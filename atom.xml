<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>bleem</title>
  
  <subtitle>The secret integer between three and four</subtitle>
  <link href="https://mritd.com/atom.xml" rel="self"/>
  
  <link href="https://mritd.com/"/>
  <updated>2021-07-29T06:13:00.000Z</updated>
  <id>https://mritd.com/</id>
  
  <author>
    <name>bleem</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>k0s 折腾笔记</title>
    <link href="https://mritd.com/2021/07/29/test-the-k0s-cluster/"/>
    <id>https://mritd.com/2021/07/29/test-the-k0s-cluster/</id>
    <published>2021-07-29T06:13:00.000Z</published>
    <updated>2021-07-29T06:13:00.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>最近两年一直在使用 kubeadm 部署 kubernetes 集群，总体来说配合一些自己小脚本还有一些自动化工具还算是方便；但是全容器化稳定性确实担忧，也遇到过莫名其妙的证书过期错误，最后重启大法解决这种问题；所以也在探索比较方便的二进制部署方式，比如这个 k0s。</p></blockquote><h2 id="一、k0s-介绍"><a href="#一、k0s-介绍" class="headerlink" title="一、k0s 介绍"></a>一、k0s 介绍</h2><blockquote><p>The Simple, Solid &amp; Certified Kubernetes Distribution.</p></blockquote><p>k0s 可以认为是一个下游的 Kubernetes 发行版，与原生 Kubernetes 相比，k0s 并未阉割大量 Kubernetes 功能；k0s 主要阉割部分基本上只有<strong>树内 Cloud provider</strong>，其他的都与原生 Kubernetes 相同。</p><p><strong>k0s 自行编译 Kubernetes 源码生成 Kubernetes 二进制文件，然后在安装后将二进制文件释放到宿主机再启动；这种情况下所有功能几乎与原生 Kubernetes 没有差异。</strong></p><h2 id="二、k0sctl-使用"><a href="#二、k0sctl-使用" class="headerlink" title="二、k0sctl 使用"></a>二、k0sctl 使用</h2><p>k0sctl 是 k0s 为了方便快速部署集群所提供的工具，有点类似于 kubeadm，但是其扩展性要比 kubeadm 好得多。在多节点的情况下，k0sctl 通过 ssh 链接目标主机然后按照步骤释放文件并启动 Kubernetes 相关服务，从而完成集群初始化。</p><h3 id="2-1、k0sctl-安装集群"><a href="#2-1、k0sctl-安装集群" class="headerlink" title="2.1、k0sctl 安装集群"></a>2.1、k0sctl 安装集群</h3><p>安装过程中会自动下载相关镜像，需要保证所有节点可以扶墙，如何离线安装后面讲解。<strong>安装前保证目标机器的 hostname 为非域名形式，否则可能会出现一些问题。</strong>以下是一个简单的启动集群示例:</p><p><strong>首先安装 k0sctl</strong></p><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment"># 安装 k0sctl</span>wget https://github.com/k0sproject/k0sctl/releases/download/v0.9.0/k0sctl-linux-x64chmod +x k0sctl-linux-x64mv k0sctl-linux-x64 /usr/<span class="hljs-built_in">local</span>/bin/k0sctl</code></pre></div><p><strong>然后编写 k0sctl.yaml 配置文件</strong></p><div class="hljs code-wrapper"><pre><code class="hljs yaml"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">k0sctl.k0sproject.io/v1beta1</span><span class="hljs-attr">kind:</span> <span class="hljs-string">Cluster</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">k0s-cluster</span><span class="hljs-attr">spec:</span>  <span class="hljs-attr">hosts:</span>  <span class="hljs-bullet">-</span> <span class="hljs-attr">ssh:</span>      <span class="hljs-attr">address:</span> <span class="hljs-number">10.0</span><span class="hljs-number">.0</span><span class="hljs-number">.11</span>      <span class="hljs-attr">user:</span> <span class="hljs-string">root</span>      <span class="hljs-attr">port:</span> <span class="hljs-number">22</span>      <span class="hljs-attr">keyPath:</span> <span class="hljs-string">/Users/bleem/.ssh/id_rsa</span>    <span class="hljs-attr">role:</span> <span class="hljs-string">controller+worker</span>  <span class="hljs-bullet">-</span> <span class="hljs-attr">ssh:</span>      <span class="hljs-attr">address:</span> <span class="hljs-number">10.0</span><span class="hljs-number">.0</span><span class="hljs-number">.12</span>      <span class="hljs-attr">user:</span> <span class="hljs-string">root</span>      <span class="hljs-attr">port:</span> <span class="hljs-number">22</span>      <span class="hljs-attr">keyPath:</span> <span class="hljs-string">/Users/bleem/.ssh/id_rsa</span>    <span class="hljs-attr">role:</span> <span class="hljs-string">controller+worker</span>  <span class="hljs-bullet">-</span> <span class="hljs-attr">ssh:</span>      <span class="hljs-attr">address:</span> <span class="hljs-number">10.0</span><span class="hljs-number">.0</span><span class="hljs-number">.13</span>      <span class="hljs-attr">user:</span> <span class="hljs-string">root</span>      <span class="hljs-attr">port:</span> <span class="hljs-number">22</span>      <span class="hljs-attr">keyPath:</span> <span class="hljs-string">/Users/bleem/.ssh/id_rsa</span>    <span class="hljs-attr">role:</span> <span class="hljs-string">controller+worker</span>  <span class="hljs-bullet">-</span> <span class="hljs-attr">ssh:</span>      <span class="hljs-attr">address:</span> <span class="hljs-number">10.0</span><span class="hljs-number">.0</span><span class="hljs-number">.14</span>      <span class="hljs-attr">user:</span> <span class="hljs-string">root</span>      <span class="hljs-attr">port:</span> <span class="hljs-number">22</span>      <span class="hljs-attr">keyPath:</span> <span class="hljs-string">/Users/bleem/.ssh/id_rsa</span>    <span class="hljs-attr">role:</span> <span class="hljs-string">worker</span>  <span class="hljs-bullet">-</span> <span class="hljs-attr">ssh:</span>      <span class="hljs-attr">address:</span> <span class="hljs-number">10.0</span><span class="hljs-number">.0</span><span class="hljs-number">.15</span>      <span class="hljs-attr">user:</span> <span class="hljs-string">root</span>      <span class="hljs-attr">port:</span> <span class="hljs-number">22</span>      <span class="hljs-attr">keyPath:</span> <span class="hljs-string">/Users/bleem/.ssh/id_rsa</span>    <span class="hljs-attr">role:</span> <span class="hljs-string">worker</span>  <span class="hljs-attr">k0s:</span>    <span class="hljs-attr">version:</span> <span class="hljs-number">1.21</span><span class="hljs-number">.2</span><span class="hljs-string">+k0s.1</span>    <span class="hljs-attr">config:</span>      <span class="hljs-attr">apiVersion:</span> <span class="hljs-string">k0s.k0sproject.io/v1beta1</span>      <span class="hljs-attr">kind:</span> <span class="hljs-string">Cluster</span>      <span class="hljs-attr">metadata:</span>        <span class="hljs-attr">name:</span> <span class="hljs-string">k0s</span>      <span class="hljs-attr">spec:</span>        <span class="hljs-attr">api:</span>          <span class="hljs-attr">address:</span> <span class="hljs-number">10.0</span><span class="hljs-number">.0</span><span class="hljs-number">.11</span>          <span class="hljs-attr">port:</span> <span class="hljs-number">6443</span>          <span class="hljs-attr">k0sApiPort:</span> <span class="hljs-number">9443</span>          <span class="hljs-attr">sans:</span>          <span class="hljs-bullet">-</span> <span class="hljs-number">10.0</span><span class="hljs-number">.0</span><span class="hljs-number">.11</span>          <span class="hljs-bullet">-</span> <span class="hljs-number">10.0</span><span class="hljs-number">.0</span><span class="hljs-number">.12</span>          <span class="hljs-bullet">-</span> <span class="hljs-number">10.0</span><span class="hljs-number">.0</span><span class="hljs-number">.13</span>        <span class="hljs-attr">storage:</span>          <span class="hljs-attr">type:</span> <span class="hljs-string">etcd</span>          <span class="hljs-attr">etcd:</span>            <span class="hljs-attr">peerAddress:</span> <span class="hljs-number">10.0</span><span class="hljs-number">.0</span><span class="hljs-number">.11</span>        <span class="hljs-attr">network:</span>          <span class="hljs-attr">kubeProxy:</span>            <span class="hljs-attr">disabled:</span> <span class="hljs-literal">false</span>            <span class="hljs-attr">mode:</span> <span class="hljs-string">ipvs</span></code></pre></div><p><strong>最后执行 <code>apply</code> 命令安装即可，安装前确保你的操作机器可以 ssh 免密登陆所有目标机器:</strong></p><div class="hljs code-wrapper"><pre><code class="hljs sh">➜  tmp k0sctl apply -c bak.yaml⠀⣿⣿⡇⠀⠀⢀⣴⣾⣿⠟⠁⢸⣿⣿⣿⣿⣿⣿⣿⡿⠛⠁⠀⢸⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⠀█████████ █████████ ███⠀⣿⣿⡇⣠⣶⣿⡿⠋⠀⠀⠀⢸⣿⡇⠀⠀⠀⣠⠀⠀⢀⣠⡆⢸⣿⣿⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀███          ███    ███⠀⣿⣿⣿⣿⣟⠋⠀⠀⠀⠀⠀⢸⣿⡇⠀⢰⣾⣿⠀⠀⣿⣿⡇⢸⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⠀███          ███    ███⠀⣿⣿⡏⠻⣿⣷⣤⡀⠀⠀⠀⠸⠛⠁⠀⠸⠋⠁⠀⠀⣿⣿⡇⠈⠉⠉⠉⠉⠉⠉⠉⠉⢹⣿⣿⠀███          ███    ███⠀⣿⣿⡇⠀⠀⠙⢿⣿⣦⣀⠀⠀⠀⣠⣶⣶⣶⣶⣶⣶⣿⣿⡇⢰⣶⣶⣶⣶⣶⣶⣶⣶⣾⣿⣿⠀█████████    ███    ██████████k0sctl 0.0.0 Copyright 2021, k0sctl authors.Anonymized telemetry of usage will be sent to the authors.By continuing to use k0sctl you agree to these terms:https://k0sproject.io/licenses/eulaINFO ==&gt; Running phase: Connect to hostsINFO [ssh] 10.0.0.15:22: connectedINFO [ssh] 10.0.0.11:22: connectedINFO [ssh] 10.0.0.12:22: connectedINFO [ssh] 10.0.0.14:22: connectedINFO [ssh] 10.0.0.13:22: connectedINFO ==&gt; Running phase: Detect host operating systemsINFO [ssh] 10.0.0.11:22: is running Ubuntu 20.04.2 LTSINFO [ssh] 10.0.0.12:22: is running Ubuntu 20.04.2 LTSINFO [ssh] 10.0.0.14:22: is running Ubuntu 20.04.2 LTSINFO [ssh] 10.0.0.13:22: is running Ubuntu 20.04.2 LTSINFO [ssh] 10.0.0.15:22: is running Ubuntu 20.04.2 LTSINFO ==&gt; Running phase: Prepare hostsINFO ==&gt; Running phase: Gather host factsINFO [ssh] 10.0.0.11:22: discovered ens33 as private interfaceINFO [ssh] 10.0.0.13:22: discovered ens33 as private interfaceINFO [ssh] 10.0.0.12:22: discovered ens33 as private interfaceINFO ==&gt; Running phase: Download k0s on hostsINFO [ssh] 10.0.0.11:22: downloading k0s 1.21.2+k0s.1INFO [ssh] 10.0.0.13:22: downloading k0s 1.21.2+k0s.1INFO [ssh] 10.0.0.12:22: downloading k0s 1.21.2+k0s.1INFO [ssh] 10.0.0.15:22: downloading k0s 1.21.2+k0s.1INFO [ssh] 10.0.0.14:22: downloading k0s 1.21.2+k0s.1......</code></pre></div><p>稍等片刻后带有三个 Master 和两个 Node 的集群将安装完成:</p><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment"># 注意: 目标机器 hostname 不应当为域名形式，这里的样例是已经修复了这个问题</span>k1.node ➜ ~ k0s kubectl get node -o wideNAME      STATUS   ROLES    AGE   VERSION       INTERNAL-IP   EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION     CONTAINER-RUNTIMEk1.node   Ready    &lt;none&gt;   10m   v1.21.2+k0s   10.0.0.11     &lt;none&gt;        Ubuntu 20.04.2 LTS   5.4.0-77-generic   containerd://1.4.6k2.node   Ready    &lt;none&gt;   10m   v1.21.2+k0s   10.0.0.12     &lt;none&gt;        Ubuntu 20.04.2 LTS   5.4.0-77-generic   containerd://1.4.6k3.node   Ready    &lt;none&gt;   10m   v1.21.2+k0s   10.0.0.13     &lt;none&gt;        Ubuntu 20.04.2 LTS   5.4.0-77-generic   containerd://1.4.6k4.node   Ready    &lt;none&gt;   10m   v1.21.2+k0s   10.0.0.14     &lt;none&gt;        Ubuntu 20.04.2 LTS   5.4.0-77-generic   containerd://1.4.6k5.node   Ready    &lt;none&gt;   10m   v1.21.2+k0s   10.0.0.15     &lt;none&gt;        Ubuntu 20.04.2 LTS   5.4.0-77-generic   containerd://1.4.6</code></pre></div><h3 id="2-2、k0sctl-的扩展方式"><a href="#2-2、k0sctl-的扩展方式" class="headerlink" title="2.2、k0sctl 的扩展方式"></a>2.2、k0sctl 的扩展方式</h3><p>与 kubeadm 不同，k0sctl 几乎提供了所有安装细节的可定制化选项，其通过三种行为来完成扩展:</p><ul><li><strong>文件上传:</strong> k0sctl 允许定义在安装前的文件上传，在安装之前 k0sctl 会把已经定义的相关文件全部上传到目标主机，包括不限于 k0s 本身二进制文件、离线镜像包、其他安装文件、其他辅助脚本等。</li><li><strong>Manifests 与 Helm:</strong> 当将特定的文件上传到 master 节点的 <code>/var/lib/k0s/manifests</code> 目录时，k0s 在安装过程中会自动应用这些配置，类似 kubelet 的 static pod 一样，只不过 k0s 允许全部资源(包括不限于 deployment、daemonset、namespace 等)；同样也可以直接在 <code>k0sctl.yaml</code> 添加 Helm 配置，k0s 也会以同样的方式帮你管理。</li><li><strong>辅助脚本:</strong> 可以在每个主机下配置 <code>hooks</code> 选项来实现执行一些特定的脚本(文档里没有，需要看源码)，以便在特定情况下做点骚操作。</li></ul><h3 id="2-3、k0sctl-使用离线镜像包"><a href="#2-3、k0sctl-使用离线镜像包" class="headerlink" title="2.3、k0sctl 使用离线镜像包"></a>2.3、k0sctl 使用离线镜像包</h3><p>基于上面的扩展，k0s 还方便的帮我们集成了离线镜像包的自动导入，我们只需要定义一个文件上传，将镜像包上传到 <code>/var/lib/k0s/images/</code> 目录后，k0s 会自定将其倒入到 containerd 中而无需我们手动干预:</p><div class="hljs code-wrapper"><pre><code class="hljs yaml"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">k0sctl.k0sproject.io/v1beta1</span><span class="hljs-attr">kind:</span> <span class="hljs-string">Cluster</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">k0s-cluster</span><span class="hljs-attr">spec:</span>  <span class="hljs-attr">hosts:</span>  <span class="hljs-bullet">-</span> <span class="hljs-attr">ssh:</span>      <span class="hljs-attr">address:</span> <span class="hljs-number">10.0</span><span class="hljs-number">.0</span><span class="hljs-number">.11</span>      <span class="hljs-attr">user:</span> <span class="hljs-string">root</span>      <span class="hljs-attr">port:</span> <span class="hljs-number">22</span>      <span class="hljs-attr">keyPath:</span> <span class="hljs-string">/Users/bleem/.ssh/id_rsa</span>    <span class="hljs-attr">role:</span> <span class="hljs-string">controller+worker</span>    <span class="hljs-comment"># files 配置将会在安装前将相关文件上传到目标主机</span>    <span class="hljs-attr">files:</span>    <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">image-bundle</span>      <span class="hljs-attr">src:</span> <span class="hljs-string">/Users/bleem/tmp/bundle_file</span>      <span class="hljs-comment"># 在该目录下的 image 压缩包将会被自动导入到 containerd 中</span>      <span class="hljs-attr">dstDir:</span> <span class="hljs-string">/var/lib/k0s/images/</span>      <span class="hljs-attr">perm:</span> <span class="hljs-number">0755</span><span class="hljs-string">......</span></code></pre></div><p><strong>关于 image 压缩包(bundle_file)如何下载以及自己自定义问题请参考官方 <a href="https://docs.k0sproject.io/v1.21.2+k0s.1/airgap-install/">Airgap install</a> 文档。</strong></p><p><img src="https://cdn.oss.link/markdown/LJIS7j.png"></p><h3 id="2-4、切换-CNI-插件"><a href="#2-4、切换-CNI-插件" class="headerlink" title="2.4、切换 CNI 插件"></a>2.4、切换 CNI 插件</h3><p>默认情况下 k0s 内部集成了两个 CNI 插件: calico 和 kube-router；如果我们使用其他的 CNI 插件例如 flannel，我们只需要将默认的 CNI 插件设置为 <code>custom</code>，然后将 flannel 的部署 yaml 上传到一台 master 的 <code>/var/lib/k0s/manifests</code> 目录即可，k0s 会自动帮我门执行 <code>apply -f xxxx.yaml</code> 这种操作。</p><p>下面是切换到 flannel 的样例，需要注意的是 flannel 官方镜像不会帮你安装 CNI 的二进制文件，我们需要借助文件上传自己安装(<a href="https://github.com/containernetworking/plugins/releases">CNI GitHub 插件下载地址</a>):</p><div class="hljs code-wrapper"><pre><code class="hljs yaml"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">k0sctl.k0sproject.io/v1beta1</span><span class="hljs-attr">kind:</span> <span class="hljs-string">Cluster</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">k0s-cluster</span><span class="hljs-attr">spec:</span>  <span class="hljs-attr">hosts:</span>  <span class="hljs-bullet">-</span> <span class="hljs-attr">ssh:</span>      <span class="hljs-attr">address:</span> <span class="hljs-number">10.0</span><span class="hljs-number">.0</span><span class="hljs-number">.11</span>      <span class="hljs-attr">user:</span> <span class="hljs-string">root</span>      <span class="hljs-attr">port:</span> <span class="hljs-number">22</span>      <span class="hljs-attr">keyPath:</span> <span class="hljs-string">/Users/bleem/.ssh/id_rsa</span>    <span class="hljs-attr">role:</span> <span class="hljs-string">controller+worker</span>    <span class="hljs-attr">files:</span>    <span class="hljs-comment"># 将 flannel 的 yaml 放到 manifests 里(需要单独创建一个目录)</span>    <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">flannel</span>      <span class="hljs-attr">src:</span> <span class="hljs-string">/Users/bleem/tmp/kube-flannel.yaml</span>      <span class="hljs-attr">dstDir:</span> <span class="hljs-string">/var/lib/k0s/manifests/flannel</span>      <span class="hljs-attr">perm:</span> <span class="hljs-number">0644</span>    <span class="hljs-comment"># 自己安装一下 CNI 插件</span>    <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">cni-plugins</span>      <span class="hljs-attr">src:</span> <span class="hljs-string">/Users/bleem/tmp/cni-plugins/*</span>      <span class="hljs-attr">dstDir:</span> <span class="hljs-string">/opt/cni/bin/</span>      <span class="hljs-attr">perm:</span> <span class="hljs-number">0755</span>  <span class="hljs-attr">k0s:</span>    <span class="hljs-attr">version:</span> <span class="hljs-string">v1.21.2+k0s.1</span>    <span class="hljs-attr">config:</span>      <span class="hljs-attr">apiVersion:</span> <span class="hljs-string">k0s.k0sproject.io/v1beta1</span>      <span class="hljs-attr">kind:</span> <span class="hljs-string">Cluster</span>      <span class="hljs-attr">metadata:</span>        <span class="hljs-attr">name:</span> <span class="hljs-string">k0s</span>      <span class="hljs-attr">spec:</span>        <span class="hljs-attr">api:</span>          <span class="hljs-attr">address:</span> <span class="hljs-number">10.0</span><span class="hljs-number">.0</span><span class="hljs-number">.11</span>          <span class="hljs-attr">port:</span> <span class="hljs-number">6443</span>          <span class="hljs-attr">k0sApiPort:</span> <span class="hljs-number">9443</span>          <span class="hljs-attr">sans:</span>          <span class="hljs-bullet">-</span> <span class="hljs-number">10.0</span><span class="hljs-number">.0</span><span class="hljs-number">.11</span>          <span class="hljs-bullet">-</span> <span class="hljs-number">10.0</span><span class="hljs-number">.0</span><span class="hljs-number">.12</span>          <span class="hljs-bullet">-</span> <span class="hljs-number">10.0</span><span class="hljs-number">.0</span><span class="hljs-number">.13</span>        <span class="hljs-attr">storage:</span>          <span class="hljs-attr">type:</span> <span class="hljs-string">etcd</span>        <span class="hljs-attr">network:</span>          <span class="hljs-attr">podCIDR:</span> <span class="hljs-number">10.244</span><span class="hljs-number">.0</span><span class="hljs-number">.0</span><span class="hljs-string">/16</span>          <span class="hljs-attr">serviceCIDR:</span> <span class="hljs-number">10.96</span><span class="hljs-number">.0</span><span class="hljs-number">.0</span><span class="hljs-string">/12</span>          <span class="hljs-comment"># 这里指定 CNI 为 custom 自定义类型，这样</span>          <span class="hljs-comment"># k0s 就不会安装 calico/kube-router 了</span>          <span class="hljs-attr">provider:</span> <span class="hljs-string">custom</span></code></pre></div><h3 id="2-5、上传-k0s-二进制文件"><a href="#2-5、上传-k0s-二进制文件" class="headerlink" title="2.5、上传 k0s 二进制文件"></a>2.5、上传 k0s 二进制文件</h3><p>除了普通文件、镜像压缩包等，默认情况下 k0sctl 在安装集群时还会在目标机器上下载 k0s 二进制文件；当然在离线环境下这一步也可以通过一个简单的配置来实现离线上传:</p><div class="hljs code-wrapper"><pre><code class="hljs yaml"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">k0sctl.k0sproject.io/v1beta1</span><span class="hljs-attr">kind:</span> <span class="hljs-string">Cluster</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">k0s-cluster</span><span class="hljs-attr">spec:</span>  <span class="hljs-attr">hosts:</span>  <span class="hljs-bullet">-</span> <span class="hljs-attr">ssh:</span>      <span class="hljs-attr">address:</span> <span class="hljs-number">10.0</span><span class="hljs-number">.0</span><span class="hljs-number">.11</span>      <span class="hljs-attr">user:</span> <span class="hljs-string">root</span>      <span class="hljs-attr">port:</span> <span class="hljs-number">22</span>      <span class="hljs-attr">keyPath:</span> <span class="hljs-string">/Users/bleem/.ssh/id_rsa</span>    <span class="hljs-attr">role:</span> <span class="hljs-string">controller+worker</span>    <span class="hljs-comment"># 声明需要上传二进制文件</span>    <span class="hljs-attr">uploadBinary:</span> <span class="hljs-literal">true</span>    <span class="hljs-comment"># 指定二进制文件位置</span>    <span class="hljs-attr">k0sBinaryPath:</span> <span class="hljs-string">/Users/bleem/tmp/k0s</span>    <span class="hljs-attr">files:</span>    <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">flannel</span>      <span class="hljs-attr">src:</span> <span class="hljs-string">/Users/bleem/tmp/kube-flannel.yaml</span>      <span class="hljs-attr">dstDir:</span> <span class="hljs-string">/var/lib/k0s/manifests/flannel</span>      <span class="hljs-attr">perm:</span> <span class="hljs-number">0644</span><span class="hljs-string">......</span></code></pre></div><h3 id="2-6、更换镜像版本"><a href="#2-6、更换镜像版本" class="headerlink" title="2.6、更换镜像版本"></a>2.6、更换镜像版本</h3><p>默认情况下 k0s 版本号与 Kubernetes 保持一致，但是如果期望某个组件使用特定的版本，则可以直接配置这些内置组件的镜像名称:</p><div class="hljs code-wrapper"><pre><code class="hljs yaml"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">k0sctl.k0sproject.io/v1beta1</span><span class="hljs-attr">kind:</span> <span class="hljs-string">Cluster</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">k0s-cluster</span><span class="hljs-attr">spec:</span>  <span class="hljs-attr">hosts:</span>  <span class="hljs-bullet">-</span> <span class="hljs-attr">ssh:</span>      <span class="hljs-attr">address:</span> <span class="hljs-number">10.0</span><span class="hljs-number">.0</span><span class="hljs-number">.11</span>      <span class="hljs-attr">user:</span> <span class="hljs-string">root</span>      <span class="hljs-attr">port:</span> <span class="hljs-number">22</span>      <span class="hljs-attr">keyPath:</span> <span class="hljs-string">/Users/bleem/.ssh/id_rsa</span>    <span class="hljs-attr">role:</span> <span class="hljs-string">controller+worker</span>    <span class="hljs-attr">uploadBinary:</span> <span class="hljs-literal">true</span>    <span class="hljs-attr">k0sBinaryPath:</span> <span class="hljs-string">/Users/bleem/tmp/k0s</span>    <span class="hljs-attr">files:</span>    <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">flannel</span>      <span class="hljs-attr">src:</span> <span class="hljs-string">/Users/bleem/tmp/kube-flannel.yaml</span>      <span class="hljs-attr">dstDir:</span> <span class="hljs-string">/var/lib/k0s/manifests/flannel</span>      <span class="hljs-attr">perm:</span> <span class="hljs-number">0644</span><span class="hljs-string">......</span>  <span class="hljs-attr">k0s:</span>    <span class="hljs-attr">version:</span> <span class="hljs-string">v1.21.2+k0s.1</span>    <span class="hljs-attr">config:</span>      <span class="hljs-attr">apiVersion:</span> <span class="hljs-string">k0s.k0sproject.io/v1beta1</span>      <span class="hljs-attr">kind:</span> <span class="hljs-string">Cluster</span>      <span class="hljs-attr">metadata:</span>        <span class="hljs-attr">name:</span> <span class="hljs-string">k0s</span>      <span class="hljs-attr">spec:</span>        <span class="hljs-attr">api:</span>          <span class="hljs-attr">address:</span> <span class="hljs-number">10.0</span><span class="hljs-number">.0</span><span class="hljs-number">.11</span>          <span class="hljs-attr">port:</span> <span class="hljs-number">6443</span>          <span class="hljs-attr">k0sApiPort:</span> <span class="hljs-number">9443</span>          <span class="hljs-attr">sans:</span>          <span class="hljs-bullet">-</span> <span class="hljs-number">10.0</span><span class="hljs-number">.0</span><span class="hljs-number">.11</span>          <span class="hljs-bullet">-</span> <span class="hljs-number">10.0</span><span class="hljs-number">.0</span><span class="hljs-number">.12</span>          <span class="hljs-bullet">-</span> <span class="hljs-number">10.0</span><span class="hljs-number">.0</span><span class="hljs-number">.13</span>        <span class="hljs-comment"># 指定内部组件的镜像使用的版本</span>        <span class="hljs-attr">images:</span>          <span class="hljs-comment">#konnectivity:</span>          <span class="hljs-comment">#  image: us.gcr.io/k8s-artifacts-prod/kas-network-proxy/proxy-agent</span>          <span class="hljs-comment">#  version: v0.0.21</span>          <span class="hljs-comment">#metricsserver:</span>          <span class="hljs-comment">#  image: gcr.io/k8s-staging-metrics-server/metrics-server</span>          <span class="hljs-comment">#  version: v0.3.7</span>          <span class="hljs-attr">kubeproxy:</span>            <span class="hljs-attr">image:</span> <span class="hljs-string">k8s.gcr.io/kube-proxy</span>            <span class="hljs-attr">version:</span> <span class="hljs-string">v1.21.3</span>          <span class="hljs-comment">#coredns:</span>          <span class="hljs-comment">#  image: docker.io/coredns/coredns</span>          <span class="hljs-comment">#  version: 1.7.0</span>          <span class="hljs-comment">#calico:</span>          <span class="hljs-comment">#  cni:</span>          <span class="hljs-comment">#    image: docker.io/calico/cni</span>          <span class="hljs-comment">#    version: v3.18.1</span>          <span class="hljs-comment">#  node:</span>          <span class="hljs-comment">#    image: docker.io/calico/node</span>          <span class="hljs-comment">#    version: v3.18.1</span>          <span class="hljs-comment">#  kubecontrollers:</span>          <span class="hljs-comment">#    image: docker.io/calico/kube-controllers</span>          <span class="hljs-comment">#    version: v3.18.1</span>          <span class="hljs-comment">#kuberouter:</span>          <span class="hljs-comment">#  cni:</span>          <span class="hljs-comment">#    image: docker.io/cloudnativelabs/kube-router</span>          <span class="hljs-comment">#    version: v1.2.1</span>          <span class="hljs-comment">#  cniInstaller:</span>          <span class="hljs-comment">#    image: quay.io/k0sproject/cni-node</span>          <span class="hljs-comment">#    version: 0.1.0</span>          <span class="hljs-attr">default_pull_policy:</span> <span class="hljs-string">IfNotPresent</span>          <span class="hljs-comment">#default_pull_policy: Never</span></code></pre></div><h3 id="2-7、调整-master-组件参数"><a href="#2-7、调整-master-组件参数" class="headerlink" title="2.7、调整 master 组件参数"></a>2.7、调整 master 组件参数</h3><p>熟悉 Kubernetes 的应该清楚，master 上三大组件: apiserver、controller、scheduler 管控整个集群；在 k0sctl 安装集群的过程中也允许自定义这些组件的参数，这些调整通过修改使用的 <code>k0sctl.yaml</code> 配置文件完成。</p><ul><li><code>spec.api.extraArgs</code>: 用于自定义 kube-apiserver 的自定义参数(kv map)</li><li><code>spec.scheduler.extraArgs</code>: 用于自定义 kube-scheduler 的自定义参数(kv map)</li><li><code>spec.controllerManager.extraArgs</code>: 用于自定义 kube-controller-manager 自定义参数(kv map)</li><li><code>spec.workerProfiles</code>: 用于覆盖 kubelet-config.yaml 中的配置，该配置最终将于默认的 kubelet-config.yaml 合并</li></ul><p>除此之外在 <code>Host</code> 配置中还有一个 <code>InstallFlags</code> 配置用于传递 k0s 安装时的其他配置选项。</p><h2 id="三、k0s-HA-搭建"><a href="#三、k0s-HA-搭建" class="headerlink" title="三、k0s HA 搭建"></a>三、k0s HA 搭建</h2><blockquote><p>其实上面的第二部分主要都是介绍 k0sctl 一些基础功能，为的就是给下面这部分 HA 生产级部署做铺垫。</p></blockquote><p>就目前来说，k0s HA 仅支持独立负载均衡器的 HA 架构；<strong>即外部需要有一个高可用的 4 层负载均衡器，其他所有 Node 节点链接这个负载均衡器实现 master 的高可用。</strong>在使用 k0sctl 命令搭建 HA 集群时很简单，只需要添加一个外部负载均衡器地址即可；<strong>以下是一个完整的，全离线状态下的 HA 集群搭建配置。</strong></p><h3 id="3-1、外部负载均衡器"><a href="#3-1、外部负载均衡器" class="headerlink" title="3.1、外部负载均衡器"></a>3.1、外部负载均衡器</h3><p><strong>在搭建之前我们假设已经有一个外部的高可用的 4 层负载均衡器，且负载均衡器已经负载了以下端口:</strong></p><ul><li><code>6443(for Kubernetes API)</code>: 负载均衡器 6443 负载所有 master 节点的 6443</li><li><code>9443 (for controller join API)</code>: 负载均衡器 9443 负载所有 master 节点的 9443</li><li><code>8132 (for Konnectivity agent)</code>: 负载均衡器 8132 负载所有 master 节点的 8132</li><li><code>8133 (for Konnectivity server)</code>: 负载均衡器 8133 负载所有 master 节点的 8133</li></ul><p>以下为一个 nginx 4 层代理的样例:</p><div class="hljs code-wrapper"><pre><code class="hljs sh">error_log syslog:server=unix:/dev/<span class="hljs-built_in">log</span> notice;worker_processes auto;events &#123;multi_accept on;use epoll;worker_connections 1024;&#125;stream &#123;    upstream kube_apiserver &#123;        least_conn;        server 10.0.0.11:6443;        server 10.0.0.12:6443;        server 10.0.0.13:6443;    &#125;    upstream konnectivity_agent &#123;        least_conn;        server 10.0.0.11:8132;        server 10.0.0.12:8132;        server 10.0.0.13:8132;    &#125;    upstream konnectivity_server &#123;        least_conn;        server 10.0.0.11:8133;        server 10.0.0.12:8133;        server 10.0.0.13:8133;    &#125;    upstream controller_join_api &#123;        least_conn;        server 10.0.0.11:9443;        server 10.0.0.12:9443;        server 10.0.0.13:9443;    &#125;        server &#123;        listen        0.0.0.0:6443;        proxy_pass    kube_apiserver;        proxy_timeout 10m;        proxy_connect_timeout 1s;    &#125;    server &#123;        listen        0.0.0.0:8132;        proxy_pass    konnectivity_agent;        proxy_timeout 10m;        proxy_connect_timeout 1s;    &#125;    server &#123;        listen        0.0.0.0:8133;        proxy_pass    konnectivity_server;        proxy_timeout 10m;        proxy_connect_timeout 1s;    &#125;    server &#123;        listen        0.0.0.0:9443;        proxy_pass    controller_join_api;        proxy_timeout 10m;        proxy_connect_timeout 1s;    &#125;&#125;</code></pre></div><h3 id="3-2、搭建-HA-集群"><a href="#3-2、搭建-HA-集群" class="headerlink" title="3.2、搭建 HA 集群"></a>3.2、搭建 HA 集群</h3><p>以下为 k0sctl 的 HA + 离线部署样例配置:</p><div class="hljs code-wrapper"><pre><code class="hljs yaml"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">k0sctl.k0sproject.io/v1beta1</span><span class="hljs-attr">kind:</span> <span class="hljs-string">Cluster</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">k0s-cluster</span><span class="hljs-attr">spec:</span>  <span class="hljs-attr">hosts:</span>  <span class="hljs-bullet">-</span> <span class="hljs-attr">ssh:</span>      <span class="hljs-attr">address:</span> <span class="hljs-number">10.0</span><span class="hljs-number">.0</span><span class="hljs-number">.11</span>      <span class="hljs-attr">user:</span> <span class="hljs-string">root</span>      <span class="hljs-attr">port:</span> <span class="hljs-number">22</span>      <span class="hljs-attr">keyPath:</span> <span class="hljs-string">/Users/bleem/.ssh/id_rsa</span>    <span class="hljs-comment"># role 支持的值</span>    <span class="hljs-comment"># &#x27;controller&#x27; 单 master</span>    <span class="hljs-comment"># &#x27;worker&#x27; 单 worker</span>    <span class="hljs-comment"># &#x27;controller+worker&#x27; master 和 worker 都运行 </span>    <span class="hljs-attr">role:</span> <span class="hljs-string">controller+worker</span>        <span class="hljs-comment"># 从本地 上传 k0s bin 文件，不要在目标机器下载</span>    <span class="hljs-attr">uploadBinary:</span> <span class="hljs-literal">true</span>    <span class="hljs-attr">k0sBinaryPath:</span> <span class="hljs-string">/Users/bleem/tmp/k0s</span>        <span class="hljs-comment"># 上传其他文件</span>    <span class="hljs-attr">files:</span>    <span class="hljs-comment"># 上传 flannel 配置，使用自定的 flannel 替换内置的 calico</span>    <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">flannel</span>      <span class="hljs-attr">src:</span> <span class="hljs-string">/Users/bleem/tmp/kube-flannel.yaml</span>      <span class="hljs-attr">dstDir:</span> <span class="hljs-string">/var/lib/k0s/manifests/flannel</span>      <span class="hljs-attr">perm:</span> <span class="hljs-number">0644</span>        <span class="hljs-comment"># 上传打包好的 image 镜像包，k0s 会自动导入到 containerd</span>    <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">image-bundle</span>      <span class="hljs-attr">src:</span> <span class="hljs-string">/Users/bleem/tmp/bundle_file</span>      <span class="hljs-attr">dstDir:</span> <span class="hljs-string">/var/lib/k0s/images/</span>      <span class="hljs-attr">perm:</span> <span class="hljs-number">0755</span>        <span class="hljs-comment"># 使用 flannel 后每个机器要上传对应的 CNI 插件</span>    <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">cni-plugins</span>      <span class="hljs-attr">src:</span> <span class="hljs-string">/Users/bleem/tmp/cni-plugins/*</span>      <span class="hljs-attr">dstDir:</span> <span class="hljs-string">/opt/cni/bin/</span>      <span class="hljs-attr">perm:</span> <span class="hljs-number">0755</span>  <span class="hljs-bullet">-</span> <span class="hljs-attr">ssh:</span>      <span class="hljs-attr">address:</span> <span class="hljs-number">10.0</span><span class="hljs-number">.0</span><span class="hljs-number">.12</span>      <span class="hljs-attr">user:</span> <span class="hljs-string">root</span>      <span class="hljs-attr">port:</span> <span class="hljs-number">22</span>      <span class="hljs-attr">keyPath:</span> <span class="hljs-string">/Users/bleem/.ssh/id_rsa</span>    <span class="hljs-attr">role:</span> <span class="hljs-string">controller+worker</span>    <span class="hljs-attr">uploadBinary:</span> <span class="hljs-literal">true</span>    <span class="hljs-attr">k0sBinaryPath:</span> <span class="hljs-string">/Users/bleem/tmp/k0s</span>    <span class="hljs-attr">files:</span>    <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">image-bundle</span>      <span class="hljs-attr">src:</span> <span class="hljs-string">/Users/bleem/tmp/bundle_file</span>      <span class="hljs-attr">dstDir:</span> <span class="hljs-string">/var/lib/k0s/images/</span>      <span class="hljs-attr">perm:</span> <span class="hljs-number">0755</span>    <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">cni-plugins</span>      <span class="hljs-attr">src:</span> <span class="hljs-string">/Users/bleem/tmp/cni-plugins/*</span>      <span class="hljs-attr">dstDir:</span> <span class="hljs-string">/opt/cni/bin/</span>      <span class="hljs-attr">perm:</span> <span class="hljs-number">0755</span>  <span class="hljs-bullet">-</span> <span class="hljs-attr">ssh:</span>      <span class="hljs-attr">address:</span> <span class="hljs-number">10.0</span><span class="hljs-number">.0</span><span class="hljs-number">.13</span>      <span class="hljs-attr">user:</span> <span class="hljs-string">root</span>      <span class="hljs-attr">port:</span> <span class="hljs-number">22</span>      <span class="hljs-attr">keyPath:</span> <span class="hljs-string">/Users/bleem/.ssh/id_rsa</span>    <span class="hljs-attr">role:</span> <span class="hljs-string">controller+worker</span>    <span class="hljs-attr">uploadBinary:</span> <span class="hljs-literal">true</span>    <span class="hljs-attr">k0sBinaryPath:</span> <span class="hljs-string">/Users/bleem/tmp/k0s</span>    <span class="hljs-attr">files:</span>    <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">image-bundle</span>      <span class="hljs-attr">src:</span> <span class="hljs-string">/Users/bleem/tmp/bundle_file</span>      <span class="hljs-attr">dstDir:</span> <span class="hljs-string">/var/lib/k0s/images/</span>      <span class="hljs-attr">perm:</span> <span class="hljs-number">0755</span>    <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">cni-plugins</span>      <span class="hljs-attr">src:</span> <span class="hljs-string">/Users/bleem/tmp/cni-plugins/*</span>      <span class="hljs-attr">dstDir:</span> <span class="hljs-string">/opt/cni/bin/</span>      <span class="hljs-attr">perm:</span> <span class="hljs-number">0755</span>  <span class="hljs-bullet">-</span> <span class="hljs-attr">ssh:</span>      <span class="hljs-attr">address:</span> <span class="hljs-number">10.0</span><span class="hljs-number">.0</span><span class="hljs-number">.14</span>      <span class="hljs-attr">user:</span> <span class="hljs-string">root</span>      <span class="hljs-attr">port:</span> <span class="hljs-number">22</span>      <span class="hljs-attr">keyPath:</span> <span class="hljs-string">/Users/bleem/.ssh/id_rsa</span>    <span class="hljs-attr">role:</span> <span class="hljs-string">worker</span>    <span class="hljs-attr">uploadBinary:</span> <span class="hljs-literal">true</span>    <span class="hljs-attr">k0sBinaryPath:</span> <span class="hljs-string">/Users/bleem/tmp/k0s</span>    <span class="hljs-attr">files:</span>    <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">image-bundle</span>      <span class="hljs-attr">src:</span> <span class="hljs-string">/Users/bleem/tmp/bundle_file</span>      <span class="hljs-attr">dstDir:</span> <span class="hljs-string">/var/lib/k0s/images/</span>      <span class="hljs-attr">perm:</span> <span class="hljs-number">0755</span>    <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">cni-plugins</span>      <span class="hljs-attr">src:</span> <span class="hljs-string">/Users/bleem/tmp/cni-plugins/*</span>      <span class="hljs-attr">dstDir:</span> <span class="hljs-string">/opt/cni/bin/</span>      <span class="hljs-attr">perm:</span> <span class="hljs-number">0755</span>  <span class="hljs-bullet">-</span> <span class="hljs-attr">ssh:</span>      <span class="hljs-attr">address:</span> <span class="hljs-number">10.0</span><span class="hljs-number">.0</span><span class="hljs-number">.15</span>      <span class="hljs-attr">user:</span> <span class="hljs-string">root</span>      <span class="hljs-attr">port:</span> <span class="hljs-number">22</span>      <span class="hljs-attr">keyPath:</span> <span class="hljs-string">/Users/bleem/.ssh/id_rsa</span>    <span class="hljs-attr">role:</span> <span class="hljs-string">worker</span>    <span class="hljs-attr">uploadBinary:</span> <span class="hljs-literal">true</span>    <span class="hljs-attr">k0sBinaryPath:</span> <span class="hljs-string">/Users/bleem/tmp/k0s</span>    <span class="hljs-attr">files:</span>    <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">image-bundle</span>      <span class="hljs-attr">src:</span> <span class="hljs-string">/Users/bleem/tmp/bundle_file</span>      <span class="hljs-attr">dstDir:</span> <span class="hljs-string">/var/lib/k0s/images/</span>      <span class="hljs-attr">perm:</span> <span class="hljs-number">0755</span>    <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">cni-plugins</span>      <span class="hljs-attr">src:</span> <span class="hljs-string">/Users/bleem/tmp/cni-plugins/*</span>      <span class="hljs-attr">dstDir:</span> <span class="hljs-string">/opt/cni/bin/</span>      <span class="hljs-attr">perm:</span> <span class="hljs-number">0755</span>  <span class="hljs-attr">k0s:</span>    <span class="hljs-attr">version:</span> <span class="hljs-string">v1.21.2+k0s.1</span>    <span class="hljs-attr">config:</span>      <span class="hljs-attr">apiVersion:</span> <span class="hljs-string">k0s.k0sproject.io/v1beta1</span>      <span class="hljs-attr">kind:</span> <span class="hljs-string">Cluster</span>      <span class="hljs-attr">metadata:</span>        <span class="hljs-attr">name:</span> <span class="hljs-string">k0s</span>      <span class="hljs-attr">spec:</span>        <span class="hljs-attr">api:</span>          <span class="hljs-comment"># 此处填写外部的负载均衡器地址，所有 kubelet 会链接这个地址</span>          <span class="hljs-attr">externalAddress:</span> <span class="hljs-number">10.0</span><span class="hljs-number">.0</span><span class="hljs-number">.20</span>          <span class="hljs-comment"># 不要忘了为外部负载均衡器添加 api 证书的 SAN</span>          <span class="hljs-attr">sans:</span>          <span class="hljs-bullet">-</span> <span class="hljs-number">10.0</span><span class="hljs-number">.0</span><span class="hljs-number">.11</span>          <span class="hljs-bullet">-</span> <span class="hljs-number">10.0</span><span class="hljs-number">.0</span><span class="hljs-number">.12</span>          <span class="hljs-bullet">-</span> <span class="hljs-number">10.0</span><span class="hljs-number">.0</span><span class="hljs-number">.13</span>          <span class="hljs-bullet">-</span> <span class="hljs-number">10.0</span><span class="hljs-number">.0</span><span class="hljs-number">.20</span>        <span class="hljs-comment"># 存储类型使用 etcd，etcd 集群由 k0s 自动管理</span>        <span class="hljs-attr">storage:</span>          <span class="hljs-attr">type:</span> <span class="hljs-string">etcd</span>        <span class="hljs-attr">network:</span>          <span class="hljs-attr">podCIDR:</span> <span class="hljs-number">10.244</span><span class="hljs-number">.0</span><span class="hljs-number">.0</span><span class="hljs-string">/16</span>          <span class="hljs-attr">serviceCIDR:</span> <span class="hljs-number">10.96</span><span class="hljs-number">.0</span><span class="hljs-number">.0</span><span class="hljs-string">/12</span>          <span class="hljs-comment"># 网络插件使用 custom，然后让 flannel 接管</span>          <span class="hljs-attr">provider:</span> <span class="hljs-string">custom</span>          <span class="hljs-attr">kubeProxy:</span>            <span class="hljs-attr">disabled:</span> <span class="hljs-literal">false</span>            <span class="hljs-comment"># 开启 kubelet 的 ipvs 模式</span>            <span class="hljs-attr">mode:</span> <span class="hljs-string">ipvs</span>        <span class="hljs-comment"># 不发送任何匿名统计信息</span>        <span class="hljs-attr">telemetry:</span>          <span class="hljs-attr">enabled:</span> <span class="hljs-literal">false</span>        <span class="hljs-attr">images:</span>          <span class="hljs-attr">default_pull_policy:</span> <span class="hljs-string">IfNotPresent</span></code></pre></div><p>最后只需要执行 <code>k0sctl apply -c k0sctl.yaml</code> 稍等几分钟集群就搭建好了，安装过程中可以看到相关文件的上传流程:</p><p><img src="https://cdn.oss.link/markdown/4rQzJU.png"></p><h3 id="3-3、证书续签和管理"><a href="#3-3、证书续签和管理" class="headerlink" title="3.3、证书续签和管理"></a>3.3、证书续签和管理</h3><p>kubeadm 集群默认证书有效期是一年，到期要通过 kubeadm 重新签署；k0s 集群也差不多一样，但是不同的是 k0s 集群更加暴力；<strong>只要 CA(默认 10年) 不丢，k0s 每次重启都强行重新生成一年有效期的证书，所以在 HA 的环境下，快到期时重启一下 k0s 服务就行。</strong></p><p><strong>k0sctl 安装完的集群默认只有一个 <code>k0scontroller.service</code> 服务，master、node 上所有服务都由这个服务启动，所以到期之前 <code>systemctl restart k0scontroller.service</code> 一下就行。</strong></p><h2 id="四、集群备份和恢复"><a href="#四、集群备份和恢复" class="headerlink" title="四、集群备份和恢复"></a>四、集群备份和恢复</h2><p>k0sctl 提供了集群备份和恢复功能，默认情况下只需要执行 <code>k0sctl backup</code> 即可完成集群备份，该命令会在当前目录下生成一个 <code>k0s_backup_TIMESTAMP.tar.gz</code> 备份文件。</p><p>需要恢复集群时使用 <code>k0sctl apply --restore-from k0s_backup_TIMESTAMP.tar.gz</code> 命令进行恢复即可；需要注意的是恢复命令等同于在新机器重新安装集群，所以有一定风险。</p><p><strong>经过连续两天的测试，感觉这个备份恢复功能并不算靠谱，还是推荐使用 Velero 备份集群。</strong></p><h2 id="五、其他高级功能"><a href="#五、其他高级功能" class="headerlink" title="五、其他高级功能"></a>五、其他高级功能</h2><h3 id="5-1、Etcd-替换"><a href="#5-1、Etcd-替换" class="headerlink" title="5.1、Etcd 替换"></a>5.1、Etcd 替换</h3><p>在小规模集群场景下可能并不需要特别完善的 Etcd 作为存储，k0s 借助于 kine 库可以实现使用 SQLite 或 MySQL 等传统数据库作为集群存储；如果想要切换存储只需要调整 <code>k0sctl.yaml</code> 配置即可:</p><div class="hljs code-wrapper"><pre><code class="hljs yaml"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">k0s.k0sproject.io/v1beta1</span><span class="hljs-attr">kind:</span> <span class="hljs-string">Cluster</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">k0s</span><span class="hljs-attr">spec:</span>  <span class="hljs-attr">storage:</span>    <span class="hljs-attr">type:</span> <span class="hljs-string">kine</span>    <span class="hljs-attr">kine:</span>      <span class="hljs-attr">dataSource:</span> <span class="hljs-string">&quot;sqlite:///var/lib/k0s/db/state.db?more=rwc&amp;_journal=WAL&amp;cache=shared&quot;</span></code></pre></div><h3 id="5-2、集群用户管理"><a href="#5-2、集群用户管理" class="headerlink" title="5.2、集群用户管理"></a>5.2、集群用户管理</h3><p>使用 k0sctl 搭建的集群通过 <code>k0s</code> 命令可以很方便的为集群添加用户，以下是添加样例:</p><div class="hljs code-wrapper"><pre><code class="hljs sh">k0s kubeconfig create --groups <span class="hljs-string">&quot;system:masters&quot;</span> testUser &gt; k0s.config</code></pre></div><h3 id="5-3、Containerd-配置"><a href="#5-3、Containerd-配置" class="headerlink" title="5.3、Containerd 配置"></a>5.3、Containerd 配置</h3><p>在不做配置的情况下 k0s 集群使用默认的 Containerd 配置，如果需要自己定义特殊配置，可以在安装时通过文件上传方式将 Containerd 配置文件上传到 <code>/etc/k0s/containerd.toml</code> 位置，该配置将会被 k0s 启动的 Containerd 读取并使用。</p><h2 id="六、总结"><a href="#六、总结" class="headerlink" title="六、总结"></a>六、总结</h2><p>k0s 是个不错的项目，对于二进制宿主机部署 Kubernetes 集群很方便，由于其直接采用 Kubernetes 二进制文件启动，所以基本没有功能阉割，而 k0sctl 又为自动化安装提供了良好的扩展性，所以值得一试。不过目前来说 k0s 在细节部分还有一定瑕疵，比如 <code>konnectivity</code> 服务在安装时无法选择性关闭等；k0s 综合来说是个不错的工具，也推荐看看源码，里面很多设计很新颖也比较利于了解集群引导过程。</p>]]></content>
    
    
    <summary type="html">发现一个宿主机二进制部署 Kubernetes 的好工具 -&gt; k0s</summary>
    
    
    
    <category term="Kubernetes" scheme="https://mritd.com/categories/kubernetes/"/>
    
    
    <category term="Kubernetes" scheme="https://mritd.com/tags/kubernetes/"/>
    
    <category term="k0s" scheme="https://mritd.com/tags/k0s/"/>
    
  </entry>
  
  <entry>
    <title>Caddy2 file server 自动重定向问题</title>
    <link href="https://mritd.com/2021/07/02/fix-caddy2-fileserver-auto-redirect/"/>
    <id>https://mritd.com/2021/07/02/fix-caddy2-fileserver-auto-redirect/</id>
    <published>2021-07-02T08:44:00.000Z</published>
    <updated>2021-07-02T08:44:00.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="一、事情起因"><a href="#一、事情起因" class="headerlink" title="一、事情起因"></a>一、事情起因</h2><p>自打很多年前开始使用静态博客工具来发布博客，现在基本上博客源码编译后就是一堆 html 等静态文件；一开始使用 nginx 作为静态文件服务器，后来切换到的 Caddy2；不过最近在 Google Search Console 中发现了大量的无效链接，给出的提示是 “网页会自动重定向”。</p><p><img src="https://cdn.oss.link/markdown/V71ZCH.png"></p><p>经过测试后发现这些链接地址在访问时都会重定向一下，然后在结尾加上 <code>/</code>；没办法我就开始探索这个 <code>/</code> 是怎么来的了。</p><h2 id="二、源码分析"><a href="#二、源码分析" class="headerlink" title="二、源码分析"></a>二、源码分析</h2><p>没办法，也不知道那个配置影响的，只能去翻 file server 的源码，在几经查找之后找到了以下代码(而且还带着注释):</p><p><img src="https://cdn.oss.link/markdown/LbCAVp.png"></p><p>从代码逻辑上看，只要 <code>*fsrv.CanonicalURIs</code> 这个变量为 <code>true</code>，那么就会触发自动重定像，并在 “目录” 尾部补上 <code>/</code>；注释里也说的很清楚是为了目录规范化，如果想看详细讨论可以参考那两个 issue。</p><h2 id="三、解决方案"><a href="#三、解决方案" class="headerlink" title="三、解决方案"></a>三、解决方案</h2><h3 id="3-1、Admin-API"><a href="#3-1、Admin-API" class="headerlink" title="3.1、Admin API"></a>3.1、Admin API</h3><p>翻了这个 <code>*fsrv.CanonicalURIs</code> 变量以后，突然发现 Caddyfile 里其实是不支持这个配置的；所以比较 low 的办法就是利用 Admin API，先把 json 弄出来，然后加上配置再。POST 回去:</p><div class="hljs code-wrapper"><pre><code class="hljs diff">&#123;&quot;apps&quot;: &#123;&quot;http&quot;: &#123;&quot;servers&quot;: &#123;&quot;srv0&quot;: &#123;&quot;listen&quot;: [&quot;:80&quot;],&quot;routes&quot;: [&#123;&quot;handle&quot;: [&#123;<span class="hljs-addition">+&quot;canonical_uris&quot;: false,</span>&quot;handler&quot;: &quot;file_server&quot;,&quot;hide&quot;: [&quot;./Caddyfile&quot;]&#125;]&#125;]&#125;&#125;&#125;&#125;&#125;</code></pre></div><div class="hljs code-wrapper"><pre><code class="hljs sh">curl -XPOST http://localhost:2019/load -H <span class="hljs-string">&quot;Content-Type: application/json&quot;</span> -d @caddy.json</code></pre></div><h3 id="3-2、升级版本"><a href="#3-2、升级版本" class="headerlink" title="3.2、升级版本"></a>3.2、升级版本</h3><p>现在可以直接从 master 构建 Caddy，或者等待 <code>v2.4.4</code> 版本发布，这两种方式产生的 Caddy 二进制文件已经支持了这个配置选项，配置样例如下:</p><div class="hljs code-wrapper"><pre><code class="hljs go">:<span class="hljs-number">80</span>file_server &#123;disable_canonical_uris&#125;</code></pre></div>]]></content>
    
    
    <summary type="html">Google Search Console 上看到好多无效链接，说我博客很多链接自动重定向了，研究半天发现是 Caddy2 的 file_server 问题，折腾两天顺手 PR 一下。</summary>
    
    
    
    <category term="Golang" scheme="https://mritd.com/categories/golang/"/>
    
    <category term="Caddy" scheme="https://mritd.com/categories/golang/caddy/"/>
    
    
    <category term="Caddy" scheme="https://mritd.com/tags/caddy/"/>
    
  </entry>
  
  <entry>
    <title>Caddyfile 语法浅析</title>
    <link href="https://mritd.com/2021/06/30/understand-caddyfile-syntax/"/>
    <id>https://mritd.com/2021/06/30/understand-caddyfile-syntax/</id>
    <published>2021-06-30T09:28:00.000Z</published>
    <updated>2021-06-30T09:28:00.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>发现大部分人在切换 Caddy 时遇到的比较大的困难就是这个 Caddyfile 不知道怎么写，一开始我也是很懵逼的状态，今天决定写写这个 Caddyfile 配置语法，顺便自己也完整的学学。</p></blockquote><h2 id="一、Caddy-配置体系"><a href="#一、Caddy-配置体系" class="headerlink" title="一、Caddy 配置体系"></a>一、Caddy 配置体系</h2><p>在 Caddy1 时代，Caddy 自创了一种被称之为 Caddyfile 的配置文件格式，当然可以理解为创造了一种语法，这里面深入的说就涉及到了编译原理相关知识，这里不再展开细谈(因为我也不会)；Caddyfile 由内部的语法解析器进行语法、词法分析最后 “序列化” 到 Go 的配置结构体中。</p><p>随着 Caddy 壮大，到了 Caddy2 时代人们已经并不满足于单纯的 Caddyfile 配置，因为学习 Caddyfile 是有代价的，负载均衡器选型的切换本身就代价很大，还要去花心思学习 Caddyfile 语法，这无异非常痛苦。<strong>所以 Caddy2 在经过取舍过后决定使用 json 作为内部标准配置，然后其他类型的配置通过 <code>Config Adapters</code> 将其转换为 json 再使用，而 Caddyfile 的 Adapter 作为官方支持的内置 Adapter 存在。</strong></p><p><strong>最终要说明的是: Caddyfile 里支持哪些指令是由 Caddyfile 的 Adapter 决定的，内部的 json 配置对应的指令名称可能跟 Caddyfile 不同，也可能内部 json 支持一些指令，而 Caddyfile 根本不支持。</strong></p><h2 id="二、Caddyfile-基本结构"><a href="#二、Caddyfile-基本结构" class="headerlink" title="二、Caddyfile 基本结构"></a>二、Caddyfile 基本结构</h2><p><strong>开局一张图，文章全靠编(下面是官方的语法结构图)</strong></p><p><img src="https://cdn.oss.link/markdown/HUABKX.jpg"></p><h3 id="2-1、全局选项"><a href="#2-1、全局选项" class="headerlink" title="2.1、全局选项"></a>2.1、全局选项</h3><p>在一个 Caddyfile 内(空白文本文件)，<strong>如果仅以两个大括号括起来的配置就是全局配置项</strong>，例如下面的配置:</p><div class="hljs code-wrapper"><pre><code class="hljs sh">&#123;debughttp_port  8080https_port 8443&#125;</code></pre></div><p>那么一共有哪些<strong>全局配置项</strong>呢？当然是看 <a href="https://caddyserver.com/docs/caddyfile/options">官方文档</a>:</p><div class="hljs code-wrapper"><pre><code class="hljs sh">&#123;<span class="hljs-comment"># General Options</span>debughttp_port  &lt;port&gt;https_port &lt;port&gt;order &lt;dir1&gt; first|last|[before|after &lt;dir2&gt;]storage &lt;module_name&gt; &#123;&lt;options...&gt;&#125;storage_clean_interval &lt;duration&gt;admin   off|&lt;addr&gt; &#123;origins &lt;origins...&gt;enforce_origin&#125;<span class="hljs-built_in">log</span> [name] &#123;output  &lt;writer_module&gt; ...format  &lt;encoder_module&gt; ...level   &lt;level&gt;include &lt;namespaces...&gt;exclude &lt;namespaces...&gt;&#125;grace_period &lt;duration&gt;<span class="hljs-comment"># TLS Options</span>auto_https off|disable_redirects|ignore_loaded_certsemail &lt;yours&gt;default_sni &lt;name&gt;local_certsskip_install_trustacme_ca &lt;directory_url&gt;acme_ca_root &lt;pem_file&gt;acme_eab &lt;key_id&gt; &lt;mac_key&gt;acme_dns &lt;provider&gt; ...on_demand_tls &#123;ask      &lt;endpoint&gt;interval &lt;duration&gt;burst    &lt;n&gt;&#125;key_type ed25519|p256|p384|rsa2048|rsa4096cert_issuer &lt;name&gt; ...ocsp_stapling offpreferred_chains [smallest] &#123;root_common_name &lt;common_names...&gt;any_common_name  &lt;common_names...&gt;&#125;<span class="hljs-comment"># Server Options</span>servers [&lt;listener_address&gt;] &#123;listener_wrappers &#123;&lt;listener_wrappers...&gt;&#125;timeouts &#123;read_body   &lt;duration&gt;read_header &lt;duration&gt;write       &lt;duration&gt;idle        &lt;duration&gt;&#125;max_header_size &lt;size&gt;protocol &#123;allow_h2cexperimental_http3strict_sni_host&#125;&#125;&#125;</code></pre></div><p>这些全局配置具体都什么意思这里就不细说了，请自行查阅文档；当然文档也可能并不一定准确，有些兴趣的可以去查看 Caddy 源码，这些都在源码中定义了 <a href="https://github.com/caddyserver/caddy/blob/v2.4.3/caddyconfig/httpcaddyfile/options.go#L28">caddyconfig/httpcaddyfile/options.go:28</a></p><h3 id="2-2、代码块"><a href="#2-2、代码块" class="headerlink" title="2.2、代码块"></a>2.2、代码块</h3><p>叫代码块可能不太恰当，也可以叫做配置块或配置片段；这是 Caddyfile 比较棒的一个功能，配置片段可以实现类似代码这种引用使用，方便组合配置文件；配置片段的语法如下:</p><div class="hljs code-wrapper"><pre><code class="hljs sh">(配置片段名字) &#123;    <span class="hljs-comment"># 这里写配置片段的内容</span>&#125;</code></pre></div><p>下面是一个配置片段示例(不能运行，只是举例):</p><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment"># 定义一个叫 TLS_INTERMEDIATE 的配置片段</span>(TLS_INTERMEDIATE) &#123;    protocols tls1.2 tls1.3    ciphers TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256 TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256 TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384 TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384 TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256 TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256&#125;www.mritd.com &#123;    <span class="hljs-comment"># 重定向</span>    redir https://mritd.com&#123;uri&#125;    <span class="hljs-comment"># 这里引用上面的 TLS_INTERMEDIATE 配置</span>    import TLS_INTERMEDIATE&#125;</code></pre></div><p>这种写法与下面的配置等价，目的就是增加配置的重用和规范化:</p><div class="hljs code-wrapper"><pre><code class="hljs sh">www.mritd.com &#123;    <span class="hljs-comment"># 重定向</span>    redir https://mritd.com&#123;uri&#125;    protocols tls1.2 tls1.3    ciphers TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256 TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256 TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384 TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384 TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256 TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256&#125;</code></pre></div><h3 id="2-3、站点配置"><a href="#2-3、站点配置" class="headerlink" title="2.3、站点配置"></a>2.3、站点配置</h3><p>站点配置是 Caddyfile 的核心中的核心，从开局的图上也可以看到，能在 “Top Level” 上存在的只有三种配置，其中就包含了这个站点配置块，站点配置块格式如下:</p><div class="hljs code-wrapper"><pre><code class="hljs sh">站点域名 &#123;    <span class="hljs-comment"># 其他配置</span>&#125;</code></pre></div><p>以下是两个合法的站点配置示例:</p><div class="hljs code-wrapper"><pre><code class="hljs sh">example1.com &#123;root * /www/example.comfile_server&#125;example2.com &#123;reverse_proxy localhost:9000&#125;</code></pre></div><h3 id="2-4、自定义匹配器"><a href="#2-4、自定义匹配器" class="headerlink" title="2.4、自定义匹配器"></a>2.4、自定义匹配器</h3><p>请求匹配器是 Caddy 内置的一种针对请求的过滤工具，有点类似于 nginx 配置中的 <code>location /api &#123;...&#125;</code>，只不过 Caddyfile 中的匹配器更加强大；标准的请求匹配器列表如下:</p><ul><li><a href="https://caddyserver.com/docs/caddyfile/matchers#expression">expression</a>: 表达式匹配(<a href="https://github.com/google/cel-spec">CEL</a>)</li><li><a href="https://caddyserver.com/docs/caddyfile/matchers#file">file</a>: 文件匹配</li><li><a href="https://caddyserver.com/docs/caddyfile/matchers#header">header</a>: 请求头匹配</li><li><a href="https://caddyserver.com/docs/caddyfile/matchers#header-regexp">header_regexp</a>: 请求头正则匹配</li><li><a href="https://caddyserver.com/docs/caddyfile/matchers#host">host</a>: 域名匹配</li><li><a href="https://caddyserver.com/docs/caddyfile/matchers#method">method</a>: HTTP 请求方法匹配</li><li><a href="https://caddyserver.com/docs/caddyfile/matchers#not">not</a>: 对其他匹配器取反</li><li><a href="https://caddyserver.com/docs/caddyfile/matchers#path">path</a>: 请求路径匹配</li><li><a href="https://caddyserver.com/docs/caddyfile/matchers#path-regexp">path_regexp</a>: 请求路径正则匹配</li><li><a href="https://caddyserver.com/docs/caddyfile/matchers#protocol">protocol</a>: 请求协议匹配</li><li><a href="https://caddyserver.com/docs/caddyfile/matchers#query">query</a>: 请求查询参数匹配</li><li><a href="https://caddyserver.com/docs/caddyfile/matchers#remote-ip">remote_ip</a>: 请求 IP 匹配</li></ul><p>自定义命名匹配器的作用是组合多个标准匹配器，然后实现复用，自定义命名匹配器语法如下:</p><div class="hljs code-wrapper"><pre><code class="hljs go"># @ 后面跟一个自定义名称@api &#123;    # 标准匹配器组合    path /api<span class="hljs-comment">/*</span><span class="hljs-comment">    host example.com</span><span class="hljs-comment">&#125;</span></code></pre></div><p>然后这个自定义的命名匹配器可以在其他位置引用:</p><div class="hljs code-wrapper"><pre><code class="hljs go">example.com &#123;    @api &#123;        # 标准匹配器组合        path /api<span class="hljs-comment">/*</span><span class="hljs-comment">        host example.com</span><span class="hljs-comment">    &#125;</span><span class="hljs-comment">    </span><span class="hljs-comment">    reverse_proxy @api 127.0.0.1:9000</span><span class="hljs-comment">&#125;</span></code></pre></div><h2 id="三、Caddyfile-语法细节"><a href="#三、Caddyfile-语法细节" class="headerlink" title="三、Caddyfile 语法细节"></a>三、Caddyfile 语法细节</h2><h3 id="3-1、Blocks"><a href="#3-1、Blocks" class="headerlink" title="3.1、Blocks"></a>3.1、Blocks</h3><p>Caddyfile 中的配置块可以理解为代码中的作用域，其包含两个大括号范围内的所有配置:</p><div class="hljs code-wrapper"><pre><code class="hljs go">... &#123;    ...&#125;</code></pre></div><p>当 Caddyfile 中只有一个站点配置，且不需要其他全局配置等信息时，Blocks 可以被省略，例如:</p><div class="hljs code-wrapper"><pre><code class="hljs go">example.com &#123;    reverse_proxy /api<span class="hljs-comment">/* localhost:9001</span><span class="hljs-comment">&#125;</span></code></pre></div><p>这个配置可以直接简写为:</p><div class="hljs code-wrapper"><pre><code class="hljs go">example.comreverse_proxy /api<span class="hljs-comment">/* localhost:9001</span></code></pre></div><p>这么做的目的是方便单站点快速配置，但是一般不常用也不推荐使用。在同一个 Caddyfile 中可以包含多个站点配置，只要地址不同即可:</p><div class="hljs code-wrapper"><pre><code class="hljs go">example.com &#123;    ...&#125;abcd.com &#123;    ...&#125;</code></pre></div><h3 id="3-2、Directives"><a href="#3-2、Directives" class="headerlink" title="3.2、Directives"></a>3.2、Directives</h3><p>指令是指描述站点配置的一些关键字，例如下面的站点配置文件:</p><div class="hljs code-wrapper"><pre><code class="hljs go">example.com &#123;    reverse_proxy /api<span class="hljs-comment">/* localhost:9001</span><span class="hljs-comment">&#125;</span></code></pre></div><p>在这个配置文件中 <code>reverse_proxy</code> 就是一个指令，同时指令还可能包含子指令(Subdirectives)，下面的配置中 <code>lb_policy</code> 就是 <code>reverse_proxy</code> 的一个子指令:</p><div class="hljs code-wrapper"><pre><code class="hljs go">example.com &#123;    reverse_proxy localhost:<span class="hljs-number">9000</span> localhost:<span class="hljs-number">9001</span> &#123;        lb_policy first    &#125;&#125;</code></pre></div><h3 id="3-3、Tokens-and-quotes"><a href="#3-3、Tokens-and-quotes" class="headerlink" title="3.3、Tokens and quotes"></a>3.3、Tokens and quotes</h3><p>在 Caddyfile 被 Caddy 读取后，Caddy 会将配置文件解析为一个个的 Token；Caddyfile 中所有 Token 都认为是空格分割，所以如果某些指令需要传递参数时我们需要通过合理的空格和引号来确保 Token 正确解析:</p><div class="hljs code-wrapper"><pre><code class="hljs go">example.com &#123;    # 这里 localhost:<span class="hljs-number">9000</span> localhost:<span class="hljs-number">9001</span> 空格分割就认为是两个 Token    reverse_proxy localhost:<span class="hljs-number">9000</span> localhost:<span class="hljs-number">9001</span>&#125;</code></pre></div><p>如果某些参数需要包含空格，那么需要使用双引号包裹:</p><div class="hljs code-wrapper"><pre><code class="hljs go">example.com &#123;    file_server &#123;        # 双引号包裹住有空格的参数        root <span class="hljs-string">&quot;/data/Application Data/html&quot;</span>    &#125;&#125;</code></pre></div><p>如果这个参数里需要包含双引号，只需要通过反斜线转义即可，例如 <code>&quot;\&quot;a b\&quot;&quot;</code>；如果有太多的双引号或者空格，可以使用 Go 语言中类似的反引号来定义 “绝对字符串”:</p><div class="hljs code-wrapper"><pre><code class="hljs go">example.com &#123;    file_server &#123;        # 反引号包裹        root <span class="hljs-string">`/data/Application Data/html`</span>    &#125;&#125;</code></pre></div><h3 id="3-4、Addresses"><a href="#3-4、Addresses" class="headerlink" title="3.4、Addresses"></a>3.4、Addresses</h3><p>Caddyfile 中的地址其实是一种很宽泛的格式，在上面讲站点配置时其实前面的字符串并不一定是域名，准确的说应该是地址:</p><div class="hljs code-wrapper"><pre><code class="hljs go">地址 &#123;    # 站点具体配置&#125;</code></pre></div><p>在 Caddyfile 中以下格式全部都是合法的地址:</p><ul><li><code>localhost</code></li><li><code>example.com</code></li><li><code>:443</code></li><li><code>http://example.com</code></li><li><code>localhost:8080</code></li><li><code>127.0.0.1</code></li><li><code>[::1]:2015</code></li><li><code>example.com/foo/*</code></li><li><code>*.example.com</code></li><li><code>http://</code></li></ul><p><strong>需要注意的是: 自动 HTTPS 是 Caddy 服务器的一个重要特性，但是自动 HTTPS 会隐式进行，除非在地址中明确的写明 <code>http://example.com</code> 这种格式时 Caddy 才会单纯监听 HTTP 协议，否则域名格式的地址 Caddy 都会进行 HTTPS 证书申请。</strong></p><p>如果地址中指定了域名，那么只有匹配到域名的请求才会接受；例如地址为 <code>localhost</code> 的站点不会响应 <code>127.0.0.1</code> 方式的访问请求。同时地址中可以采用 <code>*</code> 作为通配符，通配符作用域仅在域名的英文句号 <code>.</code> 之内，意思就是说 <code>*.example.com</code> 会匹配 <code>test.example.com</code> 但不会匹配 <code>abc.test.example.com</code>。</p><p>如果多个域名/地址共享一个站点配置，可以采用英文逗号分隔的方式写在一起:</p><div class="hljs code-wrapper"><pre><code class="hljs go">example.com,www.example.com,localhost,<span class="hljs-number">127.0</span><span class="hljs-number">.0</span><span class="hljs-number">.1</span>:<span class="hljs-number">8080</span> &#123;    file_server &#123;        root /data/html    &#125;&#125;</code></pre></div><h3 id="3-5、Matchers"><a href="#3-5、Matchers" class="headerlink" title="3.5、Matchers"></a>3.5、Matchers</h3><p>匹配器其实在第一部分已经介绍过，这里仅做一下简单说明；匹配器一般紧跟在指令之后，其大致格式分为以下三种:</p><ul><li><code>*</code>: 匹配所有请求(通配符)</li><li><code>/path</code>: 匹配特定路径</li><li><code>@name</code>: 自定义命名匹配器</li></ul><p>匹配器的用法样例如下:</p><div class="hljs code-wrapper"><pre><code class="hljs go"># 自定义一个叫 websockets 的匹配器@websockets &#123;    # 匹配请求头 Connection 中包含 Upgrade 的请求    header Connection *Upgrade*        # 匹配请求头 Upgrade 为 websocket 的请求    header Upgrade    websocket&#125;# 反向代理时使用 @websockets 匹配器reverse_proxy @websockets localhost:<span class="hljs-number">6001</span></code></pre></div><p>具体更细节的官方匹配器使用限于篇幅这里不再详细说明，请自行阅读 <a href="https://caddyserver.com/docs/caddyfile/matchers">官方文档</a></p><h3 id="3-6、Placeholders"><a href="#3-6、Placeholders" class="headerlink" title="3.6、Placeholders"></a>3.6、Placeholders</h3><p>占位符可以理解为 Caddyfile 内部的变量替换符号，占位符同样以大括号包裹，同时支持转义:</p><div class="hljs code-wrapper"><pre><code class="hljs go"># 标准占位符&#123;system.hostname&#125;# 避免冲突可进行转义\&#123;system.hostname\&#125;</code></pre></div><p>Caddyfile 内部可用的占位符有很多，但是并非在所有情况下都可用，比如 HTTP 相关的占位符仅在处理 HTTP 请求相关配置中才可用；同时占位符也支持简写，下面是官方目前支持的占位符列表:</p><p><img src="https://cdn.oss.link/markdown/MMqfJ4.png"></p><h3 id="3-7、Snippets"><a href="#3-7、Snippets" class="headerlink" title="3.7、Snippets"></a>3.7、Snippets</h3><p>片段上面也介绍过了，这里说一下片段更高级的用法: <strong>支持参数传递</strong>；下面是定义一个通用日志格式，然后通过参数引用实现不同站点使用不同日志文件的配置:</p><div class="hljs code-wrapper"><pre><code class="hljs go">(LOG_COMMON) &#123;    log &#123;        format formatted <span class="hljs-string">&quot;[&#123;ts&#125;] &#123;request&gt;remote_addr&#125; &#123;request&gt;proto&#125; &#123;request&gt;method&#125; &lt;- &#123;status&#125; -&gt; &#123;request&gt;host&#125; &#123;request&gt;uri&#125; &#123;request&gt;headers&gt;User-Agent&gt;[0]&#125;&quot;</span>  &#123;            time_format <span class="hljs-string">&quot;iso8601&quot;</span>        &#125;                # &#123;args<span class="hljs-number">.0</span>&#125; 声明引用传入的第一个参数        output file <span class="hljs-string">&quot;&#123;args.0&#125;&quot;</span> &#123;            roll_size <span class="hljs-number">100</span>mb            roll_keep <span class="hljs-number">3</span>            roll_keep_for <span class="hljs-number">7</span>d        &#125;    &#125;&#125;example.com &#123;    # 此时 /data/log/example.com.log 作为 <span class="hljs-string">&quot;&#123;args.0&#125;&quot;</span> 被传入    <span class="hljs-keyword">import</span> LOG_COMMON /data/log/example.com.log&#125;</code></pre></div><h3 id="3-8、Comments"><a href="#3-8、Comments" class="headerlink" title="3.8、Comments"></a>3.8、Comments</h3><p>注释没啥好说的，以 <code>#</code> 作为开头就行了。</p><h3 id="3-9、Environment-variables"><a href="#3-9、Environment-variables" class="headerlink" title="3.9、Environment variables"></a>3.9、Environment variables</h3><p>环境变量和占位符类似，不同的是占位符是 Caddyfile 内置的变量，而环境变量是引用系统环境变量；环境变量的使用格式如下(推荐全大写):</p><div class="hljs code-wrapper"><pre><code class="hljs go"># 引用一个叫 SITE_ADDRESS 的环境变量&#123;$SITE_ADDRESS&#125; &#123;    # 站点具体配置...&#125;</code></pre></div><p>上面的配置在 Caddy 启动时会读取 <code>SITE_ADDRESS</code> 作为监听地址，如果 <code>SITE_ADDRESS</code> 读取不到则会报错退出；如果想要为 <code>SITE_ADDRESS</code> 设置默认值，那么只需要使用如下格式即可:</p><div class="hljs code-wrapper"><pre><code class="hljs go">&#123;$SITE_ADDRESS:localhost&#125; &#123;    # 站点具体配置...&#125;</code></pre></div><h2 id="四、其他补充"><a href="#四、其他补充" class="headerlink" title="四、其他补充"></a>四、其他补充</h2><p>Caddyfile 并不是万能的，但是 Caddyfile 因为更易于编写和维护所以使用比较广泛；在第一部分介绍 Caddy 的配置文件体系时已经说明了，实际上 Caddy 内部是使用 json 作为配置的；这时就可能出现一些极端情况，比如说真的某个配置只能通过 json 配置，那么这时候可以考虑先通过 json 管理 API 进行动态修改，然后再去向官方发 issue，有能力也可以直接 PR；API 动态修改的流程如下:</p><p>首先假设你已经有一个能够正常启动的 Caddyfile，但是某个配置选项不支持，这时候你可以通过 API 获取内部的 json 配置:</p><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment"># 结尾一定要有 /</span>➜ ~ curl localhost:2019/config/&#123;<span class="hljs-string">&quot;apps&quot;</span>:&#123;<span class="hljs-string">&quot;http&quot;</span>:&#123;<span class="hljs-string">&quot;servers&quot;</span>:&#123;<span class="hljs-string">&quot;srv0&quot;</span>:&#123;<span class="hljs-string">&quot;listen&quot;</span>:[<span class="hljs-string">&quot;:80&quot;</span>],<span class="hljs-string">&quot;routes&quot;</span>:[&#123;<span class="hljs-string">&quot;handle&quot;</span>:[&#123;<span class="hljs-string">&quot;canonical_uris&quot;</span>:<span class="hljs-literal">false</span>,<span class="hljs-string">&quot;handler&quot;</span>:<span class="hljs-string">&quot;file_server&quot;</span>,<span class="hljs-string">&quot;hide&quot;</span>:[<span class="hljs-string">&quot;./Caddyfile&quot;</span>]&#125;]&#125;]&#125;&#125;&#125;&#125;&#125;</code></pre></div><p>得到这个配置以后，你可以通过格式化工具格式化 json，然后添加特定选项，再将其保存到一个配置文件中，然后重新 load 回去即可:</p><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment"># 假设修改后的 json 文件叫 caddy.json</span>➜ ~ curl -XPOST http://localhost:2019/load -H <span class="hljs-string">&quot;Content-Type: application/json&quot;</span> -d @caddy.json</code></pre></div>]]></content>
    
    
    <summary type="html">发现大部分人在切换 Caddy 时遇到的比较大的困难就是这个 Caddyfile 不知道怎么写，一开始我也是很懵逼的状态，今天决定写写这个 Caddyfile 配置语法，顺便自己也完整的学学。</summary>
    
    
    
    <category term="Caddy" scheme="https://mritd.com/categories/caddy/"/>
    
    
    <category term="Caddy" scheme="https://mritd.com/tags/caddy/"/>
    
    <category term="Caddyfile" scheme="https://mritd.com/tags/caddyfile/"/>
    
  </entry>
  
  <entry>
    <title>Golang Context 源码分析</title>
    <link href="https://mritd.com/2021/06/27/golang-context-source-code/"/>
    <id>https://mritd.com/2021/06/27/golang-context-source-code/</id>
    <published>2021-06-27T07:57:00.000Z</published>
    <updated>2021-06-27T07:57:00.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>本文所有源码分析基于 Go 1.16.4，阅读时请自行切换版本。</p></blockquote><h2 id="一、Context-介绍"><a href="#一、Context-介绍" class="headerlink" title="一、Context 介绍"></a>一、Context 介绍</h2><p>标准库中的 Context 是一个接口，其具体实现有很多种；Context 在 Go 1.7 中被添加入标准库，主要用于跨多个 Goroutine 设置截止时间、同步信号、传递上下文请求值等。</p><p>由于需要跨多个 Goroutine 传递信号，所以多个 Context 往往需要关联到一起，形成类似一个树的结构。这种树状的关联关系需要有一个根(root) Context，然后其他 Context 关联到 root Context 成为它的子(child) Context；这种关联可以是多级的，所以在角色上 Context 分为三种: </p><ul><li>root(根) Context</li><li>parent(父) Context</li><li>child(子) Context</li></ul><h2 id="二、Context-类型"><a href="#二、Context-类型" class="headerlink" title="二、Context 类型"></a>二、Context 类型</h2><h3 id="2-1、Context-的创建"><a href="#2-1、Context-的创建" class="headerlink" title="2.1、Context 的创建"></a>2.1、Context 的创建</h3><p>标准库中定义的 Context 创建方法大致如下:</p><ul><li>context.Background(): 该方法用于创建 root Context，且不可取消</li><li>context.TODO(): 该方法同样用于创建 root Context(不准确)，也不可取消，TODO 通常代表不知道要使用哪个 Context，所以后面可能有调整</li><li>context.WithCancel(parent Context): 从 parent Context 创建一个带有取消方法的 child Context，该 Context 可以手动调用 cancel</li><li>context.WithDeadline(parent Context, d time.Time): 从 parent Context 创建一个带有取消方法的 child Context，不同的是当到达 d 时间后该 Context 将自动取消</li><li>context.WithTimeout(parent Context, timeout time.Duration): 与 WithDeadline 类似，只不过指定的是一个从当前时间开始的超时时间</li><li>context.WithValue(parent Context, key, val interface{}): 从 parent Context 创建一个 child Context，该 Context 可以存储一个键值对，同时这是一个不可取消的 Context</li></ul><h3 id="2-2、Context-内部类型"><a href="#2-2、Context-内部类型" class="headerlink" title="2.2、Context 内部类型"></a>2.2、Context 内部类型</h3><p>在阅读源码后会发现，Context 各种创建方法其实主要只使用到了 4 种类型的 Context 实现:</p><h4 id="2-2-1、emptyCtx"><a href="#2-2-1、emptyCtx" class="headerlink" title="2.2.1、emptyCtx"></a>2.2.1、emptyCtx</h4><p><code>emptyCtx</code> 实际上就是个 int，其对 Context 接口的主要实现(<code>Deadline</code>、<code>Done</code>、<code>Err</code>、<code>Value</code>)全部返回了 <code>nil</code>，也就是说其实是一个 “啥也不干” 的 Context；<strong>它通常用于创建 root Context，标准库中 <code>context.Background()</code> 和 <code>context.TODO()</code> 返回的就是这个 <code>emptyCtx</code>。</strong></p><div class="hljs code-wrapper"><pre><code class="hljs go"><span class="hljs-comment">// An emptyCtx is never canceled, has no values, and has no deadline. It is not</span><span class="hljs-comment">// struct&#123;&#125;, since vars of this type must have distinct addresses.</span><span class="hljs-keyword">type</span> emptyCtx <span class="hljs-keyword">int</span><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(*emptyCtx)</span> <span class="hljs-title">Deadline</span><span class="hljs-params">()</span> <span class="hljs-params">(deadline time.Time, ok <span class="hljs-keyword">bool</span>)</span></span> &#123;<span class="hljs-keyword">return</span>&#125;<span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(*emptyCtx)</span> <span class="hljs-title">Done</span><span class="hljs-params">()</span> &lt;-<span class="hljs-title">chan</span> <span class="hljs-title">struct</span></span>&#123;&#125; &#123;<span class="hljs-keyword">return</span> <span class="hljs-literal">nil</span>&#125;<span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(*emptyCtx)</span> <span class="hljs-title">Err</span><span class="hljs-params">()</span> <span class="hljs-title">error</span></span> &#123;<span class="hljs-keyword">return</span> <span class="hljs-literal">nil</span>&#125;<span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(*emptyCtx)</span> <span class="hljs-title">Value</span><span class="hljs-params">(key <span class="hljs-keyword">interface</span>&#123;&#125;)</span> <span class="hljs-title">interface</span></span>&#123;&#125; &#123;<span class="hljs-keyword">return</span> <span class="hljs-literal">nil</span>&#125;<span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(e *emptyCtx)</span> <span class="hljs-title">String</span><span class="hljs-params">()</span> <span class="hljs-title">string</span></span> &#123;<span class="hljs-keyword">switch</span> e &#123;<span class="hljs-keyword">case</span> background:<span class="hljs-keyword">return</span> <span class="hljs-string">&quot;context.Background&quot;</span><span class="hljs-keyword">case</span> todo:<span class="hljs-keyword">return</span> <span class="hljs-string">&quot;context.TODO&quot;</span>&#125;<span class="hljs-keyword">return</span> <span class="hljs-string">&quot;unknown empty Context&quot;</span>&#125;<span class="hljs-keyword">var</span> (background = <span class="hljs-built_in">new</span>(emptyCtx)todo       = <span class="hljs-built_in">new</span>(emptyCtx))<span class="hljs-comment">// Background returns a non-nil, empty Context. It is never canceled, has no</span><span class="hljs-comment">// values, and has no deadline. It is typically used by the main function,</span><span class="hljs-comment">// initialization, and tests, and as the top-level Context for incoming</span><span class="hljs-comment">// requests.</span><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-title">Background</span><span class="hljs-params">()</span> <span class="hljs-title">Context</span></span> &#123;<span class="hljs-keyword">return</span> background&#125;<span class="hljs-comment">// TODO returns a non-nil, empty Context. Code should use context.TODO when</span><span class="hljs-comment">// it&#x27;s unclear which Context to use or it is not yet available (because the</span><span class="hljs-comment">// surrounding function has not yet been extended to accept a Context</span><span class="hljs-comment">// parameter).</span><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-title">TODO</span><span class="hljs-params">()</span> <span class="hljs-title">Context</span></span> &#123;<span class="hljs-keyword">return</span> todo&#125;</code></pre></div><h4 id="2-2-2、cancelCtx"><a href="#2-2-2、cancelCtx" class="headerlink" title="2.2.2、cancelCtx"></a>2.2.2、cancelCtx</h4><p><code>cancelCtx</code> 内部包含一个 <code>Context</code> 接口实例，还有一个 <code>children map[canceler]struct&#123;&#125;</code>；这两个变量的作用就是保证 <code>cancelCtx</code> 可以在 parent Context 和 child Context 两种角色之间转换:</p><ul><li>作为其他 Context 实例的 parent Context 时，将其他 Context 实例存储在 <code>children map[canceler]struct&#123;&#125;</code> 中建立关联关系</li><li>作为其他 Context 实例的 child Context 时，将其他 Context 实例存储在 “Context” 变量里建立关联</li></ul><p><strong><code>cancelCtx</code> 被定义为一个可以取消的 Context，而由于 Context 的树形结构，当作为 parent Context 取消时需要同步取消节点下所有 child Context，这时候只需要遍历 <code>children map[canceler]struct&#123;&#125;</code> 然后逐个取消即可。</strong></p><div class="hljs code-wrapper"><pre><code class="hljs go"><span class="hljs-comment">// A cancelCtx can be canceled. When canceled, it also cancels any children</span><span class="hljs-comment">// that implement canceler.</span><span class="hljs-keyword">type</span> cancelCtx <span class="hljs-keyword">struct</span> &#123;Contextmu       sync.Mutex            <span class="hljs-comment">// protects following fields</span>done     <span class="hljs-keyword">chan</span> <span class="hljs-keyword">struct</span>&#123;&#125;         <span class="hljs-comment">// created lazily, closed by first cancel call</span>children <span class="hljs-keyword">map</span>[canceler]<span class="hljs-keyword">struct</span>&#123;&#125; <span class="hljs-comment">// set to nil by the first cancel call</span>err      error                 <span class="hljs-comment">// set to non-nil by the first cancel call</span>&#125;<span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(c *cancelCtx)</span> <span class="hljs-title">Value</span><span class="hljs-params">(key <span class="hljs-keyword">interface</span>&#123;&#125;)</span> <span class="hljs-title">interface</span></span>&#123;&#125; &#123;<span class="hljs-keyword">if</span> key == &amp;cancelCtxKey &#123;<span class="hljs-keyword">return</span> c&#125;<span class="hljs-keyword">return</span> c.Context.Value(key)&#125;<span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(c *cancelCtx)</span> <span class="hljs-title">Done</span><span class="hljs-params">()</span> &lt;-<span class="hljs-title">chan</span> <span class="hljs-title">struct</span></span>&#123;&#125; &#123;c.mu.Lock()<span class="hljs-keyword">if</span> c.done == <span class="hljs-literal">nil</span> &#123;c.done = <span class="hljs-built_in">make</span>(<span class="hljs-keyword">chan</span> <span class="hljs-keyword">struct</span>&#123;&#125;)&#125;d := c.donec.mu.Unlock()<span class="hljs-keyword">return</span> d&#125;<span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(c *cancelCtx)</span> <span class="hljs-title">Err</span><span class="hljs-params">()</span> <span class="hljs-title">error</span></span> &#123;c.mu.Lock()err := c.errc.mu.Unlock()<span class="hljs-keyword">return</span> err&#125;<span class="hljs-keyword">type</span> stringer <span class="hljs-keyword">interface</span> &#123;String() <span class="hljs-keyword">string</span>&#125;<span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-title">contextName</span><span class="hljs-params">(c Context)</span> <span class="hljs-title">string</span></span> &#123;<span class="hljs-keyword">if</span> s, ok := c.(stringer); ok &#123;<span class="hljs-keyword">return</span> s.String()&#125;<span class="hljs-keyword">return</span> reflectlite.TypeOf(c).String()&#125;<span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(c *cancelCtx)</span> <span class="hljs-title">String</span><span class="hljs-params">()</span> <span class="hljs-title">string</span></span> &#123;<span class="hljs-keyword">return</span> contextName(c.Context) + <span class="hljs-string">&quot;.WithCancel&quot;</span>&#125;<span class="hljs-comment">// cancel closes c.done, cancels each of c&#x27;s children, and, if</span><span class="hljs-comment">// removeFromParent is true, removes c from its parent&#x27;s children.</span><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(c *cancelCtx)</span> <span class="hljs-title">cancel</span><span class="hljs-params">(removeFromParent <span class="hljs-keyword">bool</span>, err error)</span></span> &#123;<span class="hljs-keyword">if</span> err == <span class="hljs-literal">nil</span> &#123;<span class="hljs-built_in">panic</span>(<span class="hljs-string">&quot;context: internal error: missing cancel error&quot;</span>)&#125;c.mu.Lock()<span class="hljs-keyword">if</span> c.err != <span class="hljs-literal">nil</span> &#123;c.mu.Unlock()<span class="hljs-keyword">return</span> <span class="hljs-comment">// already canceled</span>&#125;c.err = err<span class="hljs-keyword">if</span> c.done == <span class="hljs-literal">nil</span> &#123;c.done = closedchan&#125; <span class="hljs-keyword">else</span> &#123;<span class="hljs-built_in">close</span>(c.done)&#125;<span class="hljs-keyword">for</span> child := <span class="hljs-keyword">range</span> c.children &#123;<span class="hljs-comment">// <span class="hljs-doctag">NOTE:</span> acquiring the child&#x27;s lock while holding parent&#x27;s lock.</span>child.cancel(<span class="hljs-literal">false</span>, err)&#125;c.children = <span class="hljs-literal">nil</span>c.mu.Unlock()<span class="hljs-keyword">if</span> removeFromParent &#123;removeChild(c.Context, c)&#125;&#125;</code></pre></div><h4 id="2-2-3、timerCtx"><a href="#2-2-3、timerCtx" class="headerlink" title="2.2.3、timerCtx"></a>2.2.3、timerCtx</h4><p><code>timerCtx</code> 实际上是在 <code>cancelCtx</code> 之上构建的，唯一的区别就是增加了计时器和截止时间；<strong>有了这两个配置以后就可以在特定时间进行自动取消，<code>WithDeadline(parent Context, d time.Time)</code> 和 <code>WithTimeout(parent Context, timeout time.Duration)</code> 方法返回的都是这个 <code>timerCtx</code>。</strong></p><div class="hljs code-wrapper"><pre><code class="hljs go"><span class="hljs-comment">// A timerCtx carries a timer and a deadline. It embeds a cancelCtx to</span><span class="hljs-comment">// implement Done and Err. It implements cancel by stopping its timer then</span><span class="hljs-comment">// delegating to cancelCtx.cancel.</span><span class="hljs-keyword">type</span> timerCtx <span class="hljs-keyword">struct</span> &#123;cancelCtxtimer *time.Timer <span class="hljs-comment">// Under cancelCtx.mu.</span>deadline time.Time&#125;<span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(c *timerCtx)</span> <span class="hljs-title">Deadline</span><span class="hljs-params">()</span> <span class="hljs-params">(deadline time.Time, ok <span class="hljs-keyword">bool</span>)</span></span> &#123;<span class="hljs-keyword">return</span> c.deadline, <span class="hljs-literal">true</span>&#125;<span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(c *timerCtx)</span> <span class="hljs-title">String</span><span class="hljs-params">()</span> <span class="hljs-title">string</span></span> &#123;<span class="hljs-keyword">return</span> contextName(c.cancelCtx.Context) + <span class="hljs-string">&quot;.WithDeadline(&quot;</span> +c.deadline.String() + <span class="hljs-string">&quot; [&quot;</span> +time.Until(c.deadline).String() + <span class="hljs-string">&quot;])&quot;</span>&#125;<span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(c *timerCtx)</span> <span class="hljs-title">cancel</span><span class="hljs-params">(removeFromParent <span class="hljs-keyword">bool</span>, err error)</span></span> &#123;c.cancelCtx.cancel(<span class="hljs-literal">false</span>, err)<span class="hljs-keyword">if</span> removeFromParent &#123;<span class="hljs-comment">// Remove this timerCtx from its parent cancelCtx&#x27;s children.</span>removeChild(c.cancelCtx.Context, c)&#125;c.mu.Lock()<span class="hljs-keyword">if</span> c.timer != <span class="hljs-literal">nil</span> &#123;c.timer.Stop()c.timer = <span class="hljs-literal">nil</span>&#125;c.mu.Unlock()&#125;</code></pre></div><h4 id="2-2-4、valueCtx"><a href="#2-2-4、valueCtx" class="headerlink" title="2.2.4、valueCtx"></a>2.2.4、valueCtx</h4><p><code>valueCtx</code> 内部同样包含了一个 <code>Context</code> 接口实例，目的也是可以作为 child Context，同时为了保证其 “Value” 特性，其内部包含了两个无限制变量 <code>key, val interface&#123;&#125;</code>；<strong>在调用 <code>valueCtx.Value(key interface&#123;&#125;)</code> 会进行递归向上查找，但是这个查找只负责查找 “直系” Context，也就是说可以无限递归查找 parent Context 是否包含这个 key，但是无法查找兄弟 Context 是否包含。</strong></p><div class="hljs code-wrapper"><pre><code class="hljs go"><span class="hljs-comment">// A valueCtx carries a key-value pair. It implements Value for that key and</span><span class="hljs-comment">// delegates all other calls to the embedded Context.</span><span class="hljs-keyword">type</span> valueCtx <span class="hljs-keyword">struct</span> &#123;Contextkey, val <span class="hljs-keyword">interface</span>&#123;&#125;&#125;<span class="hljs-comment">// stringify tries a bit to stringify v, without using fmt, since we don&#x27;t</span><span class="hljs-comment">// want context depending on the unicode tables. This is only used by</span><span class="hljs-comment">// *valueCtx.String().</span><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-title">stringify</span><span class="hljs-params">(v <span class="hljs-keyword">interface</span>&#123;&#125;)</span> <span class="hljs-title">string</span></span> &#123;<span class="hljs-keyword">switch</span> s := v.(<span class="hljs-keyword">type</span>) &#123;<span class="hljs-keyword">case</span> stringer:<span class="hljs-keyword">return</span> s.String()<span class="hljs-keyword">case</span> <span class="hljs-keyword">string</span>:<span class="hljs-keyword">return</span> s&#125;<span class="hljs-keyword">return</span> <span class="hljs-string">&quot;&lt;not Stringer&gt;&quot;</span>&#125;<span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(c *valueCtx)</span> <span class="hljs-title">String</span><span class="hljs-params">()</span> <span class="hljs-title">string</span></span> &#123;<span class="hljs-keyword">return</span> contextName(c.Context) + <span class="hljs-string">&quot;.WithValue(type &quot;</span> +reflectlite.TypeOf(c.key).String() +<span class="hljs-string">&quot;, val &quot;</span> + stringify(c.val) + <span class="hljs-string">&quot;)&quot;</span>&#125;<span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(c *valueCtx)</span> <span class="hljs-title">Value</span><span class="hljs-params">(key <span class="hljs-keyword">interface</span>&#123;&#125;)</span> <span class="hljs-title">interface</span></span>&#123;&#125; &#123;<span class="hljs-keyword">if</span> c.key == key &#123;<span class="hljs-keyword">return</span> c.val&#125;<span class="hljs-keyword">return</span> c.Context.Value(key)&#125;</code></pre></div><h2 id="三、cancelCtx-源码分析"><a href="#三、cancelCtx-源码分析" class="headerlink" title="三、cancelCtx 源码分析"></a>三、cancelCtx 源码分析</h2><h3 id="3-1、cancelCtx-是如何被创建的"><a href="#3-1、cancelCtx-是如何被创建的" class="headerlink" title="3.1、cancelCtx 是如何被创建的"></a>3.1、cancelCtx 是如何被创建的</h3><p>cancelCtx 在调用 <code>context.WithCancel</code> 方法时创建(暂不考虑其他衍生类型)，创建方法比较简单:</p><div class="hljs code-wrapper"><pre><code class="hljs go"><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-title">WithCancel</span><span class="hljs-params">(parent Context)</span> <span class="hljs-params">(ctx Context, cancel CancelFunc)</span></span> &#123;<span class="hljs-keyword">if</span> parent == <span class="hljs-literal">nil</span> &#123;<span class="hljs-built_in">panic</span>(<span class="hljs-string">&quot;cannot create context from nil parent&quot;</span>)&#125;c := newCancelCtx(parent)propagateCancel(parent, &amp;c)<span class="hljs-keyword">return</span> &amp;c, <span class="hljs-function"><span class="hljs-keyword">func</span><span class="hljs-params">()</span></span> &#123; c.cancel(<span class="hljs-literal">true</span>, Canceled) &#125;&#125;</code></pre></div><p><code>newCancelCtx</code> 方法就是将 parent Context 设置到内部变量中，<strong>值得分析的是 <code>propagateCancel(parent, &amp;c)</code> 方法和被其调用的 <code>parentCancelCtx(parent Context) (*cancelCtx, bool)</code> 方法，这两个方法保证了 Context 链可以从顶端到底端的及联 cancel</strong>，关于这两个方法的分析如下:</p><div class="hljs code-wrapper"><pre><code class="hljs go"><span class="hljs-comment">// propagateCancel arranges for child to be canceled when parent is.</span><span class="hljs-comment">// propagateCancel 这个方法主要负责保证当 parent Context 被取消时，child Context 也会被及联取消</span><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-title">propagateCancel</span><span class="hljs-params">(parent Context, child canceler)</span></span> &#123;<span class="hljs-comment">// 针对于 context.Background()/TODO() 创建的 Context(emptyCtx)，其 done channel 将永远为 nil</span><span class="hljs-comment">// 对于其他的标准的可取消的 Context(cancelCtx、timerCtx) 调用 Done() 方法将会延迟初始化 done channel(调用时创建)</span><span class="hljs-comment">// 所以 done channel 为 nil 时说明 parent context 必然永远不会被取消，所以就无需及联到 child Context</span>done := parent.Done()<span class="hljs-keyword">if</span> done == <span class="hljs-literal">nil</span> &#123;<span class="hljs-keyword">return</span> <span class="hljs-comment">// parent is never canceled</span>&#125;<span class="hljs-comment">// 如果 done channel 不是 nil，说明 parent Context 是一个可以取消的 Context</span><span class="hljs-comment">// 这里需要立即判断一下 done channel 是否可读取，如果可以读取说明上面无锁阶段</span><span class="hljs-comment">// parent Context 已经被取消了，那么应该立即取消 child Context</span><span class="hljs-keyword">select</span> &#123;<span class="hljs-keyword">case</span> &lt;-done:<span class="hljs-comment">// parent is already canceled</span>child.cancel(<span class="hljs-literal">false</span>, parent.Err())<span class="hljs-keyword">return</span><span class="hljs-keyword">default</span>:&#125;<span class="hljs-comment">// parentCancelCtx 用于获取 parent Context 的底层可取消 Context(cancelCtx)</span><span class="hljs-comment">//</span><span class="hljs-comment">// 如果 parent Context 本身就是 *cancelCtx 或者是标准库中基于 cancelCtx 衍生的 Context 会返回 true</span><span class="hljs-comment">// 如果 parent Context 已经取消/或者根本无法取消 会返回 false</span><span class="hljs-comment">// 如果 parent Context 无法转换为一个 *cancelCtx 也会返回 false</span><span class="hljs-comment">// 如果 parent Context 是一个自定义深度包装的 cancelCtx(自己定义了 done channel) 则也会返回 false</span><span class="hljs-keyword">if</span> p, ok := parentCancelCtx(parent); ok &#123; <span class="hljs-comment">// ok 为 true 说明 parent Context 为 标准库的 cancelCtx 或者至少可以完全转换为 *cancelCtx</span><span class="hljs-comment">// 先对 parent Context 加锁，防止更改</span>p.mu.Lock()<span class="hljs-comment">// 因为 ok 为 true 就已经确定了 parent Context 一定为 *cancelCtx，而 cancelCtx 取消时必然设置 err</span><span class="hljs-comment">// 所以并发加锁情况下如果 parent Context 的 err 不为空说明已经被取消了</span><span class="hljs-keyword">if</span> p.err != <span class="hljs-literal">nil</span> &#123;<span class="hljs-comment">// parent has already been canceled</span><span class="hljs-comment">// parent Context 已经被取消，则直接及联取消 child Context</span>child.cancel(<span class="hljs-literal">false</span>, p.err)&#125; <span class="hljs-keyword">else</span> &#123;<span class="hljs-comment">// 在 ok 为 true 时确定了 parent Context 一定为 *cancelCtx，此时 err 为 nil</span><span class="hljs-comment">// 这说明 parent Context 还没被取消，这时候要在 parent Context 的 children map 中关联 child Context</span><span class="hljs-comment">// 这个 children map 在 parent Context 被取消时会被遍历然后批量调用 child Context 的取消方法</span><span class="hljs-keyword">if</span> p.children == <span class="hljs-literal">nil</span> &#123;p.children = <span class="hljs-built_in">make</span>(<span class="hljs-keyword">map</span>[canceler]<span class="hljs-keyword">struct</span>&#123;&#125;)&#125;p.children[child] = <span class="hljs-keyword">struct</span>&#123;&#125;&#123;&#125;&#125;p.mu.Unlock()&#125; <span class="hljs-keyword">else</span> &#123; <span class="hljs-comment">// ok 为 false，说明: &quot;parent Context 已经取消&quot; 或 &quot;根本无法取消&quot; 或 &quot;无法转换为一个 *cancelCtx&quot; 或 &quot;是一个自定义深度包装的 cancelCtx&quot;</span>atomic.AddInt32(&amp;goroutines, +<span class="hljs-number">1</span>)<span class="hljs-comment">// 由于代码在方法开始时就判断了 parent Context &quot;已经取消&quot;、&quot;根本无法取消&quot; 这两种情况</span><span class="hljs-comment">// 所以这两种情况在这里不会发生，因此 &lt;-parent.Done() 不会产生 panic</span><span class="hljs-comment">// </span><span class="hljs-comment">// 唯一剩下的可能就是 parent Context &quot;无法转换为一个 *cancelCtx&quot; 或 &quot;是一个被覆盖了 done channel 的自定义 cancelCtx&quot;</span><span class="hljs-comment">// 这种两种情况下无法通过 parent Context 的 children map 建立关联，只能通过创建一个 Goroutine 来完成及联取消的操作</span><span class="hljs-keyword">go</span> <span class="hljs-function"><span class="hljs-keyword">func</span><span class="hljs-params">()</span></span> &#123;<span class="hljs-keyword">select</span> &#123;<span class="hljs-keyword">case</span> &lt;-parent.Done():child.cancel(<span class="hljs-literal">false</span>, parent.Err())<span class="hljs-keyword">case</span> &lt;-child.Done():&#125;&#125;()&#125;&#125;<span class="hljs-comment">// parentCancelCtx returns the underlying *cancelCtx for parent.</span><span class="hljs-comment">// It does this by looking up parent.Value(&amp;cancelCtxKey) to find</span><span class="hljs-comment">// the innermost enclosing *cancelCtx and then checking whether</span><span class="hljs-comment">// parent.Done() matches that *cancelCtx. (If not, the *cancelCtx</span><span class="hljs-comment">// has been wrapped in a custom implementation providing a</span><span class="hljs-comment">// different done channel, in which case we should not bypass it.)</span><span class="hljs-comment">// parentCancelCtx 负责从 parent Context 中取出底层的 cancelCtx</span><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-title">parentCancelCtx</span><span class="hljs-params">(parent Context)</span> <span class="hljs-params">(*cancelCtx, <span class="hljs-keyword">bool</span>)</span></span> &#123;<span class="hljs-comment">// 如果 parent context 的 done 为 nil 说明不支持 cancel，那么就不可能是 cancelCtx</span><span class="hljs-comment">// 如果 parent context 的 done 为 可复用的 closedchan 说明 parent context 已经 cancel 了</span><span class="hljs-comment">// 此时取出 cancelCtx 没有意义(具体为啥没意义后面章节会有分析)</span>done := parent.Done()<span class="hljs-keyword">if</span> done == closedchan || done == <span class="hljs-literal">nil</span> &#123;<span class="hljs-keyword">return</span> <span class="hljs-literal">nil</span>, <span class="hljs-literal">false</span>&#125;<span class="hljs-comment">// 如果 parent context 属于原生的 *cancelCtx 或衍生类型(timerCtx) 需要继续进行后续判断</span><span class="hljs-comment">// 如果 parent context 无法转换到 *cancelCtx，则认为非 cancelCtx，返回 nil,fasle</span>p, ok := parent.Value(&amp;cancelCtxKey).(*cancelCtx)<span class="hljs-keyword">if</span> !ok &#123;<span class="hljs-keyword">return</span> <span class="hljs-literal">nil</span>, <span class="hljs-literal">false</span>&#125;p.mu.Lock()<span class="hljs-comment">// 经过上面的判断后，说明 parent context 可以被转换为 *cancelCtx，这时存在多种情况:</span><span class="hljs-comment">//   - parent context 就是 *cancelCtx</span><span class="hljs-comment">//   - parent context 是标准库中的 timerCtx</span><span class="hljs-comment">//   - parent context 是个自己自定义包装的 cancelCtx</span><span class="hljs-comment">//</span><span class="hljs-comment">// 针对这 3 种情况需要进行判断，判断方法就是: </span><span class="hljs-comment">//   判断 parent context 通过 Done() 方法获取的 done channel 与 Value 查找到的 context 的 done channel 是否一致</span><span class="hljs-comment">// </span><span class="hljs-comment">// 一致情况说明 parent context 为 cancelCtx 或 timerCtx 或 自定义的 cancelCtx 且未重写 Done()，</span><span class="hljs-comment">// 这种情况下可以认为拿到了底层的 *cancelCtx</span><span class="hljs-comment">// </span><span class="hljs-comment">// 不一致情况说明 parent context 是一个自定义的 cancelCtx 且重写了 Done() 方法，并且并未返回标准 *cancelCtx 的</span><span class="hljs-comment">// 的 done channel，这种情况需要单独处理，故返回 nil, false</span>ok = p.done == donep.mu.Unlock()<span class="hljs-keyword">if</span> !ok &#123;<span class="hljs-keyword">return</span> <span class="hljs-literal">nil</span>, <span class="hljs-literal">false</span>&#125;<span class="hljs-keyword">return</span> p, <span class="hljs-literal">true</span>&#125;</code></pre></div><h3 id="3-2、cancelCtx-是如何取消的"><a href="#3-2、cancelCtx-是如何取消的" class="headerlink" title="3.2、cancelCtx 是如何取消的"></a>3.2、cancelCtx 是如何取消的</h3><p>在上面的 cancelCtx 创建源码中可以看到，cancelCtx 内部跨多个 Goroutine 实现信号传递其实靠的就是一个 done channel；<strong>如果要取消这个 Context，那么就需要让所有 <code>&lt;-c.Done()</code> 停止阻塞，这时候最简单的办法就是把这个 channel 直接 close 掉，或者干脆换成一个已经被 close 的 channel，</strong>事实上官方也是怎么做的。</p><div class="hljs code-wrapper"><pre><code class="hljs go"><span class="hljs-comment">// cancel closes c.done, cancels each of c&#x27;s children, and, if</span><span class="hljs-comment">// removeFromParent is true, removes c from its parent&#x27;s children.</span><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(c *cancelCtx)</span> <span class="hljs-title">cancel</span><span class="hljs-params">(removeFromParent <span class="hljs-keyword">bool</span>, err error)</span></span> &#123;    <span class="hljs-comment">// 首先判断 err 是不是 nil，如果不是 nil 则直接 panic</span>    <span class="hljs-comment">// 这么做的目的是因为 cancel 方法是个私有方法，标准库内任何调用 cancel</span>    <span class="hljs-comment">// 的方法保证了一定会传入 err，如果没传那就是非正常调用，所以可以直接 panic</span><span class="hljs-keyword">if</span> err == <span class="hljs-literal">nil</span> &#123;<span class="hljs-built_in">panic</span>(<span class="hljs-string">&quot;context: internal error: missing cancel error&quot;</span>)&#125;<span class="hljs-comment">// 对 context 加锁，防止并发更改</span>c.mu.Lock()<span class="hljs-comment">// 如果加锁后有并发访问，那么二次判断 err 可以防止重复 cancel 调用</span><span class="hljs-keyword">if</span> c.err != <span class="hljs-literal">nil</span> &#123;c.mu.Unlock()<span class="hljs-keyword">return</span> <span class="hljs-comment">// already canceled</span>&#125;<span class="hljs-comment">// 这里设置了内部的 err，所以上面的判断 c.err != nil 与这里是对应的</span><span class="hljs-comment">// 也就是说加锁后一定有一个 Goroutine 先 cannel，cannel 后 c.err 一定不为 nil</span>c.err = err<span class="hljs-comment">// 判断内部的 done channel 是不是为 nil，因为在 context.WithCancel 创建 cancelCtx 的</span><span class="hljs-comment">// 时候并未立即初始化 done channel(延迟初始化)，所以这里可能为 nil</span><span class="hljs-comment">// 如果 done channel 为 nil，那么就把它设置成共享可重用的一个已经被关闭的 channel</span><span class="hljs-keyword">if</span> c.done == <span class="hljs-literal">nil</span> &#123;c.done = closedchan&#125; <span class="hljs-keyword">else</span> &#123; <span class="hljs-comment">// 如果 done channel 已经被初始化，则直接 close 它</span><span class="hljs-built_in">close</span>(c.done)&#125;<span class="hljs-comment">// 如果当前 Context 下面还有关联的 child Context，且这些 child Context 都是</span><span class="hljs-comment">// 可以转换成 *cancelCtx 的 Context(见上面的 propagateCancel 方法分析)，那么</span><span class="hljs-comment">// 直接遍历 childre map，并且调用 child Context 的 cancel 即可</span><span class="hljs-comment">// 如果关联的 child Context 不能转换成 *cancelCtx，那么由 propagateCancel 方法</span><span class="hljs-comment">// 中已经创建了单独的 Goroutine 来关闭这些 child Context</span><span class="hljs-keyword">for</span> child := <span class="hljs-keyword">range</span> c.children &#123;<span class="hljs-comment">// <span class="hljs-doctag">NOTE:</span> acquiring the child&#x27;s lock while holding parent&#x27;s lock.</span>child.cancel(<span class="hljs-literal">false</span>, err)&#125;<span class="hljs-comment">// 清除 c.children map 并解锁</span>c.children = <span class="hljs-literal">nil</span>c.mu.Unlock()    <span class="hljs-comment">// 如果 removeFromParent 为 true，那么从 parent Context 中清理掉自己</span><span class="hljs-keyword">if</span> removeFromParent &#123;removeChild(c.Context, c)&#125;&#125;</code></pre></div><h3 id="3-3、parentCancelCtx-为什么不取出已取消的-cancelCtx"><a href="#3-3、parentCancelCtx-为什么不取出已取消的-cancelCtx" class="headerlink" title="3.3、parentCancelCtx 为什么不取出已取消的 cancelCtx"></a>3.3、parentCancelCtx 为什么不取出已取消的 cancelCtx</h3><p>在上面的 3.1 章节中分析 <code>parentCancelCtx</code> 方法时有这么一段:</p><div class="hljs code-wrapper"><pre><code class="hljs go"><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-title">parentCancelCtx</span><span class="hljs-params">(parent Context)</span> <span class="hljs-params">(*cancelCtx, <span class="hljs-keyword">bool</span>)</span></span> &#123;<span class="hljs-comment">// 如果 parent context 的 done 为 nil 说明不支持 cancel，那么就不可能是 cancelCtx</span><span class="hljs-comment">// 如果 parent context 的 done 为 可复用的 closedchan 说明 parent context 已经 cancel 了</span><span class="hljs-comment">// 此时取出 cancelCtx 没有意义(具体为啥没意义后面章节会有分析)</span>done := parent.Done()<span class="hljs-keyword">if</span> done == closedchan || done == <span class="hljs-literal">nil</span> &#123;<span class="hljs-keyword">return</span> <span class="hljs-literal">nil</span>, <span class="hljs-literal">false</span>&#125;<span class="hljs-comment">// ...... 省略</span>&#125;</code></pre></div><p>现在来仔细说明一下 “为什么没有意义？” 这个问题:</p><p>首先是调用 <code>parentCancelCtx</code> 方法的位置，在 context 包中只有两个位置调用了 <code>parentCancelCtx</code> 方法；一个是在创建 cancelCtx 的 <code>func WithCancel(parent Context)</code> 的 <code>propagateCancel(parent, &amp;c)</code> 方法中，另一个就是 <code>cancel</code> 方法的 <code>removeChild(c.Context, c)</code> 调用中；下面分析一下这两个方法的目的。</p><h4 id="3-3-1、propagateCancel-parent-amp-c"><a href="#3-3-1、propagateCancel-parent-amp-c" class="headerlink" title="3.3.1、propagateCancel(parent, &amp;c)"></a>3.3.1、propagateCancel(parent, &amp;c)</h4><p><code>propagateCancel</code> 负责保证当 parent cancelCtx 在取消时能正确传递到 child Context；<strong>那么它需要通过 <code>parentCancelCtx</code> 来确定 parent Context 是否是一个 cancelCtx，如果是那就把 child Context 加到 parent Context 的 children map 中，然后 parent Context 在 cancel 时会自动遍历 map 调用 child Context 的 cancel；如果不是那就开 Goroutine 阻塞读 parent Context 的 done channel然后再调用 child Context 的 cancel。</strong></p><div class="hljs code-wrapper"><pre><code class="hljs go"><span class="hljs-keyword">if</span> p, ok := parentCancelCtx(parent); ok &#123;    p.mu.Lock()    <span class="hljs-keyword">if</span> p.err != <span class="hljs-literal">nil</span> &#123;        <span class="hljs-comment">// parent has already been canceled</span>        child.cancel(<span class="hljs-literal">false</span>, p.err)    &#125; <span class="hljs-keyword">else</span> &#123;        <span class="hljs-keyword">if</span> p.children == <span class="hljs-literal">nil</span> &#123;            p.children = <span class="hljs-built_in">make</span>(<span class="hljs-keyword">map</span>[canceler]<span class="hljs-keyword">struct</span>&#123;&#125;)        &#125;        p.children[child] = <span class="hljs-keyword">struct</span>&#123;&#125;&#123;&#125;    &#125;    p.mu.Unlock()&#125; <span class="hljs-keyword">else</span> &#123;    atomic.AddInt32(&amp;goroutines, +<span class="hljs-number">1</span>)    <span class="hljs-keyword">go</span> <span class="hljs-function"><span class="hljs-keyword">func</span><span class="hljs-params">()</span></span> &#123;        <span class="hljs-keyword">select</span> &#123;        <span class="hljs-keyword">case</span> &lt;-parent.Done():            child.cancel(<span class="hljs-literal">false</span>, parent.Err())        <span class="hljs-keyword">case</span> &lt;-child.Done():        &#125;    &#125;()&#125;</code></pre></div><p>所以在这个方法调用时，<strong>如果 <code>parentCancelCtx</code> 取出一个已取消的 cancelCtx，那么 parent Context 的 children map 在 cancel 时已经清空了，这时要是再给设置上就有问题了，同样业务需求中 <code>propagateCancel</code> 为了就是控制传播，明明 parent Context 已经 cancel 了，再去传播就没意义了。</strong></p><h4 id="3-3-2、removeChild-c-Context-c"><a href="#3-3-2、removeChild-c-Context-c" class="headerlink" title="3.3.2、removeChild(c.Context, c)"></a>3.3.2、removeChild(c.Context, c)</h4><p>同上面的 3.3.1 一样，**<code>removeChild(c.Context, c)</code> 目的是在 cancel 时断开与 parent Context 的关联，同样是为了处理 children map 的问题；此时如果 <code>parentCancelCtx</code> 也取出一个已经 cancel 的 parent Context，由于 parent Context 在 cancel 时已经清空了 childre map，这里再尝试 remove 也没有任何意义。**</p><div class="hljs code-wrapper"><pre><code class="hljs go"><span class="hljs-comment">// removeChild removes a context from its parent.</span><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-title">removeChild</span><span class="hljs-params">(parent Context, child canceler)</span></span> &#123;p, ok := parentCancelCtx(parent)<span class="hljs-keyword">if</span> !ok &#123;<span class="hljs-keyword">return</span>&#125;p.mu.Lock()<span class="hljs-keyword">if</span> p.children != <span class="hljs-literal">nil</span> &#123;<span class="hljs-built_in">delete</span>(p.children, child)&#125;p.mu.Unlock()&#125;</code></pre></div><h2 id="四、timerCtx-源码分析"><a href="#四、timerCtx-源码分析" class="headerlink" title="四、timerCtx 源码分析"></a>四、timerCtx 源码分析</h2><h3 id="4-1、timerCtx-是如何创建的"><a href="#4-1、timerCtx-是如何创建的" class="headerlink" title="4.1、timerCtx 是如何创建的"></a>4.1、timerCtx 是如何创建的</h3><p>timerCtx 的创建主要通过 <code>context.WithDeadline</code> 方法，同时 <code>context.WithTimeout</code> 实际上也是调用的 <code>context.WithDeadline</code>:</p><div class="hljs code-wrapper"><pre><code class="hljs go"><span class="hljs-comment">// WithDeadline returns a copy of the parent context with the deadline adjusted</span><span class="hljs-comment">// to be no later than d. If the parent&#x27;s deadline is already earlier than d,</span><span class="hljs-comment">// WithDeadline(parent, d) is semantically equivalent to parent. The returned</span><span class="hljs-comment">// context&#x27;s Done channel is closed when the deadline expires, when the returned</span><span class="hljs-comment">// cancel function is called, or when the parent context&#x27;s Done channel is</span><span class="hljs-comment">// closed, whichever happens first.</span><span class="hljs-comment">//</span><span class="hljs-comment">// Canceling this context releases resources associated with it, so code should</span><span class="hljs-comment">// call cancel as soon as the operations running in this Context complete.</span><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-title">WithDeadline</span><span class="hljs-params">(parent Context, d time.Time)</span> <span class="hljs-params">(Context, CancelFunc)</span></span> &#123;    <span class="hljs-comment">// 与 cancelCtx 一样先检查一下 parent Context</span><span class="hljs-keyword">if</span> parent == <span class="hljs-literal">nil</span> &#123;<span class="hljs-built_in">panic</span>(<span class="hljs-string">&quot;cannot create context from nil parent&quot;</span>)&#125;        <span class="hljs-comment">// 判断 parent Context 是否支持 Deadline，如果支持的话需要判断 parent Context 的截止时间</span>    <span class="hljs-comment">// 假设 parent Context 的截止时间早于当前设置的截止时间，那就意味着 parent Context 肯定会先</span>    <span class="hljs-comment">// 被 cancel，同样由于 parent Context 的 cancel 会导致当前这个 child Context 也会被 cancel</span>    <span class="hljs-comment">// 所以这时候直接返回一个 cancelCtx 就行了，计时器已经没有必要存在了</span><span class="hljs-keyword">if</span> cur, ok := parent.Deadline(); ok &amp;&amp; cur.Before(d) &#123;<span class="hljs-comment">// The current deadline is already sooner than the new one.</span><span class="hljs-keyword">return</span> WithCancel(parent)&#125;    <span class="hljs-comment">// 创建一个 timerCtx</span>c := &amp;timerCtx&#123;cancelCtx: newCancelCtx(parent),deadline:  d,&#125;    <span class="hljs-comment">// 与 cancelCtx 一样的传播操作</span>propagateCancel(parent, c)    <span class="hljs-comment">// 判断当前时间已经已经过了截止日期，如果超过了直接 cancel</span>dur := time.Until(d)<span class="hljs-keyword">if</span> dur &lt;= <span class="hljs-number">0</span> &#123;c.cancel(<span class="hljs-literal">true</span>, DeadlineExceeded) <span class="hljs-comment">// deadline has already passed</span><span class="hljs-keyword">return</span> c, <span class="hljs-function"><span class="hljs-keyword">func</span><span class="hljs-params">()</span></span> &#123; c.cancel(<span class="hljs-literal">false</span>, Canceled) &#125;&#125;    <span class="hljs-comment">// 所有 check 都没问题的情况下，创建一个定时器，在到时间后自动 cancel</span>c.mu.Lock()<span class="hljs-keyword">defer</span> c.mu.Unlock()<span class="hljs-keyword">if</span> c.err == <span class="hljs-literal">nil</span> &#123;c.timer = time.AfterFunc(dur, <span class="hljs-function"><span class="hljs-keyword">func</span><span class="hljs-params">()</span></span> &#123;c.cancel(<span class="hljs-literal">true</span>, DeadlineExceeded)&#125;)&#125;<span class="hljs-keyword">return</span> c, <span class="hljs-function"><span class="hljs-keyword">func</span><span class="hljs-params">()</span></span> &#123; c.cancel(<span class="hljs-literal">true</span>, Canceled) &#125;&#125;</code></pre></div><h3 id="4-2、timerCtx-是如何取消的"><a href="#4-2、timerCtx-是如何取消的" class="headerlink" title="4.2、timerCtx 是如何取消的"></a>4.2、timerCtx 是如何取消的</h3><p>了解了 cancelCtx 的取消流程以后再来看 timerCtx 的取消就相对简单的多，主要就是调用一下里面的 cancelCtx 的 cancel，然后再把定时器停掉:</p><div class="hljs code-wrapper"><pre><code class="hljs go"><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(c *timerCtx)</span> <span class="hljs-title">cancel</span><span class="hljs-params">(removeFromParent <span class="hljs-keyword">bool</span>, err error)</span></span> &#123;c.cancelCtx.cancel(<span class="hljs-literal">false</span>, err)<span class="hljs-keyword">if</span> removeFromParent &#123;<span class="hljs-comment">// Remove this timerCtx from its parent cancelCtx&#x27;s children.</span>removeChild(c.cancelCtx.Context, c)&#125;c.mu.Lock()<span class="hljs-keyword">if</span> c.timer != <span class="hljs-literal">nil</span> &#123;c.timer.Stop()c.timer = <span class="hljs-literal">nil</span>&#125;c.mu.Unlock()&#125;</code></pre></div><h2 id="五、valueCtx-源码分析"><a href="#五、valueCtx-源码分析" class="headerlink" title="五、valueCtx 源码分析"></a>五、valueCtx 源码分析</h2><p>相对于 cancelCtx 还有 timerCtx，valueCtx 实在是过于简单，因为它没有及联的取消逻辑，也没有过于复杂的 kv 存储:</p><div class="hljs code-wrapper"><pre><code class="hljs go"><span class="hljs-comment">// WithValue returns a copy of parent in which the value associated with key is</span><span class="hljs-comment">// val.</span><span class="hljs-comment">//</span><span class="hljs-comment">// Use context Values only for request-scoped data that transits processes and</span><span class="hljs-comment">// APIs, not for passing optional parameters to functions.</span><span class="hljs-comment">//</span><span class="hljs-comment">// The provided key must be comparable and should not be of type</span><span class="hljs-comment">// string or any other built-in type to avoid collisions between</span><span class="hljs-comment">// packages using context. Users of WithValue should define their own</span><span class="hljs-comment">// types for keys. To avoid allocating when assigning to an</span><span class="hljs-comment">// interface&#123;&#125;, context keys often have concrete type</span><span class="hljs-comment">// struct&#123;&#125;. Alternatively, exported context key variables&#x27; static</span><span class="hljs-comment">// type should be a pointer or interface.</span><span class="hljs-comment">// WithValue 方法负责创建 valueCtx</span><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-title">WithValue</span><span class="hljs-params">(parent Context, key, val <span class="hljs-keyword">interface</span>&#123;&#125;)</span> <span class="hljs-title">Context</span></span> &#123;    <span class="hljs-comment">// parent 检测</span><span class="hljs-keyword">if</span> parent == <span class="hljs-literal">nil</span> &#123;<span class="hljs-built_in">panic</span>(<span class="hljs-string">&quot;cannot create context from nil parent&quot;</span>)&#125;    <span class="hljs-comment">// key 检测</span><span class="hljs-keyword">if</span> key == <span class="hljs-literal">nil</span> &#123;<span class="hljs-built_in">panic</span>(<span class="hljs-string">&quot;nil key&quot;</span>)&#125;    <span class="hljs-comment">// key 必须是可比较的</span><span class="hljs-keyword">if</span> !reflectlite.TypeOf(key).Comparable() &#123;<span class="hljs-built_in">panic</span>(<span class="hljs-string">&quot;key is not comparable&quot;</span>)&#125;<span class="hljs-keyword">return</span> &amp;valueCtx&#123;parent, key, val&#125;&#125;<span class="hljs-comment">// A valueCtx carries a key-value pair. It implements Value for that key and</span><span class="hljs-comment">// delegates all other calls to the embedded Context.</span><span class="hljs-keyword">type</span> valueCtx <span class="hljs-keyword">struct</span> &#123;Contextkey, val <span class="hljs-keyword">interface</span>&#123;&#125;&#125;<span class="hljs-comment">// stringify tries a bit to stringify v, without using fmt, since we don&#x27;t</span><span class="hljs-comment">// want context depending on the unicode tables. This is only used by</span><span class="hljs-comment">// *valueCtx.String().</span><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-title">stringify</span><span class="hljs-params">(v <span class="hljs-keyword">interface</span>&#123;&#125;)</span> <span class="hljs-title">string</span></span> &#123;<span class="hljs-keyword">switch</span> s := v.(<span class="hljs-keyword">type</span>) &#123;<span class="hljs-keyword">case</span> stringer:<span class="hljs-keyword">return</span> s.String()<span class="hljs-keyword">case</span> <span class="hljs-keyword">string</span>:<span class="hljs-keyword">return</span> s&#125;<span class="hljs-keyword">return</span> <span class="hljs-string">&quot;&lt;not Stringer&gt;&quot;</span>&#125;<span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(c *valueCtx)</span> <span class="hljs-title">String</span><span class="hljs-params">()</span> <span class="hljs-title">string</span></span> &#123;<span class="hljs-keyword">return</span> contextName(c.Context) + <span class="hljs-string">&quot;.WithValue(type &quot;</span> +reflectlite.TypeOf(c.key).String() +<span class="hljs-string">&quot;, val &quot;</span> + stringify(c.val) + <span class="hljs-string">&quot;)&quot;</span>&#125;<span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(c *valueCtx)</span> <span class="hljs-title">Value</span><span class="hljs-params">(key <span class="hljs-keyword">interface</span>&#123;&#125;)</span> <span class="hljs-title">interface</span></span>&#123;&#125; &#123;    <span class="hljs-comment">// 先判断当前 Context 里有没有这个 key</span><span class="hljs-keyword">if</span> c.key == key &#123;<span class="hljs-keyword">return</span> c.val&#125;    <span class="hljs-comment">// 如果没有递归向上查找</span><span class="hljs-keyword">return</span> c.Context.Value(key)&#125;</code></pre></div><h2 id="六、结尾"><a href="#六、结尾" class="headerlink" title="六、结尾"></a>六、结尾</h2><p>分析 Context 源码断断续续经历了 3、4 天，说心里话发现里面复杂情况有很多，网上其他文章很多都是只提了一嘴，但是没有深入具体逻辑，尤其是 cancelCtx 的相关调用；我甚至觉得我有些地方可能理解的也不完全正确，目前就先写到这里，如果有不对的地方欢迎补充。</p>]]></content>
    
    
    <summary type="html">好久之前就想仔细看看这个 Context，最近稍微有点时间就分析了一手</summary>
    
    
    
    <category term="Golang" scheme="https://mritd.com/categories/golang/"/>
    
    
    <category term="Golang" scheme="https://mritd.com/tags/golang/"/>
    
    <category term="Context" scheme="https://mritd.com/tags/context/"/>
    
  </entry>
  
  <entry>
    <title>Caddy2 的 ListenerWrapper</title>
    <link href="https://mritd.com/2021/06/15/caddy2-listenerwrapper/"/>
    <id>https://mritd.com/2021/06/15/caddy2-listenerwrapper/</id>
    <published>2021-06-15T06:04:00.000Z</published>
    <updated>2021-06-15T06:04:00.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>本文所有源码分析基于 Caddy2 v2.4.2 版本进行，未来版本可能源码会有变化，阅读本文时请自行将源码切换到 v2.4.2 版本。</p></blockquote><h2 id="一、这玩意是什么？"><a href="#一、这玩意是什么？" class="headerlink" title="一、这玩意是什么？"></a>一、这玩意是什么？</h2><p>Caddy2 对配置文件中的 <code>listener_wrappers</code> 配置有以下描述:</p><blockquote><p>Allows configuring listener wrappers, which can modify the behaviour of the base listener. They are applied in the given order.</p></blockquote><p>同时对于 <code>tls</code> 这个 <code>listener_wrappers</code> 还做了一下说明:</p><blockquote><p>There is a special no-op tls listener wrapper provided as a standard module which marks where TLS should be handled in the chain of listener wrappers. It should only be used if another listener wrapper must be placed in front of the TLS handshake.</p></blockquote><p>综上所述，简单的理解就是 <code>listener_wrappers</code> 在 Caddy2 中用于改变链接行为，这个行为可以理解为我们可以自定义接管链接，这些 “接管” 更偏向于底层，比如在 TLS 握手之前做点事情或者在 TLS 握手之后做点事情，这样我们就可以实现一些魔法操作。</p><h2 id="二、加载与初始化"><a href="#二、加载与初始化" class="headerlink" title="二、加载与初始化"></a>二、加载与初始化</h2><p>在 Caddy2 启动时首先会进行配置文件解析，例如解析 Caddyfile、json 等格式的配置文件，<code>listener_wrappers</code> 在配置文件中被定义为一个 <code>ServerOption</code>:</p><p><strong><a href="https://github.com/caddyserver/caddy/blob/v2.4.2/caddyconfig/httpcaddyfile/serveroptions.go#L47">caddyconfig/httpcaddyfile/serveroptions.go:47</a></strong></p><p><img src="https://cdn.oss.link/markdown/s7XueV.jpg"></p><p>该配置最终会被注入到 Server 的 listenerWrappers 属性中(先解析为 <code>ListenerWrappersRaw</code> 然后再实例化)</p><p><strong><a href="https://github.com/caddyserver/caddy/blob/v2.4.2/modules/caddyhttp/server.go#L132">modules/caddyhttp/server.go:132</a></strong> </p><p><img src="https://cdn.oss.link/markdown/vs5icq.jpg"></p><p>最后在 App 的启动过程中遍历 listenerWrappers 并逐个应用，在应用 <code>listenerWrappers</code> 时有个比较重要的顺序处理:</p><p><strong>首先 Caddy2 会尝试在 <code>net.Listener</code> 上应用一部分 <code>listenerWrappers</code>，当触及到 <code>tls</code> 这个 token 的 <code>listenerWrappers</code> 之后终止应用；终止前已被应用的这部分 <code>listenerWrappers</code> 被认为是 TLS 握手之前的自定义处理，然后在 TLS 握手之后再次应用剩下的 <code>listenerWrappers</code>，后面这部分被认为是 TLS 握手之后的自定义处理。</strong></p><p><strong><a href="https://github.com/caddyserver/caddy/blob/v2.4.2/modules/caddyhttp/app.go#L318">modules/caddyhttp/app.go:318</a></strong><br><img src="https://cdn.oss.link/markdown/6KQipF.jpg"></p><p>最终对 ListenerWrapper 加载流程分析如下:</p><ul><li>首先解析配置文件，并将配置转化为 Server 的 <code>ListenerWrappersRaw []json.RawMessage</code></li><li>然后通过 <code>ctx.LoadModule(srv, &quot;ListenerWrappersRaw&quot;)</code> 实例化 ListenerWrapper</li><li>在 <code>ctx.LoadModule</code> 时，如果发现了 <code>tls</code> 指令则按照配置文件顺序排序 ListenerWrapper 切片，否则将 <code>tls</code> 这个特殊的 ListenerWrapper 放在首位；<strong>这意味着在配置中不写 <code>tls</code> 时，所有 ListenerWrapper 永远处于 TLS 握手之后</strong></li><li>最后在 App 启动时按照切片顺序应用 ListenerWrapper，需要注意的是 ListenerWrapper 接口针对的是 <code>net.Listener</code> 的处理，其底层是 <code>net.Conn</code>；<strong>这意味着 ListenerWrapper 不会对 UDP(<code>net.PacketConn</code>) 做处理，代码中也可以看到 ListenerWrapper 并未对 HTTP3 处理</strong></li></ul><h2 id="三、具体实际应用"><a href="#三、具体实际应用" class="headerlink" title="三、具体实际应用"></a>三、具体实际应用</h2><p>说了半天，也分析了源码，那么最终回到问题原点: <strong>ListenerWrapper 能干什么？</strong>答案就是自定义协议，例如神奇的 <a href="https://github.com/imgk/caddy-trojan">caddy-trojan</a> 插件。</p><p>caddy-trojan 插件实现了 ListenerWrapper，在 App 启动时通过源码可以看到，<strong>TLS 握手完成后原始的 TCP 链接将交由这个 ListenerWrapper 处理:</strong></p><div class="hljs code-wrapper"><pre><code class="hljs go"><span class="hljs-comment">// finish wrapping listener where we left off before TLS</span><span class="hljs-keyword">for</span> i := lnWrapperIdx; i &lt; <span class="hljs-built_in">len</span>(srv.listenerWrappers); i++ &#123;ln = srv.listenerWrappers[i].WrapListener(ln)&#125;</code></pre></div><p>该插件对 <code>WrapListener</code> 方法的实现如下:</p><div class="hljs code-wrapper"><pre><code class="hljs go"><span class="hljs-comment">// WrapListener implements caddy.ListenWrapper</span><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(m *ListenerWrapper)</span> <span class="hljs-title">WrapListener</span><span class="hljs-params">(l net.Listener)</span> <span class="hljs-title">net</span>.<span class="hljs-title">Listener</span></span> &#123;ln := NewListener(l, m.upstream, m.logger)<span class="hljs-comment">// 异步后台捕获新链接</span><span class="hljs-keyword">go</span> ln.loop()<span class="hljs-keyword">return</span> ln&#125;</code></pre></div><p>所以这个 wrapper 核心处理在 <code>loop()</code> 中:</p><div class="hljs code-wrapper"><pre><code class="hljs go"><span class="hljs-comment">// loop is ...</span><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(l *Listener)</span> <span class="hljs-title">loop</span><span class="hljs-params">()</span></span> &#123;<span class="hljs-keyword">for</span> &#123;conn, err := l.Listener.Accept()<span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;<span class="hljs-keyword">select</span> &#123;<span class="hljs-keyword">case</span> &lt;-l.closed:<span class="hljs-keyword">return</span><span class="hljs-keyword">default</span>:l.logger.Error(fmt.Sprintf(<span class="hljs-string">&quot;accept net.Conn error: %v&quot;</span>, err))&#125;<span class="hljs-keyword">continue</span>&#125;<span class="hljs-keyword">go</span> <span class="hljs-function"><span class="hljs-keyword">func</span><span class="hljs-params">(c net.Conn, lg *zap.Logger, up *Upstream)</span></span> &#123;b := <span class="hljs-built_in">make</span>([]<span class="hljs-keyword">byte</span>, HeaderLen+<span class="hljs-number">2</span>)<span class="hljs-keyword">if</span> _, err := io.ReadFull(c, b); err != <span class="hljs-literal">nil</span> &#123;<span class="hljs-keyword">if</span> errors.Is(err, io.EOF) &#123;lg.Error(fmt.Sprintf(<span class="hljs-string">&quot;read prefix error: read tcp %v -&gt; %v: read: %v&quot;</span>, c.RemoteAddr(), c.LocalAddr(), err))&#125; <span class="hljs-keyword">else</span> &#123;lg.Error(fmt.Sprintf(<span class="hljs-string">&quot;read prefix error: %v&quot;</span>, err))&#125;c.Close()<span class="hljs-keyword">return</span>&#125;<span class="hljs-comment">// check the net.Conn</span><span class="hljs-keyword">if</span> ok := up.Validate(ByteSliceToString(b[:HeaderLen])); !ok &#123;<span class="hljs-keyword">select</span> &#123;<span class="hljs-keyword">case</span> &lt;-l.closed:c.Close()<span class="hljs-keyword">default</span>:l.conns &lt;- &amp;rawConn&#123;Conn: c, r: bytes.NewReader(b)&#125;&#125;<span class="hljs-keyword">return</span>&#125;<span class="hljs-keyword">defer</span> c.Close()lg.Info(fmt.Sprintf(<span class="hljs-string">&quot;handle trojan net.Conn from %v&quot;</span>, c.RemoteAddr()))nr, nw, err := Handle(io.Reader(c), io.Writer(c))<span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;lg.Error(fmt.Sprintf(<span class="hljs-string">&quot;handle net.Conn error: %v&quot;</span>, err))&#125;up.Consume(ByteSliceToString(b[:HeaderLen]), nr, nw)&#125;(conn, l.logger, l.upstream)&#125;&#125;</code></pre></div><p><strong>可以看到，当新链接进入时，首先对包头做检测 <code>if ok := up.Validate(ByteSliceToString(b[:HeaderLen]))</code>；如果检测通过那么这个链接就完全插件自己处理后续逻辑了；如果不通过则将此链接返回给 Caddy2，让 Caddy2 继续处理。</strong></p><p>这里面涉及到一个一开始让我不解的问题: “链接不可重复读”，后来看源码才明白作者处理方式很简单: <strong>包装一个 <code>rawConn</code>，在验证部分由于已经读了一点数据，如果验证不通过就把它存起来，然后让下一个读操作先读这个 buffer，从而实现原始数据组装。</strong></p><div class="hljs code-wrapper"><pre><code class="hljs go"><span class="hljs-comment">// rawConn is ...</span><span class="hljs-keyword">type</span> rawConn <span class="hljs-keyword">struct</span> &#123;net.Connr *bytes.Reader&#125;<span class="hljs-comment">// Read is ...</span><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(c *rawConn)</span> <span class="hljs-title">Read</span><span class="hljs-params">(b []<span class="hljs-keyword">byte</span>)</span> <span class="hljs-params">(<span class="hljs-keyword">int</span>, error)</span></span> &#123;<span class="hljs-keyword">if</span> c.r == <span class="hljs-literal">nil</span> &#123;<span class="hljs-keyword">return</span> c.Conn.Read(b)&#125;n, err := c.r.Read(b)<span class="hljs-keyword">if</span> errors.Is(err, io.EOF) &#123;c.r = <span class="hljs-literal">nil</span><span class="hljs-keyword">return</span> n, <span class="hljs-literal">nil</span>&#125;<span class="hljs-keyword">return</span> n, err&#125;</code></pre></div><h2 id="四、思考和总结"><a href="#四、思考和总结" class="headerlink" title="四、思考和总结"></a>四、思考和总结</h2><p>ListenerWrapper 是 Caddy2 一个强大的扩展能力，在 ListenerWrapper 基础上我们可以实现对 TCP 链接自定义处理，我们因此可以创造一些奇奇怪怪的协议。同时我们通过让链接重新交由 Caddy2 处理又能做到完美的伪装: <strong>当你去尝试访问时，如果密码学验证不通过，那么后续行为就与标准 Caddy2 表现一致，主动探测基本无效。对任何自己创造的 ListenerWrapper 来说，如果开启了类似 AEAD 这种加密，探测行为本身就会被转接到对抗密码学原理上。</strong></p>]]></content>
    
    
    <summary type="html">最近分析了一下 Caddy2 的 ListenerWrapper，觉得很厉害索性写下来</summary>
    
    
    
    <category term="Golang" scheme="https://mritd.com/categories/golang/"/>
    
    <category term="Caddy" scheme="https://mritd.com/categories/golang/caddy/"/>
    
    
    <category term="Caddy" scheme="https://mritd.com/tags/caddy/"/>
    
  </entry>
  
  <entry>
    <title>JetBrains 常用插件</title>
    <link href="https://mritd.com/2021/06/06/jetbrains-plugins/"/>
    <id>https://mritd.com/2021/06/06/jetbrains-plugins/</id>
    <published>2021-06-06T05:23:00.000Z</published>
    <updated>2021-06-06T05:23:00.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>所谓工欲善其事，必先利其器；这篇文章分享一些日常 Coding 中常用 JetBrains 系列 IDE 插件(本文所有插件可直接从 Marketplace 搜索并安装)。</p></blockquote><h2 id="One-Dark-theme"><a href="#One-Dark-theme" class="headerlink" title="One Dark theme"></a>One Dark theme</h2><p>上来先整点没用的吧，主题配色这个东西根据个人喜好；我比较喜欢花花绿绿的感觉，在防止眼疲劳的同时还有点 RMB 的感觉(RMB 也花花绿绿的)，毕竟 Coding 的时候要 “酷一点”，然后才能心情愉悦的写 BUG(代码和我只要有一个能跑就行)。</p><p><img src="https://cdn.oss.link/markdown/kikZwf.jpg"></p><h2 id="Gopher"><a href="#Gopher" class="headerlink" title="Gopher"></a>Gopher</h2><p>也是啥用没有的插件，唯一的效果就是在各种 Loading 的时候进度条变成了 Go 的吉祥物(Golang 天下第一，嘶吼)…当然还有个同款，Gopher 变成了彩虹猫，需要的自己搜吧。</p><p><img src="https://cdn.oss.link/markdown/uVYpSQ.jpg"></p><h2 id="Extra-Icons"><a href="#Extra-Icons" class="headerlink" title="Extra Icons"></a>Extra Icons</h2><p>这个插件为项目里一些特殊文件增加 Icon，比如 <code>.gitignore</code> 文件、CI 配置等，可以让人看着更舒服一些。</p><p><img src="https://cdn.oss.link/markdown/RQek39.jpg"></p><h2 id="GitToolBox"><a href="#GitToolBox" class="headerlink" title="GitToolBox"></a>GitToolBox</h2><p>GitToolBox 会在光标定位到某一行代码时显示其最近改动等提交信息，方便在甩锅时精确定位到背锅侠。</p><p><img src="https://cdn.oss.link/markdown/zyLD6C.png"></p><h2 id="String-Manipulation"><a href="#String-Manipulation" class="headerlink" title="String Manipulation"></a>String Manipulation</h2><p>这个就牛逼了，非常有实用价值的一个插件；当某些规范下你必须进行 “驼峰/下划线/短横线/全大写/全小写/大写加下划线/小写加下划线…” 疯狂转换时，String Manipulation 只需要选中右键即可完成。</p><p><img src="https://cdn.oss.link/markdown/GBB7KC.gif"></p><h2 id="Randomness"><a href="#Randomness" class="headerlink" title="Randomness"></a>Randomness</h2><p>Randomness 在需要测试数据时很有用，它可以快速生成一些随机性的垃圾数据填充进来；例如写示例配置时。</p><p><img src="https://cdn.oss.link/markdown/Cj9bsm.png"></p><h2 id="Translation"><a href="#Translation" class="headerlink" title="Translation"></a>Translation</h2><p>英文渣必备插件，除了能翻译一些单词之外，还能自动识别文档进行翻译，甚至还带单词本。</p><p><img src="https://cdn.oss.link/markdown/psZGNv.png"></p><h2 id="carbon-now-sh"><a href="#carbon-now-sh" class="headerlink" title="carbon-now-sh"></a>carbon-now-sh</h2><p>当你想把你的代码发到某个群里装X，或者写博客不想让别人复制只想展示时，carbon-now-sh 可以帮你把选中的代码传输到 <a href="https://carbon.now.sh/">https://carbon.now.sh/</a> 来生成漂亮的图片。</p><p><img src="https://cdn.oss.link/markdown/qXDcrg.png"></p>]]></content>
    
    
    <summary type="html">分享一些好玩的和有用的 JetBrains 系列 IDE 插件</summary>
    
    
    
    <category term="Golang" scheme="https://mritd.com/categories/golang/"/>
    
    
    <category term="JetBrains" scheme="https://mritd.com/tags/jetbrains/"/>
    
  </entry>
  
  <entry>
    <title>寥寥浮生，莫问前程</title>
    <link href="https://mritd.com/2021/06/04/liao-liao-fu-sheng-mo-wen-qian-cheng/"/>
    <id>https://mritd.com/2021/06/04/liao-liao-fu-sheng-mo-wen-qian-cheng/</id>
    <published>2021-06-04T13:41:00.000Z</published>
    <updated>2021-06-04T13:41:00.000Z</updated>
    
    <content type="html"><![CDATA[<audio  autoplay="autoplay">  <source src="https://cdn.oss.link/bgm/night-piano9.mp3" type="audio/mpeg" loop="loop" />Your browser does not support the audio element.</audio><p><img src="https://cdn.oss.link/markdown/pHOIRa.png"></p><center>月下映双鱼，白首雁归去，自古无愁难成句。    --- 双鱼</center><h2 id="给岁月以青春，而不是给青春以岁月"><a href="#给岁月以青春，而不是给青春以岁月" class="headerlink" title="给岁月以青春，而不是给青春以岁月"></a>给岁月以青春，而不是给青春以岁月</h2><blockquote><p>给时光以生命，而不是给生命以时光。</p></blockquote><p>每当谈起岁月，总显得是那么残忍，残忍到能真切的感受到 “岁月如刀”，然后一刀又一刀；今天莫名其妙的看了一下我的生日(是的，已经忘记了年龄)，然后得到了一个很有意思的答案: </p><p><strong>在这个世界上生活了 9684 天，生命进度条以 80 岁算正好 <code>33%</code>。</strong></p><p>所以我的青春已经开始向我挥手告别，或许有些艰难，或许有点感慨，但终究都已不再重要。我庆幸的是我的青春不会像大多数人一样无趣，虽然充满血腥却显得美丽；但同样也会留下许多遗憾，缺了孩子气的天真，没体会过关心，没了热血冲动的经历。</p><p>以前每当回想起来的时候，总是带着那么一点悲伤；今天突然想起一句话: “给时光以生命，而不是给生命以时光。”，<strong>所以当我们回首青春，应当给岁月以青春，而不是给青春以岁月；这段青春，感谢你砥砺前行，让岁月不在苦涩。</strong></p><h2 id="勇敢的质疑，勇敢的被质疑"><a href="#勇敢的质疑，勇敢的被质疑" class="headerlink" title="勇敢的质疑，勇敢的被质疑"></a>勇敢的质疑，勇敢的被质疑</h2><blockquote><p>世上没有绝对的真理，这就是我要对你们说的话。</p></blockquote><p>质疑事物的能力，看似简单，我却缺少了太多；父母曾经无数次对我说过 “你应该怎样怎样…”，小到吃饭喝水，大到人生规划，最后事实证明基本没有对的；有些看似亘古不变的东西让我忘却了它存在的意义，以至于我选择了随波逐流，却从未质疑过一个基本的对与错。</p><p>同样，这个世界不只有黑与白，还有很多灰。别人的对我质疑是否真正重要？这是否是自己放弃的理由？…让质疑我的人去质疑吧，这是他的权利，但我活不成他的样子。</p><p><strong>这世界上从来就没有绝对的真理，浩瀚星河里我太过渺小，而命运又太过伟大；我抱着敬畏之心质疑一切，也接受这个世界对我的质疑。</strong></p><h2 id="对未知的结论永远应该是未知"><a href="#对未知的结论永远应该是未知" class="headerlink" title="对未知的结论永远应该是未知"></a>对未知的结论永远应该是未知</h2><blockquote><p>我们用眼睛观察，而不全然接受。</p></blockquote><p>我曾不止一次对只是看到表象的事物作出定论，以为笑容背后就是开心，以为乌云密布就会下雨。我把太多的归因算到我所看到或是模糊看到的东西上，当 “墨菲定律” 和 “幸存者偏差” 双重作用时，悲观的人会更悲观，快乐的人会更快乐。</p><p>现在人到中年，终于学会了放弃；<strong>我所看到的不一定是真的，我没看清的也不一定是假的，对待未知的结论永远应该是未知，所以赌不赌看自己，莫要让整个世界买单。</strong></p><h2 id="成功无法复制，经验或许没用"><a href="#成功无法复制，经验或许没用" class="headerlink" title="成功无法复制，经验或许没用"></a>成功无法复制，经验或许没用</h2><blockquote><p>世界上没有完全相同的两片树叶。</p></blockquote><p>太多的人喜欢对别人讲述 “自己的经验”，一件事怎么做才能成功，甚至认为自己是 “苦口婆心”。我曾经也做过类似的傻事，直到前些年遇到了一个出租车司机；</p><p>他以前是一个搞自动化测试的，几年前跟我一样一个人来到北京打拼，我们有着差不多相似的经历，但结果却并不相同也不相似；直到现在我还记得那一段路上他所诉说的艰苦和努力，还有命运的不公。</p><p>我那时才意识到: <strong>成功根本无法复制，经验也或许没用，别人的一生之所以那样只是命运垂青，或是开了个玩笑。怎么做事，我无法说服别人，也没人可以教导我。</strong></p><h2 id="对抗随机性等于向既定轨道回归"><a href="#对抗随机性等于向既定轨道回归" class="headerlink" title="对抗随机性等于向既定轨道回归"></a>对抗随机性等于向既定轨道回归</h2><blockquote><p>已经在开往地狱的路上，那就应该和魔鬼一起笑着去。</p></blockquote><p>这是前两天在 Twitter 上看到老刘(@Yachen Liu)发的感悟:</p><blockquote><p>对于多数人，大概在初中至大学阶段，思维方式、思维能力、性格等终身基本不变的特征就已经定型了，与此对应的人生未来轨道也已经基本确定，之后的时间不过是不断的对抗随机性向既定轨道回归。</p></blockquote><p>我很认同这句话，环境会改变人的，很多性格、习惯其实都是那个懵懵懂懂的年纪养成的；所以现在开始不去尝试对抗随机性，给自己一点机会，<strong>我期望我短暂的生命中会有些其他的东西，因为我不想回到以前的轨道上。</strong></p><h2 id="一切达观，都是对悲苦的省略"><a href="#一切达观，都是对悲苦的省略" class="headerlink" title="一切达观，都是对悲苦的省略"></a>一切达观，都是对悲苦的省略</h2><p>写到最后，以大雪将至里的话结尾吧:</p><blockquote><p>和所有的人一样，在他的一生里，也曾怀有过自己的想象和梦想，其中的一些是他自己实现的，有一些是命运赠予给他的，很多是从来都无法实现的，或者是刚刚得到，就又被从手中掠夺走的。但是他一直还活着。</p><p>他想不起来，他是从哪儿来的，最终他也不知道，他将要去向何方。但是，这生来死去之间的时光，他的一生，他可以不含遗憾地去回看，用一个戛然而止的微笑，然后就只是巨大的惊讶。</p></blockquote><p>BGM 取自 “<a href="https://www.youtube.com/watch?v=NRh3ubYJmw4">石进 - 夜的钢琴曲</a>“ 系列第九曲。</p>]]></content>
    
    
    <summary type="html">月下映双鱼，白首雁归去，自古无愁难成句。 --- 双鱼</summary>
    
    
    
    <category term="随笔" scheme="https://mritd.com/categories/%E9%9A%8F%E7%AC%94/"/>
    
    
    <category term="浮生" scheme="https://mritd.com/tags/%E6%B5%AE%E7%94%9F/"/>
    
  </entry>
  
  <entry>
    <title>搓一个 Telegram Bot</title>
    <link href="https://mritd.com/2021/06/03/make-a-telegram-bot/"/>
    <id>https://mritd.com/2021/06/03/make-a-telegram-bot/</id>
    <published>2021-06-03T04:20:00.000Z</published>
    <updated>2021-06-03T04:20:00.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>在一个月光如雪的晚上和 PMheart 在 Telegram 闲聊，突然发现群里一个人的昵称是当前时间，然后观察一会儿发现还在不停变化… 最可气的是他还弄个 “东半球🌏最准报时” 的头衔，我一开始以为是 Telegram 又出的什么 “高级功能”(毕竟微信炸💩都是 Telegram 好久之前玩剩下的)，几经 Google 我发现其实就是自己写个 Bot，然后定时 rename；<strong>那我要不整个 “东半球最浪漫诗人” 岂不是太面了🤔。</strong></p></blockquote><h2 id="一、Telegram-Bot-介绍"><a href="#一、Telegram-Bot-介绍" class="headerlink" title="一、Telegram Bot 介绍"></a>一、Telegram Bot 介绍</h2><p>在 Telegram 官方的文档描述中，其 Bot Api 实质上分为两种，这两种 Api 用途也各不相同:</p><h3 id="1-1、标准-Bot"><a href="#1-1、标准-Bot" class="headerlink" title="1.1、标准 Bot"></a>1.1、标准 Bot</h3><p>由用户自行联系 <code>BotFather</code> (人如其名)交互式创建，该 Bot 是官方所认为的标准 Bot，其主要目的就是作为一个真正的 Bot；我们可以通过一个 Token 调用 Telegram Api 来控制它，玩法很多，包括不限于发送告警、作为群管机器人、交互式的帮你做各种自动化等等；同时这个 Bot 具有严格的隐私权限控制，比如拉到群里可以控制 Bot 对群消息是否可见等等(Telegram 这点做的非常 OJBK)；借助于这类 Bot，也有些脑洞大开的大哥在浪的边缘疯狂试探，比如下面的扫雷机器人:</p><p><img src="https://cdn.oss.link/markdown/I3IcQn.png" alt="I3IcQn"></p><p>还有算命的:<br><img src="https://cdn.oss.link/markdown/pIZYSS.png" alt="pIZYSS"></p><p>Github 真正干活的:<br><img src="https://cdn.oss.link/markdown/9wPxeI.png" alt="9wPxeI"></p><p>我的证书告警:<br><img src="https://cdn.oss.link/markdown/HAvQny.png" alt="HAvQny"></p><p>当然肯定有高铁动车组的(🔞我是正经人):<br><img src="https://cdn.oss.link/markdown/1zmI2m.png" alt="1zmI2m"></p><h3 id="1-2、客户端-Bot"><a href="#1-2、客户端-Bot" class="headerlink" title="1.2、客户端 Bot"></a>1.2、客户端 Bot</h3><p>准确的官方介绍是 <code>TDLib – build your own Telegram</code>，从这个介绍可以看出，这一个 “Bot” Api 本质上并不是让你写 Bot，而是作为开发一个第三方 Telegram 客户端用的；所以这个 Api 的权限很大，可以完整的模拟一个用户；目前我发现被滥用最多的就是用这个 Api 作为恶意拉人、发广告等，简直是币圈割韭菜御用。</p><p><strong>TDLib 本质上是一个 C++ 的 lib，官方提供了引导页面来帮助你用主流语言跨语言调用来使用它:</strong></p><p><img src="https://cdn.oss.link/markdown/yTAC3k.png" alt="yTAC3k"></p><h2 id="二、标准-Bot-使用"><a href="#二、标准-Bot-使用" class="headerlink" title="二、标准 Bot 使用"></a>二、标准 Bot 使用</h2><p>标准 Bot 使用相对简单，按照官方文档跟 <code>BotFather</code> 聊天创建一个即可:</p><p><img src="https://cdn.oss.link/markdown/gPhI8O.png" alt="gPhI8O"></p><p>当创建完成后在 Bot 设置界面你可以获取一个 <code>Token</code>，使用这个 Token 连接 Bot Api 地址就可以开始控制你的 Bot；Golang 开发可以考虑使用 <a href="https://github.com/tucnak/telebot">https://github.com/tucnak/telebot</a> 这个库，用法相当简单:</p><div class="hljs code-wrapper"><pre><code class="hljs go"><span class="hljs-keyword">package</span> main<span class="hljs-keyword">import</span> (<span class="hljs-string">&quot;log&quot;</span><span class="hljs-string">&quot;time&quot;</span>tb <span class="hljs-string">&quot;gopkg.in/tucnak/telebot.v2&quot;</span>)<span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-title">main</span><span class="hljs-params">()</span></span> &#123;b, err := tb.NewBot(tb.Settings&#123;<span class="hljs-comment">// You can also set custom API URL.</span><span class="hljs-comment">// If field is empty it equals to &quot;https://api.telegram.org&quot;.</span>URL: <span class="hljs-string">&quot;http://195.129.111.17:8012&quot;</span>,Token:  <span class="hljs-string">&quot;TOKEN_HERE&quot;</span>,Poller: &amp;tb.LongPoller&#123;Timeout: <span class="hljs-number">10</span> * time.Second&#125;,&#125;)<span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;log.Fatal(err)<span class="hljs-keyword">return</span>&#125;b.Handle(<span class="hljs-string">&quot;/hello&quot;</span>, <span class="hljs-function"><span class="hljs-keyword">func</span><span class="hljs-params">(m *tb.Message)</span></span> &#123;b.Send(m.Sender, <span class="hljs-string">&quot;Hello World!&quot;</span>)&#125;)b.Start()&#125;</code></pre></div><p>基于这个库，我为了方便使用写了一个命令行小工具，方便我发送告警信息等: <a href="https://github.com/mritd/tgsend">tgsend</a></p><p><img src="https://cdn.oss.link/markdown/BCSMIV.png" alt="BCSMIV"></p><p><img src="https://cdn.oss.link/markdown/warOjJ.png" alt="warOjJ"></p><h2 id="三、客户端-Api-使用"><a href="#三、客户端-Api-使用" class="headerlink" title="三、客户端 Api 使用"></a>三、客户端 Api 使用</h2><p>标准 Bot Api 很丰富，日常干活啥的也完全能满足，但是！<strong>人如果不会装X那和咸鱼有什么区别？</strong>我的 “东半球最浪漫诗人” 得提上日程。</p><h3 id="3-1、TDLib-构建"><a href="#3-1、TDLib-构建" class="headerlink" title="3.1、TDLib 构建"></a>3.1、TDLib 构建</h3><p>关于 Api 易用性，开发生态环境，这一点说实话，Telegram 能把所有国内 IM 按在地上摩擦，就像这样:</p><p><img src="https://cdn.oss.link/markdown/iI1jz8.jpg" alt="iI1jz8"></p><p>Telegram 官方提供了完整的 “点一点” 构建 TDLib 引导页面: <a href="https://tdlib.github.io/td/build.html">https://tdlib.github.io/td/build.html</a></p><p>勾选好自己的语言、操作系统、系统版本、甚至是编译的内存大小等设置后，无脑复制下面的命令执行就行:</p><p><img src="https://cdn.oss.link/markdown/DnZdvD.png" alt="DnZdvD"></p><h3 id="3-2、Telegram-API-HASH"><a href="#3-2、Telegram-API-HASH" class="headerlink" title="3.2、Telegram API HASH"></a>3.2、Telegram API HASH</h3><p>TDLib 构建完成后，需要自行申请一个 API_HASH，API_HASH 类似一个让 Telegram 识别你的客户端的 “合法标识”；API_HASH 申请需要登陆 <a href="https://my.telegram.org/">https://my.telegram.org/</a>，然后选择 <strong>API development tools</strong>:</p><p><img src="https://cdn.oss.link/markdown/qp2BjK.png" alt="qp2BjK"></p><p>然后填写相关信息，最后 Telegram 就会为你生成好 API_HASH:</p><p><img src="https://cdn.oss.link/markdown/QGoh8Q.png" alt="QGoh8Q"></p><h3 id="3-3、TDLib-使用"><a href="#3-3、TDLib-使用" class="headerlink" title="3.3、TDLib 使用"></a>3.3、TDLib 使用</h3><p>TDLib 构建好了，API HASH 也有了，那么根据自己选择的语言找一个靠谱的 SDK 使用即可；比如 Golang 开发，我选择了 <a href="https://github.com/Arman92/go-tdlib">https://github.com/Arman92/go-tdlib</a>，这个库使用相当简单:</p><div class="hljs code-wrapper"><pre><code class="hljs go"><span class="hljs-keyword">package</span> main<span class="hljs-keyword">import</span> (<span class="hljs-string">&quot;fmt&quot;</span><span class="hljs-string">&quot;github.com/Arman92/go-tdlib&quot;</span>)<span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-title">main</span><span class="hljs-params">()</span></span> &#123;tdlib.SetLogVerbosityLevel(<span class="hljs-number">1</span>)tdlib.SetFilePath(<span class="hljs-string">&quot;./errors.txt&quot;</span>)<span class="hljs-comment">// Create new instance of client</span>client := tdlib.NewClient(tdlib.Config&#123;APIID:               <span class="hljs-string">&quot;187786&quot;</span>,APIHash:             <span class="hljs-string">&quot;e782045df67ba48e441ccb105da8fc85&quot;</span>,SystemLanguageCode:  <span class="hljs-string">&quot;en&quot;</span>,DeviceModel:         <span class="hljs-string">&quot;Server&quot;</span>,SystemVersion:       <span class="hljs-string">&quot;1.0.0&quot;</span>,ApplicationVersion:  <span class="hljs-string">&quot;1.0.0&quot;</span>,UseMessageDatabase:  <span class="hljs-literal">true</span>,UseFileDatabase:     <span class="hljs-literal">true</span>,UseChatInfoDatabase: <span class="hljs-literal">true</span>,UseTestDataCenter:   <span class="hljs-literal">false</span>,DatabaseDirectory:   <span class="hljs-string">&quot;./tdlib-db&quot;</span>,FileDirectory:       <span class="hljs-string">&quot;./tdlib-files&quot;</span>,IgnoreFileNames:     <span class="hljs-literal">false</span>,&#125;)<span class="hljs-keyword">for</span> &#123;currentState, _ := client.Authorize()<span class="hljs-keyword">if</span> currentState.GetAuthorizationStateEnum() == tdlib.AuthorizationStateWaitPhoneNumberType &#123;fmt.Print(<span class="hljs-string">&quot;Enter phone: &quot;</span>)<span class="hljs-keyword">var</span> number <span class="hljs-keyword">string</span>fmt.Scanln(&amp;number)_, err := client.SendPhoneNumber(number)<span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;fmt.Printf(<span class="hljs-string">&quot;Error sending phone number: %v&quot;</span>, err)&#125;&#125; <span class="hljs-keyword">else</span> <span class="hljs-keyword">if</span> currentState.GetAuthorizationStateEnum() == tdlib.AuthorizationStateWaitCodeType &#123;fmt.Print(<span class="hljs-string">&quot;Enter code: &quot;</span>)<span class="hljs-keyword">var</span> code <span class="hljs-keyword">string</span>fmt.Scanln(&amp;code)_, err := client.SendAuthCode(code)<span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;fmt.Printf(<span class="hljs-string">&quot;Error sending auth code : %v&quot;</span>, err)&#125;&#125; <span class="hljs-keyword">else</span> <span class="hljs-keyword">if</span> currentState.GetAuthorizationStateEnum() == tdlib.AuthorizationStateWaitPasswordType &#123;fmt.Print(<span class="hljs-string">&quot;Enter Password: &quot;</span>)<span class="hljs-keyword">var</span> password <span class="hljs-keyword">string</span>fmt.Scanln(&amp;password)_, err := client.SendAuthPassword(password)<span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;fmt.Printf(<span class="hljs-string">&quot;Error sending auth password: %v&quot;</span>, err)&#125;&#125; <span class="hljs-keyword">else</span> <span class="hljs-keyword">if</span> currentState.GetAuthorizationStateEnum() == tdlib.AuthorizationStateReadyType &#123;fmt.Println(<span class="hljs-string">&quot;Authorization Ready! Let&#x27;s rock&quot;</span>)<span class="hljs-keyword">break</span>&#125;&#125;<span class="hljs-comment">// Main loop</span><span class="hljs-keyword">for</span> update := <span class="hljs-keyword">range</span> client.RawUpdates &#123;<span class="hljs-comment">// Show all updates</span>fmt.Println(update.Data)fmt.Print(<span class="hljs-string">&quot;\n\n&quot;</span>)&#125;&#125;</code></pre></div><p>由于 Telegram 是允许多客户端登陆的(<strong>跟我一起喊:微信垃圾、张小龙垃圾</strong>)，所以使用 TDLib 我们可以完全控制我们的账户行为；那么 “东半球最浪漫诗人” 实现就相对简单:</p><ul><li>自己爬一点古诗名句: <a href="https://github.com/mritd/poetbot/blob/master/poet.txt">https://github.com/mritd/poetbot/blob/master/poet.txt</a></li><li>写点代码定时 rename: <a href="https://github.com/mritd/poetbot/blob/master/main.go#L165">https://github.com/mritd/poetbot/blob/master/main.go#L165</a></li></ul><p>核心代码就这几行:</p><div class="hljs code-wrapper"><pre><code class="hljs go"><span class="hljs-comment">// 一个定时任务工具</span>cn := cron.New()<span class="hljs-comment">// 默认 30s 执行一次</span>_, err = cn.AddFunc(c.String(<span class="hljs-string">&quot;cron&quot;</span>), <span class="hljs-function"><span class="hljs-keyword">func</span><span class="hljs-params">()</span></span> &#123;    rand.Seed(time.Now().Unix())    <span class="hljs-comment">// 随机取一句诗</span>    name := data[rand.Intn(<span class="hljs-built_in">len</span>(data)<span class="hljs-number">-1</span>)]    logger.Infof(<span class="hljs-string">&quot;update name to [%s]...&quot;</span>, name)    <span class="hljs-comment">// 调用 TDLib 改名</span>    _, err := client.SetName(name, <span class="hljs-string">&quot;&quot;</span>)    <span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;        logger.Error(err)    &#125;&#125;)</code></pre></div><p>效果嘛，就这样:</p><p><img src="https://cdn.oss.link/markdown/A15b2P.png" alt="A15b2P"></p><p><img src="https://cdn.oss.link/markdown/3MXSCI.png" alt="3MXSCI"></p>]]></content>
    
    
    <summary type="html">Telegram 确实好用，搓个 Bot 也好玩</summary>
    
    
    
    <category term="Golang" scheme="https://mritd.com/categories/golang/"/>
    
    
    <category term="Golang" scheme="https://mritd.com/tags/golang/"/>
    
    <category term="Telegram" scheme="https://mritd.com/tags/telegram/"/>
    
  </entry>
  
  <entry>
    <title>nerdctl 初试</title>
    <link href="https://mritd.com/2021/06/01/nerdctl-test/"/>
    <id>https://mritd.com/2021/06/01/nerdctl-test/</id>
    <published>2021-06-01T13:46:00.000Z</published>
    <updated>2021-06-01T13:46:00.000Z</updated>
    
    <content type="html"><![CDATA[<p>自从 Containerd 发布 1.5 以后，nerdctl 工具配合 Containerd 的情况下基本已经可以替换掉 Docker 和 Docker Compose；由于天下苦 Docker 久已，没忍住今天试了试。</p><h2 id="一、nerdctl-安装"><a href="#一、nerdctl-安装" class="headerlink" title="一、nerdctl 安装"></a>一、nerdctl 安装</h2><p>nerdctl 官方发布包包含两个安装版本:</p><ul><li>Minimal: 仅包含 nerdctl 二进制文件以及 rootless 模式下的辅助安装脚本</li><li>Full: 看名字就能知道是个全量包，其包含了 Containerd、CNI、runc、BuildKit 等完整组件</li></ul><p>这时候用脚趾头想我都要一把梭，在一把梭之前先卸载以前安装的 Docker 以及 Containerd 等组件(以下以 Ubuntu 20.04 为例):</p><div class="hljs code-wrapper"><pre><code class="hljs sh">apt purge docker.io containerd -y</code></pre></div><p>然后下载安装包解压启动即可(一把梭真香):</p><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment"># 下载压缩包</span>wget https://github.com/containerd/nerdctl/releases/download/v0.8.2/nerdctl-full-0.8.2-linux-amd64.tar.gz<span class="hljs-comment"># 解压安装</span>tar Cxzvvf /usr/<span class="hljs-built_in">local</span> nerdctl-full-0.8.2-linux-amd64.tar.gz<span class="hljs-comment"># 启动 containerd 和 buildkitd</span>systemctl <span class="hljs-built_in">enable</span> --now containerdsystemctl <span class="hljs-built_in">enable</span> --now buildkit</code></pre></div><h2 id="二、使用"><a href="#二、使用" class="headerlink" title="二、使用"></a>二、使用</h2><p>启动完成后就可以通过 <code>ctr</code>、<code>crictl</code> 命令测试 containerd 是否工作正常了；没问题的话继续折腾 <code>nerdctl</code>。</p><h3 id="2-1、Docker-CLI-兼容"><a href="#2-1、Docker-CLI-兼容" class="headerlink" title="2.1、Docker CLI 兼容"></a>2.1、Docker CLI 兼容</h3><p>Docker CLI 的兼容具体情况可以从 <a href="https://github.com/containerd/nerdctl#command-reference">https://github.com/containerd/nerdctl#command-reference</a> 中查看相关说明；既然是为了兼容 Docker CLI，那么在运行时只需要把 <code>docker</code> 命令换成 <code>nerdctl</code> 命令即可:</p><div class="hljs code-wrapper"><pre><code class="hljs sh">vm.node ➜ ~ nerdctl run -d --name <span class="hljs-built_in">test</span> -p 8080:80 nginx:alpine80342ff329574ab290c212b2b786b52dd0c3f3209ee8e9e06878259dd1186879vm.node ➜  ~ nerdctl psCONTAINER ID    IMAGE                             COMMAND                   CREATED          STATUS    PORTS                   NAMES80342ff32957    docker.io/library/nginx:alpine    <span class="hljs-string">&quot;/docker-entrypoint.…&quot;</span>    3 seconds ago    Up        0.0.0.0:8080-&gt;80/tcp    <span class="hljs-built_in">test</span>vm.node ➜ ~ curl 10.0.0.5:8080&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt;&lt;title&gt;Welcome to nginx!&lt;/title&gt;&lt;style&gt;    body &#123;        width: 35em;        margin: 0 auto;        font-family: Tahoma, Verdana, Arial, sans-serif;    &#125;&lt;/style&gt;&lt;/head&gt;&lt;body&gt;&lt;h1&gt;Welcome to nginx!&lt;/h1&gt;&lt;p&gt;If you see this page, the nginx web server is successfully installed andworking. Further configuration is required.&lt;/p&gt;&lt;p&gt;For online documentation and support please refer to&lt;a href=<span class="hljs-string">&quot;http://nginx.org/&quot;</span>&gt;nginx.org&lt;/a&gt;.&lt;br/&gt;Commercial support is available at&lt;a href=<span class="hljs-string">&quot;http://nginx.com/&quot;</span>&gt;nginx.com&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;&lt;em&gt;Thank you <span class="hljs-keyword">for</span> using nginx.&lt;/em&gt;&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;</code></pre></div><p><strong>唯一需要注意的是部分命令选项还是有一定不兼容，比如 <code>run</code> 的时候 <code>-d</code> 和 <code>-t</code> 不能一起用，<code>--restart</code> 策略不支持等，但是通过列表可以看到大部分 cli 都已经完成了。</strong></p><h3 id="2-2、Docker-Compose-兼容"><a href="#2-2、Docker-Compose-兼容" class="headerlink" title="2.2、Docker Compose 兼容"></a>2.2、Docker Compose 兼容</h3><p>由于环境不同吧，说实话 Docker Compose 兼容才是吸引最大的一点；因为现实环境中很少有直接 <code>docker run...</code> 这么干的，大部分不重要服务都是通过 <code>docker-compose</code> 启动的；而目前来说 <code>nerdctl</code> 配合 CNI 等已经完成了大部分 Compose 的兼容:</p><p><strong>docker-compose.yaml</strong></p><div class="hljs code-wrapper"><pre><code class="hljs yaml"><span class="hljs-attr">version:</span> <span class="hljs-string">&#x27;3.7&#x27;</span><span class="hljs-attr">services:</span>  <span class="hljs-attr">cloudreve:</span>    <span class="hljs-attr">image:</span> <span class="hljs-string">mritd/cloudreve:relativepath</span>    <span class="hljs-attr">container_name:</span> <span class="hljs-string">cloudreve</span>    <span class="hljs-attr">restart:</span> <span class="hljs-string">always</span>    <span class="hljs-attr">ports:</span>    <span class="hljs-bullet">-</span> <span class="hljs-string">&quot;5212:5212&quot;</span>    <span class="hljs-bullet">-</span> <span class="hljs-string">&quot;5443:5443&quot;</span>    <span class="hljs-attr">volumes:</span>    <span class="hljs-bullet">-</span> <span class="hljs-string">./config:/etc/cloudreve</span>    <span class="hljs-bullet">-</span> <span class="hljs-string">data:/data</span>    <span class="hljs-bullet">-</span> <span class="hljs-string">shared:/downloads</span>    <span class="hljs-attr">command:</span> [<span class="hljs-string">&quot;-c&quot;</span>,<span class="hljs-string">&quot;/etc/cloudreve/conf.ini&quot;</span>]<span class="hljs-attr">volumes:</span>  <span class="hljs-attr">shared:</span>  <span class="hljs-attr">data:</span></code></pre></div><p>运行测试:</p><div class="hljs code-wrapper"><pre><code class="hljs sh">vm.node ➜ nerdctl compose up -dINFO[0000] Creating network test_defaultINFO[0000] Ensuring image mritd/cloudreve:relativepathINFO[0000] Creating container cloudreve</code></pre></div><p><strong>不过目前比较尴尬的是 compose 还不支持 <code>ps</code> 命令，同时如果 volume 了宿主机目录，如果目录不存在也不会自动创建；<code>logs</code> 命令似乎也有 BUG。</strong></p><h2 id="三、总结"><a href="#三、总结" class="headerlink" title="三、总结"></a>三、总结</h2><p>nerdctl 目前还有很多不完善的地方，比如 <code>cp</code> 等命令不支持，<code>compose</code> 命令不完善，BuildKit 还不支持多平台交叉编译等；所以简单玩玩倒是可以，距离生产使用还需要一些时间，但是总体来说<strong>未来可期</strong>，相信不久以后我们会离 Docker 越来越远。</p>]]></content>
    
    
    <summary type="html">今天突然想起了 nerdctl，装一个玩玩...</summary>
    
    
    
    <category term="Containerd" scheme="https://mritd.com/categories/containerd/"/>
    
    
    <category term="nerdctl" scheme="https://mritd.com/tags/nerdctl/"/>
    
  </entry>
  
  <entry>
    <title>Golang 监控 HTTPS 证书过期时间</title>
    <link href="https://mritd.com/2021/05/31/golang-check-certificate-expiration-time/"/>
    <id>https://mritd.com/2021/05/31/golang-check-certificate-expiration-time/</id>
    <published>2021-05-31T04:47:00.000Z</published>
    <updated>2021-05-31T04:47:00.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="一、业务需求"><a href="#一、业务需求" class="headerlink" title="一、业务需求"></a>一、业务需求</h2><p>由于近几年 Let’s Encrypt 的兴起以及 HTTPS 的普及，个人用户终于可以免费 “绿” 一把了；但是 Let’s Encrypt ACME 申请的证书目前只有 3 个月，过期就要更换，最尴尬的是某些比较重要的东西(比如扶墙服务)证书一旦过期会耽误大事；而不同环境下自动更换证书工具也不一定靠谱，极端时候还是需要自己手动更换，所以催生了我想写个证书过期时间检测的小玩具的想法。</p><h2 id="二、HTTPS-证书链"><a href="#二、HTTPS-证书链" class="headerlink" title="二、HTTPS 证书链"></a>二、HTTPS 证书链</h2><p>了解证书加密体系的应该知道，TLS 证书是链式信任的，所以中间任何一个证书过期、失效都会导致整个信任链断裂，不过单纯的 Let’s Encrypt ACME 证书检测可能只关注末端证书即可，除非哪天 Let’s Encrypt 倒下…</p><h2 id="三、Go-的-HTTP-请求"><a href="#三、Go-的-HTTP-请求" class="headerlink" title="三、Go 的 HTTP 请求"></a>三、Go 的 HTTP 请求</h2><p>Go 在发送 HTTP 请求后，在响应体中会包含一个 <code>TLS *tls.ConnectionState</code> 结构体，该结构体中目前存放了服务端返回的整个证书链:</p><div class="hljs code-wrapper"><pre><code class="hljs go"><span class="hljs-comment">// ConnectionState records basic TLS details about the connection.</span><span class="hljs-keyword">type</span> ConnectionState <span class="hljs-keyword">struct</span> &#123;<span class="hljs-comment">// Version is the TLS version used by the connection (e.g. VersionTLS12).</span>Version <span class="hljs-keyword">uint16</span><span class="hljs-comment">// HandshakeComplete is true if the handshake has concluded.</span>HandshakeComplete <span class="hljs-keyword">bool</span><span class="hljs-comment">// DidResume is true if this connection was successfully resumed from a</span><span class="hljs-comment">// previous session with a session ticket or similar mechanism.</span>DidResume <span class="hljs-keyword">bool</span><span class="hljs-comment">// CipherSuite is the cipher suite negotiated for the connection (e.g.</span><span class="hljs-comment">// TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256, TLS_AES_128_GCM_SHA256).</span>CipherSuite <span class="hljs-keyword">uint16</span><span class="hljs-comment">// NegotiatedProtocol is the application protocol negotiated with ALPN.</span>NegotiatedProtocol <span class="hljs-keyword">string</span><span class="hljs-comment">// NegotiatedProtocolIsMutual used to indicate a mutual NPN negotiation.</span><span class="hljs-comment">//</span><span class="hljs-comment">// Deprecated: this value is always true.</span>NegotiatedProtocolIsMutual <span class="hljs-keyword">bool</span><span class="hljs-comment">// ServerName is the value of the Server Name Indication extension sent by</span><span class="hljs-comment">// the client. It&#x27;s available both on the server and on the client side.</span>ServerName <span class="hljs-keyword">string</span><span class="hljs-comment">// PeerCertificates are the parsed certificates sent by the peer, in the</span><span class="hljs-comment">// order in which they were sent. The first element is the leaf certificate</span><span class="hljs-comment">// that the connection is verified against.</span><span class="hljs-comment">//</span><span class="hljs-comment">// On the client side, it can&#x27;t be empty. On the server side, it can be</span><span class="hljs-comment">// empty if Config.ClientAuth is not RequireAnyClientCert or</span><span class="hljs-comment">// RequireAndVerifyClientCert.</span>PeerCertificates []*x509.Certificate<span class="hljs-comment">// VerifiedChains is a list of one or more chains where the first element is</span><span class="hljs-comment">// PeerCertificates[0] and the last element is from Config.RootCAs (on the</span><span class="hljs-comment">// client side) or Config.ClientCAs (on the server side).</span><span class="hljs-comment">//</span><span class="hljs-comment">// On the client side, it&#x27;s set if Config.InsecureSkipVerify is false. On</span><span class="hljs-comment">// the server side, it&#x27;s set if Config.ClientAuth is VerifyClientCertIfGiven</span><span class="hljs-comment">// (and the peer provided a certificate) or RequireAndVerifyClientCert.</span>VerifiedChains [][]*x509.Certificate<span class="hljs-comment">// SignedCertificateTimestamps is a list of SCTs provided by the peer</span><span class="hljs-comment">// through the TLS handshake for the leaf certificate, if any.</span>SignedCertificateTimestamps [][]<span class="hljs-keyword">byte</span><span class="hljs-comment">// OCSPResponse is a stapled Online Certificate Status Protocol (OCSP)</span><span class="hljs-comment">// response provided by the peer for the leaf certificate, if any.</span>OCSPResponse []<span class="hljs-keyword">byte</span><span class="hljs-comment">// TLSUnique contains the &quot;tls-unique&quot; channel binding value (see RFC 5929,</span><span class="hljs-comment">// Section 3). This value will be nil for TLS 1.3 connections and for all</span><span class="hljs-comment">// resumed connections.</span><span class="hljs-comment">//</span><span class="hljs-comment">// Deprecated: there are conditions in which this value might not be unique</span><span class="hljs-comment">// to a connection. See the Security Considerations sections of RFC 5705 and</span><span class="hljs-comment">// RFC 7627, and https://mitls.org/pages/attacks/3SHAKE#channelbindings.</span>TLSUnique []<span class="hljs-keyword">byte</span><span class="hljs-comment">// ekm is a closure exposed via ExportKeyingMaterial.</span>ekm <span class="hljs-function"><span class="hljs-keyword">func</span><span class="hljs-params">(label <span class="hljs-keyword">string</span>, context []<span class="hljs-keyword">byte</span>, length <span class="hljs-keyword">int</span>)</span> <span class="hljs-params">([]<span class="hljs-keyword">byte</span>, error)</span></span>&#125;</code></pre></div><p>根据源码注释可以看到，<code>PeerCertificates</code> 包含了服务端所有证书，那么如果需要检测证书过期时间只需要遍历这个证书切片即可。</p><h2 id="四、代码实现"><a href="#四、代码实现" class="headerlink" title="四、代码实现"></a>四、代码实现</h2><p>基本需求确定，且确立代码可行性后直接开始 coding:</p><div class="hljs code-wrapper"><pre><code class="hljs go"><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-title">checkSSL</span><span class="hljs-params">(beforeTime time.Duration)</span> <span class="hljs-title">error</span></span> &#123;client := &amp;http.Client&#123;Transport: &amp;http.Transport&#123;<span class="hljs-comment">// 注意如果证书已过期，那么只有在关闭证书校验的情况下链接才能建立成功</span>TLSClientConfig: &amp;tls.Config&#123;InsecureSkipVerify: <span class="hljs-literal">true</span>&#125;,&#125;,<span class="hljs-comment">// 10s 超时后认为服务挂了</span>Timeout: <span class="hljs-number">10</span> * time.Second,&#125;resp, err := client.Get(<span class="hljs-string">&quot;https://mritd.com&quot;</span>)<span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;<span class="hljs-keyword">return</span> err&#125;<span class="hljs-keyword">defer</span> <span class="hljs-function"><span class="hljs-keyword">func</span><span class="hljs-params">()</span></span> &#123; _ = resp.Body.Close() &#125;()<span class="hljs-comment">// 遍历所有证书</span><span class="hljs-keyword">for</span> _, cert := <span class="hljs-keyword">range</span> resp.TLS.PeerCertificates &#123;<span class="hljs-comment">// 检测证书是否已经过期</span><span class="hljs-keyword">if</span> !cert.NotAfter.After(time.Now()) &#123;<span class="hljs-keyword">return</span> NewWebSiteError(fmt.Sprintf(<span class="hljs-string">&quot;Website [https://mritd.com] certificate has expired: %s&quot;</span>, cert.NotAfter.Local().Format(<span class="hljs-string">&quot;2006-01-02 15:04:05&quot;</span>)))&#125;<span class="hljs-comment">// 检测证书距离当前时间 是否小于 beforeTime</span><span class="hljs-comment">// 例如 beforeTime = 7d，那么在证书过期前 6d 开始就发出警告</span><span class="hljs-keyword">if</span> cert.NotAfter.Sub(time.Now()) &lt; beforeTime &#123;<span class="hljs-keyword">return</span> NewWebSiteError(fmt.Sprintf(<span class="hljs-string">&quot;Website [https://mritd.com] certificate will expire, remaining time: %fh&quot;</span>, cert.NotAfter.Sub(time.Now()).Hours()))&#125;&#125;<span class="hljs-keyword">return</span> <span class="hljs-literal">nil</span>&#125;</code></pre></div><h2 id="五、整合告警"><a href="#五、整合告警" class="headerlink" title="五、整合告警"></a>五、整合告警</h2><p>基本检测逻辑完成后，可以尝试集成告警服务，例如 Email、Telegram、微信通知等；告警的实现暂时不在本文讨论范围内，具体完整实现可以参考 <a href="https://github.com/mritd/certmonitor">https://github.com/mritd/certmonitor</a>，certmonitor 集成了 Telegram，最终效果如下:</p><p><img src="https://cdn.oss.link/markdown/ixjtRl.png" alt="ixjtRl"></p><h2 id="六、其他改进"><a href="#六、其他改进" class="headerlink" title="六、其他改进"></a>六、其他改进</h2><p>有些情况下某些服务不一定是完全基于 HTTPS 的，所以协议上可以后续去尝试使用 tls 客户端直接链接，还可能需要考虑未来基于 QUIC 的 HTTP3 等，复杂点也要支持文件证书检测… 给我时间我能给自己提一万个需求(今天就先码到这)…</p>]]></content>
    
    
    <summary type="html">迫于需要节省成本(穷)，一直使用 Let&#39;s Encrypt 签发的证书，为了防止证书过期，自己撮了一个小工具</summary>
    
    
    
    <category term="Golang" scheme="https://mritd.com/categories/golang/"/>
    
    
    <category term="Golang" scheme="https://mritd.com/tags/golang/"/>
    
  </entry>
  
  <entry>
    <title>Kubernetes 切换到 Containerd</title>
    <link href="https://mritd.com/2021/05/29/use-containerd-with-kubernetes/"/>
    <id>https://mritd.com/2021/05/29/use-containerd-with-kubernetes/</id>
    <published>2021-05-29T15:01:00.000Z</published>
    <updated>2021-05-29T15:01:00.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="一、环境准备"><a href="#一、环境准备" class="headerlink" title="一、环境准备"></a>一、环境准备</h2><ul><li>Ubuntu 20.04 x5</li><li>Etcd 3.4.16</li><li>Kubernetes 1.21.1</li><li>Containerd 1.3.3</li></ul><h3 id="1-1、处理-IPVS"><a href="#1-1、处理-IPVS" class="headerlink" title="1.1、处理 IPVS"></a>1.1、处理 IPVS</h3><p>由于 Kubernetes 新版本 Service 实现切换到 IPVS，所以需要确保内核加载了 IPVS modules；以下命令将设置系统启动自动加载 IPVS 相关模块，执行完成后需要重启。</p><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment"># Kernel modules</span>cat &gt; /etc/modules-load.d/50-kubernetes.conf &lt;&lt;<span class="hljs-string">EOF</span><span class="hljs-string"># Load some kernel modules needed by kubernetes at boot</span><span class="hljs-string">nf_conntrack</span><span class="hljs-string">br_netfilter</span><span class="hljs-string">ip_vs</span><span class="hljs-string">ip_vs_lc</span><span class="hljs-string">ip_vs_wlc</span><span class="hljs-string">ip_vs_rr</span><span class="hljs-string">ip_vs_wrr</span><span class="hljs-string">ip_vs_lblc</span><span class="hljs-string">ip_vs_lblcr</span><span class="hljs-string">ip_vs_dh</span><span class="hljs-string">ip_vs_sh</span><span class="hljs-string">ip_vs_fo</span><span class="hljs-string">ip_vs_nq</span><span class="hljs-string">ip_vs_sed</span><span class="hljs-string">EOF</span><span class="hljs-comment"># sysctl</span>cat &gt; /etc/sysctl.d/50-kubernetes.conf &lt;&lt;<span class="hljs-string">EOF</span><span class="hljs-string">net.ipv4.ip_forward=1</span><span class="hljs-string">net.bridge.bridge-nf-call-iptables=1</span><span class="hljs-string">net.bridge.bridge-nf-call-ip6tables=1</span><span class="hljs-string">fs.inotify.max_user_watches=525000</span><span class="hljs-string">EOF</span></code></pre></div><p>重启完成后务必检查相关 module 加载以及内核参数设置:</p><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment"># check ipvs modules</span>➜ ~ lsmod | grep ip_vsip_vs_sed              16384  0ip_vs_nq               16384  0ip_vs_fo               16384  0ip_vs_sh               16384  0ip_vs_dh               16384  0ip_vs_lblcr            16384  0ip_vs_lblc             16384  0ip_vs_wrr              16384  0ip_vs_rr               16384  0ip_vs_wlc              16384  0ip_vs_lc               16384  0ip_vs                 155648  22 ip_vs_wlc,ip_vs_rr,ip_vs_dh,ip_vs_lblcr,ip_vs_sh,ip_vs_fo,ip_vs_nq,ip_vs_lblc,ip_vs_wrr,ip_vs_lc,ip_vs_sednf_conntrack          139264  1 ip_vsnf_defrag_ipv6         24576  2 nf_conntrack,ip_vslibcrc32c              16384  5 nf_conntrack,btrfs,xfs,raid456,ip_vs<span class="hljs-comment"># check sysctl</span>➜ ~ sysctl -a | grep ip_forwardnet.ipv4.ip_forward = 1net.ipv4.ip_forward_update_priority = 1net.ipv4.ip_forward_use_pmtu = 0➜ ~ sysctl -a | grep bridge-nf-callnet.bridge.bridge-nf-call-arptables = 1net.bridge.bridge-nf-call-ip6tables = 1net.bridge.bridge-nf-call-iptables = 1</code></pre></div><h3 id="1-2、安装-Containerd"><a href="#1-2、安装-Containerd" class="headerlink" title="1.2、安装 Containerd"></a>1.2、安装 Containerd</h3><p>Containerd 在 Ubuntu 20 中已经在默认官方仓库中包含，所以只需要 apt 安装即可:</p><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment"># 其他软件包后面可能会用到，所以顺手装了</span>apt install containerd bridge-utils nfs-common tree -y</code></pre></div><p>安装成功后可以通过执行 <code>ctr images ls</code> 命令验证，<strong>本章节不会对 Containerd 配置做说明，Containerd 配置文件将在 Kubernetes 安装时进行配置。</strong></p><h2 id="二、安装-kubernetes"><a href="#二、安装-kubernetes" class="headerlink" title="二、安装 kubernetes"></a>二、安装 kubernetes</h2><h3 id="2-1、安装-Etcd-集群"><a href="#2-1、安装-Etcd-集群" class="headerlink" title="2.1、安装 Etcd 集群"></a>2.1、安装 Etcd 集群</h3><p>Etcd 对于 Kubernetes 来说是核心中的核心，所以个人还是比较喜欢在宿主机安装；宿主机安装情况下为了方便我打包了一些 <strong><code>*-pack</code></strong> 的工具包，用于快速处理:</p><p>安装 CFSSL 和 ETCD</p><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment"># 下载安装包</span>wget https://github.com/mritd/etcd-pack/releases/download/v3.4.16/etcd_v3.4.16.runwget https://github.com/mritd/cfssl-pack/releases/download/v1.5.0/cfssl_v1.5.0.run<span class="hljs-comment"># 安装 cfssl 和 etcd</span>chmod +x *.run./etcd_v3.4.16.run install./cfssl_v1.5.0.run install</code></pre></div><p>安装完成后，<strong>自行调整 <code>/etc/cfssl/etcd/etcd-csr.json</code> 相关 IP</strong>，然后执行同目录下 <code>create.sh</code> 生成证书。</p><div class="hljs code-wrapper"><pre><code class="hljs sh">➜ ~ cat /etc/cfssl/etcd/etcd-csr.json&#123;    <span class="hljs-string">&quot;key&quot;</span>: &#123;        <span class="hljs-string">&quot;algo&quot;</span>: <span class="hljs-string">&quot;rsa&quot;</span>,        <span class="hljs-string">&quot;size&quot;</span>: 2048    &#125;,    <span class="hljs-string">&quot;names&quot;</span>: [        &#123;            <span class="hljs-string">&quot;O&quot;</span>: <span class="hljs-string">&quot;etcd&quot;</span>,            <span class="hljs-string">&quot;OU&quot;</span>: <span class="hljs-string">&quot;etcd Security&quot;</span>,            <span class="hljs-string">&quot;L&quot;</span>: <span class="hljs-string">&quot;Beijing&quot;</span>,            <span class="hljs-string">&quot;ST&quot;</span>: <span class="hljs-string">&quot;Beijing&quot;</span>,            <span class="hljs-string">&quot;C&quot;</span>: <span class="hljs-string">&quot;CN&quot;</span>        &#125;    ],    <span class="hljs-string">&quot;CN&quot;</span>: <span class="hljs-string">&quot;etcd&quot;</span>,    <span class="hljs-string">&quot;hosts&quot;</span>: [        <span class="hljs-string">&quot;127.0.0.1&quot;</span>,        <span class="hljs-string">&quot;localhost&quot;</span>,        <span class="hljs-string">&quot;*.etcd.node&quot;</span>,        <span class="hljs-string">&quot;*.kubernetes.node&quot;</span>,        <span class="hljs-string">&quot;10.0.0.11&quot;</span>,        <span class="hljs-string">&quot;10.0.0.12&quot;</span>,        <span class="hljs-string">&quot;10.0.0.13&quot;</span>    ]&#125;<span class="hljs-comment"># 复制到 3 台 master</span>➜ ~ <span class="hljs-keyword">for</span> ip <span class="hljs-keyword">in</span> `seq 1 3`; <span class="hljs-keyword">do</span> scp /etc/cfssl/etcd/*.pem root@10.0.0.1<span class="hljs-variable">$ip</span>:/etc/etcd/ssl; <span class="hljs-keyword">done</span></code></pre></div><p>证书生成完成后调整每台机器的 Etcd 配置文件，然后修复权限启动。</p><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment"># 复制配置</span><span class="hljs-keyword">for</span> ip <span class="hljs-keyword">in</span> `seq 1 3`; <span class="hljs-keyword">do</span> scp /etc/etcd/etcd.cluster.yaml root@10.0.0.1<span class="hljs-variable">$ip</span>:/etc/etcd/etcd.yaml; <span class="hljs-keyword">done</span><span class="hljs-comment"># 修复权限</span><span class="hljs-keyword">for</span> ip <span class="hljs-keyword">in</span> `seq 1 3`; <span class="hljs-keyword">do</span> ssh root@10.0.0.1<span class="hljs-variable">$ip</span> chown -R etcd:etcd /etc/etcd; <span class="hljs-keyword">done</span><span class="hljs-comment"># 每台机器启动</span>systemctl start etcd</code></pre></div><p>启动完成后通过 <code>etcdctl</code> 验证集群状态:</p><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment"># 稳妥点应该执行 etcdctl endpoint health</span>➜ ~ etcdctl member list55fcbe0adaa45350, started, etcd3, https://10.0.0.13:2380, https://10.0.0.13:2379, <span class="hljs-literal">false</span>cebdf10928a06f3c, started, etcd1, https://10.0.0.11:2380, https://10.0.0.11:2379, <span class="hljs-literal">false</span>f7a9c20602b8532e, started, etcd2, https://10.0.0.12:2380, https://10.0.0.12:2379, <span class="hljs-literal">false</span></code></pre></div><h3 id="2-2、安装-kubeadm"><a href="#2-2、安装-kubeadm" class="headerlink" title="2.2、安装 kubeadm"></a>2.2、安装 kubeadm</h3><p>kubeadm 国内用户建议使用 aliyun 的安装源:</p><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment"># kubeadm</span>apt-get install -y apt-transport-httpscurl https://mirrors.aliyun.com/kubernetes/apt/doc/apt-key.gpg | apt-key add -cat &lt;&lt;<span class="hljs-string">EOF &gt;/etc/apt/sources.list.d/kubernetes.list</span><span class="hljs-string">deb https://mirrors.aliyun.com/kubernetes/apt/ kubernetes-xenial main</span><span class="hljs-string">EOF</span>apt update<span class="hljs-comment"># ebtables、ethtool kubelet 可能会用，具体忘了，反正从官方文档上看到的</span>apt install kubelet kubeadm kubectl ebtables ethtool -y</code></pre></div><h3 id="2-3、安装-kube-apiserver-proxy"><a href="#2-3、安装-kube-apiserver-proxy" class="headerlink" title="2.3、安装 kube-apiserver-proxy"></a>2.3、安装 kube-apiserver-proxy</h3><p>kube-apiserver-proxy 是我自己编译的一个仅开启四层代理的 Nginx，其主要负责监听 <code>127.0.0.1:6443</code> 并负载到所有的 Api Server 地址(<code>0.0.0.0:5443</code>):</p><div class="hljs code-wrapper"><pre><code class="hljs sh">wget https://github.com/mritd/kube-apiserver-proxy-pack/releases/download/v1.20.0/kube-apiserver-proxy_v1.20.0.runchmod +x *.run./kube-apiserver-proxy_v1.20.0.run install</code></pre></div><p>安装完成后根据 IP 地址不同自行调整 Nginx 配置文件，然后启动:</p><div class="hljs code-wrapper"><pre><code class="hljs sh">➜ ~ cat /etc/kubernetes/apiserver-proxy.conferror_log syslog:server=unix:/dev/<span class="hljs-built_in">log</span> notice;worker_processes auto;events &#123;        multi_accept on;        use epoll;        worker_connections 1024;&#125;stream &#123;    upstream kube_apiserver &#123;        least_conn;        server 10.0.0.11:5443;        server 10.0.0.12:5443;        server 10.0.0.13:5443;    &#125;    server &#123;        listen        0.0.0.0:6443;        proxy_pass    kube_apiserver;        proxy_timeout 10m;        proxy_connect_timeout 1s;    &#125;&#125;systemctl start kube-apiserver-proxy</code></pre></div><h3 id="2-4、安装-kubeadm-config"><a href="#2-4、安装-kubeadm-config" class="headerlink" title="2.4、安装 kubeadm-config"></a>2.4、安装 kubeadm-config</h3><p>kubeadm-config 是一系列配置文件的组合以及 kubeadm 安装所需的必要镜像文件的打包，安装完成后将会自动配置 Containerd、ctrictl 等:</p><div class="hljs code-wrapper"><pre><code class="hljs sh">wget https://github.com/mritd/kubeadm-config-pack/releases/download/v1.21.1/kubeadm-config_v1.21.1.runchmod +x *.run<span class="hljs-comment"># --load 选项用于将 kubeadm 所需镜像 load 到 containerd 中</span>./kubeadm-config_v1.21.1.run install --load</code></pre></div><h4 id="2-4-1、containerd-配置"><a href="#2-4-1、containerd-配置" class="headerlink" title="2.4.1、containerd 配置"></a>2.4.1、containerd 配置</h4><p>Containerd 配置位于 <code>/etc/containerd/config.toml</code>，其配置如下:</p><div class="hljs code-wrapper"><pre><code class="hljs toml"><span class="hljs-attr">version</span> = <span class="hljs-number">2</span><span class="hljs-comment"># 指定存储根目录</span><span class="hljs-attr">root</span> = <span class="hljs-string">&quot;/data/containerd&quot;</span><span class="hljs-attr">state</span> = <span class="hljs-string">&quot;/run/containerd&quot;</span><span class="hljs-comment"># OOM 评分</span><span class="hljs-attr">oom_score</span> = -<span class="hljs-number">999</span><span class="hljs-section">[grpc]</span>  <span class="hljs-attr">address</span> = <span class="hljs-string">&quot;/run/containerd/containerd.sock&quot;</span><span class="hljs-section">[metrics]</span>  <span class="hljs-attr">address</span> = <span class="hljs-string">&quot;127.0.0.1:1234&quot;</span><span class="hljs-section">[plugins]</span>  <span class="hljs-section">[plugins.&quot;io.containerd.grpc.v1.cri&quot;]</span>    <span class="hljs-comment"># sandbox 镜像</span>    <span class="hljs-attr">sandbox_image</span> = <span class="hljs-string">&quot;k8s.gcr.io/pause:3.4.1&quot;</span>    <span class="hljs-section">[plugins.&quot;io.containerd.grpc.v1.cri&quot;.containerd]</span>      <span class="hljs-attr">snapshotter</span> = <span class="hljs-string">&quot;overlayfs&quot;</span>      <span class="hljs-attr">default_runtime_name</span> = <span class="hljs-string">&quot;runc&quot;</span>      <span class="hljs-section">[plugins.&quot;io.containerd.grpc.v1.cri&quot;.containerd.runtimes]</span>        <span class="hljs-section">[plugins.&quot;io.containerd.grpc.v1.cri&quot;.containerd.runtimes.runc]</span>          <span class="hljs-attr">runtime_type</span> = <span class="hljs-string">&quot;io.containerd.runc.v2&quot;</span>          <span class="hljs-comment"># 开启 systemd cgroup</span>          <span class="hljs-section">[plugins.&quot;io.containerd.grpc.v1.cri&quot;.containerd.runtimes.runc.options]</span>            <span class="hljs-attr">SystemdCgroup</span> = <span class="hljs-literal">true</span></code></pre></div><h4 id="2-4-2、crictl-配置"><a href="#2-4-2、crictl-配置" class="headerlink" title="2.4.2、crictl 配置"></a>2.4.2、crictl 配置</h4><p>在切换到 Containerd 以后意味着以前的 <code>docker</code> 命令将不再可用，containerd 默认自带了一个 <code>ctr</code> 命令，同时 CRI 规范会自带一个 <code>crictl</code> 命令；<code>crictl</code> 命令配置文件存放在 <code>/etc/crictl.yaml</code> 中:</p><div class="hljs code-wrapper"><pre><code class="hljs yaml"><span class="hljs-attr">runtime-endpoint:</span> <span class="hljs-string">unix:///run/containerd/containerd.sock</span><span class="hljs-attr">image-endpoint:</span> <span class="hljs-string">unix:///run/containerd/containerd.sock</span><span class="hljs-attr">pull-image-on-create:</span> <span class="hljs-literal">true</span></code></pre></div><h4 id="2-4-3、kubeadm-配置"><a href="#2-4-3、kubeadm-配置" class="headerlink" title="2.4.3、kubeadm 配置"></a>2.4.3、kubeadm 配置</h4><p>kubeadm 配置目前分为 2 个，一个是用于首次引导启动的 init 配置，另一个是用于其他节点 join 到 master 的配置；其中比较重要的 init 配置如下:</p><div class="hljs code-wrapper"><pre><code class="hljs yaml"><span class="hljs-comment"># /etc/kubernetes/kubeadm.yaml</span><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">kubeadm.k8s.io/v1beta2</span><span class="hljs-attr">kind:</span> <span class="hljs-string">InitConfiguration</span><span class="hljs-comment"># kubeadm token create</span><span class="hljs-attr">bootstrapTokens:</span><span class="hljs-bullet">-</span> <span class="hljs-attr">token:</span> <span class="hljs-string">&quot;c2t0rj.cofbfnwwrb387890&quot;</span><span class="hljs-attr">nodeRegistration:</span>  <span class="hljs-comment"># CRI 地址(Containerd)</span>  <span class="hljs-attr">criSocket:</span> <span class="hljs-string">unix:///run/containerd/containerd.sock</span>  <span class="hljs-attr">kubeletExtraArgs:</span>    <span class="hljs-attr">runtime-cgroups:</span> <span class="hljs-string">&quot;/system.slice/containerd.service&quot;</span>    <span class="hljs-attr">rotate-server-certificates:</span> <span class="hljs-string">&quot;true&quot;</span><span class="hljs-attr">localAPIEndpoint:</span>  <span class="hljs-attr">advertiseAddress:</span> <span class="hljs-string">&quot;10.0.0.11&quot;</span>  <span class="hljs-attr">bindPort:</span> <span class="hljs-number">5443</span><span class="hljs-comment"># kubeadm certs certificate-key</span><span class="hljs-attr">certificateKey:</span> <span class="hljs-string">31f1e534733a1607e5ba67b2834edd3a7debba41babb1fac1bee47072a98d88b</span><span class="hljs-meta">---</span><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">kubeadm.k8s.io/v1beta2</span><span class="hljs-attr">kind:</span> <span class="hljs-string">ClusterConfiguration</span><span class="hljs-attr">clusterName:</span> <span class="hljs-string">&quot;kuberentes&quot;</span><span class="hljs-attr">kubernetesVersion:</span> <span class="hljs-string">&quot;v1.21.1&quot;</span><span class="hljs-attr">certificatesDir:</span> <span class="hljs-string">&quot;/etc/kubernetes/pki&quot;</span><span class="hljs-comment"># Other components of the current control plane only connect to the apiserver on the current host.</span><span class="hljs-comment"># This is the expected behavior, see: https://github.com/kubernetes/kubeadm/issues/2271</span><span class="hljs-attr">controlPlaneEndpoint:</span> <span class="hljs-string">&quot;127.0.0.1:6443&quot;</span><span class="hljs-attr">etcd:</span>  <span class="hljs-attr">external:</span>    <span class="hljs-attr">endpoints:</span>    <span class="hljs-bullet">-</span> <span class="hljs-string">&quot;https://10.0.0.11:2379&quot;</span>    <span class="hljs-bullet">-</span> <span class="hljs-string">&quot;https://10.0.0.12:2379&quot;</span>    <span class="hljs-bullet">-</span> <span class="hljs-string">&quot;https://10.0.0.13:2379&quot;</span>    <span class="hljs-attr">caFile:</span> <span class="hljs-string">&quot;/etc/etcd/ssl/etcd-ca.pem&quot;</span>    <span class="hljs-attr">certFile:</span> <span class="hljs-string">&quot;/etc/etcd/ssl/etcd.pem&quot;</span>    <span class="hljs-attr">keyFile:</span> <span class="hljs-string">&quot;/etc/etcd/ssl/etcd-key.pem&quot;</span><span class="hljs-attr">networking:</span>  <span class="hljs-attr">serviceSubnet:</span> <span class="hljs-string">&quot;10.66.0.0/16&quot;</span>  <span class="hljs-attr">podSubnet:</span> <span class="hljs-string">&quot;10.88.0.1/16&quot;</span>  <span class="hljs-attr">dnsDomain:</span> <span class="hljs-string">&quot;cluster.local&quot;</span><span class="hljs-attr">apiServer:</span>  <span class="hljs-attr">extraArgs:</span>    <span class="hljs-attr">v:</span> <span class="hljs-string">&quot;4&quot;</span>    <span class="hljs-attr">alsologtostderr:</span> <span class="hljs-string">&quot;true&quot;</span><span class="hljs-comment">#    audit-log-maxage: &quot;21&quot;</span><span class="hljs-comment">#    audit-log-maxbackup: &quot;10&quot;</span><span class="hljs-comment">#    audit-log-maxsize: &quot;100&quot;</span><span class="hljs-comment">#    audit-log-path: &quot;/var/log/kube-audit/audit.log&quot;</span><span class="hljs-comment">#    audit-policy-file: &quot;/etc/kubernetes/audit-policy.yaml&quot;</span>    <span class="hljs-attr">authorization-mode:</span> <span class="hljs-string">&quot;Node,RBAC&quot;</span>    <span class="hljs-attr">event-ttl:</span> <span class="hljs-string">&quot;720h&quot;</span>    <span class="hljs-attr">runtime-config:</span> <span class="hljs-string">&quot;api/all=true&quot;</span>    <span class="hljs-attr">service-node-port-range:</span> <span class="hljs-string">&quot;30000-50000&quot;</span>    <span class="hljs-attr">service-cluster-ip-range:</span> <span class="hljs-string">&quot;10.66.0.0/16&quot;</span><span class="hljs-comment">#    insecure-bind-address: &quot;0.0.0.0&quot;</span><span class="hljs-comment">#    insecure-port: &quot;8080&quot;</span>    <span class="hljs-comment"># The fraction of requests that will be closed gracefully(GOAWAY) to prevent</span>    <span class="hljs-comment"># HTTP/2 clients from getting stuck on a single apiserver.</span>    <span class="hljs-attr">goaway-chance:</span> <span class="hljs-string">&quot;0.001&quot;</span><span class="hljs-comment">#  extraVolumes:</span><span class="hljs-comment">#  - name: &quot;audit-config&quot;</span><span class="hljs-comment">#    hostPath: &quot;/etc/kubernetes/audit-policy.yaml&quot;</span><span class="hljs-comment">#    mountPath: &quot;/etc/kubernetes/audit-policy.yaml&quot;</span><span class="hljs-comment">#    readOnly: true</span><span class="hljs-comment">#    pathType: &quot;File&quot;</span><span class="hljs-comment">#  - name: &quot;audit-log&quot;</span><span class="hljs-comment">#    hostPath: &quot;/var/log/kube-audit&quot;</span><span class="hljs-comment">#    mountPath: &quot;/var/log/kube-audit&quot;</span><span class="hljs-comment">#    pathType: &quot;DirectoryOrCreate&quot;</span>  <span class="hljs-attr">certSANs:</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">&quot;*.kubernetes.node&quot;</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">&quot;10.0.0.11&quot;</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">&quot;10.0.0.12&quot;</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">&quot;10.0.0.13&quot;</span>  <span class="hljs-attr">timeoutForControlPlane:</span> <span class="hljs-string">1m</span><span class="hljs-attr">controllerManager:</span>  <span class="hljs-attr">extraArgs:</span>    <span class="hljs-attr">v:</span> <span class="hljs-string">&quot;4&quot;</span>    <span class="hljs-attr">node-cidr-mask-size:</span> <span class="hljs-string">&quot;19&quot;</span>    <span class="hljs-attr">deployment-controller-sync-period:</span> <span class="hljs-string">&quot;10s&quot;</span>    <span class="hljs-attr">experimental-cluster-signing-duration:</span> <span class="hljs-string">&quot;8670h&quot;</span>    <span class="hljs-attr">node-monitor-grace-period:</span> <span class="hljs-string">&quot;20s&quot;</span>    <span class="hljs-attr">pod-eviction-timeout:</span> <span class="hljs-string">&quot;2m&quot;</span>    <span class="hljs-attr">terminated-pod-gc-threshold:</span> <span class="hljs-string">&quot;30&quot;</span><span class="hljs-attr">scheduler:</span>  <span class="hljs-attr">extraArgs:</span>    <span class="hljs-attr">v:</span> <span class="hljs-string">&quot;4&quot;</span><span class="hljs-meta">---</span><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">kubelet.config.k8s.io/v1beta1</span><span class="hljs-attr">kind:</span> <span class="hljs-string">KubeletConfiguration</span><span class="hljs-attr">failSwapOn:</span> <span class="hljs-literal">false</span><span class="hljs-attr">oomScoreAdj:</span> <span class="hljs-number">-900</span><span class="hljs-attr">cgroupDriver:</span> <span class="hljs-string">&quot;systemd&quot;</span><span class="hljs-attr">kubeletCgroups:</span> <span class="hljs-string">&quot;/system.slice/kubelet.service&quot;</span><span class="hljs-attr">nodeStatusUpdateFrequency:</span> <span class="hljs-string">5s</span><span class="hljs-attr">rotateCertificates:</span> <span class="hljs-literal">true</span><span class="hljs-attr">evictionSoft:</span>  <span class="hljs-attr">&quot;imagefs.available&quot;:</span> <span class="hljs-string">&quot;15%&quot;</span>  <span class="hljs-attr">&quot;memory.available&quot;:</span> <span class="hljs-string">&quot;512Mi&quot;</span>  <span class="hljs-attr">&quot;nodefs.available&quot;:</span> <span class="hljs-string">&quot;15%&quot;</span>  <span class="hljs-attr">&quot;nodefs.inodesFree&quot;:</span> <span class="hljs-string">&quot;10%&quot;</span><span class="hljs-attr">evictionSoftGracePeriod:</span>  <span class="hljs-attr">&quot;imagefs.available&quot;:</span> <span class="hljs-string">&quot;3m&quot;</span>  <span class="hljs-attr">&quot;memory.available&quot;:</span> <span class="hljs-string">&quot;1m&quot;</span>  <span class="hljs-attr">&quot;nodefs.available&quot;:</span> <span class="hljs-string">&quot;3m&quot;</span>  <span class="hljs-attr">&quot;nodefs.inodesFree&quot;:</span> <span class="hljs-string">&quot;1m&quot;</span><span class="hljs-attr">evictionHard:</span>  <span class="hljs-attr">&quot;imagefs.available&quot;:</span> <span class="hljs-string">&quot;10%&quot;</span>  <span class="hljs-attr">&quot;memory.available&quot;:</span> <span class="hljs-string">&quot;256Mi&quot;</span>  <span class="hljs-attr">&quot;nodefs.available&quot;:</span> <span class="hljs-string">&quot;10%&quot;</span>  <span class="hljs-attr">&quot;nodefs.inodesFree&quot;:</span> <span class="hljs-string">&quot;5%&quot;</span><span class="hljs-attr">evictionMaxPodGracePeriod:</span> <span class="hljs-number">30</span><span class="hljs-attr">imageGCLowThresholdPercent:</span> <span class="hljs-number">70</span><span class="hljs-attr">imageGCHighThresholdPercent:</span> <span class="hljs-number">80</span><span class="hljs-attr">kubeReserved:</span>  <span class="hljs-attr">&quot;cpu&quot;:</span> <span class="hljs-string">&quot;500m&quot;</span>  <span class="hljs-attr">&quot;memory&quot;:</span> <span class="hljs-string">&quot;512Mi&quot;</span>  <span class="hljs-attr">&quot;ephemeral-storage&quot;:</span> <span class="hljs-string">&quot;1Gi&quot;</span><span class="hljs-meta">---</span><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">kubeproxy.config.k8s.io/v1alpha1</span><span class="hljs-attr">kind:</span> <span class="hljs-string">KubeProxyConfiguration</span><span class="hljs-comment"># kube-proxy specific options here</span><span class="hljs-attr">clusterCIDR:</span> <span class="hljs-string">&quot;10.88.0.1/16&quot;</span><span class="hljs-attr">mode:</span> <span class="hljs-string">&quot;ipvs&quot;</span><span class="hljs-attr">oomScoreAdj:</span> <span class="hljs-number">-900</span><span class="hljs-attr">ipvs:</span>  <span class="hljs-attr">minSyncPeriod:</span> <span class="hljs-string">5s</span>  <span class="hljs-attr">syncPeriod:</span> <span class="hljs-string">5s</span>  <span class="hljs-attr">scheduler:</span> <span class="hljs-string">&quot;wrr&quot;</span></code></pre></div><p>init 配置具体含义请自行参考官方文档，相对于 init 配置，join 配置比较简单，<strong>不过需要注意的是如果需要 join 为 master 则需要 <code>controlPlane</code> 这部分，否则请注释掉 <code>controlPlane</code>。</strong></p><div class="hljs code-wrapper"><pre><code class="hljs yaml"><span class="hljs-comment"># /etc/kubernetes/kubeadm-join.yaml</span><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">kubeadm.k8s.io/v1beta2</span><span class="hljs-attr">kind:</span> <span class="hljs-string">JoinConfiguration</span><span class="hljs-attr">controlPlane:</span>  <span class="hljs-attr">localAPIEndpoint:</span>    <span class="hljs-attr">advertiseAddress:</span> <span class="hljs-string">&quot;10.0.0.12&quot;</span>    <span class="hljs-attr">bindPort:</span> <span class="hljs-number">5443</span>  <span class="hljs-attr">certificateKey:</span> <span class="hljs-string">31f1e534733a1607e5ba67b2834edd3a7debba41babb1fac1bee47072a98d88b</span><span class="hljs-attr">discovery:</span>  <span class="hljs-attr">bootstrapToken:</span>    <span class="hljs-attr">apiServerEndpoint:</span> <span class="hljs-string">&quot;127.0.0.1:6443&quot;</span>    <span class="hljs-attr">token:</span> <span class="hljs-string">&quot;c2t0rj.cofbfnwwrb387890&quot;</span>    <span class="hljs-comment"># Please replace with the &quot;--discovery-token-ca-cert-hash&quot; value printed</span>    <span class="hljs-comment"># after the kubeadm init command is executed successfully</span>    <span class="hljs-attr">caCertHashes:</span>    <span class="hljs-bullet">-</span> <span class="hljs-string">&quot;sha256:97590810ae34a82501717e33acfca76f16044f1a365c5ad9a1c66433c386c75c&quot;</span><span class="hljs-attr">nodeRegistration:</span>  <span class="hljs-attr">criSocket:</span> <span class="hljs-string">unix:///run/containerd/containerd.sock</span>  <span class="hljs-attr">kubeletExtraArgs:</span>    <span class="hljs-attr">runtime-cgroups:</span> <span class="hljs-string">&quot;/system.slice/containerd.service&quot;</span>    <span class="hljs-attr">rotate-server-certificates:</span> <span class="hljs-string">&quot;true&quot;</span></code></pre></div><h3 id="2-5、拉起-master"><a href="#2-5、拉起-master" class="headerlink" title="2.5、拉起 master"></a>2.5、拉起 master</h3><p>在调整好配置后，拉起 master 节点只需要一条命令:</p><div class="hljs code-wrapper"><pre><code class="hljs sh">kubeadm init --config /etc/kubernetes/kubeadm.yaml --upload-certs --ignore-preflight-errors=Swap</code></pre></div><p>拉起完成后记得保存相关 Token 以便于后续使用。</p><h3 id="2-6、拉起其他-master"><a href="#2-6、拉起其他-master" class="headerlink" title="2.6、拉起其他 master"></a>2.6、拉起其他 master</h3><p>在第一个 master 启动完成后，使用 <code>join</code> 命令让其他 master 加入即可；<strong>需要注意的是 <code>kubeadm-join.yaml</code> 配置中需要替换 <code>caCertHashes</code> 为第一个 master 拉起后的 <code>discovery-token-ca-cert-hash</code> 的值。</strong></p><div class="hljs code-wrapper"><pre><code class="hljs sh">kubeadm join 127.0.0.1:6443 --config /etc/kubernetes/kubeadm-join.yaml --ignore-preflight-errors=Swap</code></pre></div><h3 id="2-7、拉起其他-node"><a href="#2-7、拉起其他-node" class="headerlink" title="2.7、拉起其他 node"></a>2.7、拉起其他 node</h3><p>node 节点拉起与拉起其他 master 节点一样，唯一不同的是需要注释掉配置中的 <code>controlPlane</code> 部分。</p><div class="hljs code-wrapper"><pre><code class="hljs yaml"><span class="hljs-comment"># /etc/kubernetes/kubeadm-join.yaml</span><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">kubeadm.k8s.io/v1beta2</span><span class="hljs-attr">kind:</span> <span class="hljs-string">JoinConfiguration</span><span class="hljs-comment">#controlPlane:</span><span class="hljs-comment">#  localAPIEndpoint:</span><span class="hljs-comment">#    advertiseAddress: &quot;10.0.0.12&quot;</span><span class="hljs-comment">#    bindPort: 5443</span><span class="hljs-comment">#  certificateKey: 31f1e534733a1607e5ba67b2834edd3a7debba41babb1fac1bee47072a98d88b</span><span class="hljs-attr">discovery:</span>  <span class="hljs-attr">bootstrapToken:</span>    <span class="hljs-attr">apiServerEndpoint:</span> <span class="hljs-string">&quot;127.0.0.1:6443&quot;</span>    <span class="hljs-attr">token:</span> <span class="hljs-string">&quot;c2t0rj.cofbfnwwrb387890&quot;</span>    <span class="hljs-comment"># Please replace with the &quot;--discovery-token-ca-cert-hash&quot; value printed</span>    <span class="hljs-comment"># after the kubeadm init command is executed successfully</span>    <span class="hljs-attr">caCertHashes:</span>    <span class="hljs-bullet">-</span> <span class="hljs-string">&quot;sha256:97590810ae34a82501717e33acfca76f16044f1a365c5ad9a1c66433c386c75c&quot;</span><span class="hljs-attr">nodeRegistration:</span>  <span class="hljs-attr">criSocket:</span> <span class="hljs-string">unix:///run/containerd/containerd.sock</span>  <span class="hljs-attr">kubeletExtraArgs:</span>    <span class="hljs-attr">runtime-cgroups:</span> <span class="hljs-string">&quot;/system.slice/containerd.service&quot;</span>    <span class="hljs-attr">rotate-server-certificates:</span> <span class="hljs-string">&quot;true&quot;</span></code></pre></div><div class="hljs code-wrapper"><pre><code class="hljs sh">kubeadm join 127.0.0.1:6443 --config /etc/kubernetes/kubeadm-join.yaml --ignore-preflight-errors=Swap</code></pre></div><h3 id="2-8、其他处理"><a href="#2-8、其他处理" class="headerlink" title="2.8、其他处理"></a>2.8、其他处理</h3><p>由于 kubelet 开启了证书轮转，所以新集群会有大量 csr 请求，批量允许即可:</p><div class="hljs code-wrapper"><pre><code class="hljs sh">kubectl get csr | grep Pending | awk <span class="hljs-string">&#x27;&#123;print $1&#125;&#x27;</span> | xargs kubectl certificate approve</code></pre></div><p>同时为了 master 节点也能负载 pod，需要调整污点:</p><div class="hljs code-wrapper"><pre><code class="hljs sh">kubectl taint nodes --all node-role.kubernetes.io/master-</code></pre></div><p>后续 CNI 等不在本文内容范围内。</p><h2 id="三、Containerd-常用操作"><a href="#三、Containerd-常用操作" class="headerlink" title="三、Containerd 常用操作"></a>三、Containerd 常用操作</h2><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment"># 列出镜像</span>ctr images ls<span class="hljs-comment"># 列出 k8s 镜像</span>ctr -n k8s.io images ls<span class="hljs-comment"># 导入镜像</span>ctr -n k8s.io images import xxxx.tar<span class="hljs-comment"># 导出镜像</span>ctr -n k8s.io images <span class="hljs-built_in">export</span> kube-scheduler.tar k8s.gcr.io/kube-scheduler:v1.21.1</code></pre></div><h2 id="四、资源仓库"><a href="#四、资源仓库" class="headerlink" title="四、资源仓库"></a>四、资源仓库</h2><p>本文中所有 <code>*-pack</code> 仓库地址如下:</p><ul><li><a href="https://github.com/mritd/cfssl-pack">https://github.com/mritd/cfssl-pack</a></li><li><a href="https://github.com/mritd/etcd-pack">https://github.com/mritd/etcd-pack</a></li><li><a href="https://github.com/mritd/kube-apiserver-proxy-pack">https://github.com/mritd/kube-apiserver-proxy-pack</a></li><li><a href="https://github.com/mritd/kubeadm-config-pack">https://github.com/mritd/kubeadm-config-pack</a></li></ul>]]></content>
    
    
    <summary type="html">换到 Containerd 小半年了，没事写写。</summary>
    
    
    
    <category term="Kubernetes" scheme="https://mritd.com/categories/kubernetes/"/>
    
    
    <category term="Linux" scheme="https://mritd.com/tags/linux/"/>
    
    <category term="Kubernetes" scheme="https://mritd.com/tags/kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>MySQL 表结构对比</title>
    <link href="https://mritd.com/2021/05/29/mysql-schema-diff/"/>
    <id>https://mritd.com/2021/05/29/mysql-schema-diff/</id>
    <published>2021-05-29T10:12:00.000Z</published>
    <updated>2021-05-29T10:12:00.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="一、安装-mysql-schema-diff"><a href="#一、安装-mysql-schema-diff" class="headerlink" title="一、安装 mysql-schema-diff"></a>一、安装 mysql-schema-diff</h2><p>Ubuntu 20.04 系统使用如下命令安装:</p><div class="hljs code-wrapper"><pre><code class="hljs sh">apt install libmysql-diff-perl -y</code></pre></div><p>安装完成后使用 <code>--help</code> 应该能看到相关提示</p><div class="hljs code-wrapper"><pre><code class="hljs sh">➜ ~ mysql-schema-diff --<span class="hljs-built_in">help</span>Usage: mysql-schema-diff [ options ] &lt;database1&gt; &lt;database2&gt;Options:  -?,  --<span class="hljs-built_in">help</span>                show this <span class="hljs-built_in">help</span>  -A,  --apply               interactively patch database1 to match database2  -B,  --batch-apply         non-interactively patch database1 to match database2  -d,  --debug[=N]           <span class="hljs-built_in">enable</span> debugging [level N, default 1]  -l,  --list-tables         output the list off all used tables  -o,  --only-both           only output changes <span class="hljs-keyword">for</span> tables <span class="hljs-keyword">in</span> both databases  -k,  --keep-old-tables     don<span class="hljs-string">&#x27;t output DROP TABLE commands</span><span class="hljs-string">  -c,  --keep-old-columns    don&#x27;</span>t output DROP COLUMN commands  -n,  --no-old-defs         suppress comments describing old definitions  -t,  --table-re=REGEXP     restrict comparisons to tables matching REGEXP  -i,  --tolerant            ignore DEFAULT, AUTO_INCREMENT, COLLATE, and formatting changes  -S,  --single-transaction  perform DB dump <span class="hljs-keyword">in</span> transaction  -h,  --host=...            connect to host  -P,  --port=...            use this port <span class="hljs-keyword">for</span> connection  -u,  --user=...            user <span class="hljs-keyword">for</span> login <span class="hljs-keyword">if</span> not current user  -p,  --password[=...]      password to use when connecting to server  -s,  --socket=...          socket to use when connecting to server<span class="hljs-keyword">for</span> &lt;databaseN&gt; only, <span class="hljs-built_in">where</span> N == 1 or 2,       --hostN=...           connect to host       --portN=...           use this port <span class="hljs-keyword">for</span> connection       --userN=...           user <span class="hljs-keyword">for</span> login <span class="hljs-keyword">if</span> not current user       --passwordN[=...]     password to use when connecting to server       --socketN=...         socket to use when connecting to serverDatabases can be either files or database names.If there is an ambiguity, the file will be preferred;to prevent this prefix the database argument with `db:<span class="hljs-string">&#x27;.</span></code></pre></div><h2 id="二、生成差异-SQL"><a href="#二、生成差异-SQL" class="headerlink" title="二、生成差异 SQL"></a>二、生成差异 SQL</h2><p>安装完成后可直接使用该工具生成<strong>差异 SQL 文件</strong>，<code>mysql-schema-diff</code> 工具使用如下:</p><div class="hljs code-wrapper"><pre><code class="hljs sh">Usage: mysql-schema-diff [ options ] &lt;database1&gt; &lt;database2&gt;Options:  -?,  --<span class="hljs-built_in">help</span>                show this <span class="hljs-built_in">help</span>  -A,  --apply               interactively patch database1 to match database2  -B,  --batch-apply         non-interactively patch database1 to match database2  -d,  --debug[=N]           <span class="hljs-built_in">enable</span> debugging [level N, default 1]  -l,  --list-tables         output the list off all used tables  -o,  --only-both           only output changes <span class="hljs-keyword">for</span> tables <span class="hljs-keyword">in</span> both databases  -k,  --keep-old-tables     don<span class="hljs-string">&#x27;t output DROP TABLE commands</span><span class="hljs-string">  -c,  --keep-old-columns    don&#x27;</span>t output DROP COLUMN commands  -n,  --no-old-defs         suppress comments describing old definitions  -t,  --table-re=REGEXP     restrict comparisons to tables matching REGEXP  -i,  --tolerant            ignore DEFAULT, AUTO_INCREMENT, COLLATE, and formatting changes  -S,  --single-transaction  perform DB dump <span class="hljs-keyword">in</span> transaction  -h,  --host=...            connect to host  -P,  --port=...            use this port <span class="hljs-keyword">for</span> connection  -u,  --user=...            user <span class="hljs-keyword">for</span> login <span class="hljs-keyword">if</span> not current user  -p,  --password[=...]      password to use when connecting to server  -s,  --socket=...          socket to use when connecting to server<span class="hljs-keyword">for</span> &lt;databaseN&gt; only, <span class="hljs-built_in">where</span> N == 1 or 2,       --hostN=...           connect to host       --portN=...           use this port <span class="hljs-keyword">for</span> connection       --userN=...           user <span class="hljs-keyword">for</span> login <span class="hljs-keyword">if</span> not current user       --passwordN[=...]     password to use when connecting to server       --socketN=...         socket to use when connecting to server</code></pre></div><p><strong>通俗的说，通过 <code>--hostN</code> 等参数指定两个数据库地址，例如 <code>--password1</code> 指定第一个数据库密码，<code>--password2</code> 指定第二个数据库密码；然后最后仅跟 <code>数据库1[.表名] 数据库2[.表名]</code>，表名如果不写则默认对比两个数据库。</strong>以下为样例命令:</p><div class="hljs code-wrapper"><pre><code class="hljs sh">mysql-schema-diff \    --host1 127.0.0.1 --port1 3300 \    --user1 bleem --password1=Bleem77965badf \    --host2 127.0.0.1 --port2 3306 \    --user2 bleem --password2=asnfskdf667asd8 \    testdb testdb</code></pre></div><p>注意，首次运行后请根据生成的 SQL 判断<strong>对比是否正确</strong>，比如说<strong>想把比较新的测试库更改同步到生产库</strong>，那么 SQL 里全是<strong>DROP 字样的删除动作，这说明 <code>--hostN</code> 等参数指定反了(变成了生产库同步测试库)，此时只需要将 <code>--hostN</code> 参数调换一下即可(1改成2，2改成1)，这样生成的 SQL 就会变为 ADD 字样的添加动作。</strong></p><h2 id="三、网络问题"><a href="#三、网络问题" class="headerlink" title="三、网络问题"></a>三、网络问题</h2><p>mysql-schema-diff 工具需要在运行时能同时连接两个数据库，常规情况下可以通过 SSH 打洞来临时解决访问问题；<strong>如果实在无法打通网络环境，mysql-schema-diff 还支持文件对比</strong>，以下为一些文件对比的示例:</p><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment"># compare table definitions in two files</span>mysql-schema-diff db1.mysql db2.mysql<span class="hljs-comment"># compare table definitions in a file &#x27;db1.mysql&#x27; with a database &#x27;db2&#x27;</span>mysql-schema-diff db1.mysql db2<span class="hljs-comment"># interactively upgrade schema of database &#x27;db1&#x27; to be like the</span><span class="hljs-comment"># schema described in the file &#x27;db2.mysql&#x27;</span>mysql-schema-diff -A db1 db2.mysql<span class="hljs-comment"># compare table definitions in two databases on a remote machine</span>mysql-schema-diff --host=remote.host.com --user=myaccount db1 db2<span class="hljs-comment"># compare table definitions in a local database &#x27;foo&#x27; with a</span><span class="hljs-comment"># database &#x27;bar&#x27; on a remote machine, when a file foo already</span><span class="hljs-comment"># exists in the current directory</span>mysql-schema-diff --host2=remote.host.com --password=secret db:foo bar</code></pre></div>]]></content>
    
    
    <summary type="html">最近在疯狂码业务代码，由于开发频繁导致生产库结构与测试库不一样，改动太多不想手动搞，记录一下这个小工具。</summary>
    
    
    
    <category term="Linux" scheme="https://mritd.com/categories/linux/"/>
    
    
    <category term="Linux" scheme="https://mritd.com/tags/linux/"/>
    
    <category term="MySQL" scheme="https://mritd.com/tags/mysql/"/>
    
  </entry>
  
  <entry>
    <title>Longhorn 微服务化存储初试</title>
    <link href="https://mritd.com/2021/03/06/longhorn-storage-test/"/>
    <id>https://mritd.com/2021/03/06/longhorn-storage-test/</id>
    <published>2021-03-06T05:40:22.000Z</published>
    <updated>2021-03-06T05:40:22.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="一、Longhorn-安装"><a href="#一、Longhorn-安装" class="headerlink" title="一、Longhorn 安装"></a>一、Longhorn 安装</h2><h3 id="1-1、准备工作"><a href="#1-1、准备工作" class="headerlink" title="1.1、准备工作"></a>1.1、准备工作</h3><p>Longhorn 官方推荐的最小配置如下，如果数据并不算太重要可适当缩减和调整，具体请自行斟酌:</p><ul><li>3 Nodes</li><li>4 vCPUs per Node</li><li>4 GiB per Node</li><li>SSD/NVMe or similar performance block device on the node for storage(We don’t recommend using spinning disks with Longhorn, due to low IOPS.)</li></ul><p>本次安装测试环境如下:</p><ul><li>Ubuntu 20.04(8c16g)</li><li>Disk 200g</li><li>Kubernetes 1.20.4(kubeadm)</li><li>Longhorn 1.1.0</li></ul><h3 id="1-2、安装-Longhorn-Helm"><a href="#1-2、安装-Longhorn-Helm" class="headerlink" title="1.2、安装 Longhorn(Helm)"></a>1.2、安装 Longhorn(Helm)</h3><p>安装 Longhorn 推荐使用 Helm，因为在卸载时 kubectl 无法直接使用 delete 卸载，需要进行其他清理工作；helm 安装命令如下:</p><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment"># add Longhorn repo</span>helm repo add longhorn https://charts.longhorn.io<span class="hljs-comment"># update</span>helm repo update<span class="hljs-comment"># create namespace</span>kubectl create namespace longhorn-system<span class="hljs-comment"># install</span>helm install longhorn longhorn/longhorn --namespace longhorn-system --values longhorn-values.yaml</code></pre></div><p>其中 <code>longhorn-values.yaml</code> 请从 <a href="https://github.com/longhorn/charts/blob/longhorn-1.1.0/charts/longhorn/values.yaml">Charts 仓库</a> 下载，本文仅修改了以下两项:</p><div class="hljs code-wrapper"><pre><code class="hljs yaml"><span class="hljs-attr">defaultSettings:</span>  <span class="hljs-comment"># 默认存储目录(默认为 /var/lib/longhorn)</span>  <span class="hljs-attr">defaultDataPath:</span> <span class="hljs-string">&quot;/data/longhorn&quot;</span>  <span class="hljs-comment"># 默认副本数量</span>  <span class="hljs-attr">defaultReplicaCount:</span> <span class="hljs-number">2</span></code></pre></div><p>安装完成后 Pod 运行情况如下所示:</p><p><img src="https://cdn.oss.link/markdown/M5ZYyb1614931770247.png" alt="M5ZYyb1614931770247"></p><p>此后可通过集群 Ingress 或者 NodePort 等方式暴露 service <code>longhorn-frontend</code> 的 80 端口来访问 Longhorn UI；<strong>注意，Ingress 等负载均衡其如果采用 HTTPS 访问请确保向 Longhorn UI 传递了 <code>X-Forwarded-Proto: https</code> 头，否则可能导致 Websocket 不安全链接以及跨域等问题，后果就是 UI 出现一些神奇的小问题(我排查了好久…)。</strong></p><p><img src="https://cdn.oss.link/markdown/vIFU281614931897632.png" alt="vIFU281614931897632"></p><h3 id="1-3、卸载-Longhorn"><a href="#1-3、卸载-Longhorn" class="headerlink" title="1.3、卸载 Longhorn"></a>1.3、卸载 Longhorn</h3><p>如果在安装过程中有任何操作错误，或想重新安装验证相关设置，可通过以下命令卸载 Longhorn:</p><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment"># 卸载</span>helm uninstall longhorn -n longhorn-system</code></pre></div><h2 id="二、Longhorn-架构"><a href="#二、Longhorn-架构" class="headerlink" title="二、Longhorn 架构"></a>二、Longhorn 架构</h2><h3 id="2-1、Design"><a href="#2-1、Design" class="headerlink" title="2.1、Design"></a>2.1、Design</h3><p>Longhorn 总体设计分为两层: <strong>数据平面和控制平面</strong>；Longhorn Engine 是一个存储控制器，对应数据平面；Longhorn Manager 对应控制平面。</p><h4 id="2-1-1、Longhorn-Manager"><a href="#2-1-1、Longhorn-Manager" class="headerlink" title="2.1.1、Longhorn Manager"></a>2.1.1、Longhorn Manager</h4><p>Longhorn Manager 使用 Operator 模式，作为 Daemonset 运行在每个节点上；Longhorn Manager 负责接收 Longhorn UI 以及 Kubernetes Volume 插件的 API 调用，然后创建和管理 Volume；</p><p>Longhorn Manager 在与 kubernetes API 通信并创建 Longhorn Volume CRD(heml 安装直接创建了相关 CRD，查看代码后发现 Manager 里面似乎也会判断并创建一下)，此后 Longhorn Manager watch 相关 CRD 资源和 Kubernetes 原生资源(PV/PVC…)，一但集群内创建了 Longhorn Volume 则 Longhorn Manager 负责创建物理 Volume。</p><p>当 Longhorn Manager 创建 Volume 时，Longhorn Manager 首先会在 Volume 所在节点创建 Longhorn Engine 实例(对比实际行为后发现所谓的 “实例” 其实只是运行了一个 Linux 进程，并非创建 Pod)，然后根据副本数量在所需放置副本的节点上创建对应的副本。</p><h4 id="2-1-2、Longhorn-Engine"><a href="#2-1-2、Longhorn-Engine" class="headerlink" title="2.1.2、Longhorn Engine"></a>2.1.2、Longhorn Engine</h4><p>Longhorn Engine 始终与其使用 Volume 的 Pod 在同一节点上，它跨存储在多个节点上的多个副本同步复制卷；同时数据的多路径保证 Longhorn Volume 的 HA，单个副本或者 Engine 出现问题不会影响到所有副本或 Pod 对 Volume 的访问。</p><p>下图中展示了 Longhorn 的 HA 架构，每个 Kubernetes Volume 将会对应一个 Longhorn Engine，每个 Engine 管理 Volume 的多个副本，Engine 与 副本实质都会是一个单独的 Linux 进程运行:</p><p><img src="https://cdn.oss.link/markdown/Yxljct1614932095536.png" alt="Yxljct1614932095536"></p><p><strong>注意: 图中的 Engine 并非是单独的一个 Pod，而是每一个 Volume 会对应一个 golang exec 出来的 Linux 进程。</strong></p><h3 id="2-2、CSI-Plugin"><a href="#2-2、CSI-Plugin" class="headerlink" title="2.2、CSI Plugin"></a>2.2、CSI Plugin</h3><p>CSI 部分不做过多介绍，具体参考 <a href="https://mritd.com/2020/08/19/how-to-write-a-csi-driver-for-kubernetes/">如何编写 CSI 插件</a>；以下为简要说明:</p><ul><li>Kubernetes CSI 被抽象为具体的 CSI 容器并通过 gRPC 调用目标 plugin</li><li>Longhorn CSI Plugin 负责接收标准 CSI 容器发起的 gRPC 调用</li><li>Longhorn CSI Plugin 将 Kubernetes CSI gRPC 调用转换为自己的 Longhorn API 调用，并将其转发到 Longhorn Manager 控制平面</li><li>Longhorn 某些功能使用了 iSCSI，所以可能需要在节点上安装 open-iscsi 或 iscsiadm</li></ul><h3 id="2-3、Longhorn-UI"><a href="#2-3、Longhorn-UI" class="headerlink" title="2.3、Longhorn UI"></a>2.3、Longhorn UI</h3><p>Longhorn UI 向外暴露一个 Dashboard，并用过 Longhorn API 与 Longhorn Manager 控制平面交互；Longhorn UI 在架构上类似于 Longhorn CSI Plugin 的替代者，只不过一个是通过 Web UI 转化为 Longhorn API，另一个是将 CSI gRPC 转换为 Longhorn API。</p><h3 id="2-4、Replicas-And-Snapshots"><a href="#2-4、Replicas-And-Snapshots" class="headerlink" title="2.4、Replicas And Snapshots"></a>2.4、Replicas And Snapshots</h3><p>在 Longhorn 微服务架构中，副本也作为单独的进程运行，其实质存储文件采用 Linux 的稀释文件方式；每个副本均包含 Longhorn Volume 的快照链，快照就像一个 Image 层，其中最旧的快照用作基础层，而较新的快照位于顶层。如果数据会覆盖旧快照中的数据，则仅将其包含在新快照中；整个快照链展示了数据的当前状态。</p><p>在进行快照时，Longhorn 会创建差异磁盘(differencing disk)文件，每个差异磁盘文件被看作是一个快照，当 Longhorn 读取文件时从上层开始依次查找，其示例图如下:</p><p><img src="https://cdn.oss.link/markdown/tOvsJ91614932116138.jpg" alt="tOvsJ91614932116138"></p><p><strong>为了提高读取性能，Longhorn 维护了一个读取索引，该索引记录了每个 4K 存储块中哪个差异磁盘包含有效数据；读取索引会占用一定的内存，每个 4K 块占用一个字节，字节大小的读取索引意味着每个卷最多可以拍摄 254 个快照，在大约 1TB 的卷中读取索引大约会消耗256MB 的内存。</strong></p><h3 id="2-5、Backups-and-Secondary-Storage"><a href="#2-5、Backups-and-Secondary-Storage" class="headerlink" title="2.5、Backups and Secondary Storage"></a>2.5、Backups and Secondary Storage</h3><p>由于数据大小、网络延迟等限制，跨区域同步复制无法做到很高的时效性，所以 Longhorn 提供了称之为 Secondary Storage 的备份方案；Secondary Storage 依赖外部的 NFS、S3 等存储设施，一旦在 Longhorn 中配置了 Backup Storage，Longhorn 将会通过卷的指定版本快照完成备份；<strong>备份过程中 Longhorn 将会抹平快照信息，这意味着快照历史变更将会丢失，相同的原始卷备份是增量的，通过不断的应用差异磁盘文件完成；为了避免海量小文件带来的性能瓶颈，Longhorn 采用 2MB 分块进行备份，任何边界内 4k 块变动都会触发 2MB 块的备份行为；Longhorn 的备份功能为跨集群、跨区域提供完善的灾难恢复机制。</strong>Longhorn 备份机制如下图所示:</p><p><img src="https://cdn.oss.link/markdown/6EShuz1614932147884.jpg" alt="6EShuz1614932147884"></p><h3 id="2-6、Longhorn-Pods"><a href="#2-6、Longhorn-Pods" class="headerlink" title="2.6、Longhorn Pods"></a>2.6、Longhorn Pods</h3><p>上面的大部分其实来源于对官方文档 <a href="https://longhorn.io/docs/1.1.0/concepts/">Architecture and Concepts</a> 的翻译；在翻译以及阅读文档过程中，通过对比文档与实际行为，还有阅读源码发现了一些细微差异，这里着重介绍一下这些 Pod 都是怎么回事:</p><p><img src="https://cdn.oss.link/markdown/1EUF4y-1615008575-QcVGUt.png" alt="1EUF4y-1615008575-QcVGUt"></p><h4 id="2-6-1、longhorn-manager"><a href="#2-6-1、longhorn-manager" class="headerlink" title="2.6.1、longhorn-manager"></a>2.6.1、longhorn-manager</h4><p>longhorn-manager 与文档描述一致，其通过 Helm 安装时直接以 Daemonset 方式 Create 出来，然后 longhorn-manager 开启 HTTP API(9500) 等待其他组件请求；<strong>同时 longhorn-manager 还会使用 Operator 模式监听各种资源，包括不限于 Longhorn CRD 以及集群的 PV(C) 等资源，然后作出对应的响应。</strong></p><h4 id="2-6-2、longhorn-driver-deployer"><a href="#2-6-2、longhorn-driver-deployer" class="headerlink" title="2.6.2、longhorn-driver-deployer"></a>2.6.2、longhorn-driver-deployer</h4><p>Helm 安装时创建了 longhorn-driver-deployer Deployment，longhorn-driver-deployer 实际上也是 longhorn-manager 镜像启动，只不过启动后会沟通 longhorn-manager HTTP API，然后创建所有 CSI 相关容器，包括 <code>csi-provisioner</code>、<code>csi-snapshotter</code>、<code>longhorn-csi-plugin</code> 等。</p><h4 id="2-6-3、instance-manager-e"><a href="#2-6-3、instance-manager-e" class="headerlink" title="2.6.3、instance-manager-e"></a>2.6.3、instance-manager-e</h4><p>上面所说的每个 Engine 对应一个 Linux 进程其实就是通过这个 Pod 完成的，<strong>instance-manager-e 由 longhorn-manager 创建，创建完成后 instance-manager-e 监听 gRPC 8500 端口，其只要职责就是接收 gRPC 请求，并启动 Engine 进程；从上面我们 Engine 介绍可以得知 Engine 与 Volume 绑定，所以理论上集群内 Volume 被创建时有某个 “东西” 创建了 CRD <code>engines.longhorn.io</code>，然后又有人 watch 了 <code>engines.longhorn.io</code> 并通知 instance-manager-e 启动 Engine 进程；这里不负责任的推测是 longhorn-manager 干的，但是没看代码不敢说死…</strong></p><p><img src="https://cdn.oss.link/markdown/hfk9uQ1614943191222.png" alt="hfk9uQ1614943191222"></p><p>同理 <code>instance-manager-r</code> 是负责启动副本的 Linux 进程的，工作原理与 <code>instance-manager-e</code> 相同，通过简单的查看代码(IDE 没打开…哈哈哈)推测，<code>instance-manager-e/-r</code> 应该是 longhorn-manager Operator 下的产物，其维护了一个自己的 “Daemonset”，但是 kubectl 是看不到的。</p><h4 id="2-6-4、longhorn-ui"><a href="#2-6-4、longhorn-ui" class="headerlink" title="2.6.4、longhorn-ui"></a>2.6.4、longhorn-ui</h4><p>longhorn-ui 很简单，就是个 UI 界面，然后 HTTP API 沟通 longhorn-manager，这里不再做过多说明。</p><h2 id="三、Longhorn-使用"><a href="#三、Longhorn-使用" class="headerlink" title="三、Longhorn 使用"></a>三、Longhorn 使用</h2><h3 id="3-1、常规使用"><a href="#3-1、常规使用" class="headerlink" title="3.1、常规使用"></a>3.1、常规使用</h3><p>默认情况下 Helm 安装完成后会自动创建 StorageClass，如果集群中只有 Longhorn 作为存储，那么 Longhorn 的 StorageClass 将作为默认 StorageClass。关于 StorageClass、PV、PVC 如果使用这里不做过多描述，请参考官方 <a href="https://longhorn.io/docs/1.1.0/references/examples/">Example</a> 文档；</p><p><strong>需要注意的是 Longhorn 作为块存储仅支持 <code>ReadWriteOnce</code> 模式，如果想支持 <code>ReadWriteMany</code> 模式，则需要在节点安装 <code>nfs-common</code>，Longhorn 将会自动创建 <code>share-manager</code> 容器然后通过 NFSV4 共享这个 Volume 从而实现 <code>ReadWriteMany</code>；</strong>具体请参考 <a href="https://longhorn.io/docs/1.1.0/advanced-resources/rwx-workloads/">Support for ReadWriteMany (RWX) workloads</a>。</p><h3 id="3-2、添加删除磁盘"><a href="#3-2、添加删除磁盘" class="headerlink" title="3.2、添加删除磁盘"></a>3.2、添加删除磁盘</h3><p>如果出现磁盘损坏重建或者添加删除磁盘，请直接访问 UI 界面，通过下拉菜单操作即可；在操作前请将节点调整到维护模式并驱逐副本，具体请参考 <a href="https://longhorn.io/docs/1.1.0/volumes-and-nodes/disks-or-nodes-eviction/">Evicting Replicas on Disabled Disks or Nodes</a>。</p><p><img src="https://cdn.oss.link/markdown/0tedBl1614945096177.png" alt="0tedBl1614945096177"></p><p><strong>需要注意的是添加新磁盘时，磁盘挂载的软连接路径不能工作，请使用原始挂载路径或通过 <code>mount --bind</code> 命令设置新路径。</strong></p><p><img src="https://cdn.oss.link/markdown/Ps8QRR1614945234562.png" alt="Ps8QRR1614945234562"></p><h3 id="3-3、创建快照及回滚"><a href="#3-3、创建快照及回滚" class="headerlink" title="3.3、创建快照及回滚"></a>3.3、创建快照及回滚</h3><p>当创建好 Volume 以后可以用过 Longhorn UI 在线对 Volume 创建快照，<strong>但是回滚快照过程需要 Workload(Pod) 离线，同时 Volume 必须以维护模式 reattach 到某一个 Host 节点上，然后在 Longhorn UI 进行调整；</strong>以下为快照创建回滚测试:</p><p><strong>test.pvc.yaml</strong></p><div class="hljs code-wrapper"><pre><code class="hljs yaml"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span><span class="hljs-attr">kind:</span> <span class="hljs-string">PersistentVolumeClaim</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">longhorn-simple-pvc</span><span class="hljs-attr">spec:</span>  <span class="hljs-attr">accessModes:</span>    <span class="hljs-bullet">-</span> <span class="hljs-string">ReadWriteOnce</span>  <span class="hljs-attr">storageClassName:</span> <span class="hljs-string">longhorn</span>  <span class="hljs-attr">resources:</span>    <span class="hljs-attr">requests:</span>      <span class="hljs-attr">storage:</span> <span class="hljs-string">1Gi</span></code></pre></div><p><strong>test.po.yaml</strong></p><div class="hljs code-wrapper"><pre><code class="hljs yaml"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span><span class="hljs-attr">kind:</span> <span class="hljs-string">Pod</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">longhorn-simple-pod</span>  <span class="hljs-attr">namespace:</span> <span class="hljs-string">default</span><span class="hljs-attr">spec:</span>  <span class="hljs-attr">restartPolicy:</span> <span class="hljs-string">Always</span>  <span class="hljs-attr">containers:</span>    <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">volume-test</span>      <span class="hljs-attr">image:</span> <span class="hljs-string">nginx:stable-alpine</span>      <span class="hljs-attr">imagePullPolicy:</span> <span class="hljs-string">IfNotPresent</span>      <span class="hljs-attr">livenessProbe:</span>        <span class="hljs-attr">exec:</span>          <span class="hljs-attr">command:</span>            <span class="hljs-bullet">-</span> <span class="hljs-string">ls</span>            <span class="hljs-bullet">-</span> <span class="hljs-string">/data/lost+found</span>        <span class="hljs-attr">initialDelaySeconds:</span> <span class="hljs-number">5</span>        <span class="hljs-attr">periodSeconds:</span> <span class="hljs-number">5</span>      <span class="hljs-attr">volumeMounts:</span>        <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">volv</span>          <span class="hljs-attr">mountPath:</span> <span class="hljs-string">/data</span>      <span class="hljs-attr">ports:</span>        <span class="hljs-bullet">-</span> <span class="hljs-attr">containerPort:</span> <span class="hljs-number">80</span>  <span class="hljs-attr">volumes:</span>    <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">volv</span>      <span class="hljs-attr">persistentVolumeClaim:</span>        <span class="hljs-attr">claimName:</span> <span class="hljs-string">longhorn-simple-pvc</span></code></pre></div><h4 id="3-3-1、创建快照"><a href="#3-3-1、创建快照" class="headerlink" title="3.3.1、创建快照"></a>3.3.1、创建快照</h4><p>首先创建相关资源:</p><div class="hljs code-wrapper"><pre><code class="hljs sh">kubectl create -f test.pvc.yamlkubectl create -f test.po.yaml</code></pre></div><p>创建完成后在 Longhorn UI 中可以看到刚刚创建出的 Volume:</p><p><img src="https://cdn.oss.link/markdown/L3hnYi-1615000011-UBMIEE.png" alt="L3hnYi-1615000011-UBMIEE"></p><p>点击 Name 链接进入到 Volume 详情，然后点击 <code>Take Snapshot</code> 按钮即可拍摄快照；<strong>有些情况下 UI 响应缓慢可能导致 <code>Take Snapshot</code> 按钮变灰，刷新两次即可恢复。</strong></p><p><img src="https://cdn.oss.link/markdown/TkKNCz-1615001806-jjaakH.png" alt="TkKNCz-1615001806-jjaakH"></p><p>快照在回滚后仍然可以进行交叉创建</p><p><img src="https://cdn.oss.link/markdown/ykQs66-1615002223-B2Z9Sa.png" alt="ykQs66-1615002223-B2Z9Sa"></p><h4 id="3-3-2、回滚快照"><a href="#3-3-2、回滚快照" class="headerlink" title="3.3.2、回滚快照"></a>3.3.2、回滚快照</h4><p><strong>回滚快照时必须停止 Pod:</strong></p><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment"># 停止</span>kubectl delete -f test.po.yaml</code></pre></div><p><strong>然后重新将 Volume Attach 到宿主机:</strong></p><p><img src="https://cdn.oss.link/markdown/uuAFT8-1615001937-bGA3Sq.png" alt="uuAFT8-1615001937-bGA3Sq"></p><p><strong>注意要开启维护模式</strong></p><p><img src="https://cdn.oss.link/markdown/bFcbW3-1615002020-Q8nG15.png" alt="bFcbW3-1615002020-Q8nG15"></p><p>稍等片刻等待所有副本 “Running” 然后 Revert 即可</p><p><img src="https://cdn.oss.link/markdown/xLZdSP-1615002098-p8sueg.png" alt="xLZdSP-1615002098-p8sueg"></p><p>回滚完成后，需要 Detach Volume，以便供重新创建的 Pod 使用</p><p><img src="https://cdn.oss.link/markdown/JYZZLe-1615002351-x6A6TE.png" alt="JYZZLe-1615002351-x6A6TE"></p><h4 id="3-3-3、定时快照"><a href="#3-3-3、定时快照" class="headerlink" title="3.3.3、定时快照"></a>3.3.3、定时快照</h4><p>除了手动创建快照之外，Longhorn 还支持定时对 Volume 进行快照处理；要使用定时任务，请进入 Volume 详情页面，在 <code>Recurring Snapshot and Backup Schedule</code> 选项卡下新增定时任务即可:</p><p><img src="https://cdn.oss.link/markdown/iUpPnA-1615006838-wM4tBA.png" alt="iUpPnA-1615006838-wM4tBA"></p><p><strong>如果不想为内核 Volume 都手动设置自动快照，可以用过调整 StorageClass 来实现为每个自动创建的 PV 进行自动快照，具体请阅读 <a href="https://longhorn.io/docs/1.1.0/snapshots-and-backups/scheduling-backups-and-snapshots/#set-up-recurring-jobs-using-a-storageclass">Set up Recurring Jobs using a StorageClass</a> 文档。</strong></p><h3 id="3-4、Volume-扩容"><a href="#3-4、Volume-扩容" class="headerlink" title="3.4、Volume 扩容"></a>3.4、Volume 扩容</h3><p>Longhorn 支持对 Volume 进行扩容，扩容方式和回滚快照类似，都需要 Deacth Volume 并开启维护模式。</p><p><strong>首先停止 Workload</strong></p><div class="hljs code-wrapper"><pre><code class="hljs sh">➜ ~ kubectl <span class="hljs-built_in">exec</span> -it longhorn-simple-pod -- df -hFilesystem                Size      Used Available Use% Mounted onoverlay                 199.9G      4.7G    195.2G   2% /tmpfs                    64.0M         0     64.0M   0% /devtmpfs                     7.8G         0      7.8G   0% /sys/fs/cgroup/dev/longhorn/pvc-1c9e23f4-af29-4a48-9560-87983267b8d3                        975.9M      2.5M    957.4M   0% /data/dev/sda4                60.0G      7.7G     52.2G  13% /etc/hosts/dev/sda4                60.0G      7.7G     52.2G  13% /dev/termination-log/dev/sdc1               199.9G      4.7G    195.2G   2% /etc/hostname/dev/sdc1               199.9G      4.7G    195.2G   2% /etc/resolv.confshm                      64.0M         0     64.0M   0% /dev/shmtmpfs                     7.8G     12.0K      7.8G   0% /run/secrets/kubernetes.io/serviceaccounttmpfs                     7.8G         0      7.8G   0% /proc/acpitmpfs                    64.0M         0     64.0M   0% /proc/kcoretmpfs                    64.0M         0     64.0M   0% /proc/keystmpfs                    64.0M         0     64.0M   0% /proc/timer_listtmpfs                    64.0M         0     64.0M   0% /proc/sched_debugtmpfs                     7.8G         0      7.8G   0% /proc/scsitmpfs                     7.8G         0      7.8G   0% /sys/firmware➜ ~ kubectl delete -f test.po.yamlpod <span class="hljs-string">&quot;longhorn-simple-pod&quot;</span> deleted</code></pre></div><p><strong>然后直接使用 <code>kubectl</code> 编辑 PVC，调整 <code>spec.resources.requests.storage</code></strong></p><p><img src="https://cdn.oss.link/markdown/LT9aSo-1615006002-L1veZr.png" alt="LT9aSo-1615006002-L1veZr"></p><p>保存后可以从 Longhorn UI 中看到 Volume 在自动 resize</p><p><img src="https://cdn.oss.link/markdown/zG4ugo-1615006070-f7POdb.png" alt="zG4ugo-1615006070-f7POdb"></p><p>重新创建 Workload 可以看到 Volume 已经扩容成功</p><div class="hljs code-wrapper"><pre><code class="hljs sh">➜ ~ kubectl create -f test.po.yamlpod/longhorn-simple-pod created➜ ~ kubectl <span class="hljs-built_in">exec</span> -it longhorn-simple-pod -- df -hFilesystem                Size      Used Available Use% Mounted onoverlay                 199.9G      6.9G    193.0G   3% /tmpfs                    64.0M         0     64.0M   0% /devtmpfs                     7.8G         0      7.8G   0% /sys/fs/cgroup/dev/longhorn/pvc-1c9e23f4-af29-4a48-9560-87983267b8d3                          4.9G      4.0M      4.9G   0% /data/dev/sda4                60.0G      7.6G     52.4G  13% /etc/hosts/dev/sda4                60.0G      7.6G     52.4G  13% /dev/termination-log/dev/sdc1               199.9G      6.9G    193.0G   3% /etc/hostname/dev/sdc1               199.9G      6.9G    193.0G   3% /etc/resolv.confshm                      64.0M         0     64.0M   0% /dev/shmtmpfs                     7.8G     12.0K      7.8G   0% /run/secrets/kubernetes.io/serviceaccounttmpfs                     7.8G         0      7.8G   0% /proc/acpitmpfs                    64.0M         0     64.0M   0% /proc/kcoretmpfs                    64.0M         0     64.0M   0% /proc/keystmpfs                    64.0M         0     64.0M   0% /proc/timer_listtmpfs                    64.0M         0     64.0M   0% /proc/sched_debugtmpfs                     7.8G         0      7.8G   0% /proc/scsitmpfs                     7.8G         0      7.8G   0% /sys/firmware</code></pre></div><p><strong>Volume 扩展过程中 Longhorn 会自动处理文件系统相关调整，但是并不是百分百会处理，一般 Longhorn 仅在以下情况做自动处理：</strong></p><ul><li>扩展后大小大约当前大小(进行扩容)</li><li>Longhorn Volume 中存在一个 Linux 文件系统</li><li>Longhorn Volume 中的 Linux 文件系统为 ext4 或 xfs</li><li>Longhorn Volume 使用 <code>block device</code> 作为 frontend</li></ul><p>非这几种情况外，如还原到更小容量的 Snapshot，可能需要手动调整文件系统，具体请参考 <a href="https://longhorn.io/docs/1.1.0/volumes-and-nodes/expansion/#filesystem-expansion">Filesystem expansion</a> 章节文档。</p><h2 id="四、总结"><a href="#四、总结" class="headerlink" title="四、总结"></a>四、总结</h2><p>总体来说目前 Longhorn 是一个比较清量级的存储解决方案，微服务化使其更加可靠，同时官方文档完善社区响应也比较迅速；最主要的是 Longhorn 采用的技术方案不会过于复杂，通过文档以及阅读源码至少可以比较快速的了解其背后实现，而反观一些其他大型存储要么文档不全，要么实现技术复杂，普通用户很难窥视其核心；综合来说在小型存储选择上比较推荐 Longhorn，至于稳定性么，很不负责的说我也不知道，毕竟我也是新手，备份还没折腾呢…</p>]]></content>
    
    
    <summary type="html">Longhorn 是 Rancher Labs 开源的一个轻量级云原生微服务化存储方案，本篇文章将详细介绍 Longhorn 安装使用以及其设计架构</summary>
    
    
    
    <category term="Kubernetes" scheme="https://mritd.com/categories/kubernetes/"/>
    
    
    <category term="Kubernetes" scheme="https://mritd.com/tags/kubernetes/"/>
    
    <category term="CSI" scheme="https://mritd.com/tags/csi/"/>
    
    <category term="Longhorn" scheme="https://mritd.com/tags/longhorn/"/>
    
  </entry>
  
  <entry>
    <title>Caddy2 简明教程</title>
    <link href="https://mritd.com/2021/01/07/lets-start-using-caddy2/"/>
    <id>https://mritd.com/2021/01/07/lets-start-using-caddy2/</id>
    <published>2021-01-07T09:44:30.000Z</published>
    <updated>2021-01-07T09:44:30.000Z</updated>
    
    <content type="html"><![CDATA[<p>Caddy 是一个 Go 编写的 Web 服务器，类似于 Nginx，Caddy 提供了更加强大的功能，随着 v2 版本发布 Caddy 已经可以作为中小型站点 Web 服务器的另一个选择；相较于 Nginx 来说使用 Caddy 的优势如下:</p><ul><li>自动的 HTTPS 证书申请(ACME HTTP/DNS 挑战)</li><li>自动证书续期以及 OCSP stapling 等</li><li>更高的安全性包括但不限于 TLS 配置以及内存安全等</li><li>友好且强大的配置文件支持</li><li>支持 API 动态调整配置(有木有人可以搞个 Dashboard？)</li><li>支持 HTTP3(QUIC)</li><li>支持动态后端，例如连接 Consul、作为 k8s ingress 等</li><li>后端多种负载策略以及健康检测等</li><li>本身 Go 编写，高度模块化的系统方便扩展(CoreDNS 基于 Caddy1 开发)</li><li>……</li></ul><p>就目前来说，Caddy 对于我个人印象唯一的缺点就是性能没有 Nginx 高，但是这是个仁者见仁智者见智的问题；相较于提供的这些便利性，在性能可接受的情况下完全有理由切换到 Caddy。</p><h2 id="一、编译-Caddy2"><a href="#一、编译-Caddy2" class="headerlink" title="一、编译 Caddy2"></a>一、编译 Caddy2</h2><blockquote><p>注意: 在 Caddy1 时代，Caddy 官方发布的预编译二进制文件是不允许进行商业使用的，Caddy2 以后已经全部切换到 Apache 2.0 License，具体请参考 <a href="https://github.com/caddyserver/caddy/issues/2786">issue#2786</a>。</p></blockquote><p>在默认情况下 Caddy2 官方提供了预编译的二进制文件，以及<a href="https://caddyserver.com/download">自定义 build 下载页面</a>，不过对于需要集成一些第三方插件时，我们仍需采用官方提供的 <a href="https://github.com/caddyserver/xcaddy">xcaddy</a> 来进行自行编译；以下为具体的编译过程:</p><h3 id="1-1、Golang-环境安装"><a href="#1-1、Golang-环境安装" class="headerlink" title="1.1、Golang 环境安装"></a>1.1、Golang 环境安装</h3><blockquote><p><strong>本部分编译环境默认为 Ubuntu 20.04 系统，同时使用 root 用户，其他环境请自行调整相关目录以及配置；编译时自行处理好科学上网相关配置，也可以直接用国外 VPS 服务器编译。</strong></p></blockquote><p>首先下载 go 语言的 SDK 压缩包，其他平台可以从 <a href="https://golang.org/dl/">https://golang.org/dl/</a> 下载对应的压缩包:</p><div class="hljs code-wrapper"><pre><code class="hljs sh">wget https://golang.org/dl/go1.15.6.linux-amd64.tar.gz</code></pre></div><p>下载完成后解压并配置相关变量:</p><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment"># 解压</span>tar -zxvf go1.15.6.linux-amd64.tar.gz<span class="hljs-comment"># 移动到任意目录</span>mkdir -p /opt/devtoolsmv go /opt/devtools/go<span class="hljs-comment"># 创建 go 相关目录</span>mkdir -p <span class="hljs-variable">$&#123;HOME&#125;</span>/gopath/&#123;src,bin,pkg&#125;<span class="hljs-comment"># 调整变量配置，将以下变量加入到 shell 初始化配置中</span><span class="hljs-comment"># bash 用户请编辑 ~/.bashrc</span><span class="hljs-comment"># zsh 用户请编辑 ~/.zshrc</span><span class="hljs-built_in">export</span> GOROOT=<span class="hljs-string">&#x27;/opt/devtools/go&#x27;</span><span class="hljs-built_in">export</span> GOPATH=<span class="hljs-string">&quot;<span class="hljs-variable">$&#123;HOME&#125;</span>/gopath&quot;</span><span class="hljs-built_in">export</span> GOPROXY=<span class="hljs-string">&#x27;https://goproxy.cn&#x27;</span> <span class="hljs-comment"># 如果已经解决了科学上网问题，GOPROXY 变量可以删除，否则可能会起反作用</span><span class="hljs-built_in">export</span> PATH=<span class="hljs-string">&quot;<span class="hljs-variable">$&#123;GOROOT&#125;</span>/bin:<span class="hljs-variable">$&#123;GOPATH&#125;</span>/bin:<span class="hljs-variable">$&#123;PATH&#125;</span>&quot;</span><span class="hljs-comment"># 让配置生效</span><span class="hljs-comment"># bash 用户替换成 ~/.basrc</span><span class="hljs-comment"># 重新退出登录也可以</span><span class="hljs-built_in">source</span> ~/.zshrc</code></pre></div><p>配置完成后，应该在命令行执行 <code>go version</code> 有成功返回:</p><div class="hljs code-wrapper"><pre><code class="hljs sh">bleem ➜ ~ go versiongo version go1.15.6 linux/amd64</code></pre></div><h3 id="1-2、安装-xcaddy"><a href="#1-2、安装-xcaddy" class="headerlink" title="1.2、安装 xcaddy"></a>1.2、安装 xcaddy</h3><p>按照官方文档直接命令行执行 <code>go get -u github.com/caddyserver/xcaddy/cmd/xcaddy</code> 安装即可:</p><div class="hljs code-wrapper"><pre><code class="hljs sh">bleem ➜ ~ go get -u github.com/caddyserver/xcaddy/cmd/xcaddygo: downloading github.com/caddyserver/xcaddy v0.1.7go: found github.com/caddyserver/xcaddy/cmd/xcaddy <span class="hljs-keyword">in</span> github.com/caddyserver/xcaddy v0.1.7go: downloading github.com/Masterminds/semver/v3 v3.1.0go: github.com/Masterminds/semver/v3 upgrade =&gt; v3.1.1go: downloading github.com/Masterminds/semver/v3 v3.1.1.....</code></pre></div><p>安装完成后应当在命令行可以直接执行 <code>xcaddy</code> 命令:</p><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment"># xcaddy 并没有提供完善的命令行支持，所以 `--help` 报错很正常</span>bleem ➜  ~ xcaddy --<span class="hljs-built_in">help</span>go: cannot match <span class="hljs-string">&quot;all&quot;</span>: working directory is not part of a module2021/01/07 12:15:56 [ERROR] <span class="hljs-built_in">exec</span> [go list -m -f=&#123;&#123;<span class="hljs-keyword">if</span> .Replace&#125;&#125;&#123;&#123;.Path&#125;&#125; =&gt; &#123;&#123;.Replace&#125;&#125;&#123;&#123;end&#125;&#125; all]: <span class="hljs-built_in">exit</span> status 1:</code></pre></div><h3 id="1-3、编译-Caddy2"><a href="#1-3、编译-Caddy2" class="headerlink" title="1.3、编译 Caddy2"></a>1.3、编译 Caddy2</h3><p>编译之前系统需要安装 <code>jq</code>、<code>curl</code>、<code>git</code> 命令，没有的请使用 <code>apt install -y curl git jq</code> 命令安装；</p><p>自行编译的目的是增加第三方插件方便使用，其中官方列出的插件可以从 <a href="https://caddyserver.com/download">Download</a> 页面获取到:</p><p><img src="https://cdn.oss.link/markdown/XkJZ2R1609993722272.png" alt="XkJZ2R1609993722272"></p><p>其他插件可以从 GitHub 上寻找或者自行编写，整理好这些插件列表以后只需要使用 <code>xcaddy</code> 编译即可:</p><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment"># 获取最新版本号，其实直接去 GitHub realse 页复制以下就行</span><span class="hljs-comment"># 这里转化为脚本是为了方便自动化</span><span class="hljs-built_in">export</span> version=$(curl -s <span class="hljs-string">&quot;https://api.github.com/repos/caddyserver/caddy/releases/latest&quot;</span> | jq -r .tag_name)<span class="hljs-comment"># 使用 xcaddy 编译</span>xcaddy build <span class="hljs-variable">$&#123;version&#125;</span> --output ./caddy_<span class="hljs-variable">$&#123;version&#125;</span> \        --with github.com/abiosoft/caddy-exec \        --with github.com/caddy-dns/cloudflare \        --with github.com/caddy-dns/dnspod \        --with github.com/caddy-dns/duckdns \        --with github.com/caddy-dns/gandi \        --with github.com/caddy-dns/route53 \        --with github.com/greenpau/caddy-auth-jwt \        --with github.com/greenpau/caddy-auth-portal \        --with github.com/greenpau/caddy-trace \        --with github.com/hairyhenderson/caddy-teapot-module \        --with github.com/kirsch33/realip \        --with github.com/porech/caddy-maxmind-geolocation \        --with github.com/caddyserver/format-encoder \        --with github.com/mholt/caddy-webdav</code></pre></div><p>编译过程日志如下所示，稍等片刻后将会生成编译好的二进制文件:</p><p><img src="https://cdn.oss.link/markdown/Kr2tG61609993987722.png" alt="Kr2tG61609993987722"></p><p>编译成功后可以通过 <code>list-modules</code> 子命令查看被添加的插件是否成功编译到了 caddy 中:</p><div class="hljs code-wrapper"><pre><code class="hljs sh">bleem ➜  ~ ./caddy_v2.3.0 list-modulesadmin.api.loadadmin.api.metricscaddy.adapters.caddyfilecaddy.listeners.tlscaddy.logging.encoders.consolecaddy.logging.encoders.filtercaddy.logging.encoders.filter.deletecaddy.logging.encoders.filter.ip_maskcaddy.logging.encoders.formattedcaddy.logging.encoders.jsoncaddy.logging.encoders.logfmtcaddy.logging.encoders.single_fieldcaddy.logging.writers.discardcaddy.logging.writers.filecaddy.logging.writers.netcaddy.logging.writers.stderrcaddy.logging.writers.stdoutcaddy.storage.file_systemdns.providers.cloudflaredns.providers.dnspoddns.providers.duckdnsdns.providers.gandidns.providers.route53<span class="hljs-built_in">exec</span>httphttp.authentication.hashes.bcrypthttp.authentication.hashes.scrypthttp.authentication.providers.http_basichttp.authentication.providers.jwt......</code></pre></div><h2 id="二、安装-Caddy2"><a href="#二、安装-Caddy2" class="headerlink" title="二、安装 Caddy2"></a>二、安装 Caddy2</h2><h3 id="2-1、宿主机安装"><a href="#2-1、宿主机安装" class="headerlink" title="2.1、宿主机安装"></a>2.1、宿主机安装</h3><p>宿主机安装 Caddy2 需要使用 systemd 进行守护，幸运的是 Caddy2 官方提供了各种平台的安装包以及 <a href="https://github.com/caddyserver/dist">systemd 配置文件仓库</a>；目前推荐的方式是直接采用包管理器安装标准版本的 Caddy2，然后替换自编译的可执行文件:</p><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment"># 安装标准版本 Caddy2</span>sudo apt install -y debian-keyring debian-archive-keyring apt-transport-httpscurl -1sLf <span class="hljs-string">&#x27;https://dl.cloudsmith.io/public/caddy/stable/cfg/gpg/gpg.155B6D79CA56EA34.key&#x27;</span> | sudo apt-key add -curl -1sLf <span class="hljs-string">&#x27;https://dl.cloudsmith.io/public/caddy/stable/cfg/setup/config.deb.txt?distro=debian&amp;version=any-version&#x27;</span> | sudo tee -a /etc/apt/sources.list.d/caddy-stable.listsudo apt updatesudo apt install caddy<span class="hljs-comment"># 替换二进制文件</span>systemctl stop caddyrm -f /usr/bin/caddymv ./caddy_v2.3.0 /usr/bin/caddy</code></pre></div><h3 id="2-2、Docker-安装"><a href="#2-2、Docker-安装" class="headerlink" title="2.2、Docker 安装"></a>2.2、Docker 安装</h3><p>Docker 用户可以通过 Dockerfile 自行编译 image，目前我编写了一个基于 xcaddy 的 <a href="https://github.com/mritd/dockerfile/blob/master/caddy/Dockerfile">Dockerfile</a>，如果有其他插件需要集成自行修改重新编译即可；当前 Dockerfile 预编译的镜像已经推送到了 <a href="https://hub.docker.com/repository/docker/mritd/caddy">Docker Hub</a> 中，镜像名称为 <code>mritd/caddy</code>。</p><h2 id="三、配置-Caddy2"><a href="#三、配置-Caddy2" class="headerlink" title="三、配置 Caddy2"></a>三、配置 Caddy2</h2><p>Caddy2 的配置文件核心采用 json，但是 json 可读性不强，所以官方维护了一个转换器，抽象出称之为 Caddyfile 的新配置格式；关于 Caddyfile 的完整语法请查看官方文档 <a href="https://caddyserver.com/docs/caddyfile">https://caddyserver.com/docs/caddyfile</a>，本文仅做一些基本使用的样例。</p><h3 id="3-1、配置片段"><a href="#3-1、配置片段" class="headerlink" title="3.1、配置片段"></a>3.1、配置片段</h3><p>Caddyfile 支持类似代码中 function 一样的配置片段，<strong>这些配置片段可以在任意位置被 <code>import</code>，同时可以接受参数，</strong>以下为配置片断示例:</p><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment"># 括号内为片段名称，可以自行定义</span>(TLS) &#123;    protocols tls1.2 tls1.3    ciphers TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256 TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256 TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384 TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384 TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256 TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256&#125;<span class="hljs-comment"># 在任意位置可以引用此片段从而达到配置复用</span>import TLS</code></pre></div><h3 id="3-2、配置模块化"><a href="#3-2、配置模块化" class="headerlink" title="3.2、配置模块化"></a>3.2、配置模块化</h3><p><code>import</code> 指令除了支持引用配置片段以外，还支持引用外部文件，同时支持通配符，有了这个命令以后我们就可以方便的将配置文件进行模块化处理:</p><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment"># 引用外部的 /etc/caddy/*.caddy</span>import /etc/caddy/*.caddy</code></pre></div><h3 id="3-3、站点配置"><a href="#3-3、站点配置" class="headerlink" title="3.3、站点配置"></a>3.3、站点配置</h3><p>针对于站点域名配置，Caddyfile 比较自由化，其格式如下:</p><div class="hljs code-wrapper"><pre><code class="hljs sh">地址 &#123;    站点配置&#125;</code></pre></div><p>关于这个 “地址” 接受多种格式，以下都为合法的地址格式:</p><ul><li><code>localhost</code></li><li><code>example.com</code></li><li><code>:443</code></li><li><code>http://example.com</code></li><li><code>localhost:8080</code></li><li><code>127.0.0.1</code></li><li><code>[::1]:2015</code></li><li><code>example.com/foo/*</code></li><li><code>*.example.com</code></li><li><code>http://</code></li></ul><h3 id="3-4、环境变量"><a href="#3-4、环境变量" class="headerlink" title="3.4、环境变量"></a>3.4、环境变量</h3><p>Caddyfile 支持直接引用系统环境变量，通过此功能可以将一些敏感信息从配置文件中剔除:</p><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment"># 引用环境变量 GANDI_API_TOKEN</span>dns gandi &#123;<span class="hljs-variable">$GANDI_API_TOKEN</span>&#125;</code></pre></div><h3 id="3-5、配置片段参数支持"><a href="#3-5、配置片段参数支持" class="headerlink" title="3.5、配置片段参数支持"></a>3.5、配置片段参数支持</h3><p>针对于配置片段，Caddyfile 还支持类似于函数代码的参数支持，通过参数支持可以让外部引用时动态修改配置信息:</p><div class="hljs code-wrapper"><pre><code class="hljs sh">(LOG) &#123;    <span class="hljs-built_in">log</span> &#123;        format json  &#123;            time_format <span class="hljs-string">&quot;iso8601&quot;</span>        &#125;        <span class="hljs-comment"># &quot;&#123;args.0&#125;&quot; 引用传入的第一个参数，此处用于动态传入日志文件名称</span>        output file <span class="hljs-string">&quot;&#123;args.0&#125;&quot;</span> &#123;            roll_size 100mb            roll_keep 3            roll_keep_for 7d        &#125;    &#125;&#125;<span class="hljs-comment"># 引用片段</span>import LOG <span class="hljs-string">&quot;/data/logs/mritd.com.log&quot;</span></code></pre></div><h3 id="3-6、自动证书申请"><a href="#3-6、自动证书申请" class="headerlink" title="3.6、自动证书申请"></a>3.6、自动证书申请</h3><p>在启动 Caddy2 之前，如果目标域名(例如: <code>www.example.com</code>)已经解析到了本机，那么 Caddy2 启动后会尝试自动通过 ACME HTTP 挑战申请证书；如果期望使用 DNS 的方式申请证书则需要其他 DNS 插件支持，比如上面编译的 <code>--with github.com/caddy-dns/gandi</code> 为 gandi 服务商的 DNS 插件；关于使用 DNS 挑战的配置编写方式需要具体去看其插件文档，目前 gandi 的配置如下:</p><div class="hljs code-wrapper"><pre><code class="hljs sh">tls &#123;dns gandi &#123;env.GANDI_API_TOKEN&#125;&#125;</code></pre></div><p>配置完成后 Caddy2 会通过 ACME DNS 挑战申请证书，<strong>值得注意的是即使通过 DNS 申请证书默认也不会申请泛域名证书，如果想要调整这种细节配置请使用 json 配置或管理 API。</strong></p><h3 id="3-7、完整模块化配置样例"><a href="#3-7、完整模块化配置样例" class="headerlink" title="3.7、完整模块化配置样例"></a>3.7、完整模块化配置样例</h3><p>了解了以上基础配置信息，我们就可以实际编写一个站点配置了；以下为本站的 Caddy 配置样例:</p><p>目录结构:</p><div class="hljs code-wrapper"><pre><code class="hljs sh">caddy├── Caddyfile├── mritd.com.caddy└── mritd.me.caddy</code></pre></div><h4 id="3-7-1、Caddyfile"><a href="#3-7-1、Caddyfile" class="headerlink" title="3.7.1、Caddyfile"></a>3.7.1、Caddyfile</h4><p><strong>Caddyfile 主要包含一些通用的配置，并将其抽到配置片段中，类似与 nginx 的 <code>nginx.conf</code> 主配置；在最后部分通过 <code>import</code> 关键字引入其他具体站点配置，类似 nginx 的 <code>vhost</code> 配置。</strong></p><div class="hljs code-wrapper"><pre><code class="hljs sh">(LOG) &#123;    <span class="hljs-built_in">log</span> &#123;        <span class="hljs-comment"># 日志格式参考 https://github.com/caddyserver/format-encoder 插件文档</span>        format formatted <span class="hljs-string">&quot;[&#123;ts&#125;] &#123;request&gt;remote_addr&#125; &#123;request&gt;proto&#125; &#123;request&gt;method&#125; &lt;- &#123;status&#125; -&gt; &#123;request&gt;host&#125; &#123;request&gt;uri&#125; &#123;request&gt;headers&gt;User-Agent&gt;[0]&#125;&quot;</span>  &#123;            time_format <span class="hljs-string">&quot;iso8601&quot;</span>        &#125;        output file <span class="hljs-string">&quot;&#123;args.0&#125;&quot;</span> &#123;            roll_size 100mb            roll_keep 3            roll_keep_for 7d        &#125;    &#125;&#125;(TLS) &#123;    <span class="hljs-comment"># TLS 配置采用 https://mozilla.github.io/server-side-tls/ssl-config-generator/ 生成，SSL Labs 评分 A+</span>    protocols tls1.2 tls1.3    ciphers TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256 TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256 TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384 TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384 TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256 TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256&#125;(HSTS) &#123;    <span class="hljs-comment"># HSTS (63072000 seconds)</span>    header / Strict-Transport-Security <span class="hljs-string">&quot;max-age=63072000&quot;</span>&#125;(ACME_GANDI) &#123;    <span class="hljs-comment"># 从环境变量获取 GANDI_API_TOKEN</span>    dns gandi &#123;<span class="hljs-variable">$GANDI_API_TOKEN</span>&#125;&#125;<span class="hljs-comment"># 聚合上面的配置片段为新的片段</span>(COMMON_CONFIG) &#123;    <span class="hljs-comment"># 压缩支持</span>    encode zstd gzip    <span class="hljs-comment"># TLS 配置</span>    tls &#123;        import TLS        import ACME_GANDI    &#125;    <span class="hljs-comment"># HSTS</span>    import HSTS&#125;<span class="hljs-comment"># 开启 HTTP3 实验性支持</span>&#123;    servers :443 &#123;        protocol &#123;            experimental_http3        &#125;    &#125;&#125;<span class="hljs-comment"># 引入其他具体的站点配置</span>import /etc/caddy/*.caddy</code></pre></div><h4 id="3-7-2、mritd-com-caddy"><a href="#3-7-2、mritd-com-caddy" class="headerlink" title="3.7.2、mritd.com.caddy"></a>3.7.2、mritd.com.caddy</h4><p><strong><code>mritd.com.caddy</code> 为主站点配置，主站点配置内主要编写一些路由规则，TLS 等都从配置片段引入，这样可以保持统一。</strong></p><div class="hljs code-wrapper"><pre><code class="hljs sh">www.mritd.com &#123;    <span class="hljs-comment"># 重定向到 mritd.com(默认 302)</span>    redir https://mritd.com&#123;uri&#125;    <span class="hljs-comment"># 日志</span>    import LOG <span class="hljs-string">&quot;/data/logs/mritd.com.log&quot;</span>    <span class="hljs-comment"># TLS、HSTS、ACME 等通用配置</span>    import COMMON_CONFIG&#125;mritd.com &#123;    <span class="hljs-comment"># 路由</span>    route /* &#123;        reverse_proxy mritd_com:80    &#125;    <span class="hljs-comment"># 日志</span>    import LOG <span class="hljs-string">&quot;/data/logs/mritd.com.log&quot;</span>    <span class="hljs-comment"># TLS、HSTS、ACME 等通用配置</span>    import COMMON_CONFIG&#125;</code></pre></div><h4 id="3-7-3、mritd-me-caddy"><a href="#3-7-3、mritd-me-caddy" class="headerlink" title="3.7.3、mritd.me.caddy"></a>3.7.3、mritd.me.caddy</h4><p><strong><code>mritd.me.caddy</code> 为老站点配置，目前主要将其 301 到新站点即可。</strong></p><div class="hljs code-wrapper"><pre><code class="hljs sh">www.mritd.me &#123;    <span class="hljs-comment"># 重定向到 mritd.com</span>    <span class="hljs-comment"># 最后的 &quot;code&quot; 支持三种参数</span>    <span class="hljs-comment"># temporary =&gt; 302</span>    <span class="hljs-comment"># permanent =&gt; 301</span>    <span class="hljs-comment"># html =&gt; HTML document redirect</span>    redir https://mritd.com&#123;uri&#125; permanent    <span class="hljs-comment"># 日志</span>    import LOG <span class="hljs-string">&quot;/data/logs/mritd.com.log&quot;</span>    <span class="hljs-comment"># TLS、HSTS、ACME 等通用配置</span>    import COMMON_CONFIG&#125;mritd.me &#123;    <span class="hljs-comment"># 重定向</span>    redir https://mritd.com&#123;uri&#125; permanent    <span class="hljs-comment"># 日志</span>    import LOG <span class="hljs-string">&quot;/data/logs/mritd.com.log&quot;</span>    <span class="hljs-comment"># TLS、HSTS、ACME 等通用配置</span>    import COMMON_CONFIG&#125;</code></pre></div><h2 id="四、启动与重载"><a href="#四、启动与重载" class="headerlink" title="四、启动与重载"></a>四、启动与重载</h2><p>配置文件编写完成后，通过 <code>systemctl start caddy</code> 可启动 caddy 服务器；每次配置修改后可以通过 <code>systemctl reload caddy</code> 进行配置重载，重载期间 caddy 不会重启(实际上调用 <code>caddy reload</code> 命令)，<strong>当配置文件书写错误时，重载只会失败，不会影响正在运行的 caddy 服务器。</strong></p><h2 id="五、总结"><a href="#五、总结" class="headerlink" title="五、总结"></a>五、总结</h2><p>本文只是列举了一些简单的 Caddy 使用样例，在强大的插件配合下，Caddy 可以实现各种 “神奇” 的功能，这些功能依赖于复杂的 Caddy 配置，Caddy 配置需要仔细阅读<a href="https://caddyserver.com/docs/caddyfile/directives">官方文档</a>，关于 Caddyfile 的每个配置段在文档中都有详细的描述。</p><p>值得一提的是 Caddy 本身内置了丰富的插件，例如内置 “file_server”、内置各种负载均衡策略等，这些插件组合在一起可以实现一些复杂的功能；Caddy 是采用 go 编写的，官方也给出了详细的<a href="https://caddyserver.com/docs/extending-caddy">开发文档</a>，相较于 Nginx 来说通过 Lua 或者 C 来开发编写插件来说，Caddy 的插件开发上手要容易得多；Caddy 本身针对数据存储、动态后端、配置文件转换等都内置了扩展接口，这为有特定需求的扩展开发打下了良好基础。</p><p>最终总结，综合来看目前 Caddy2 的性能损失可接受的情况下，相较于 Nginx 绝对是个绝佳选择，各种新功能都能够满足现代化 Web 站点的需求，真香警告。</p>]]></content>
    
    
    <summary type="html">最近网站证书又过期了...... 终于痛下决心(以前太懒)切换到了 Caddy2，这里记录一下 Caddy2 简单使用方式，包括从零开始编译以及配置调整。</summary>
    
    
    
    <category term="Linux" scheme="https://mritd.com/categories/linux/"/>
    
    
    <category term="Caddy" scheme="https://mritd.com/tags/caddy/"/>
    
  </entry>
  
  <entry>
    <title>Skywalking 初试</title>
    <link href="https://mritd.com/2020/11/27/how-to-deploy-skywalking-on-kubernetes/"/>
    <id>https://mritd.com/2020/11/27/how-to-deploy-skywalking-on-kubernetes/</id>
    <published>2020-11-27T07:50:32.000Z</published>
    <updated>2020-11-27T07:50:32.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>在 Skywalking 刚发布的时候就开始关注这个玩意了，一直没有时间去测试；最近正好新项目上线，顺手把 Skywalking 搞起来了，下面简单记录一下 Kubernetes 下的安装使用。</p></blockquote><h2 id="一、先决条件"><a href="#一、先决条件" class="headerlink" title="一、先决条件"></a>一、先决条件</h2><p>确保有一套运行正常的 Kubernetes 集群，本文默认为使用 Elasticsearch7 作为后端存储；<strong>如果想把 ES 放到 Kubernetes 集群里那么还得确保集群配置了正确的存储，譬如默认的 StorageClass 可用等。</strong>本文为了方便起见(其实就是穷)采用外部 ES 存储且使用 docker-compose 单节点部署，所以不需要集群的分布式存储；最后确保你本地的 <code>kubectl</code> 能够正常运行。</p><h2 id="二、基本架构"><a href="#二、基本架构" class="headerlink" title="二、基本架构"></a>二、基本架构</h2><p>Skywalking 在大体上(不准确)分为四大部分:</p><ul><li>oap-server: 无状态服务后端，主要负责处理核心逻辑，可以简单理解为一个标准 java web 项目。</li><li>skywalking-ui: UI 前端，通过 graphql 连接 oap-server 提供用户查询等 UI 展示。</li><li>agent: 各种语言实现的 agent 负责抓取应用运行数据并上报给 oap-server，核心的指标上报来源。</li><li>DB: 各种数据库，负责存储 Skywalking 的指标数据，生产环境推荐 ES、TiDB、MySQL。</li></ul><h2 id="三、部署-Skywalking"><a href="#三、部署-Skywalking" class="headerlink" title="三、部署 Skywalking"></a>三、部署 Skywalking</h2><h3 id="3-1、部署-Elasticsearch"><a href="#3-1、部署-Elasticsearch" class="headerlink" title="3.1、部署 Elasticsearch"></a>3.1、部署 Elasticsearch</h3><p>Elasticsearch 当前使用 7.9.2 版本，由于只是初次尝试还处于测试阶段所以直接 docker-compose 启动一个单点:</p><div class="hljs code-wrapper"><pre><code class="hljs yaml"><span class="hljs-comment"># 如果有需要可以进入 es 容器使用以下命令设置密码</span><span class="hljs-comment"># elasticsearch-setup-passwords interactive</span><span class="hljs-attr">version:</span> <span class="hljs-string">&#x27;3.8&#x27;</span><span class="hljs-attr">services:</span>  <span class="hljs-attr">elasticsearch:</span>    <span class="hljs-attr">container_name:</span> <span class="hljs-string">elasticsearch</span>    <span class="hljs-attr">image:</span> <span class="hljs-string">docker.elastic.co/elasticsearch/elasticsearch:7.9.2</span>    <span class="hljs-attr">restart:</span> <span class="hljs-string">always</span>    <span class="hljs-attr">network_mode:</span> <span class="hljs-string">&quot;host&quot;</span>    <span class="hljs-attr">volumes:</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">data:/data/elasticsearch</span>    <span class="hljs-attr">environment:</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">http.host=172.16.11.43</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">http.port=9200</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">transport.tcp.port=172.16.11.43</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">transport.tcp.port=9300</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">cluster.name=skyes</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">node.name=skyes</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">discovery.type=single-node</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">xpack.security.enabled=true</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">xpack.monitoring.enabled=true</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">&quot;ES_JAVA_OPTS=-Xms4096m -Xmx7168m&quot;</span><span class="hljs-attr">volumes:</span>  <span class="hljs-attr">data:</span></code></pre></div><h3 id="3-2、安装-Helm"><a href="#3-2、安装-Helm" class="headerlink" title="3.2、安装 Helm"></a>3.2、安装 Helm</h3><p>由于 Skywalking 官方给出的 Kubernetes 安装方式为 Helm 安装，所以需要本地先安装 Helm；Helm 安装方式非常简单，根据官方文档<strong>在网络没问题的情况下</strong>直接执行以下命令即可:</p><div class="hljs code-wrapper"><pre><code class="hljs sh">curl https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 | bash</code></pre></div><p>如果网络不是那么 OK 的情况下请参考<a href="https://helm.sh/docs/intro/install/">官方文档</a>的包管理器方式安装或直接下载二进制文件安装。</p><h3 id="3-3、克隆仓库初始化-Helm"><a href="#3-3、克隆仓库初始化-Helm" class="headerlink" title="3.3、克隆仓库初始化 Helm"></a>3.3、克隆仓库初始化 Helm</h3><p>Helm 部署之前按照<a href="https://github.com/apache/skywalking-kubernetes">官方文档</a>提示需要先初始化 Helm 仓库:</p><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment"># clone helm 仓库</span>git <span class="hljs-built_in">clone</span> https://github.com/apache/skywalking-kubernetes<span class="hljs-built_in">cd</span> skywalking-kubernetes/chart<span class="hljs-comment"># 即使使用外部 ES 也要添加这个 repo，否则会导致依赖错误</span>helm repo add elastic https://helm.elastic.cohelm dep up skywalking<span class="hljs-comment"># change the release name according to your scenario</span><span class="hljs-built_in">export</span> SKYWALKING_RELEASE_NAME=skywalking<span class="hljs-comment"># 如果修改了 NAMESPACE，后续部署则需要先通过 kuebctl 创建该 NAMESPACE</span><span class="hljs-comment"># change the namespace according to your scenario</span><span class="hljs-built_in">export</span> SKYWALKING_RELEASE_NAMESPACE=default</code></pre></div><h3 id="3-4、安装-Skywalking"><a href="#3-4、安装-Skywalking" class="headerlink" title="3.4、安装 Skywalking"></a>3.4、安装 Skywalking</h3><p>Helm 初始化完成后需要自行调整配置文件，配置 oap-server 使用外部 ES</p><p><strong>values-my-es.yaml</strong></p><div class="hljs code-wrapper"><pre><code class="hljs yaml"><span class="hljs-attr">oap:</span>  <span class="hljs-attr">image:</span>    <span class="hljs-attr">tag:</span> <span class="hljs-number">8.1</span><span class="hljs-number">.0</span><span class="hljs-string">-es7</span>      <span class="hljs-comment"># Set the right tag according to the existing Elasticsearch version</span>  <span class="hljs-attr">storageType:</span> <span class="hljs-string">elasticsearch7</span><span class="hljs-attr">ui:</span>  <span class="hljs-attr">image:</span>    <span class="hljs-attr">tag:</span> <span class="hljs-number">8.1</span><span class="hljs-number">.0</span><span class="hljs-attr">elasticsearch:</span>  <span class="hljs-attr">enabled:</span> <span class="hljs-literal">false</span>  <span class="hljs-attr">config:</span>               <span class="hljs-comment"># For users of an existing elasticsearch cluster,takes effect when `elasticsearch.enabled` is false</span>    <span class="hljs-attr">host:</span> <span class="hljs-number">172.16</span><span class="hljs-number">.11</span><span class="hljs-number">.43</span>    <span class="hljs-attr">port:</span>      <span class="hljs-attr">http:</span> <span class="hljs-number">9200</span>    <span class="hljs-attr">user:</span> <span class="hljs-string">&quot;elastic&quot;</span>    <span class="hljs-attr">password:</span> <span class="hljs-string">&quot;elastic&quot;</span></code></pre></div><p>调整好配置后只需要使用 Helm 安装即可:</p><div class="hljs code-wrapper"><pre><code class="hljs sh">helm install <span class="hljs-string">&quot;<span class="hljs-variable">$&#123;SKYWALKING_RELEASE_NAME&#125;</span>&quot;</span> skywalking -n <span class="hljs-string">&quot;<span class="hljs-variable">$&#123;SKYWALKING_RELEASE_NAMESPACE&#125;</span>&quot;</span> \  -f ./skywalking/values-my-es.yaml --<span class="hljs-built_in">set</span> oap.image.tag=8.2.0-es7 --<span class="hljs-built_in">set</span> ui.image.tag=8.2.0</code></pre></div><p>如果安装出错或者其他问题可以使用以下命令进行卸载:</p><div class="hljs code-wrapper"><pre><code class="hljs sh">helm uninstall <span class="hljs-string">&quot;<span class="hljs-variable">$&#123;SKYWALKING_RELEASE_NAME&#125;</span>&quot;</span> skywalking -n <span class="hljs-string">&quot;<span class="hljs-variable">$&#123;SKYWALKING_RELEASE_NAMESPACE&#125;</span>&quot;</span></code></pre></div><p>安装成功后应该在 <code>$&#123;SKYWALKING_RELEASE_NAMESPACE&#125;</code> 下看到相关 Pod:</p><div class="hljs code-wrapper"><pre><code class="hljs sh">k8s21 ➜  ~ kubectl get pod -o wide -n skywalkingNAME                              READY   STATUS      RESTARTS   AGE   IP             NODE    NOMINATED NODE   READINESS GATESskywalking-es-init-xw6tx          0/1     Completed   0          32h   10.30.0.62     k8s21   &lt;none&gt;           &lt;none&gt;skywalking-oap-64c65cc6bb-lnq82   1/1     Running     0          32h   10.30.0.61     k8s21   &lt;none&gt;           &lt;none&gt;skywalking-oap-64c65cc6bb-q7zj8   1/1     Running     0          32h   10.30.32.103   k8s22   &lt;none&gt;           &lt;none&gt;skywalking-ui-695ff9d69d-wqcm8    1/1     Running     0          32h   10.30.161.42   k8s25   &lt;none&gt;           &lt;none&gt;</code></pre></div><p>在确认 Pod 都运行正常后可以通过 <code>kubectl port-forward</code> 命令来查看 UI 界面:</p><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment"># 执行以下命令，访问 127.0.0.1:8080 即可访问到 skywalking-ui</span>kubectl port-forward -n <span class="hljs-variable">$&#123;SKYWALKING_RELEASE_NAMESPACE&#125;</span> service/skywalking-ui 8080:80</code></pre></div><p><strong>在生产环境可能需要配置正确的 Ingress 或者 NodePort 等方式暴露 skywalking-ui 服务，具体取决于生产集群服务暴露方式，请自行调整。</strong></p><h2 id="四、Agent-配置"><a href="#四、Agent-配置" class="headerlink" title="四、Agent 配置"></a>四、Agent 配置</h2><blockquote><p>由于目前仅在 Java 项目上测试，所以以下 Agent 配置仅仅对 Java 项目有效。</p></blockquote><p>Skywalking 在简单使用时不需要侵入代码，对于 jar 包启动的项目只需要在启动时增加 <code>-javaagent</code> 选项即可。</p><h3 id="4-1、Agent-获取"><a href="#4-1、Agent-获取" class="headerlink" title="4.1、Agent 获取"></a>4.1、Agent 获取</h3><p><code>javaagent</code> 可以通过下载对应的 skywalking release 安装包获取，将此 <code>agent</code> 目录解压到任意位置，稍后将添加到 java 启动参数。</p><p><img src="https://cdn.oss.link/markdown/y1q3k.png" alt="agent_dir"></p><h3 id="4-2、Agent-配置"><a href="#4-2、Agent-配置" class="headerlink" title="4.2、Agent 配置"></a>4.2、Agent 配置</h3><p>Agent 主配置文件存放在 <code>config/agent.config</code> 配置文件中，配置文件内支持环境变量读取，可以自行添加其他配置和引用其他变量；通常这个配置文件在容器化时有两种选择，<strong>一种是创建 ConfigMap，然后通过 ConfigMap 挂载到容器里进行覆盖；另一种是在默认配置里引用各种变量，在容器启动时通过环境变量注入。</strong>这里暂时使用环境变量注入的方式:</p><p><strong>agent.config</strong></p><p><img src="https://cdn.oss.link/markdown/4t67x.png" alt="agent.config"></p><p><strong>deployment.yml</strong></p><p><img src="https://cdn.oss.link/markdown/q0uw7.png" alt="deployment.yml"></p><p>调整完成后，应用运行一段时间后应该能在 UI 中看到数据</p><p><img src="https://cdn.oss.link/markdown/86vmo.png" alt="skwalking-ui"></p><h2 id="五、注意事项"><a href="#五、注意事项" class="headerlink" title="五、注意事项"></a>五、注意事项</h2><ul><li><strong>默认情况下 Helm 相关命令执行缓慢，可能需要设置 <code>http(s)_proxy</code> <code>...( ＿ ＿)ノ｜壁</code>(自行体会这个表情)</strong></li><li><strong>Skywalking 镜像一般比较大，下载缓慢，推荐预先拉取好然后 load 到每个节点</strong></li><li><strong>ES 如果设置了密码，不要忘记在 Helm 安装时调整好密码配置</strong></li><li><strong>jar 包启动时 <code>-javaagent</code> 不能放在 <code>-jar</code> 选项之后，否则可能不生效</strong></li><li><strong>集群内连接 oap-server 推荐通过 <code>skywalking-oap.skywalking.svc.cluster.local</code> 域名服务发现方式寻址</strong></li></ul>]]></content>
    
    
    <summary type="html">在 Skywalking 刚发布的时候就开始关注这个玩意了，一直没有时间去测试；最近正好新项目上线，顺手把 Skywalking 搞起来了，下面简单记录一下 Kubernetes 下的安装使用。</summary>
    
    
    
    <category term="Kubernetes" scheme="https://mritd.com/categories/kubernetes/"/>
    
    
    <category term="Skywalking" scheme="https://mritd.com/tags/skywalking/"/>
    
  </entry>
  
  <entry>
    <title>利用 etcdhosts 插件搭建分布式 CoreDNS</title>
    <link href="https://mritd.com/2020/11/17/set-up-coredns-ha-clsuter-by-etcdhosts/"/>
    <id>https://mritd.com/2020/11/17/set-up-coredns-ha-clsuter-by-etcdhosts/</id>
    <published>2020-11-17T02:01:28.000Z</published>
    <updated>2020-11-17T02:01:28.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>目前宿主机上全部采用的 dnsmasq 作为 DNS 管理，其中有一个很大的问题是需要进行 DNS 冗余，dnsmasq 每次修改都要多台机器同步，所以自己写了一个插件配合 CoreDNS 实现分布式部署，如果想了解插件编写方式请参考 <a href="https://mritd.com/2019/11/05/writing-plugin-for-coredns/">Writing Plugin for Coredns</a>。 </p></blockquote><h2 id="一、etcdhosts-插件简介"><a href="#一、etcdhosts-插件简介" class="headerlink" title="一、etcdhosts 插件简介"></a>一、etcdhosts 插件简介</h2><p>etcdhosts 顾名思义，就是将 hosts 文件存储在 Etcd 中，然后多个 CoreDNS 共享一份 hosts 文件；得益于 Etcd 提供的 watch 功能，当 Etcd 中的 hosts 文件更新时，每台 CoreDNS 服务器都会接到推送，同时完成热重载；etcdhosts 基本架构如下:</p><div class="hljs code-wrapper"><pre><code class="hljs sh">+-----------------------------------------------------------------------------+|                                                                             ||   +-----------+                                                             ||   |           |                                                             ||   |  CoreDNS  +---------------------+                                       ||   |           |                     |                                       ||   +-----------+                     |                +------------------+   ||                                     |                |                  |   ||                            +--------v---------+      |                  |   ||   +-----------+            |                  |      |                  |   ||   |           |            |                  |      | dnsctl or        |   ||   |  CoreDNS  +------------&gt;   Etcd Cluster   &lt;------+ other etcd tool  |   ||   |           |            |                  |      |                  |   ||   +-----------+            |                  |      |                  |   ||                            +---------^--------+      |                  |   ||                                      |               |                  |   ||   +-----------+                      |               +------------------+   ||   |           |                      |                                      ||   |  CoreDNS  +----------------------+                                      ||   |           |                                                             ||   +-----------+                                                             ||                                                                             ||                                                                             |+-----------------------------------------------------------------------------+</code></pre></div><h2 id="二、编译-CoreDNS"><a href="#二、编译-CoreDNS" class="headerlink" title="二、编译 CoreDNS"></a>二、编译 CoreDNS</h2><blockquote><p>etcdhosts <a href="https://github.com/ytpay/etcdhosts/releases">release</a> 页已经提供部分版本的预编译文件，可以直接下载使用。</p></blockquote><p><a href="https://github.com/ytpay/etcdhosts">etcdhosts</a> 作为一个 CoreDNS 扩展插件采用直接偶合的方式编写(未采用 gRPC 是因为考虑性能影响)，这意味着需要重新编译 CoreDNS 来集成插件，以下为 CoreDNS 编译过程(使用 docker):</p><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment"># clone source code</span>git <span class="hljs-built_in">clone</span> https://github.com/ytpay/etcdhosts.git<span class="hljs-comment"># build</span><span class="hljs-built_in">cd</span> etcdhosts &amp;&amp; ./build v1.8.0</code></pre></div><p>编译完成后将在 <code>build</code> 目录下生成各个平台的二进制文件压缩包。</p><h2 id="三、搭建-Etcd-集群"><a href="#三、搭建-Etcd-集群" class="headerlink" title="三、搭建 Etcd 集群"></a>三、搭建 Etcd 集群</h2><p>Etcd 集群搭建将直接采用 deb 安装包，具体细节这里不再阐述，本次搭建系统为 Ubuntu 20，以下为搭建步骤。</p><h3 id="2-1、安装软件包"><a href="#2-1、安装软件包" class="headerlink" title="2.1、安装软件包"></a>2.1、安装软件包</h3><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment"># 下载 cfssl 安装包，用于签署证书</span>wget https://github.com/mritd/etcd-deb/releases/download/v3.4.13/cfssl_1.4.1_amd64.deb<span class="hljs-comment"># 下载 etcd 安装包</span>wget https://github.com/mritd/etcd-deb/releases/download/v3.4.13/etcd_3.4.13_amd64.deb<span class="hljs-comment"># 执行安装</span>dpkg -i cfssl_1.4.1_amd64.deb etcd_3.4.13_amd64.deb</code></pre></div><h3 id="2-2、创建证书"><a href="#2-2、创建证书" class="headerlink" title="2.2、创建证书"></a>2.2、创建证书</h3><p>创建证书需要先修改证书配置文件(<code>etcd-csr.json</code>)然后借助 cfssl 工具来创建证书</p><p><strong><code>/etc/etcd/cfssl/etcd-csr.json</code></strong></p><div class="hljs code-wrapper"><pre><code class="hljs diff">&#123;    &quot;key&quot;: &#123;        &quot;algo&quot;: &quot;rsa&quot;,        &quot;size&quot;: 2048    &#125;,    &quot;names&quot;: [        &#123;            &quot;O&quot;: &quot;etcd&quot;,            &quot;OU&quot;: &quot;etcd Security&quot;,            &quot;L&quot;: &quot;Beijing&quot;,            &quot;ST&quot;: &quot;Beijing&quot;,            &quot;C&quot;: &quot;CN&quot;        &#125;    ],    &quot;CN&quot;: &quot;etcd&quot;,    &quot;hosts&quot;: [        &quot;127.0.0.1&quot;,        &quot;localhost&quot;,        &quot;*.etcd.node&quot;,        &quot;*.kubernetes.node&quot;,<span class="hljs-addition">+       &quot;172.16.11.71&quot;,</span><span class="hljs-addition">+       &quot;172.16.11.72&quot;,</span><span class="hljs-addition">+       &quot;172.16.11.73&quot;</span>    ]&#125;</code></pre></div><p>通过脚本创建证书</p><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-built_in">cd</span> /etc/etcd/cfssl./create.shcp *.pem /etc/etcd/ssl</code></pre></div><p><strong>证书创建完成后需要分发到其他两台机器上，保证三台节点的 <code>/etc/etcd/ssl</code> 目录证书相同。</strong></p><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment"># 复制证书</span>scp /etc/etcd/ssl/*.pem root@NODE2:/etc/etcd/sslscp /etc/etcd/ssl/*.pem root@NODE3:/etc/etcd/ssl<span class="hljs-comment"># 修复权限(三台都要修复)</span>chown -R etcd:etcd /etc/etcd/</code></pre></div><h3 id="2-3、调整集群配置"><a href="#2-3、调整集群配置" class="headerlink" title="2.3、调整集群配置"></a>2.3、调整集群配置</h3><p>证书签署完成后，简单的调整每台机器上的集群节点配置即可</p><p><strong><code>/etc/etcd/etcd.conf</code></strong></p><div class="hljs code-wrapper"><pre><code class="hljs diff"># [member]<span class="hljs-addition">+ # 节点号自行修改，推荐格式: etcd+节点IP，例如 etcd21</span><span class="hljs-addition">+ ETCD_NAME=etcd1</span>ETCD_DATA_DIR=&quot;/var/lib/etcd/data&quot;ETCD_WAL_DIR=&quot;/var/lib/etcd/wal&quot;ETCD_SNAPSHOT_COUNT=&quot;100&quot;<span class="hljs-addition">+ # 修改为当前机器 IP</span><span class="hljs-addition">+ ETCD_LISTEN_PEER_URLS=&quot;https://172.16.11.71:2380&quot;</span><span class="hljs-addition">+ # 修改为当前机器 IP</span><span class="hljs-addition">+ ETCD_LISTEN_CLIENT_URLS=&quot;https://172.16.11.71:2379,http://127.0.0.1:2379&quot;</span>ETCD_QUOTA_BACKEND_BYTES=&quot;8589934592&quot;ETCD_MAX_REQUEST_BYTES=&quot;10485760&quot;# [cluster]<span class="hljs-addition">+ # 修改为当前机器 IP</span><span class="hljs-addition">+ ETCD_INITIAL_ADVERTISE_PEER_URLS=&quot;https://172.16.11.71:2380&quot;</span># if you use different ETCD_NAME (e.g. test), set ETCD_INITIAL_CLUSTER value for this name, i.e. &quot;test=http://...&quot;<span class="hljs-addition">+ # 三台机器都要按照格式写好</span><span class="hljs-addition">+ ETCD_INITIAL_CLUSTER=&quot;etcd1=https://172.16.11.71:2380,etcd2=https://172.16.11.72:2380,etcd3=https://172.16.11.73:2380&quot;</span>ETCD_INITIAL_CLUSTER_STATE=&quot;new&quot;ETCD_INITIAL_CLUSTER_TOKEN=&quot;etcd-cluster&quot;<span class="hljs-addition">+ # 修改为当前机器 IP</span><span class="hljs-addition">+ ETCD_ADVERTISE_CLIENT_URLS=&quot;https://172.16.11.71:2379&quot;</span>ETCD_AUTO_COMPACTION_MODE=&quot;revision&quot;ETCD_AUTO_COMPACTION_RETENTION=&quot;16&quot;ETCD_QUOTA_BACKEND_BYTES=&quot;5368709120&quot;# [security]ETCD_CERT_FILE=&quot;/etc/etcd/ssl/etcd.pem&quot;ETCD_KEY_FILE=&quot;/etc/etcd/ssl/etcd-key.pem&quot;ETCD_TRUSTED_CA_FILE=&quot;/etc/etcd/ssl/etcd-root-ca.pem&quot;ETCD_CLIENT_CERT_AUTH=&quot;true&quot;ETCD_AUTO_TLS=&quot;true&quot;ETCD_PEER_CERT_FILE=&quot;/etc/etcd/ssl/etcd.pem&quot;ETCD_PEER_KEY_FILE=&quot;/etc/etcd/ssl/etcd-key.pem&quot;ETCD_PEER_CLIENT_CERT_AUTH=&quot;true&quot;ETCD_PEER_TRUSTED_CA_FILE=&quot;/etc/etcd/ssl/etcd-root-ca.pem&quot;ETCD_PEER_AUTO_TLS=&quot;true&quot;</code></pre></div><p>最后每台机器执行 <code>systemctl start etcd</code> 启动即可，验证集群是否健康可以使用如下命令测试:</p><div class="hljs code-wrapper"><pre><code class="hljs sh">etcdctl endpoint health --cert /etc/etcd/ssl/etcd.pem --key /etc/etcd/ssl/etcd-key.pem --cacert /etc/etcd/ssl/etcd-root-ca.pem --endpoints https://172.16.11.71:2379,https://172.16.11.72:2379,https://172.16.11.73:2379https://172.16.11.71:2379 is healthy: successfully committed proposal: took = 33.07493mshttps://172.16.11.72:2379 is healthy: successfully committed proposal: took = 32.132266mshttps://172.16.11.73:2379 is healthy: successfully committed proposal: took = 40.745291ms</code></pre></div><h2 id="三、搭建-CoreDNS-集群"><a href="#三、搭建-CoreDNS-集群" class="headerlink" title="三、搭建 CoreDNS 集群"></a>三、搭建 CoreDNS 集群</h2><h3 id="3-1、CoreDNS-安装"><a href="#3-1、CoreDNS-安装" class="headerlink" title="3.1、CoreDNS 安装"></a>3.1、CoreDNS 安装</h3><p>系统级 CoreDNS 安装推荐直接使用 systemd 管理，官方目前提供了 systemd 相关配置文件: <a href="https://github.com/coredns/deployment/tree/master/systemd">https://github.com/coredns/deployment/tree/master/systemd</a></p><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment"># 安装二进制文件</span>tar -zxvf coredns_1.8.0_linux_amd64.tgzmv coredns /usr/bin/coredns<span class="hljs-comment"># 安装 systemd 配置</span>wget https://raw.githubusercontent.com/coredns/deployment/master/systemd/coredns-sysusers.conf -O /usr/lib/sysusers.d/coredns-sysusers.confwget https://raw.githubusercontent.com/coredns/deployment/master/systemd/coredns-tmpfiles.conf -O /usr/lib/tmpfiles.d/coredns-tmpfiles.confwget https://raw.githubusercontent.com/coredns/deployment/master/systemd/coredns.service -O /usr/lib/systemd/system/coredns.service<span class="hljs-comment"># reload</span>systemctl daemon-reload<span class="hljs-comment"># 初始化用户</span>systemd-sysusers<span class="hljs-comment"># 初始化临时目录</span>systemd-tmpfiles --create<span class="hljs-comment"># 创建配置目录</span>mkdir -p /etc/coredns/ssl</code></pre></div><h3 id="3-2、etcdhosts-配置"><a href="#3-2、etcdhosts-配置" class="headerlink" title="3.2、etcdhosts 配置"></a>3.2、etcdhosts 配置</h3><p>etcdhosts 的配置类似官方的 etcd 插件，其配置格式如下:</p><div class="hljs code-wrapper"><pre><code class="hljs sh">etcdhosts [ZONES...] &#123;    [INLINE]    ttl SECONDS    no_reverse    fallthrough [ZONES...]    key ETCD_KEY    endpoint ETCD_ENDPOINT...    credentials ETCD_USERNAME ETCD_PASSWORD    tls ETCD_CERT ETCD_KEY ETCD_CACERT    timeout ETCD_TIMEOUT&#125;</code></pre></div><p>以下是一个简单的可启动的样例配置:</p><p><strong><code>/etc/coredns/Corefile</code></strong></p><div class="hljs code-wrapper"><pre><code class="hljs sh">. &#123;    <span class="hljs-comment"># 绑定接口地址</span>    <span class="hljs-built_in">bind</span> 172.16.11.71    <span class="hljs-comment"># cache</span>    cache 30 . &#123;        success 4096    &#125;    <span class="hljs-comment"># etcdhosts 配置</span>    etcdhosts . &#123;        fallthrough .        key /etcdhosts        timeout 5s        tls /etc/coredns/ssl/etcd.pem /etc/coredns/ssl/etcd-key.pem /etc/coredns/ssl/etcd-root-ca.pem        endpoint https://172.16.11.71:2379 https://172.16.11.72:2379 https://172.16.11.73:2379    &#125;    <span class="hljs-comment"># 上游 DNS 配置</span>    forward . 114.114.114.114:53 &#123;        max_fails 2        expire 20s        policy random        health_check 0.2s    &#125;    <span class="hljs-comment"># 日志配置</span>    errors    <span class="hljs-built_in">log</span> . <span class="hljs-string">&quot;&#123;remote&#125;:&#123;port&#125; - &#123;&gt;id&#125; \&quot;&#123;type&#125; &#123;class&#125; &#123;name&#125; &#123;proto&#125; &#123;size&#125; &#123;&gt;do&#125; &#123;&gt;bufsize&#125;\&quot; &#123;rcode&#125; &#123;&gt;rflags&#125; &#123;rsize&#125; &#123;duration&#125;&quot;</span>&#125;</code></pre></div><p>由于 etcdhosts 插件需要连接 etcd 集群，所以需要将证书复制到 <code>Corefile</code> 指定的位置:</p><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment"># 实际生产环境 coredns 与 etcd 一般不在一台机器上，请自行 scp</span>cp /etc/etcd/ssl/*.pem /etc/coredns/ssl<span class="hljs-comment"># 修复权限</span>chown -R coredns:coredns /etc/coredns</code></pre></div><p><strong>最后直接启动即可(首次启动会出现 <code>[ERROR] plugin/etcdhosts: invalid etcd response: 0</code> 错误，属于正常情况):</strong></p><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment"># 启动</span>systemctl start coredns<span class="hljs-comment"># 测试</span>dig @172.16.11.71 baidu.com; &lt;&lt;&gt;&gt; DiG 9.16.1-Ubuntu &lt;&lt;&gt;&gt; @172.16.11.71 baidu.com; (1 server found);; global options: +cmd;; Got answer:;; -&gt;&gt;HEADER&lt;&lt;- <span class="hljs-string">opcode: QUERY, status: NOERROR, id: 35323</span><span class="hljs-string">;; flags: qr rd ra; QUERY: 1, ANSWER: 2, AUTHORITY: 0, ADDITIONAL: 1</span><span class="hljs-string"></span><span class="hljs-string">;; OPT PSEUDOSECTION:</span><span class="hljs-string">; EDNS: version: 0, flags:; udp: 4096</span><span class="hljs-string">; COOKIE: 8e3137531ed0b57a (echoed)</span><span class="hljs-string">;; QUESTION SECTION:</span><span class="hljs-string">;baidu.com.                     IN      A</span><span class="hljs-string"></span><span class="hljs-string">;; ANSWER SECTION:</span><span class="hljs-string">baidu.com.              30      IN      A       220.181.38.148</span><span class="hljs-string">baidu.com.              30      IN      A       39.156.69.79</span><span class="hljs-string"></span><span class="hljs-string">;; Query time: 8 msec</span><span class="hljs-string">;; SERVER: 172.16.11.71#53(172.16.11.71)</span><span class="hljs-string">;; WHEN: Mon Nov 16 20:18:25 CST 2020</span><span class="hljs-string">;; MSG SIZE  rcvd: 100</span></code></pre></div><p><strong>最后在多台机器上通过同样的配置启动 CoreDNS 即可，此时所有 CoreDNS 服务器通过 Etcd 提供一致性的记录解析。</strong></p><h2 id="四、记录调整"><a href="#四、记录调整" class="headerlink" title="四、记录调整"></a>四、记录调整</h2><p>所有 CoreDNS 启动成功后，默认 etcdhosts 插件将会读取 Etcd 中的 <code>/etcdhosts</code> key 作为 hosts 文件载入；<strong>载入成功后将会在内存级进行 Cache，多次查询不会造成疯狂的 Etcd 请求，只有当触发 reload 时(包括 Etcd 更新)才会重新查询 Etcd。</strong>所以此时只需要向 Etcd 的 <code>/etcdhosts</code> key 写入一个 hosts 文件即可；写入 Etcd 可以使用 etcdctl 以及其他的开源工具，甚至自己开发都可以，<strong>记录更改只需要跟 Etcd 打交道，不需要理会 CoreDNS；</strong>由于本人实在是比较菜，前端页面写不出来，所以弄了一个命令行版本的工具: <a href="https://github.com/ytpay/dnsctl">dnsctl</a></p><p>dnsctl 只有一个可执行文件，<strong>默认情况下 dnsctl 读取 <code>$HOME/.dnsctl.yaml</code> 配置文件来沟通 Etcd，</strong>配置文件格式如下:</p><div class="hljs code-wrapper"><pre><code class="hljs yaml"><span class="hljs-comment"># etcd 中 etcdhosts 插件的 key</span><span class="hljs-attr">dnskey:</span> <span class="hljs-string">/etcdhosts</span><span class="hljs-comment"># etcd 集群配置</span><span class="hljs-attr">etcd:</span>  <span class="hljs-attr">cert:</span> <span class="hljs-string">/etc/etcd/ssl/etcd.pem</span>  <span class="hljs-attr">key:</span> <span class="hljs-string">/etc/etcd/ssl/etcd-key.pem</span>  <span class="hljs-attr">ca:</span> <span class="hljs-string">/etc/etcd/ssl/etcd-root-ca.pem</span>  <span class="hljs-attr">endpoints:</span>    <span class="hljs-bullet">-</span> <span class="hljs-string">https://172.16.11.71:2379</span>    <span class="hljs-bullet">-</span> <span class="hljs-string">https://172.16.11.72:2379</span>    <span class="hljs-bullet">-</span> <span class="hljs-string">https://172.16.11.73:2379</span></code></pre></div><p>dnsctl 提供如下命令</p><div class="hljs code-wrapper"><pre><code class="hljs sh">dnsctl <span class="hljs-keyword">for</span> etcdhosts pluginUsage:  dnsctl [flags]  dnsctl [<span class="hljs-built_in">command</span>]Available Commands:  config      show example config  dump        dump hosts  edit        edit hosts  <span class="hljs-built_in">help</span>        Help about any <span class="hljs-built_in">command</span>  upload      upload hosts from file  version     show hosts versionFlags:      --config string   config file (default is <span class="hljs-variable">$HOME</span>/.dnsctl.yaml)  -h, --<span class="hljs-built_in">help</span>            <span class="hljs-built_in">help</span> <span class="hljs-keyword">for</span> dnsctl  -v, --version         version <span class="hljs-keyword">for</span> dnsctlUse <span class="hljs-string">&quot;dnsctl [command] --help&quot;</span> <span class="hljs-keyword">for</span> more information about a <span class="hljs-built_in">command</span>.</code></pre></div><p>其中 <code>edit</code> 命令将会打开系统默认编辑器(例如 vim)，然后编辑完保存后会自动上传到 Etcd 中，此后 CoreDNS 的 etcdhosts 插件将会立即重载；**<code>dump</code> 命令用于将 Etcd 中的 hosts 文件保存到本地用于备份，<code>upload</code> 命令可以将已有的 hosts 文件上传到 Etcd 用于恢复。**</p>]]></content>
    
    
    <summary type="html">目前宿主机上全部采用的 dnsmasq 作为 DNS 管理，其中有一个很大的问题是需要进行 DNS 冗余，dnsmasq 每次修改都要多台机器同步，所以自己写了一个插件配合 CoreDNS 实现分布式部署，如果想了解插件编写方式请参考 [Writing Plugin for Coredns](https://mritd.com/2019/11/05/writing-plugin-for-coredns/)。</summary>
    
    
    
    <category term="Golang" scheme="https://mritd.com/categories/golang/"/>
    
    
    <category term="CoreDNS" scheme="https://mritd.com/tags/coredns/"/>
    
    <category term="etcdhosts" scheme="https://mritd.com/tags/etcdhosts/"/>
    
  </entry>
  
  <entry>
    <title>GIGABYTE Z370 AORUS Gaming 5 关闭 CFG 锁</title>
    <link href="https://mritd.com/2020/10/16/gigabyte-z370-aorus-gaming-5-disable-cfg-lock/"/>
    <id>https://mritd.com/2020/10/16/gigabyte-z370-aorus-gaming-5-disable-cfg-lock/</id>
    <published>2020-10-15T18:51:00.000Z</published>
    <updated>2020-10-15T18:51:00.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="一、解锁原理"><a href="#一、解锁原理" class="headerlink" title="一、解锁原理"></a>一、解锁原理</h2><p>由于主板不支持设置 CFG Lock，所以只能借助第三方工具强行解锁；首要前提是需要知道 CFG Lock 的设置地址，不同型号主板甚至不同版本的 BIOS 都不一定相同，所以 CFG Lock 地址不要照搬；本次操作用到的工具如下:</p><ul><li>主板的 BIOS 文件(自行去官网下载，并且版本要和当前一致)</li><li><a href="https://github.com/LongSoft/UEFITool/releases">UEFITool</a>: 用于读取 BIOS 文件并搜索 CFG Lock 所在 Section</li><li><a href="https://github.com/LongSoft/Universal-IFR-Extractor/releases">ifrextract</a>: 用于将对应 Section 的 efi 文件转换为纯文本</li><li><a href="https://github.com/datasone/grub-mod-setup_var/releases">modGRUBShell.efi</a>: 修改版的 grub UEFI Shell 提供 <code>setup_var_3</code> 命令来修改 CFG Lock</li></ul><h2 id="二、获取-CFG-Lock-地址"><a href="#二、获取-CFG-Lock-地址" class="headerlink" title="二、获取 CFG Lock 地址"></a>二、获取 CFG Lock 地址</h2><h3 id="2-1、提取-BIOS-Section"><a href="#2-1、提取-BIOS-Section" class="headerlink" title="2.1、提取 BIOS Section"></a>2.1、提取 BIOS Section</h3><p>mac 下打开 UEFITool，选择 <code>File &gt; Open image file</code>，文件选择框点击选项按钮切换成 <code>All files</code> 模式否则由于文件扩展名不同可能无法选中，然后选择主板 BIOS 文件。</p><p><img src="https://cdn.oss.link/markdown/OVx3zw-1602786074-uLkuRk.png" alt="OVx3zw-1602786074-uLkuRk"></p><p><img src="https://cdn.oss.link/markdown/4OWosE-1602786158-wwWXdf.png" alt="4OWosE-1602786158-wwWXdf"></p><p>然后 <code>command + F</code> 切换到 <code>Text</code> 模式搜索 <code>CFG Lock</code>，接着双击下面的搜索结果会定位到对应的 Section。</p><p><img src="https://cdn.oss.link/markdown/miL7Qf-1602786270-thU0US.png" alt="miL7Qf-1602786270-thU0US"></p><p>接下来右键 <code>Extract body</code> 导出到桌面等任意文件夹既可。</p><p><img src="https://cdn.oss.link/markdown/qfbB5v-1602786419-Ys0a4P.png" alt="qfbB5v-1602786419-Ys0a4P"></p><h3 id="2-2、转换为-Text-文本"><a href="#2-2、转换为-Text-文本" class="headerlink" title="2.2、转换为 Text 文本"></a>2.2、转换为 Text 文本</h3><p>提取到 <code>[Section Name].efi</code> 文件后命令行执行 <code>ifrextract [Section Name].efi cfg.txt</code> 导出为文本。</p><p><img src="https://cdn.oss.link/markdown/TfPsbJ-1602786563-uLZS4A.png" alt="TfPsbJ-1602786563-uLZS4A"></p><h3 id="2-3、确定-CFG-Lock-位置"><a href="#2-3、确定-CFG-Lock-位置" class="headerlink" title="2.3、确定 CFG Lock 位置"></a>2.3、确定 CFG Lock 位置</h3><p>导出文本后通过编辑器搜索 <code>CFG Lock</code> 字符串，<strong>其中 <code>VarStoreInfo</code> 后面的地址就是 CFG Lock 设置地址，请记录这个地址(最好用手机拍照)。</strong></p><p><img src="https://cdn.oss.link/markdown/eCjJwX-1602786663-eyW0vz.png" alt="eCjJwX-1602786663-eyW0vz"></p><h2 id="三、关闭-CFG-Lock"><a href="#三、关闭-CFG-Lock" class="headerlink" title="三、关闭 CFG Lock"></a>三、关闭 CFG Lock</h2><p>得到了 CFG Lock 地址以后一切都简单了，创建一个启动 U 盘然后执行命令既可；首先将 U 盘格式化为 GUID 分区表，然后挂载 EFI 分区，<strong>将 <code>modGRUBShell.efi</code> 重命名为 <code>BOOTX64.efi</code> 并放入 <code>EFI/BOOT</code> 目录。</strong></p><p><img src="https://cdn.oss.link/markdown/YCHA7V-1602787150-1fqjYt.png" alt="YCHA7V-1602787150-1fqjYt"></p><p>最后重启系统 BIOS 选择使用 U 盘启动，并在 grub shell 内执行 <code>setup_var_3 0x529 0x0</code> 然后重启即完成解锁；<strong>注意: <code>0x529</code> 请替换为上面找到的实际地址，实际地址 <code>0x***</code> 后面的 <code>***</code> 如果有大写字母请保持大写；</strong>这部份就不上图了，懒得拍照。</p>]]></content>
    
    
    <summary type="html">最近在狂折腾黑苹果，从以前的 Clover 换成了 OC，迫于主板 CFG Lock 导致没法继续优化，折腾好久找到了解决方案。</summary>
    
    
    
    <category term="Hackintosh" scheme="https://mritd.com/categories/hackintosh/"/>
    
    
    <category term="CFG Lock" scheme="https://mritd.com/tags/cfg-lock/"/>
    
  </entry>
  
  <entry>
    <title>网站切换到 Hexo</title>
    <link href="https://mritd.com/2020/10/08/switch-jekyll-to-hexo/"/>
    <id>https://mritd.com/2020/10/08/switch-jekyll-to-hexo/</id>
    <published>2020-10-08T11:04:00.000Z</published>
    <updated>2020-10-08T11:04:00.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="一、Hexo-安装"><a href="#一、Hexo-安装" class="headerlink" title="一、Hexo 安装"></a>一、Hexo 安装</h2><p>Hexo 安装根据官方文档直接操作即可，安装前提是需要先安装 Nodejs(这里不再阐述直接略过)</p><div class="hljs code-wrapper"><pre><code class="hljs sh">npm install -g hexo-cli</code></pre></div><p>Hexo 命令行工具安装完成后可以直接初始化一个样例项目，init 过程会 clone <code>https://github.com/hexojs/hexo-starter.git</code> 到本地，同时自动安装好相关依赖</p><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment"># mritd.com 为目录名，个人习惯直接使用网站域名作为目录名称</span>hexo init mritd.com</code></pre></div><p>进入目录启动样例站点</p><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment"># 进入目录</span><span class="hljs-built_in">cd</span> mritd.com<span class="hljs-comment"># 启动本地服务器进行预览</span>hexo serve</code></pre></div><p><img src="https://cdn.oss.link/markdown/1jb2q.png" alt="hexo_demo"></p><h2 id="二、主题设置"><a href="#二、主题设置" class="headerlink" title="二、主题设置"></a>二、主题设置</h2><p>基本的样例博客启动完成后就需要选择一个主题，主题实质上才决定博客功能，这里目前使用了 <a href="https://github.com/fluid-dev/hexo-theme-fluid">Fluid</a> 主题，这个主题目前兼具了个人博客所需的所有功能，而且作者提交比较活跃，文档也比较全面。</p><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment"># 下载主题</span>git <span class="hljs-built_in">clone</span> https://github.com/fluid-dev/hexo-theme-fluid.git themes/fluid<span class="hljs-comment"># 切换到最新版本</span>(<span class="hljs-built_in">cd</span> themes/fluid &amp;&amp; git checkout -b v1.8.3 v1.8.3)</code></pre></div><p>接下来修改 <code>_config.yml</code> 配置切换主题即可</p><div class="hljs code-wrapper"><pre><code class="hljs yaml"><span class="hljs-comment"># Extensions</span><span class="hljs-comment">## Plugins: https://hexo.io/plugins/</span><span class="hljs-comment">## Themes: https://hexo.io/themes/</span><span class="hljs-attr">theme:</span> <span class="hljs-string">fluid</span></code></pre></div><p>然后重新启动博客进行预览: <code>hexo cl &amp;&amp; hexo s</code></p><p><img src="https://cdn.oss.link/markdown/vlibl.png" alt="fluid_demo"></p><p><strong>关于主题其他配置可自行阅读 <a href="https://hexo.fluid-dev.com/docs/guide/">官方文档</a>，文档有时可能更新不及时，可同时参考仓库内的 <a href="https://github.com/fluid-dev/hexo-theme-fluid/blob/master/_config.yml"><code>_config.yml</code></a> 配置。</strong></p><h2 id="三、文章导入"><a href="#三、文章导入" class="headerlink" title="三、文章导入"></a>三、文章导入</h2><p>关于 jekyll 博客的文章如何导入到 Hexo 中网上有很多脚本；但是实际上两个静态博客框架都是支持标准的 Markdown 语法书写的文章进行渲染，唯一区别就是每篇文章上的 “头”。</p><div class="hljs code-wrapper"><pre><code class="hljs markdown">---catalog: truecategories:<span class="hljs-bullet">  -</span> [Kubernetes]<span class="hljs-bullet">  -</span> [Golang]date: 2018-11-25 11:11:28excerpt: 最近在看 kubeadm 的源码，不过有些东西光看代码还是没法太清楚，还是需要实际运行才能看到具体代码怎么跑的，还得打断点 debug；无奈的是本机是 mac，debug 得在 Linux 下，so 研究了一下 remote debugkeywords: kubeadm,debugmultilingual: falsetags:<span class="hljs-bullet">  -</span> Golang<span class="hljs-bullet">  -</span> Kubernetestitle: 远程 Debug kubeadmindex<span class="hljs-emphasis">_img: img/remote_</span>debug.jpg---具体文章内容......</code></pre></div><p>所以直接复制 jekyll 的 md 文件到 <code>source/_posts</code> 目录，并修改文档头部即可。</p><h2 id="四、自动更新"><a href="#四、自动更新" class="headerlink" title="四、自动更新"></a>四、自动更新</h2><p>目前博客部署在自己的 VPS 上，以前都是将博客生成的静态直接使用 nginx 发布出去的；但是面临的问题就是每次博客更新都要手动去 VPS 更新，虽然可以写一些 CI 脚本但是并不算智能；得益于 Golang 官方完善的标准库支持，这次直接几行代码写一个静态服务器，同时拦截特定 URL 来更新博客:</p><div class="hljs code-wrapper"><pre><code class="hljs golang"><span class="hljs-keyword">package</span> main<span class="hljs-keyword">import</span> (<span class="hljs-string">&quot;fmt&quot;</span><span class="hljs-string">&quot;net/http&quot;</span><span class="hljs-string">&quot;os&quot;</span><span class="hljs-string">&quot;os/exec&quot;</span><span class="hljs-string">&quot;path&quot;</span>)<span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-title">main</span><span class="hljs-params">()</span></span> &#123;http.Handle(<span class="hljs-string">&quot;/&quot;</span>, fileServerWithCustom404(http.Dir(<span class="hljs-string">&quot;/data&quot;</span>)))http.HandleFunc(<span class="hljs-string">&quot;/update&quot;</span>, update)fmt.Println(<span class="hljs-string">&quot;Updating WebSite...&quot;</span>)_, err := gitPull()<span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;fmt.Printf(<span class="hljs-string">&quot;WebSite update failed: %s&quot;</span>, err)&#125;fmt.Println(<span class="hljs-string">&quot;HTTP Server Listen at [:8080]...&quot;</span>)_ = http.ListenAndServe(<span class="hljs-string">&quot;:8080&quot;</span>, <span class="hljs-literal">nil</span>)&#125;<span class="hljs-comment">// POST 请求 /update 触发 git pull 更新博客</span><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-title">update</span><span class="hljs-params">(w http.ResponseWriter, r *http.Request)</span></span> &#123;<span class="hljs-keyword">if</span> r.Method != http.MethodPost &#123;w.WriteHeader(http.StatusBadRequest)_, _ = w.Write([]<span class="hljs-keyword">byte</span>(<span class="hljs-string">&quot;only support POST method.\n&quot;</span>))<span class="hljs-keyword">return</span>&#125;bs, err := gitPull()<span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;w.WriteHeader(http.StatusInternalServerError)_, _ = w.Write([]<span class="hljs-keyword">byte</span>(err.Error()))<span class="hljs-keyword">return</span>&#125;w.WriteHeader(http.StatusOK)_, _ = w.Write(bs)&#125;<span class="hljs-comment">// 包装一下 404 状态码，返回自定义的 404 页面</span><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-title">fileServerWithCustom404</span><span class="hljs-params">(fs http.FileSystem)</span> <span class="hljs-title">http</span>.<span class="hljs-title">Handler</span></span> &#123;fsh := http.FileServer(fs)<span class="hljs-keyword">return</span> http.HandlerFunc(<span class="hljs-function"><span class="hljs-keyword">func</span><span class="hljs-params">(w http.ResponseWriter, r *http.Request)</span></span> &#123;_, err := fs.Open(path.Clean(r.URL.Path))<span class="hljs-keyword">if</span> os.IsNotExist(err) &#123;r.URL.Path = <span class="hljs-string">&quot;/404.html&quot;</span>&#125;fsh.ServeHTTP(w, r)&#125;)&#125;<span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-title">gitPull</span><span class="hljs-params">()</span> <span class="hljs-params">(msg []<span class="hljs-keyword">byte</span>, err error)</span></span> &#123;cmd := exec.Command(<span class="hljs-string">&quot;git&quot;</span>, <span class="hljs-string">&quot;pull&quot;</span>)cmd.Dir = <span class="hljs-string">&quot;/data&quot;</span><span class="hljs-keyword">return</span> cmd.CombinedOutput()&#125;</code></pre></div><h2 id="五、Docker-化"><a href="#五、Docker-化" class="headerlink" title="五、Docker 化"></a>五、Docker 化</h2><p>有了上面的静态服务器，写个 Dockerfile 将 Hexo 生成的静态文件打包即可:</p><div class="hljs code-wrapper"><pre><code class="hljs Dockerfile"><span class="hljs-keyword">FROM</span> golang:<span class="hljs-number">1.15</span>-alpine3.<span class="hljs-number">12</span> AS builder<span class="hljs-keyword">ENV</span> GO111MODULE on<span class="hljs-keyword">COPY</span><span class="bash"> goserver /go/src/github.com/mritd/hexo/goserver</span><span class="hljs-keyword">WORKDIR</span><span class="bash"> /go/src/github.com/mritd/hexo/goserver</span><span class="hljs-keyword">RUN</span><span class="bash"> <span class="hljs-built_in">set</span> -e \</span><span class="bash">    &amp;&amp; go install</span><span class="hljs-keyword">FROM</span> alpine:<span class="hljs-number">3.12</span> AS dist<span class="hljs-keyword">LABEL</span><span class="bash"> maintainer=<span class="hljs-string">&quot;mritd &lt;mritd@linux.com&gt;&quot;</span></span><span class="hljs-keyword">ENV</span> TZ Asia/Shanghai<span class="hljs-keyword">ENV</span> REPO https://github.com/mritd/mritd.com.git<span class="hljs-keyword">COPY</span><span class="bash"> --from=builder /go/bin/goserver /usr/<span class="hljs-built_in">local</span>/bin/goserver</span><span class="hljs-keyword">RUN</span><span class="bash"> <span class="hljs-built_in">set</span> -e \</span><span class="bash">    &amp;&amp; apk upgrade \</span><span class="bash">    &amp;&amp; apk add bash tzdata git \</span><span class="bash">    &amp;&amp; git <span class="hljs-built_in">clone</span> <span class="hljs-variable">$&#123;REPO&#125;</span> /data \</span><span class="bash">    &amp;&amp; ln -sf /usr/share/zoneinfo/<span class="hljs-variable">$&#123;TZ&#125;</span> /etc/localtime \</span><span class="bash">    &amp;&amp; <span class="hljs-built_in">echo</span> <span class="hljs-variable">$&#123;TZ&#125;</span> &gt; /etc/timezone \</span><span class="bash">    &amp;&amp; rm -rf /var/cache/apk/*</span><span class="hljs-keyword">WORKDIR</span><span class="bash"> /data</span><span class="hljs-keyword">CMD</span><span class="bash"> [<span class="hljs-string">&quot;goserver&quot;</span>]</span></code></pre></div><p>镜像运行后将使用 <code>/data</code> 目录最为静态文件目录进行发布，Hexo 生成的静态文件(public 目录)也会完整的 clone 到当前目录，此后使用 POST 请求访问 <code>/update</code> 即可触发从 Github 更新博客内容。</p><h2 id="六、Travis-CI-集成"><a href="#六、Travis-CI-集成" class="headerlink" title="六、Travis CI 集成"></a>六、Travis CI 集成</h2><p>所有就绪以后在主仓库增加 <code>.travis.yml</code> 配置来联动 travis ci；由于每次 push 到 Github 的内容实际上已经是本地生成的 public 目录，所以 CI 只需要通知服务器更新即可；强迫症又加了一个 Telegram 通知，每次触发更新完成后 Telegram 再给自己推送一下:</p><div class="hljs code-wrapper"><pre><code class="hljs yml"><span class="hljs-attr">language:</span> <span class="hljs-string">go</span><span class="hljs-attr">git:</span>  <span class="hljs-attr">quiet:</span> <span class="hljs-literal">true</span><span class="hljs-attr">script:</span><span class="hljs-bullet">-</span> <span class="hljs-string">curl</span> <span class="hljs-string">-X</span> <span class="hljs-string">POST</span> <span class="hljs-string">$&#123;CALLBACK&#125;</span><span class="hljs-attr">after_script:</span><span class="hljs-bullet">-</span> <span class="hljs-string">curl</span> <span class="hljs-string">-X</span> <span class="hljs-string">POST</span> <span class="hljs-string">https://api.telegram.org/bot$&#123;TELEGRAM_TOKEN&#125;/sendMessage</span> <span class="hljs-string">-d</span> <span class="hljs-string">chat_id=$&#123;TELEGRAM_CHAT_ID&#125;</span> <span class="hljs-string">-d</span> <span class="hljs-string">&quot;text=mritd.com deployed.&quot;</span></code></pre></div><h2 id="七、gulp-优化"><a href="#七、gulp-优化" class="headerlink" title="七、gulp 优化"></a>七、gulp 优化</h2><p>由于目前一些配图啥的还是存储在服务器本地，所以图片等比较大的静态文件仍然是访问瓶颈，这时候可以借助 gulp 来压缩并进行优化:</p><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment"># 安装 gulp</span>npm install -g gulp<span class="hljs-comment"># 安装 gulp 插件</span>npm install gulp-htmlclean gulp-htmlmin gulp-minify-css gulp-uglify-es gulp-imagemin --save<span class="hljs-comment"># 重新 link 一下</span>npm link gulp</code></pre></div><p>接下来编写 <code>gulpfile.js</code> 指定相关的优化任务</p><div class="hljs code-wrapper"><pre><code class="hljs js"><span class="hljs-keyword">var</span> gulp = <span class="hljs-built_in">require</span>(<span class="hljs-string">&#x27;gulp&#x27;</span>);<span class="hljs-keyword">var</span> minifycss = <span class="hljs-built_in">require</span>(<span class="hljs-string">&#x27;gulp-minify-css&#x27;</span>);<span class="hljs-keyword">var</span> uglify = <span class="hljs-built_in">require</span>(<span class="hljs-string">&#x27;gulp-uglify-es&#x27;</span>).default;<span class="hljs-keyword">var</span> htmlmin = <span class="hljs-built_in">require</span>(<span class="hljs-string">&#x27;gulp-htmlmin&#x27;</span>);<span class="hljs-keyword">var</span> htmlclean = <span class="hljs-built_in">require</span>(<span class="hljs-string">&#x27;gulp-htmlclean&#x27;</span>);<span class="hljs-keyword">var</span> imagemin = <span class="hljs-built_in">require</span>(<span class="hljs-string">&#x27;gulp-imagemin&#x27;</span>);<span class="hljs-comment">// 压缩html</span>gulp.task(<span class="hljs-string">&#x27;minify-html&#x27;</span>, <span class="hljs-function"><span class="hljs-keyword">function</span>(<span class="hljs-params"></span>) </span>&#123;    <span class="hljs-keyword">return</span> gulp.src(<span class="hljs-string">&#x27;./public/**/*.html&#x27;</span>)        .pipe(htmlclean())        .pipe(htmlmin(&#123;            removeComments: <span class="hljs-literal">true</span>,            minifyJS: <span class="hljs-literal">true</span>,            minifyCSS: <span class="hljs-literal">true</span>,            minifyURLs: <span class="hljs-literal">true</span>,        &#125;))        .pipe(gulp.dest(<span class="hljs-string">&#x27;./public&#x27;</span>))&#125;);<span class="hljs-comment">// 压缩css</span>gulp.task(<span class="hljs-string">&#x27;minify-css&#x27;</span>, <span class="hljs-function"><span class="hljs-keyword">function</span>(<span class="hljs-params"></span>) </span>&#123;    <span class="hljs-keyword">return</span> gulp.src(<span class="hljs-string">&#x27;./public/css/*.css&#x27;</span>)        .pipe(minifycss(&#123;            compatibility: <span class="hljs-string">&#x27;*&#x27;</span>        &#125;))        .pipe(gulp.dest(<span class="hljs-string">&#x27;./public/css&#x27;</span>));&#125;);<span class="hljs-comment">// 压缩js</span>gulp.task(<span class="hljs-string">&#x27;minify-js&#x27;</span>, <span class="hljs-function"><span class="hljs-keyword">function</span>(<span class="hljs-params"></span>) </span>&#123;    <span class="hljs-keyword">return</span> gulp.src(<span class="hljs-string">&#x27;./public/js/*.js&#x27;</span>, <span class="hljs-string">&#x27;!./public/js/*.min.js&#x27;</span>)        .pipe(uglify())        .pipe(gulp.dest(<span class="hljs-string">&#x27;./public/js&#x27;</span>));&#125;);<span class="hljs-comment">// 压缩图片</span>gulp.task(<span class="hljs-string">&#x27;minify-images&#x27;</span>, <span class="hljs-function"><span class="hljs-keyword">function</span>(<span class="hljs-params"></span>) </span>&#123;    <span class="hljs-keyword">return</span> gulp.src(<span class="hljs-string">&#x27;./public/img/*.*&#x27;</span>)        .pipe(imagemin(        [imagemin.gifsicle(&#123;<span class="hljs-string">&#x27;optimizationLevel&#x27;</span>: <span class="hljs-number">3</span>&#125;),        imagemin.mozjpeg(&#123;<span class="hljs-string">&#x27;progressive&#x27;</span>: <span class="hljs-literal">true</span>&#125;),        imagemin.optipng(&#123;<span class="hljs-string">&#x27;optimizationLevel&#x27;</span>: <span class="hljs-number">7</span>&#125;),        imagemin.svgo()],        &#123;<span class="hljs-string">&#x27;verbose&#x27;</span>: <span class="hljs-literal">true</span>&#125;))        .pipe(gulp.dest(<span class="hljs-string">&#x27;./public/img&#x27;</span>))&#125;);<span class="hljs-comment">// 默认任务</span><span class="hljs-comment">// 这里默认没有运行 minify-js，因为我发现 js 压缩以后 PageSpeed 评分</span><span class="hljs-comment">// 莫明其妙的降低了，目前只优先考虑桌面浏览器的性能，暂不考虑移动端</span>gulp.task(<span class="hljs-string">&#x27;default&#x27;</span>, gulp.parallel(    <span class="hljs-string">&#x27;minify-html&#x27;</span>,<span class="hljs-string">&#x27;minify-css&#x27;</span>,<span class="hljs-string">&#x27;minify-images&#x27;</span>));</code></pre></div><p>最后在每次部署时执行一下 <code>gulp</code> 命令即可完成优化: <code>hexo cl &amp;&amp; hexo g &amp;&amp; gulp</code></p>]]></content>
    
    
    <summary type="html">坚持写博客大约有 5 年多的时间了，以前的博客一直采用 jekyll 框架，由于一直缺少搜索等功能，而自己又不会前端，最近干脆直接切换到 Hexo 了；这里记录一下折腾过程。</summary>
    
    
    
    <category term="Linux" scheme="https://mritd.com/categories/linux/"/>
    
    
    <category term="Hexo" scheme="https://mritd.com/tags/hexo/"/>
    
  </entry>
  
  <entry>
    <title>编写一个动态准入控制来实现自动化</title>
    <link href="https://mritd.com/2020/08/19/write-a-dynamic-admission-control-webhook/"/>
    <id>https://mritd.com/2020/08/19/write-a-dynamic-admission-control-webhook/</id>
    <published>2020-08-19T06:35:00.000Z</published>
    <updated>2020-08-19T06:35:00.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="一、准入控制介绍"><a href="#一、准入控制介绍" class="headerlink" title="一、准入控制介绍"></a>一、准入控制介绍</h2><p>在 Kubernetes 整个请求链路中，请求通过认证和授权之后、对象被持久化之前需要通过一连串的 “准入控制拦截器”；这些准入控制器负载验证请求的合法性，必要情况下也可以对请求进行修改；默认准入控制器编写在 kube-apiserver 的代码中，针对于当前 kube-apiserver 默认启用的准入控制器你可以通过以下命令查看:</p><div class="hljs code-wrapper"><pre><code class="hljs sh">kube-apiserver -h | grep enable-admission-plugins</code></pre></div><p>具体每个准入控制器的作用可以通过 <a href="https://kubernetes.io/zh/docs/reference/access-authn-authz/admission-controllers/">Using Admission Controllers</a> 文档查看。在这些准入控制器中有两个特殊的准入控制器 <code>MutatingAdmissionWebhook</code> 和 <code>ValidatingAdmissionWebhook</code>。<strong>这两个准入控制器以 WebHook 的方式提供扩展能力，从而我们可以实现自定义的一些功能。当我们在集群中创建相关 WebHook 配置后，我们配置中描述的想要关注的资源在集群中创建、修改等都会触发 WebHook，我们再编写具体的应用来响应 WebHook 即可完成特定功能。</strong></p><h2 id="二、动态准入控制"><a href="#二、动态准入控制" class="headerlink" title="二、动态准入控制"></a>二、动态准入控制</h2><p>动态准入控制实际上指的就是上面所说的两个 WebHook，在使用动态准入控制时需要一些先决条件:</p><ul><li>确保 Kubernetes 集群版本至少为 v1.16 (以便使用 <code>admissionregistration.k8s.io/v1 API</code>)或者 v1.9 (以便使用 <code>admissionregistration.k8s.io/v1beta1</code> API)。</li><li>确保启用 MutatingAdmissionWebhook 和 ValidatingAdmissionWebhook 控制器。 </li><li>确保启用 <code>admissionregistration.k8s.io/v1</code> 或 <code>admissionregistration.k8s.io/v1beta1</code> API。</li></ul><p>如果要使用 Mutating Admission Webhook，在满足先决条件后，需要在系统中 create 一个 MutatingWebhookConfiguration:</p><div class="hljs code-wrapper"><pre><code class="hljs yaml"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">admissionregistration.k8s.io/v1</span><span class="hljs-attr">kind:</span> <span class="hljs-string">MutatingWebhookConfiguration</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">&quot;mutating-webhook.mritd.me&quot;</span>  <span class="hljs-attr">namespace:</span> <span class="hljs-string">kube-addons</span><span class="hljs-attr">webhooks:</span>  <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">&quot;mutating-webhook.mritd.me&quot;</span>    <span class="hljs-attr">rules:</span>      <span class="hljs-bullet">-</span> <span class="hljs-attr">apiGroups:</span>   [<span class="hljs-string">&quot;&quot;</span>]        <span class="hljs-attr">apiVersions:</span> [<span class="hljs-string">&quot;v1&quot;</span>]        <span class="hljs-attr">operations:</span>  [<span class="hljs-string">&quot;CREATE&quot;</span>,<span class="hljs-string">&quot;UPDATE&quot;</span>]        <span class="hljs-attr">resources:</span>   [<span class="hljs-string">&quot;pods&quot;</span>]        <span class="hljs-attr">scope:</span>       <span class="hljs-string">&quot;Namespaced&quot;</span>    <span class="hljs-attr">clientConfig:</span>      <span class="hljs-attr">service:</span>        <span class="hljs-attr">name:</span> <span class="hljs-string">&quot;mutating-webhook&quot;</span>        <span class="hljs-attr">namespace:</span> <span class="hljs-string">&quot;kube-addons&quot;</span>        <span class="hljs-attr">path:</span> <span class="hljs-string">/print</span>      <span class="hljs-attr">caBundle:</span> <span class="hljs-string">$&#123;CA_BUNDLE&#125;</span>    <span class="hljs-attr">admissionReviewVersions:</span> [<span class="hljs-string">&quot;v1&quot;</span>, <span class="hljs-string">&quot;v1beta1&quot;</span>]    <span class="hljs-attr">sideEffects:</span> <span class="hljs-string">None</span>    <span class="hljs-attr">timeoutSeconds:</span> <span class="hljs-number">5</span>    <span class="hljs-attr">failurePolicy:</span> <span class="hljs-string">Ignore</span>    <span class="hljs-attr">namespaceSelector:</span>      <span class="hljs-attr">matchLabels:</span>        <span class="hljs-attr">mutating-webhook.mritd.me:</span> <span class="hljs-string">&quot;true&quot;</span></code></pre></div><p>同样要使用 Validating Admission Webhook 也需要类似的配置:</p><div class="hljs code-wrapper"><pre><code class="hljs yaml"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">admissionregistration.k8s.io/v1</span><span class="hljs-attr">kind:</span> <span class="hljs-string">ValidatingWebhookConfiguration</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">&quot;validating-webhook.mritd.me&quot;</span><span class="hljs-attr">webhooks:</span>  <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">&quot;validating-webhook.mritd.me&quot;</span>    <span class="hljs-attr">rules:</span>      <span class="hljs-bullet">-</span> <span class="hljs-attr">apiGroups:</span>   [<span class="hljs-string">&quot;&quot;</span>]        <span class="hljs-attr">apiVersions:</span> [<span class="hljs-string">&quot;v1&quot;</span>]        <span class="hljs-attr">operations:</span>  [<span class="hljs-string">&quot;CREATE&quot;</span>,<span class="hljs-string">&quot;UPDATE&quot;</span>]        <span class="hljs-attr">resources:</span>   [<span class="hljs-string">&quot;pods&quot;</span>]        <span class="hljs-attr">scope:</span>       <span class="hljs-string">&quot;Namespaced&quot;</span>    <span class="hljs-attr">clientConfig:</span>      <span class="hljs-attr">service:</span>        <span class="hljs-attr">name:</span> <span class="hljs-string">&quot;validating-webhook&quot;</span>        <span class="hljs-attr">namespace:</span> <span class="hljs-string">&quot;kube-addons&quot;</span>        <span class="hljs-attr">path:</span> <span class="hljs-string">/print</span>      <span class="hljs-attr">caBundle:</span> <span class="hljs-string">$&#123;CA_BUNDLE&#125;</span>    <span class="hljs-attr">admissionReviewVersions:</span> [<span class="hljs-string">&quot;v1&quot;</span>, <span class="hljs-string">&quot;v1beta1&quot;</span>]    <span class="hljs-attr">sideEffects:</span> <span class="hljs-string">None</span>    <span class="hljs-attr">timeoutSeconds:</span> <span class="hljs-number">5</span>    <span class="hljs-attr">failurePolicy:</span> <span class="hljs-string">Ignore</span>    <span class="hljs-attr">namespaceSelector:</span>      <span class="hljs-attr">matchLabels:</span>        <span class="hljs-attr">validating-webhook.mritd.me:</span> <span class="hljs-string">&quot;true&quot;</span></code></pre></div><p>从配置文件中可以看到，<code>webhooks.rules</code> 段落中具体指定了我们想要关注的资源及其行为，<code>webhooks.clientConfig</code> 中指定了 webhook 触发后将其发送到那个地址以及证书配置等，这些具体字段的含义可以通过官方文档 <a href="https://kubernetes.io/zh/docs/reference/access-authn-authz/extensible-admission-controllers/">Dynamic Admission Control</a> 来查看。</p><p><strong>值得注意的是 Mutating Admission Webhook 会在 Validating Admission Webhook 之前触发；Mutating Admission Webhook 可以修改用户的请求，比如自动调整镜像名称、增加注解等，而 Validating Admission Webhook 只能做校验(true or false)，不可以进行修改操作。</strong></p><h2 id="三、编写一个-WebHook"><a href="#三、编写一个-WebHook" class="headerlink" title="三、编写一个 WebHook"></a>三、编写一个 WebHook</h2><blockquote><p><strong>郑重提示: 本部分文章请结合 <a href="https://github.com/mritd/goadmission">goadmission</a> 框架源码进行阅读。</strong></p></blockquote><h3 id="3-1、大体思路"><a href="#3-1、大体思路" class="headerlink" title="3.1、大体思路"></a>3.1、大体思路</h3><p>在编写之前一般我们先大体了解一下流程并制订方案再去实现，边写边思考适合在细节实现上，对于整体的把控需要提前作好预习。针对于这个准入控制的 WebHook 来说，根据其官方文档大致总结重点如下:</p><ul><li>WebHook 接收者就是一个标准的 HTTP Server，请求方式是 POST + JSON</li><li>请求响应都是一个 AdmissionReview 对象</li><li>响应时需要请求时的 UID(<code>request.uid</code>)</li><li>响应时 Mutating Admission Webhook 可以包含对请求的修改信息，格式为 JSONPatch</li></ul><p>有了以上信息以后便可以知道编写 WebHook 需要的东西，根据这些信息目前我作出的大体方案如下:</p><ul><li>最起码我们要有个 HTTP Server，考虑到后续可能会同时处理多种 WebHook，所以需要一个带有路径匹配的 HTTP 框架，Gin 什么的虽然不错但是太重，最终选择简单轻量的 <code>gorilla/mux</code>。</li><li>应该做好适当的抽象，因为对于响应需要包含的 UID 等限制在每个请求都有可以提取出来自动化完成。</li><li>针对于 Mutating Admission Webhook 响应的 JSONPatch 可以弄个结构体然后直接反序列化。</li></ul><h3 id="3-2、AdmissionReview-对象"><a href="#3-2、AdmissionReview-对象" class="headerlink" title="3.2、AdmissionReview 对象"></a>3.2、AdmissionReview 对象</h3><p>基于 3.1 部分的分析可以知道，WebHook 接收和响应都是一个 AdmissionReview 对象，在查看源码以后可以看到 AdmissionReview 结构如下:</p><p><img src="https://cdn.oss.link/markdown/jro62.png" alt="AdmissionReview"></p><p>从代码的命名中可以很清晰的看出，在请求发送到 WebHook 时我们只需要关注内部的 AdmissionRequest(实际入参)，在我们编写的 WebHook 处理完成后只需要返回包含有 AdmissionResponse(实际返回体) 的 AdmissionReview 对象即可；总的来说 <strong>AdmissionReview 对象是个套壳，请求是里面的 AdmissionRequest，响应是里面的 AdmissionResponse</strong>。</p><h3 id="3-3、Hello-World"><a href="#3-3、Hello-World" class="headerlink" title="3.3、Hello World"></a>3.3、Hello World</h3><p>有了上面的一些基础知识，我们就可以简单的实行一个什么也不干的 WebHook 方法(本地无法直接运行，重点在于思路):</p><div class="hljs code-wrapper"><pre><code class="hljs go"><span class="hljs-comment">// printRequest 接收 AdmissionRequest 对象并将其打印到到控制台，接着不做任何处理直接返回一个 AdmissionResponse 对象</span><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-title">printRequest</span><span class="hljs-params">(request *admissionv1.AdmissionRequest)</span> <span class="hljs-params">(*admissionv1.AdmissionResponse, error)</span></span> &#123;bs, err := jsoniter.MarshalIndent(request, <span class="hljs-string">&quot;&quot;</span>, <span class="hljs-string">&quot;    &quot;</span>)<span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;<span class="hljs-keyword">return</span> <span class="hljs-literal">nil</span>, err&#125;logger.Infof(<span class="hljs-string">&quot;print request: %s&quot;</span>, <span class="hljs-keyword">string</span>(bs))<span class="hljs-keyword">return</span> &amp;admissionv1.AdmissionResponse&#123;Allowed: <span class="hljs-literal">true</span>,Result: &amp;metav1.Status&#123;Code:    http.StatusOK,Message: <span class="hljs-string">&quot;Hello World&quot;</span>,&#125;,&#125;, <span class="hljs-literal">nil</span>&#125;</code></pre></div><p>上面这个 <code>printRequest</code> 方法最细粒度的控制到只面向我们的实际请求和响应；而对于 WebHook Server 来说其接到的是 http 请求，<strong>所以我们还需要在外面包装一下，将 http 请求转换为 AdmissionReview 并提取 AdmissionRequest 再调用上面的 <code>printRequest</code> 来处理，最后将返回结果重新包装为 AdmissionReview 重新返回；整体的代码如下</strong></p><div class="hljs code-wrapper"><pre><code class="hljs go"><span class="hljs-comment">// 通用的错误返回方法</span><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-title">responseErr</span><span class="hljs-params">(handlePath, msg <span class="hljs-keyword">string</span>, httpCode <span class="hljs-keyword">int</span>, w http.ResponseWriter)</span></span> &#123;logger.Errorf(<span class="hljs-string">&quot;handle func [%s] response err: %s&quot;</span>, handlePath, msg)review := &amp;admissionv1.AdmissionReview&#123;Response: &amp;admissionv1.AdmissionResponse&#123;Allowed: <span class="hljs-literal">false</span>,Result: &amp;metav1.Status&#123;Message: msg,&#125;,&#125;,&#125;bs, err := jsoniter.Marshal(review)<span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;logger.Errorf(<span class="hljs-string">&quot;failed to marshal response: %v&quot;</span>, err)w.WriteHeader(http.StatusInternalServerError)_, _ = w.Write([]<span class="hljs-keyword">byte</span>(fmt.Sprintf(<span class="hljs-string">&quot;failed to marshal response: %s&quot;</span>, err)))&#125;w.WriteHeader(httpCode)_, err = w.Write(bs)logger.Debugf(<span class="hljs-string">&quot;write err response: %d: %v: %v&quot;</span>, httpCode, review, err)&#125;<span class="hljs-comment">// printRequest 接收 AdmissionRequest 对象并将其打印到到控制台，接着不做任何处理直接返回一个 AdmissionResponse 对象</span><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-title">printRequest</span><span class="hljs-params">(request *admissionv1.AdmissionRequest)</span> <span class="hljs-params">(*admissionv1.AdmissionResponse, error)</span></span> &#123;bs, err := jsoniter.MarshalIndent(request, <span class="hljs-string">&quot;&quot;</span>, <span class="hljs-string">&quot;    &quot;</span>)<span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;<span class="hljs-keyword">return</span> <span class="hljs-literal">nil</span>, err&#125;logger.Infof(<span class="hljs-string">&quot;print request: %s&quot;</span>, <span class="hljs-keyword">string</span>(bs))<span class="hljs-keyword">return</span> &amp;admissionv1.AdmissionResponse&#123;Allowed: <span class="hljs-literal">true</span>,Result: &amp;metav1.Status&#123;Code:    http.StatusOK,Message: <span class="hljs-string">&quot;Hello World&quot;</span>,&#125;,&#125;, <span class="hljs-literal">nil</span>&#125;<span class="hljs-comment">// http server 的处理方法</span><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-title">headler</span><span class="hljs-params">(w http.ResponseWriter, r *http.Request)</span></span> &#123;<span class="hljs-keyword">defer</span> <span class="hljs-function"><span class="hljs-keyword">func</span><span class="hljs-params">()</span></span> &#123; _ = r.Body.Close() &#125;()w.Header().Set(<span class="hljs-string">&quot;Content-Type&quot;</span>, <span class="hljs-string">&quot;application/json&quot;</span>)<span class="hljs-comment">// 读取 body，出错直接返回</span>reqBs, err := ioutil.ReadAll(r.Body)<span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;responseErr(handlePath, err.Error(), http.StatusInternalServerError, w)<span class="hljs-keyword">return</span>&#125;<span class="hljs-keyword">if</span> reqBs == <span class="hljs-literal">nil</span> || <span class="hljs-built_in">len</span>(reqBs) == <span class="hljs-number">0</span> &#123;responseErr(handlePath, <span class="hljs-string">&quot;request body is empty&quot;</span>, http.StatusBadRequest, w)<span class="hljs-keyword">return</span>&#125;logger.Debugf(<span class="hljs-string">&quot;request body: %s&quot;</span>, <span class="hljs-keyword">string</span>(reqBs))<span class="hljs-comment">// 将 body 反序列化为 AdmissionReview</span>reqReview := admissionv1.AdmissionReview&#123;&#125;<span class="hljs-keyword">if</span> _, _, err := deserializer.Decode(reqBs, <span class="hljs-literal">nil</span>, &amp;reqReview); err != <span class="hljs-literal">nil</span> &#123;responseErr(handlePath, fmt.Sprintf(<span class="hljs-string">&quot;failed to decode req: %s&quot;</span>, err), http.StatusInternalServerError, w)<span class="hljs-keyword">return</span>&#125;<span class="hljs-keyword">if</span> reqReview.Request == <span class="hljs-literal">nil</span> &#123;responseErr(handlePath, <span class="hljs-string">&quot;admission review request is empty&quot;</span>, http.StatusBadRequest, w)<span class="hljs-keyword">return</span>&#125;<span class="hljs-comment">// 提取 AdmissionRequest 并调用 printRequest 处理</span>resp, err := printRequest(reqReview.Request)<span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;responseErr(handlePath, fmt.Sprintf(<span class="hljs-string">&quot;admission func response: %s&quot;</span>, err), http.StatusForbidden, w)<span class="hljs-keyword">return</span>&#125;<span class="hljs-keyword">if</span> resp == <span class="hljs-literal">nil</span> &#123;responseErr(handlePath, <span class="hljs-string">&quot;admission func response is empty&quot;</span>, http.StatusInternalServerError, w)<span class="hljs-keyword">return</span>&#125;<span class="hljs-comment">// 复制 AdmissionRequest 中的 UID 到 AdmissionResponse 中(必须进行，否则会导致响应无效)</span>resp.UID = reqReview.Request.UID<span class="hljs-comment">// 复制 reqReview.TypeMeta 到新的响应 AdmissionReview 中</span>respReview := admissionv1.AdmissionReview&#123;TypeMeta: reqReview.TypeMeta,Response: resp,&#125;<span class="hljs-comment">// 重新序列化响应并返回</span>respBs, err := jsoniter.Marshal(respReview)<span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;responseErr(handlePath, fmt.Sprintf(<span class="hljs-string">&quot;failed to marshal response: %s&quot;</span>, err), http.StatusInternalServerError, w)logger.Errorf(<span class="hljs-string">&quot;the expected response is: %v&quot;</span>, respReview)<span class="hljs-keyword">return</span>&#125;w.WriteHeader(http.StatusOK)_, err = w.Write(respBs)logger.Debugf(<span class="hljs-string">&quot;write response: %d: %s: %v&quot;</span>, http.StatusOK, <span class="hljs-keyword">string</span>(respBs), err)&#125;</code></pre></div><h3 id="3-4、抽象出框架"><a href="#3-4、抽象出框架" class="headerlink" title="3.4、抽象出框架"></a>3.4、抽象出框架</h3><p>编写了简单的 Hello World 以后可以看出，真正在编写时我们需要实现的都是处理 AdmissionRequest 并返回 AdmissionResponse 这部份(printRequest)；外部的包装为 AdmissionReview、复制 UID、复制 TypeMeta 等都是通用的方法，所以基于这一点我们可以进行适当的抽象:</p><h4 id="3-4-1、AdmissionFunc"><a href="#3-4-1、AdmissionFunc" class="headerlink" title="3.4.1、AdmissionFunc"></a>3.4.1、AdmissionFunc</h4><p>针对每一个贴合业务的 WebHook 来说，其大致有三大属性:</p><ul><li>WebHook 的类型(Mutating/Validating)</li><li>WebHook 拦截的 URL 路径(/print_request)</li><li>WebHook 核心的处理逻辑(处理 Request 和返回 Response)</li></ul><p>我们将其抽象为 AdmissionFunc 结构体以后如下所示</p><div class="hljs code-wrapper"><pre><code class="hljs go"><span class="hljs-comment">// WebHook 类型</span><span class="hljs-keyword">const</span> (Mutating   AdmissionFuncType = <span class="hljs-string">&quot;Mutating&quot;</span>Validating AdmissionFuncType = <span class="hljs-string">&quot;Validating&quot;</span>)<span class="hljs-comment">// 每一个对应到我们业务的 WebHook 抽象的 struct</span><span class="hljs-keyword">type</span> AdmissionFunc <span class="hljs-keyword">struct</span> &#123;Type AdmissionFuncTypePath <span class="hljs-keyword">string</span>Func <span class="hljs-function"><span class="hljs-keyword">func</span><span class="hljs-params">(request *admissionv1.AdmissionRequest)</span> <span class="hljs-params">(*admissionv1.AdmissionResponse, error)</span></span>&#125;</code></pre></div><h4 id="3-4-2、HandleFunc"><a href="#3-4-2、HandleFunc" class="headerlink" title="3.4.2、HandleFunc"></a>3.4.2、HandleFunc</h4><p>我们知道 WebHook 是基于 HTTP 的，所以上面抽象出的 AdmissionFunc 还不能直接用在 HTTP 请求代码中；如果直接偶合到 HTTP 请求代码中，我们就没法为 HTTP 代码再增加其他拦截路径等等特殊的底层设置；<strong>所以站在 HTTP 层面来说还需要抽象一个 “更高层面的且包含 AdmissionFunc 全部能力的 HandleFunc” 来使用；HandleFunc 抽象 HTTP 层面的需求:</strong></p><ul><li>HTTP 请求方法</li><li>HTTP 请求路径</li><li>HTTP 处理方法</li></ul><p>以下为 HandleFunc 的抽象:</p><div class="hljs code-wrapper"><pre><code class="hljs go"><span class="hljs-keyword">type</span> HandleFunc <span class="hljs-keyword">struct</span> &#123;Path   <span class="hljs-keyword">string</span>Method <span class="hljs-keyword">string</span>Func   <span class="hljs-function"><span class="hljs-keyword">func</span><span class="hljs-params">(w http.ResponseWriter, r *http.Request)</span></span>&#125;</code></pre></div><h3 id="3-5、goadmission-框架"><a href="#3-5、goadmission-框架" class="headerlink" title="3.5、goadmission 框架"></a>3.5、goadmission 框架</h3><p>有了以上两个角度的抽象，再结合 命令行参数解析、日志处理、配置文件读取等等，我揉合出了一个 <a href="https://github.com/mritd/goadmission">goadmission</a> 框架，以方便动态准入控制的快速开发。</p><h4 id="3-5-1、基本结构"><a href="#3-5-1、基本结构" class="headerlink" title="3.5.1、基本结构"></a>3.5.1、基本结构</h4><div class="hljs code-wrapper"><pre><code class="hljs sh">.├── main.go└── pkg    ├── adfunc    │   ├── adfuncs.go    │   ├── adfuncs_json.go    │   ├── func_check_deploy_time.go    │   ├── func_disable_service_links.go    │   ├── func_image_rename.go    │   └── func_print_request.go    ├── conf    │   └── conf.go    ├── route    │   ├── route_available.go    │   ├── route_health.go    │   └── router.go    └── zaplogger        ├── config.go        └── logger.go5 directories, 13 files</code></pre></div><ul><li>main.go 为程序运行入口，在此设置命令行 flag 参数等</li><li>pkg/conf 为框架配置包，所有的配置读取只读取这个包即可</li><li>pkg/zaplogger zap log 库的日志抽象和处理(copy 自 operator-sdk)</li><li>pkg/route http 级别的路由抽象(HandleFunc)</li><li>pkg/adfunc 动态准入控制 WebHook 级别的抽(AdmissionFunc)</li></ul><h4 id="3-5-2、增加动态准入控制"><a href="#3-5-2、增加动态准入控制" class="headerlink" title="3.5.2、增加动态准入控制"></a>3.5.2、增加动态准入控制</h4><p>由于框架已经作好了路由注册等相关抽象，所以只需要新建 go 文件，然后通过 init 方法注册到全局 WebHook 组中即可，新编写的 WebHook 对已有代码不会有任何侵入:</p><p><img src="https://cdn.oss.link/markdown/lg6zc.png" alt="add_adfunc"></p><p><strong>需要注意的是所有 validating 类型的 WebHook 会在 URL 路径前自动拼接 <code>/validating</code> 路径，mutating 类型的 WebHook 会在 URL 路径前自动拼接 <code>/mutating</code> 路径；</strong>这么做是为了避免在更高层级的 HTTP Route 上添加冲突的路由。</p><p><img src="https://cdn.oss.link/markdown/nd5ez.png" alt="auto_fix_url"></p><h4 id="3-5-3、实现-image-自动修改"><a href="#3-5-3、实现-image-自动修改" class="headerlink" title="3.5.3、实现 image 自动修改"></a>3.5.3、实现 image 自动修改</h4><p>所以一切准备就绪以后，就需要 “不忘初心”，撸一个自动修改镜像名称的 WebHook:</p><div class="hljs code-wrapper"><pre><code class="hljs go"><span class="hljs-keyword">package</span> adfunc<span class="hljs-keyword">import</span> (<span class="hljs-string">&quot;fmt&quot;</span><span class="hljs-string">&quot;net/http&quot;</span><span class="hljs-string">&quot;strings&quot;</span><span class="hljs-string">&quot;sync&quot;</span><span class="hljs-string">&quot;time&quot;</span><span class="hljs-string">&quot;github.com/mritd/goadmission/pkg/conf&quot;</span>jsoniter <span class="hljs-string">&quot;github.com/json-iterator/go&quot;</span>corev1 <span class="hljs-string">&quot;k8s.io/api/core/v1&quot;</span>metav1 <span class="hljs-string">&quot;k8s.io/apimachinery/pkg/apis/meta/v1&quot;</span><span class="hljs-string">&quot;github.com/mritd/goadmission/pkg/route&quot;</span>admissionv1 <span class="hljs-string">&quot;k8s.io/api/admission/v1&quot;</span>)<span class="hljs-comment">// 只初始化一次 renameMap</span><span class="hljs-keyword">var</span> renameOnce sync.Once<span class="hljs-comment">// renameMap 保存镜像名称的替换规则，目前粗略实现为纯文本替换</span><span class="hljs-keyword">var</span> renameMap <span class="hljs-keyword">map</span>[<span class="hljs-keyword">string</span>]<span class="hljs-keyword">string</span><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-title">init</span><span class="hljs-params">()</span></span> &#123;route.Register(route.AdmissionFunc&#123;Type: route.Mutating,Path: <span class="hljs-string">&quot;/rename&quot;</span>,Func: <span class="hljs-function"><span class="hljs-keyword">func</span><span class="hljs-params">(request *admissionv1.AdmissionRequest)</span> <span class="hljs-params">(*admissionv1.AdmissionResponse, error)</span></span> &#123;<span class="hljs-comment">// init rename rules map</span>renameOnce.Do(<span class="hljs-function"><span class="hljs-keyword">func</span><span class="hljs-params">()</span></span> &#123;renameMap = <span class="hljs-built_in">make</span>(<span class="hljs-keyword">map</span>[<span class="hljs-keyword">string</span>]<span class="hljs-keyword">string</span>, <span class="hljs-number">10</span>)<span class="hljs-comment">// 将镜像重命名规则初始化到 renameMap 中，方便后续读取</span><span class="hljs-comment">// rename rule example: k8s.gcr.io/=gcrxio/k8s.gcr.io_</span><span class="hljs-keyword">for</span> _, s := <span class="hljs-keyword">range</span> conf.ImageRename &#123;ss := strings.Split(s, <span class="hljs-string">&quot;=&quot;</span>)<span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(ss) != <span class="hljs-number">2</span> &#123;logger.Fatalf(<span class="hljs-string">&quot;failed to parse image name rename rules: %s&quot;</span>, s)&#125;renameMap[ss[<span class="hljs-number">0</span>]] = ss[<span class="hljs-number">1</span>]&#125;&#125;)<span class="hljs-comment">// 这个准入控制的 WebHook 只针对 Pod 处理，非 Pod 类请求直接返回错误</span><span class="hljs-keyword">switch</span> request.Kind.Kind &#123;<span class="hljs-keyword">case</span> <span class="hljs-string">&quot;Pod&quot;</span>:<span class="hljs-comment">// 从 request 中反序列化出 Pod 实例</span><span class="hljs-keyword">var</span> pod corev1.Poderr := jsoniter.Unmarshal(request.Object.Raw, &amp;pod)<span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;errMsg := fmt.Sprintf(<span class="hljs-string">&quot;[route.Mutating] /rename: failed to unmarshal object: %v&quot;</span>, err)logger.Error(errMsg)<span class="hljs-keyword">return</span> &amp;admissionv1.AdmissionResponse&#123;Allowed: <span class="hljs-literal">false</span>,Result: &amp;metav1.Status&#123;Code:    http.StatusBadRequest,Message: errMsg,&#125;,&#125;, <span class="hljs-literal">nil</span>&#125;<span class="hljs-comment">// 后来我发现带有下面这个注解的 Pod 是没法更改成功的，这种 Pod 是由 kubelet 直接</span><span class="hljs-comment">// 启动的 static pod，在 api server 中只能看到它的 &quot;mirror&quot;，不能改的</span><span class="hljs-comment">// skip static pod</span><span class="hljs-keyword">for</span> k := <span class="hljs-keyword">range</span> pod.Annotations &#123;<span class="hljs-keyword">if</span> k == <span class="hljs-string">&quot;kubernetes.io/config.mirror&quot;</span> &#123;errMsg := fmt.Sprintf(<span class="hljs-string">&quot;[route.Mutating] /rename: pod %s has kubernetes.io/config.mirror annotation, skip image rename&quot;</span>, pod.Name)logger.Warn(errMsg)<span class="hljs-keyword">return</span> &amp;admissionv1.AdmissionResponse&#123;Allowed: <span class="hljs-literal">true</span>,Result: &amp;metav1.Status&#123;Code:    http.StatusOK,Message: errMsg,&#125;,&#125;, <span class="hljs-literal">nil</span>&#125;&#125;<span class="hljs-comment">// 遍历所有 Pod，然后生成 JSONPatch</span><span class="hljs-comment">// 注意: 返回结果必须是 JSONPatch，k8s api server 再将 JSONPatch 应用到 Pod 上 </span><span class="hljs-comment">// 由于有多个 Pod，所以最终会产生一个补丁数组</span><span class="hljs-keyword">var</span> patches []Patch<span class="hljs-keyword">for</span> i, c := <span class="hljs-keyword">range</span> pod.Spec.Containers &#123;<span class="hljs-keyword">for</span> s, t := <span class="hljs-keyword">range</span> renameMap &#123;<span class="hljs-keyword">if</span> strings.HasPrefix(c.Image, s) &#123;patches = <span class="hljs-built_in">append</span>(patches, Patch&#123;<span class="hljs-comment">// 指定 JSONPatch 动作为 replace </span>Option: PatchOptionReplace,<span class="hljs-comment">// 打补丁的绝对位置</span>Path:   fmt.Sprintf(<span class="hljs-string">&quot;/spec/containers/%d/image&quot;</span>, i),<span class="hljs-comment">// replace 为处理过的镜像名</span>Value:  strings.Replace(c.Image, s, t, <span class="hljs-number">1</span>),&#125;)<span class="hljs-comment">// 为了后期调试和留存历史，我们再为修改过的 Pod 加个注解</span>patches = <span class="hljs-built_in">append</span>(patches, Patch&#123;Option: PatchOptionAdd,Path:   <span class="hljs-string">&quot;/metadata/annotations&quot;</span>,Value: <span class="hljs-keyword">map</span>[<span class="hljs-keyword">string</span>]<span class="hljs-keyword">string</span>&#123;fmt.Sprintf(<span class="hljs-string">&quot;rename-mutatingwebhook-%d.mritd.me&quot;</span>, time.Now().Unix()): fmt.Sprintf(<span class="hljs-string">&quot;%d-%s-%s&quot;</span>, i, strings.ReplaceAll(s, <span class="hljs-string">&quot;/&quot;</span>, <span class="hljs-string">&quot;_&quot;</span>), strings.ReplaceAll(t, <span class="hljs-string">&quot;/&quot;</span>, <span class="hljs-string">&quot;_&quot;</span>)),&#125;,&#125;)<span class="hljs-keyword">break</span>&#125;&#125;&#125;<span class="hljs-comment">// 将所有 JSONPatch 序列化成 json，然后返回即可</span>patch, err := jsoniter.Marshal(patches)<span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;errMsg := fmt.Sprintf(<span class="hljs-string">&quot;[route.Mutating] /rename: failed to marshal patch: %v&quot;</span>, err)logger.Error(errMsg)<span class="hljs-keyword">return</span> &amp;admissionv1.AdmissionResponse&#123;Allowed: <span class="hljs-literal">false</span>,Result: &amp;metav1.Status&#123;Code:    http.StatusInternalServerError,Message: errMsg,&#125;,&#125;, <span class="hljs-literal">nil</span>&#125;logger.Infof(<span class="hljs-string">&quot;[route.Mutating] /rename: patches: %s&quot;</span>, <span class="hljs-keyword">string</span>(patch))<span class="hljs-keyword">return</span> &amp;admissionv1.AdmissionResponse&#123;Allowed:   <span class="hljs-literal">true</span>,Patch:     patch,PatchType: JSONPatch(),Result: &amp;metav1.Status&#123;Code:    http.StatusOK,Message: <span class="hljs-string">&quot;success&quot;</span>,&#125;,&#125;, <span class="hljs-literal">nil</span><span class="hljs-keyword">default</span>:errMsg := fmt.Sprintf(<span class="hljs-string">&quot;[route.Mutating] /rename: received wrong kind request: %s, Only support Kind: Pod&quot;</span>, request.Kind.Kind)logger.Error(errMsg)<span class="hljs-keyword">return</span> &amp;admissionv1.AdmissionResponse&#123;Allowed: <span class="hljs-literal">false</span>,Result: &amp;metav1.Status&#123;Code:    http.StatusForbidden,Message: errMsg,&#125;,&#125;, <span class="hljs-literal">nil</span>&#125;&#125;,&#125;)&#125;</code></pre></div><h2 id="四、总结"><a href="#四、总结" class="headerlink" title="四、总结"></a>四、总结</h2><ul><li>动态准入控制其实就是个 WebHook，我们弄个 HTTP Server 接收 AdmissionRequest 响应 AdmissionResponse 就行。</li><li>Request、Response 会包装到 AdmissionReview 中，我们还需要做一些边缘处理，比如复制 UID、TypeMeta 等</li><li>MutatingWebHook 想要修改东西时，要返回描述修改操作的 JSONPatch 补丁</li><li>单个 WebHook 很简单，写多个的时候要自己抽好框架，尽量优雅的作好复用和封装</li></ul>]]></content>
    
    
    <summary type="html">前段时间弄了一个 imgsync 的工具把 gcr.io 的镜像搬运到了 Docker Hub，但是即使这样我每次还是需要编辑 yaml 配置手动改镜像名称；所以我萌生了一个想法: 能不能自动化这个过程？</summary>
    
    
    
    <category term="Kubernetes" scheme="https://mritd.com/categories/kubernetes/"/>
    
    
    <category term="Admission" scheme="https://mritd.com/tags/admission/"/>
    
  </entry>
  
  <entry>
    <title>使用 etcdadm 三分钟搭建 etcd 集群</title>
    <link href="https://mritd.com/2020/08/19/use-etcdadm-to-build-etcd-cluster-in-3-minutes/"/>
    <id>https://mritd.com/2020/08/19/use-etcdadm-to-build-etcd-cluster-in-3-minutes/</id>
    <published>2020-08-19T06:20:00.000Z</published>
    <updated>2020-08-19T06:20:00.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="一、介绍"><a href="#一、介绍" class="headerlink" title="一、介绍"></a>一、介绍</h2><p>在搭建 Kubernetes 集群的过程中首先要搞定 Etcd 集群，虽然说 kubeadm 工具已经提供了默认和 master 节点绑定的 Etcd 集群自动搭建方式，但是我个人一直是手动将 Etcd 集群搭建在宿主机；<strong>因为这个玩意太重要了，毫不夸张的说 kubernetes 所有组件崩溃我们都能在一定时间以后排查问题恢复，但是一旦 Etcd 集群没了那么 Kubernetes 集群也就真没了。</strong></p><p>在很久以前我创建了 <a href="https://github.com/Gozap/edep">edep</a> 工具来实现 Etcd 集群的辅助部署，再后来由于我们的底层系统偶合了 Ubuntu，所以创建了 <a href="https://github.com/mritd/etcd-deb">etcd-deb</a> 项目来自动打 deb 包来直接安装；最近逛了一下 Kubernetes 的相关项目，发现跟我的 edep 差不多的项目 <a href="https://github.com/kubernetes-sigs/etcdadm">etcdadm</a>，试了一下 “真香”。</p><h2 id="二、安装"><a href="#二、安装" class="headerlink" title="二、安装"></a>二、安装</h2><p><a href="https://github.com/kubernetes-sigs/etcdadm">etcdadm</a> 项目是使用 go 编写的，所以很明显只有一个二进制下载下来就能用:</p><div class="hljs code-wrapper"><pre><code class="hljs sh">wget https://github.com/kubernetes-sigs/etcdadm/releases/download/v0.1.3/etcdadm-linux-amd64chmod +x etcdadm-linux-amd64</code></pre></div><h2 id="三、使用"><a href="#三、使用" class="headerlink" title="三、使用"></a>三、使用</h2><h3 id="3-1、启动引导节点"><a href="#3-1、启动引导节点" class="headerlink" title="3.1、启动引导节点"></a>3.1、启动引导节点</h3><p>类似 kubeadm 一样，etcdadm 也是先启动第一个节点，然后后续节点直接 join 即可；第一个节点启动只需要执行 <code>etcdadm init</code> 命令即可:</p><div class="hljs code-wrapper"><pre><code class="hljs sh">k1.node ➜  ~ ./etcdadm-linux-amd64 initINFO[0000] [install] extracting etcd archive /var/cache/etcdadm/etcd/v3.3.8/etcd-v3.3.8-linux-amd64.tar.gz to /tmp/etcd664686683INFO[0001] [install] verifying etcd 3.3.8 is installed <span class="hljs-keyword">in</span> /opt/bin/INFO[0001] [certificates] creating PKI assetsINFO[0001] creating a self signed etcd CA certificate and key files[certificates] Generated ca certificate and key.INFO[0001] creating a new server certificate and key files <span class="hljs-keyword">for</span> etcd[certificates] Generated server certificate and key.[certificates] server serving cert is signed <span class="hljs-keyword">for</span> DNS names [k1.node] and IPs [127.0.0.1 172.16.10.21]INFO[0002] creating a new certificate and key files <span class="hljs-keyword">for</span> etcd peering[certificates] Generated peer certificate and key.[certificates] peer serving cert is signed <span class="hljs-keyword">for</span> DNS names [k1.node] and IPs [172.16.10.21]INFO[0002] creating a new client certificate <span class="hljs-keyword">for</span> the etcdctl[certificates] Generated etcdctl-etcd-client certificate and key.INFO[0002] creating a new client certificate <span class="hljs-keyword">for</span> the apiserver calling etcd[certificates] Generated apiserver-etcd-client certificate and key.[certificates] valid certificates and keys now exist <span class="hljs-keyword">in</span> <span class="hljs-string">&quot;/etc/etcd/pki&quot;</span>INFO[0006] [health] Checking <span class="hljs-built_in">local</span> etcd endpoint healthINFO[0006] [health] Local etcd endpoint is healthyINFO[0006] To add another member to the cluster, copy the CA cert/key to its certificate dir and run:INFO[0006]      etcdadm join https://172.16.10.21:2379</code></pre></div><p>从命令行输出可以看到不同阶段 etcdadm 的相关日志输出；在 <code>init</code> 命令时可以指定一些特定参数来覆盖默认行为，比如版本号、安装目录等:</p><div class="hljs code-wrapper"><pre><code class="hljs sh">k1.node ➜  ~ ./etcdadm-linux-amd64 init --<span class="hljs-built_in">help</span>Initialize a new etcd clusterUsage:  etcdadm init [flags]Flags:      --certs-dir string                    certificates directory (default <span class="hljs-string">&quot;/etc/etcd/pki&quot;</span>)      --disk-priorities stringArray         Setting etcd disk priority (default [Nice=-10,IOSchedulingClass=best-effort,IOSchedulingPriority=2])      --download-connect-timeout duration   Maximum time <span class="hljs-keyword">in</span> seconds that you allow the connection to the server to take. (default 10s)  -h, --<span class="hljs-built_in">help</span>                                <span class="hljs-built_in">help</span> <span class="hljs-keyword">for</span> init      --install-dir string                  install directory (default <span class="hljs-string">&quot;/opt/bin/&quot;</span>)      --name string                         etcd member name      --release-url string                  URL used to download etcd (default <span class="hljs-string">&quot;https://github.com/coreos/etcd/releases/download&quot;</span>)      --server-cert-extra-sans strings      optional extra Subject Alternative Names <span class="hljs-keyword">for</span> the etcd server signing cert, can be multiple comma separated DNS names or IPs      --skip-hash-check                     Ignore snapshot integrity <span class="hljs-built_in">hash</span> value (required <span class="hljs-keyword">if</span> copied from data directory)      --snapshot string                     Etcd v3 snapshot file used to initialize member      --version string                      etcd version (default <span class="hljs-string">&quot;3.3.8&quot;</span>)Global Flags:  -l, --log-level string   <span class="hljs-built_in">set</span> <span class="hljs-built_in">log</span> level <span class="hljs-keyword">for</span> output, permitted values debug, info, warn, error, fatal and panic (default <span class="hljs-string">&quot;info&quot;</span>)</code></pre></div><h3 id="3-2、其他节点加入"><a href="#3-2、其他节点加入" class="headerlink" title="3.2、其他节点加入"></a>3.2、其他节点加入</h3><p>在首个节点启动完成后，将集群 ca 证书复制到其他节点然后执行 <code>etcdadm join ENDPOINT_ADDRESS</code> 即可:</p><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment"># 复制 ca 证书</span>k1.node ➜  ~ rsync -avR /etc/etcd/pki/ca.* 172.16.10.22:/root@172.16.10.22<span class="hljs-string">&#x27;s password:</span><span class="hljs-string">sending incremental file list</span><span class="hljs-string">/etc/etcd/</span><span class="hljs-string">/etc/etcd/pki/</span><span class="hljs-string">/etc/etcd/pki/ca.crt</span><span class="hljs-string">/etc/etcd/pki/ca.key</span><span class="hljs-string"></span><span class="hljs-string">sent 2,932 bytes  received 67 bytes  856.86 bytes/sec</span><span class="hljs-string">total size is 2,684  speedup is 0.89</span><span class="hljs-string"></span><span class="hljs-string"># 执行 join</span><span class="hljs-string">k2.node ➜  ~ ./etcdadm-linux-amd64 join https://172.16.10.21:2379</span><span class="hljs-string">INFO[0000] [certificates] creating PKI assets</span><span class="hljs-string">INFO[0000] creating a self signed etcd CA certificate and key files</span><span class="hljs-string">[certificates] Using the existing ca certificate and key.</span><span class="hljs-string">INFO[0000] creating a new server certificate and key files for etcd</span><span class="hljs-string">[certificates] Generated server certificate and key.</span><span class="hljs-string">[certificates] server serving cert is signed for DNS names [k2.node] and IPs [172.16.10.22 127.0.0.1]</span><span class="hljs-string">INFO[0000] creating a new certificate and key files for etcd peering</span><span class="hljs-string">[certificates] Generated peer certificate and key.</span><span class="hljs-string">[certificates] peer serving cert is signed for DNS names [k2.node] and IPs [172.16.10.22]</span><span class="hljs-string">INFO[0000] creating a new client certificate for the etcdctl</span><span class="hljs-string">[certificates] Generated etcdctl-etcd-client certificate and key.</span><span class="hljs-string">INFO[0001] creating a new client certificate for the apiserver calling etcd</span><span class="hljs-string">[certificates] Generated apiserver-etcd-client certificate and key.</span><span class="hljs-string">[certificates] valid certificates and keys now exist in &quot;/etc/etcd/pki&quot;</span><span class="hljs-string">INFO[0001] [membership] Checking if this member was added</span><span class="hljs-string">INFO[0001] [membership] Member was not added</span><span class="hljs-string">INFO[0001] Removing existing data dir &quot;/var/lib/etcd&quot;</span><span class="hljs-string">INFO[0001] [membership] Adding member</span><span class="hljs-string">INFO[0001] [membership] Checking if member was started</span><span class="hljs-string">INFO[0001] [membership] Member was not started</span><span class="hljs-string">INFO[0001] [membership] Removing existing data dir &quot;/var/lib/etcd&quot;</span><span class="hljs-string">INFO[0001] [install] extracting etcd archive /var/cache/etcdadm/etcd/v3.3.8/etcd-v3.3.8-linux-amd64.tar.gz to /tmp/etcd315786364</span><span class="hljs-string">INFO[0003] [install] verifying etcd 3.3.8 is installed in /opt/bin/</span><span class="hljs-string">INFO[0006] [health] Checking local etcd endpoint health</span><span class="hljs-string">INFO[0006] [health] Local etcd endpoint is healthy</span></code></pre></div><h2 id="四、细节分析"><a href="#四、细节分析" class="headerlink" title="四、细节分析"></a>四、细节分析</h2><h3 id="4-1、默认配置"><a href="#4-1、默认配置" class="headerlink" title="4.1、默认配置"></a>4.1、默认配置</h3><p>在目前 etcdadm 尚未支持配置文件，目前所有默认配置存放在 <a href="https://github.com/kubernetes-sigs/etcdadm/blob/master/constants/constants.go#L22">constants.go</a> 中，这里面包含了默认安装位置、systemd 配置、环境变量配置等，限于篇幅请自行查看代码；下面简单介绍一些一些刚须的配置:</p><h4 id="4-1-1、etcdctl"><a href="#4-1-1、etcdctl" class="headerlink" title="4.1.1、etcdctl"></a>4.1.1、etcdctl</h4><p>etcdctl 默认安装在 <code>/opt/bin</code> 目录下，同时你会发现该目录下还存在一个 <code>etcdctl.sh</code> 脚本，<strong>这个脚本将会自动读取 etcdctl 配置文件(<code>/etc/etcd/etcdctl.env</code>)，所以推荐使用这个脚本来替代 etcdctl 命令。</strong></p><h4 id="4-1-2、数据目录"><a href="#4-1-2、数据目录" class="headerlink" title="4.1.2、数据目录"></a>4.1.2、数据目录</h4><p>默认的数据目录存储在 <code>/var/lib/etcd</code> 目录，目前 etcdadm 尚未提供任何可配置方式，当然你可以自己改源码。</p><h4 id="4-2-3、配置文件"><a href="#4-2-3、配置文件" class="headerlink" title="4.2.3、配置文件"></a>4.2.3、配置文件</h4><p>配置文件总共有两个，一个是 <code>/etc/etcd/etcdctl.env</code> 用于 <code>/opt/bin/etcdctl.sh</code> 读取；另一个是 <code>/etc/etcd/etcd.env</code> 用于 systemd 读取并启动 etcd server。</p><h3 id="4-2、Join-流程"><a href="#4-2、Join-流程" class="headerlink" title="4.2、Join 流程"></a>4.2、Join 流程</h3><blockquote><p>其实很久以前由于我自己部署方式导致了我一直以来理解的一个错误，我一直以为 etcd server 证书要包含所有 server 地址，当然这个想法是怎么来的我也不知道，但是当我看了以下 Join 操作源码以后突然意识到 “为什么要包含所有？包含当前 server 不就行了么。”；当然对于 HTTPS 证书的理解一直是明白的，但是很奇怪就是不知道怎么就产生了这个想法(哈哈，我自己都觉的不可思议)…</p></blockquote><ul><li>由于预先拷贝了 ca 证书，所以 join 开始前 etcdadm 使用这个 ca 证书会签发自己需要的所有证书。</li><li>接下来 etcdadmin 通过 etcdctl-etcd-client 证书创建 client，然后调用 <code>MemberAdd</code> 添加新集群</li><li>最后老套路下载安装+启动就完成了</li></ul><h3 id="4-3、目前不足"><a href="#4-3、目前不足" class="headerlink" title="4.3、目前不足"></a>4.3、目前不足</h3><p>目前 etcdadm 虽然已经基本生产可用，但是仍有些不足的地方:</p><ul><li>不支持配置文件，很多东西无法定制</li><li>join 加入集群是在内部 api 完成，并未持久化到物理配置文件，后续重建可能忘记节点 ip</li><li>集群证书目前不支持自动续期，默认证书为 1 年很容易过期</li><li>下载动作调用了系统命令(curl)依赖性有点强</li><li>日志格式有点不友好，比如 level 和日期</li></ul>]]></content>
    
    
    <summary type="html">本文介绍一下 etcd 宿主机部署的新玩具 etcdadm，类似 kubeadm 一样可以快速的在宿主机搭建 Etcd 集群。</summary>
    
    
    
    <category term="Kubernetes" scheme="https://mritd.com/categories/kubernetes/"/>
    
    
    <category term="etcd" scheme="https://mritd.com/tags/etcd/"/>
    
  </entry>
  
  <entry>
    <title>如何编写 CSI 插件</title>
    <link href="https://mritd.com/2020/08/19/how-to-write-a-csi-driver-for-kubernetes/"/>
    <id>https://mritd.com/2020/08/19/how-to-write-a-csi-driver-for-kubernetes/</id>
    <published>2020-08-19T06:10:00.000Z</published>
    <updated>2020-08-19T06:10:00.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="一、为什么需要-CSI"><a href="#一、为什么需要-CSI" class="headerlink" title="一、为什么需要 CSI"></a>一、为什么需要 CSI</h2><p>在 Kubernetes 以前的版本中，其所有受官方支持的存储驱动全部在 Kubernetes 的主干代码中，其他第三方开发的自定义插件通过 <a href="https://github.com/kubernetes/community/blob/master/contributors/devel/sig-storage/flexvolume.md">FlexVolume</a> 插件的形势提供服务；<strong>相对于 kubernetes 的源码树来说，内置的存储我们称之为 “树内存储”，外部第三方实现我们称之为 “树外存储”；</strong>在很长一段时间里树内存储和树外存储并行开发和使用，但是随着时间推移渐渐的就出现了很严重的问题:</p><ul><li>想要添加官方支持的存储必须在树内修改，这意味着需要 Kubernetes 发版</li><li>如果树内存储出现问题则也必须等待 Kubernetes 发版才能修复</li></ul><p>为了解决这种尴尬的问题，Kubernetes 必须抽象出一个合适的存储接口，并将所有存储驱动全部适配到这个接口上，存储驱动最好与 Kubernetes 之间进行 RPC 调用完成解耦，这样就造就了 CSI(Container Storage Interface)。</p><h2 id="二、CSI-基础知识"><a href="#二、CSI-基础知识" class="headerlink" title="二、CSI 基础知识"></a>二、CSI 基础知识</h2><h3 id="2-1、CSI-Sidecar-Containers"><a href="#2-1、CSI-Sidecar-Containers" class="headerlink" title="2.1、CSI Sidecar Containers"></a>2.1、CSI Sidecar Containers</h3><p>在开发 CSI 之前我们最好熟悉一下 CSI 开发中的一些常识；了解过 Kubernetes API 开发的朋友应该清楚，所有的资源定义(Deployment、Service…)在 Kubernetes 中其实就是一个 Object，此时可以将 Kubernetes 看作是一个 Database，无论是 Operator 还是 CSI 其核心本质都是不停的 Watch 特定的 Object，一但 kubectl 或者其他客户端 “动了” 这个 Object，我们的对应实现程序就 Watch 到变更然后作出相应的响应；<strong>对于 CSI 编写者来说，这些 Watch 动作已经不必自己实现 <a href="https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/#custom-controllers">Custom Controller</a>，官方为我们提供了 <a href="https://kubernetes-csi.github.io/docs/sidecar-containers.html">CSI Sidecar Containers</a>；</strong>并且在新版本中这些 Sidecar Containers 实现极其完善，比如自动的多节点 HA(Etcd 选举)等。</p><p><strong>所以到迄今为止，所谓的 CSI 插件开发事实上并非面向 Kubernetes API 开发，而是面向 Sidecar Containers 的 gRPC 开发，Sidecar Containers 一般会和我们自己开发的 CSI 驱动程序在同一个 Pod 中启动，然后 Sidecar Containers Watch API 中 CSI 相关 Object 的变动，接着通过本地 unix 套接字调用我们编写的 CSI 驱动：</strong></p><p><img src="https://cdn.oss.link/markdown/10w5g.png" alt="CSI_Sidecar_Containers"></p><p>目前官方提供的 Sidecar Containers 如下:</p><ul><li><a href="https://kubernetes-csi.github.io/docs/external-provisioner.html">external-provisioner</a></li><li><a href="https://kubernetes-csi.github.io/docs/external-attacher.html">external-attacher</a></li><li><a href="https://kubernetes-csi.github.io/docs/external-snapshotter.html">external-snapshotter</a></li><li><a href="https://kubernetes-csi.github.io/docs/external-resizer.html">external-resizer</a></li><li><a href="https://kubernetes-csi.github.io/docs/node-driver-registrar.html">node-driver-registrar</a></li><li><a href="https://kubernetes-csi.github.io/docs/cluster-driver-registrar.html">cluster-driver-registrar (deprecated)</a></li><li><a href="https://kubernetes-csi.github.io/docs/livenessprobe.html">livenessprobe</a></li></ul><p>每个 Sidecar Container 的作用可以通过对应链接查看，需要注意的是 cluster-driver-registrar 已经停止维护，请改用 node-driver-registrar。</p><h3 id="2-2、CSI-处理阶段"><a href="#2-2、CSI-处理阶段" class="headerlink" title="2.2、CSI 处理阶段"></a>2.2、CSI 处理阶段</h3><blockquote><p>在理解了 CSI Sidecar Containers 以后，我们仍需要大致的了解 CSI 挂载过程中的大致流程，以此来针对性的实现每个阶段所需要的功能；CSI 整个流程实际上大致分为以下三大阶段:</p></blockquote><h4 id="2-2-1、Provisioning-and-Deleting"><a href="#2-2-1、Provisioning-and-Deleting" class="headerlink" title="2.2.1、Provisioning and Deleting"></a>2.2.1、Provisioning and Deleting</h4><p><a href="https://github.com/kubernetes/community/blob/master/contributors/design-proposals/storage/container-storage-interface.md#provisioning-and-deleting">Provisioning and Deleting</a> 阶段实现与外部存储供应商协调卷的创建/删除处理，简单地说就是需要实现 CreateVolume 和 DeleteVolume；假设外部存储供应商为阿里云存储那么此阶段应该完成在阿里云存储商创建一个指定大小的块设备，或者在用户删除 volume 时完成在阿里云存储上删除这个块设备；除此之外此阶段还应当响应存储拓扑分布从而保证 volume 分布在正确的集群拓扑上(此处描述不算清晰，推荐查看设计文档)。</p><h4 id="2-2-2、Attaching-and-Detaching"><a href="#2-2-2、Attaching-and-Detaching" class="headerlink" title="2.2.2、Attaching and Detaching"></a>2.2.2、Attaching and Detaching</h4><p><a href="https://github.com/kubernetes/community/blob/master/contributors/design-proposals/storage/container-storage-interface.md#attaching-and-detaching">Attaching and Detaching</a> 阶段实现将外部存储供应商提供好的卷设备挂载到本地或者从本地卸载，简单地说就是实现 ControllerPublishVolume 和 ControllerUnpublishVolume；同样以外部存储供应商为阿里云存储为例，在 Provisioning 阶段创建好的卷的块设备，在此阶段应该实现将其挂载到服务器本地或从本地卸载，在必要的情况下还需要进行格式化等操作。</p><h4 id="2-2-3、Mount-and-Umount"><a href="#2-2-3、Mount-and-Umount" class="headerlink" title="2.2.3、Mount and Umount"></a>2.2.3、Mount and Umount</h4><p>这个阶段在 CSI 设计文档中没有做详细描述，在前两个阶段完成后，当一个目标 Pod 在某个 Node 节点上调度时，kubelet 会根据前两个阶段返回的结果来创建这个 Pod；同样以外部存储供应商为阿里云存储为例，此阶段将会把已经 Attaching 的本地块设备以目录形式挂载到 Pod 中或者从 Pod 中卸载这个块设备。</p><h3 id="2-3、CSI-gRPC-Server"><a href="#2-3、CSI-gRPC-Server" class="headerlink" title="2.3、CSI gRPC Server"></a>2.3、CSI gRPC Server</h3><p>CSI 的三大阶段实际上更细粒度的划分到 CSI Sidecar Containers 中，上面已经说过我们开发 CSI 实际上是面向 CSI Sidecar Containers 编程，针对于 CSI Sidecar Containers 我们主要需要实现以下三个 gRPC Server:</p><h4 id="2-3-1、Identity-Server"><a href="#2-3-1、Identity-Server" class="headerlink" title="2.3.1、Identity Server"></a>2.3.1、Identity Server</h4><p>在当前 CSI Spec v1.3.0 中 IdentityServer 定义如下:</p><div class="hljs code-wrapper"><pre><code class="hljs go"><span class="hljs-comment">// IdentityServer is the server API for Identity service.</span><span class="hljs-keyword">type</span> IdentityServer <span class="hljs-keyword">interface</span> &#123;GetPluginInfo(context.Context, *GetPluginInfoRequest) (*GetPluginInfoResponse, error)GetPluginCapabilities(context.Context, *GetPluginCapabilitiesRequest) (*GetPluginCapabilitiesResponse, error)Probe(context.Context, *ProbeRequest) (*ProbeResponse, error)&#125;</code></pre></div><p>从代码上可以看出 IdentityServer 主要负责像 Kubernetes 提供 CSI 插件名称可选功能等，所以此 Server 是必须实现的。</p><h4 id="2-3-2、Node-Server"><a href="#2-3-2、Node-Server" class="headerlink" title="2.3.2、Node Server"></a>2.3.2、Node Server</h4><p>同样当前 CSI v1.3.0 Spec 中 NodeServer 定义如下:</p><div class="hljs code-wrapper"><pre><code class="hljs go"><span class="hljs-comment">// NodeServer is the server API for Node service.</span><span class="hljs-keyword">type</span> NodeServer <span class="hljs-keyword">interface</span> &#123;NodeStageVolume(context.Context, *NodeStageVolumeRequest) (*NodeStageVolumeResponse, error)NodeUnstageVolume(context.Context, *NodeUnstageVolumeRequest) (*NodeUnstageVolumeResponse, error)NodePublishVolume(context.Context, *NodePublishVolumeRequest) (*NodePublishVolumeResponse, error)NodeUnpublishVolume(context.Context, *NodeUnpublishVolumeRequest) (*NodeUnpublishVolumeResponse, error)NodeGetVolumeStats(context.Context, *NodeGetVolumeStatsRequest) (*NodeGetVolumeStatsResponse, error)NodeExpandVolume(context.Context, *NodeExpandVolumeRequest) (*NodeExpandVolumeResponse, error)NodeGetCapabilities(context.Context, *NodeGetCapabilitiesRequest) (*NodeGetCapabilitiesResponse, error)NodeGetInfo(context.Context, *NodeGetInfoRequest) (*NodeGetInfoResponse, error)&#125;</code></pre></div><p>在最小化的实现中，NodeServer 中仅仅需要实现 <code>NodePublishVolume</code>、<code>NodeUnpublishVolume</code>、<code>NodeGetCapabilities</code> 三个方法，在 Mount 阶段 kubelet 会通过 <a href="https://kubernetes-csi.github.io/docs/node-driver-registrar.html">node-driver-registrar</a> 容器调用这三个方法。</p><h4 id="2-3-3、Controller-Server"><a href="#2-3-3、Controller-Server" class="headerlink" title="2.3.3、Controller Server"></a>2.3.3、Controller Server</h4><p>在当前 CSI Spec v1.3.0 ControllerServer 定义如下:</p><div class="hljs code-wrapper"><pre><code class="hljs go"><span class="hljs-comment">// ControllerServer is the server API for Controller service.</span><span class="hljs-keyword">type</span> ControllerServer <span class="hljs-keyword">interface</span> &#123;CreateVolume(context.Context, *CreateVolumeRequest) (*CreateVolumeResponse, error)DeleteVolume(context.Context, *DeleteVolumeRequest) (*DeleteVolumeResponse, error)ControllerPublishVolume(context.Context, *ControllerPublishVolumeRequest) (*ControllerPublishVolumeResponse, error)ControllerUnpublishVolume(context.Context, *ControllerUnpublishVolumeRequest) (*ControllerUnpublishVolumeResponse, error)ValidateVolumeCapabilities(context.Context, *ValidateVolumeCapabilitiesRequest) (*ValidateVolumeCapabilitiesResponse, error)ListVolumes(context.Context, *ListVolumesRequest) (*ListVolumesResponse, error)GetCapacity(context.Context, *GetCapacityRequest) (*GetCapacityResponse, error)ControllerGetCapabilities(context.Context, *ControllerGetCapabilitiesRequest) (*ControllerGetCapabilitiesResponse, error)CreateSnapshot(context.Context, *CreateSnapshotRequest) (*CreateSnapshotResponse, error)DeleteSnapshot(context.Context, *DeleteSnapshotRequest) (*DeleteSnapshotResponse, error)ListSnapshots(context.Context, *ListSnapshotsRequest) (*ListSnapshotsResponse, error)ControllerExpandVolume(context.Context, *ControllerExpandVolumeRequest) (*ControllerExpandVolumeResponse, error)ControllerGetVolume(context.Context, *ControllerGetVolumeRequest) (*ControllerGetVolumeResponse, error)&#125;</code></pre></div><p>从这些方法上可以看出，大部分的核心逻辑应该在 ControllerServer 中实现，比如创建/销毁 Volume，创建/销毁 Snapshot 等；在一般情况下我们自己编写的 CSI 都会实现 <code>CreateVolume</code> 和 <code>DeleteVolume</code>，至于其他方法根据业务需求以及外部存储供应商实际情况来决定是否进行实现。</p><h4 id="2-3-4、整体部署加构图"><a href="#2-3-4、整体部署加构图" class="headerlink" title="2.3.4、整体部署加构图"></a>2.3.4、整体部署加构图</h4><p><img src="https://cdn.oss.link/markdown/vopox.jpg" alt="CSI Deploy Mechanism"></p><p><strong>从这个部署架构图上可以看出在实际上 CSI 部署时，Mount and Umount 阶段(对应 Node Server 实现)以 Daemonset 方式保证其部署到每个节点，当 Volume 创建完成后由其挂载到 Pod 中；其他阶段(Provisioning and Deleting 和 Attaching and Detaching) 只要部署多个实例保证 HA 即可(最新版本的 Sidecar Containers 已经实现了多节点自动选举)；每次 PV 创建时首先由其他两个阶段的 Sidecar Containers 做处理，处理完成后信息返回给 Kubernetes 再传递到 Node Driver(Node Server) 上，然后 Node Driver 将其 Mount 到 Pod 中。</strong></p><h2 id="三、编写一个-NFS-CSI-插件"><a href="#三、编写一个-NFS-CSI-插件" class="headerlink" title="三、编写一个 NFS CSI 插件"></a>三、编写一个 NFS CSI 插件</h2><h3 id="3-1、前置准备及分析"><a href="#3-1、前置准备及分析" class="headerlink" title="3.1、前置准备及分析"></a>3.1、前置准备及分析</h3><p>根据以上文档的描述，针对于需要编写一个 NFS CSI 插件这个需求，大致我们可以作出如下分析:</p><ul><li>三大阶段中我们只需要实现 Provisioning and Deleting 和 Mount and Umount；因为以 NFS 作为外部存储供应商来说我们并非是块设备，所以也不需要挂载到宿主机(Attaching and Detaching)。</li><li>Provisioning and Deleting 阶段我们需要实现 <code>CreateVolume</code> 和 <code>DeleteVolume</code> 逻辑，其核心逻辑应该是针对每个 PV 在 NFS Server 目录下执行 <code>mkdir</code>，并将生成的目录名称等信息返回给 Kubernetes。</li><li>Mount and Umount 阶段需要实现 Node Server 的 <code>NodePublishVolume</code> 和 <code>NodeUnpublishVolume</code> 方法，然后将上一阶段提供的目录名称等信息组合成挂载命令 Mount 到 Pod 即可。</li></ul><p>在明确了这个需求以后我们需要开始编写 gRPC Server，当然不能盲目的自己乱造轮子，<strong>因为这些 gRPC Server 需要是 <code>NonBlocking</code> 的，</strong>所以最佳实践就是参考官方给出的样例项目 <a href="https://github.com/kubernetes-csi/csi-driver-host-path">csi-driver-host-path</a>，这是一名合格的 CCE 必备的技能(CCE = Ctrl C + Ctrl V + Engineer)。</p><h3 id="3-2、Hostpath-CSI-源码分析"><a href="#3-2、Hostpath-CSI-源码分析" class="headerlink" title="3.2、Hostpath CSI 源码分析"></a>3.2、Hostpath CSI 源码分析</h3><p>针对官方给出的 CSI 样例，首先把源码弄到本地，然后通过 IDE 打开；这里默认为读者熟悉 Go 语言相关语法以及 go mod 等依赖配置，开发 IDE 默认为 GoLand</p><p><img src="https://cdn.oss.link/markdown/jlsdg.png" alt="source tree"></p><p>从源码树上可以看到，hostpath 的 CSI 实现非常简单；首先是 <code>cmd</code> 包下的命令行部分，main 方法在这里定义，然后就是 <code>pkg/hostpath</code> 包的具体实现部分，CSI 需要实现的三大 gRPC Server 全部在此。</p><h4 id="3-2-1、命令行解析"><a href="#3-2-1、命令行解析" class="headerlink" title="3.2.1、命令行解析"></a>3.2.1、命令行解析</h4><p><code>cmd</code> 包下主要代码就是一些命令行解析，方便从外部传入一些参数供 CSI 使用；针对于 NFS CSI 我们需要从外部传入 NFS Server 地址、挂载目录等参数，如果外部存储供应商为其他云存储可能就需要从命令行传入 AccessKey、AccessToken 等参数。</p><p><img src="https://cdn.oss.link/markdown/t4mje.png" alt="flag_parse"></p><p>目前 go 原生的命令行解析非常弱鸡，所以更推荐使用 <a href="https://github.com/spf13/cobra">cobra</a> 命令行库完成解析。</p><h4 id="3-2-2、Hostpath-结构体"><a href="#3-2-2、Hostpath-结构体" class="headerlink" title="3.2.2、Hostpath 结构体"></a>3.2.2、Hostpath 结构体</h4><p>从上面命令行解析的图中可以看到，在完成命令行解析后交由 <code>handle</code> 方法处理；<code>handle</code> 方法很简单，通过命令行拿到的参数创建一个 <code>hostpath</code> 结构体指针，然后 <code>Run</code> 起来就行了，所以接下来要着重看一下这个结构体</p><p><img src="https://cdn.oss.link/markdown/0dc0j.png" alt="hostpath_struct"></p><p>从代码上可以看到，<code>hostpath</code> 结构体内有一系列的字段用来存储命令行传入的特定参数，然后还有三个 gRPC Server 的引用；命令行参数解析完成后通过 <code>NewHostPathDriver</code> 方法设置到 <code>hostpath</code> 结构体内，然后通过调用结构体的 <code>Run</code> 方法创建三个 gRPC Server 并运行</p><p><img src="https://cdn.oss.link/markdown/wt4ha.png" alt="hostpath_run"></p><h4 id="3-2-3、代码分布"><a href="#3-2-3、代码分布" class="headerlink" title="3.2.3、代码分布"></a>3.2.3、代码分布</h4><p>经过这么简单的一看，基本上一个最小化的 CSI 代码分布已经可以出来了:</p><ul><li>首先需要做命令行解析，一般放在 <code>cmd</code> 包</li><li>然后需要一个一般与 CSI 插件名称相同的结构体用来承载参数</li><li>结构体内持有三个 gRPC Server 引用，并通过适当的方法使用内部参数还初始化这个三个 gRPC Server</li><li>有了这些 gRPC Server 以后通过 <code>server.go</code> 中的 <code>NewNonBlockingGRPCServer</code> 方法将其启动(这里也可以看出 server.go 里面的方法我们后面可以 copy 直接用)</li></ul><h3 id="3-3、创建-CSI-插件骨架"><a href="#3-3、创建-CSI-插件骨架" class="headerlink" title="3.3、创建 CSI 插件骨架"></a>3.3、创建 CSI 插件骨架</h3><blockquote><p>项目骨架已经提交到 Github <a href="https://github.com/mritd/csi-archetype">mritd/csi-archetype</a> 项目，可直接 clone 并使用。</p></blockquote><p>大致的研究完 Hostpath 的 CSI 源码，我们就可以根据其实现细节抽象出一个项目 CSI 骨架:</p><p><img src="https://cdn.oss.link/markdown/7y8qu.png" alt="csi_archetype"></p><p>在这个骨架中我们采用 <a href="https://github.com/spf13/cobra">corba</a> 完成命令行参数解析，同时使用 <a href="github.com/sirupsen/logrus">logrus</a> 作为日志输出库，这两个库都是 Kubernetes 以及 docker 比较常用的库；我们创建了一个叫 <code>archetype</code> 的结构体作为 CSI 的主承载类，这个结构体需要定义一些参数(parameter1…)方便后面初始化相关 gRPC Server 实现相关调用。</p><div class="hljs code-wrapper"><pre><code class="hljs go"><span class="hljs-keyword">type</span> archetype <span class="hljs-keyword">struct</span> &#123;name     <span class="hljs-keyword">string</span>nodeID   <span class="hljs-keyword">string</span>version  <span class="hljs-keyword">string</span>endpoint <span class="hljs-keyword">string</span><span class="hljs-comment">// Add CSI plugin parameters here</span>parameter1 <span class="hljs-keyword">string</span>parameter2 <span class="hljs-keyword">int</span>parameter3 time.Duration<span class="hljs-built_in">cap</span>   []*csi.VolumeCapability_AccessModecscap []*csi.ControllerServiceCapability&#125;</code></pre></div><p>与 Hostpath CSI 实现相同，我们创建一个 <code>NewCSIDriver</code> 方法来返回 <code>archetype</code> 结构体实例，在 <code>NewCSIDriver</code> 方法中将命令行解析得到的相关参数设置进结构体中并添加一些 <code>AccessModes</code> 和 <code>ServiceCapabilities</code> 方便后面 <code>Identity Server</code> 调用。</p><div class="hljs code-wrapper"><pre><code class="hljs go"><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-title">NewCSIDriver</span><span class="hljs-params">(version, nodeID, endpoint, parameter1 <span class="hljs-keyword">string</span>, parameter2 <span class="hljs-keyword">int</span>, parameter3 time.Duration)</span> *<span class="hljs-title">archetype</span></span> &#123;logrus.Infof(<span class="hljs-string">&quot;Driver: %s version: %s&quot;</span>, driverName, version)<span class="hljs-comment">// Add some check here</span><span class="hljs-keyword">if</span> parameter1 == <span class="hljs-string">&quot;&quot;</span> &#123;logrus.Fatal(<span class="hljs-string">&quot;parameter1 is empty&quot;</span>)&#125;n := &amp;archetype&#123;name:     driverName,nodeID:   nodeID,version:  version,endpoint: endpoint,parameter1: parameter1,parameter2: parameter2,parameter3: parameter3,&#125;<span class="hljs-comment">// Add access modes for CSI here</span>n.AddVolumeCapabilityAccessModes([]csi.VolumeCapability_AccessMode_Mode&#123;csi.VolumeCapability_AccessMode_MULTI_NODE_MULTI_WRITER,&#125;)<span class="hljs-comment">// Add service capabilities for CSI here</span>n.AddControllerServiceCapabilities([]csi.ControllerServiceCapability_RPC_Type&#123;csi.ControllerServiceCapability_RPC_CREATE_DELETE_VOLUME,csi.ControllerServiceCapability_RPC_CREATE_DELETE_SNAPSHOT,&#125;)<span class="hljs-keyword">return</span> n&#125;</code></pre></div><p><strong>整个骨架源码树中，命令行解析自己重构使用一些更加方便的命令行解析、日志输出库；结构体部分参考 Hostpath 结构体自己调整，<code>server.go</code> 用来创建 <code>NonBlocking</code> 的 gRPC Server(直接从 Hotspath 样例项目 copy 即可)；然后就是三大 gRPC Server 的实现，由于是 “项目骨架” 所以相关方法我们都返回未实现，后续我们主要来实现这些方法就能让自己写的这个 CSI 插件 work。</strong></p><p><img src="https://cdn.oss.link/markdown/876sk.png" alt="Unimplemented_gRPC_Server"></p><h3 id="3-4、创建-NFS-CSI-插件骨架"><a href="#3-4、创建-NFS-CSI-插件骨架" class="headerlink" title="3.4、创建 NFS CSI 插件骨架"></a>3.4、创建 NFS CSI 插件骨架</h3><p>有了 CSI 的项目骨架以后，我们只需要简单地修改名字将其重命名为 NFS CSI 插件即可；由于这篇文章是先实现好了 NFS CSI(已经 work) 再来写的，所以 NFS CSI 的源码可以直接参考 <a href="https://github.com/Gozap/csi-nfs">Gozap/csi-nfs</a> 即可，下面的部分主要介绍三大 gRPC Server 的实现</p><p><img src="https://cdn.oss.link/markdown/kk42j.png" alt="csi-nfs"></p><h3 id="3-5、实现-Identity-Server"><a href="#3-5、实现-Identity-Server" class="headerlink" title="3.5、实现 Identity Server"></a>3.5、实现 Identity Server</h3><p><img src="https://cdn.oss.link/markdown/r8etm.png" alt="Identity Server"></p><p>Identity Server 实现相对简单，总共就三个接口；<code>GetPluginInfo</code> 接口返回插件名称版本即可(注意版本号好像只能是 <code>1.1.1</code> 这种，<code>v1.1.1</code> 好像会报错)；<code>Probe</code> 接口用来做健康检测可以直接返回空 response 即可，当然最理想的情况应该是做一些业务逻辑判活；<code>GetPluginCapabilities</code> 接口看起来简单但是要清楚返回的 <code>Capabilities</code> 含义，由于我们的 NFS 插件必然需要响应 <code>CreateVolume</code> 等请求(实现 Controller Server)，所以 cap 必须给予 <code>PluginCapability_Service_CONTROLLER_SERVICE</code>，除此之外如果节点不支持均匀的创建外部存储供应商的 Volume，那么应当同时返回 <code>PluginCapability_Service_VOLUME_ACCESSIBILITY_CONSTRAINTS</code> 以表示 CSI 处理时需要根据集群拓扑作调整；具体的可以查看 gRPC 注释:</p><div class="hljs code-wrapper"><pre><code class="hljs go"><span class="hljs-keyword">const</span> (PluginCapability_Service_UNKNOWN PluginCapability_Service_Type = <span class="hljs-number">0</span><span class="hljs-comment">// CONTROLLER_SERVICE indicates that the Plugin provides RPCs for</span><span class="hljs-comment">// the ControllerService. Plugins SHOULD provide this capability.</span><span class="hljs-comment">// In rare cases certain plugins MAY wish to omit the</span><span class="hljs-comment">// ControllerService entirely from their implementation, but such</span><span class="hljs-comment">// SHOULD NOT be the common case.</span><span class="hljs-comment">// The presence of this capability determines whether the CO will</span><span class="hljs-comment">// attempt to invoke the REQUIRED ControllerService RPCs, as well</span><span class="hljs-comment">// as specific RPCs as indicated by ControllerGetCapabilities.</span>PluginCapability_Service_CONTROLLER_SERVICE PluginCapability_Service_Type = <span class="hljs-number">1</span><span class="hljs-comment">// VOLUME_ACCESSIBILITY_CONSTRAINTS indicates that the volumes for</span><span class="hljs-comment">// this plugin MAY NOT be equally accessible by all nodes in the</span><span class="hljs-comment">// cluster. The CO MUST use the topology information returned by</span><span class="hljs-comment">// CreateVolumeRequest along with the topology information</span><span class="hljs-comment">// returned by NodeGetInfo to ensure that a given volume is</span><span class="hljs-comment">// accessible from a given node when scheduling workloads.</span>PluginCapability_Service_VOLUME_ACCESSIBILITY_CONSTRAINTS PluginCapability_Service_Type = <span class="hljs-number">2</span>)</code></pre></div><h3 id="3-6、实现-Controller-Server"><a href="#3-6、实现-Controller-Server" class="headerlink" title="3.6、实现 Controller Server"></a>3.6、实现 Controller Server</h3><p>Controller Server 实际上对应着 Provisioning and Deleting 阶段；换句话说核心的创建/删除卷、快照等都应在此做实现，针对于本次编写的 NFS 插件仅做最小实现(创建/删除卷)；需要注意的是除了核心的创建删除卷要实现以外还需要实现 <code>ControllerGetCapabilities</code> 方法，该方法返回 Controller Server 的 cap:</p><p><img src="https://cdn.oss.link/markdown/pl0n3.png" alt="ControllerGetCapabilities"></p><p><code>ControllerGetCapabilities</code> 返回的实际上是在创建驱动时设置的 cscap:</p><div class="hljs code-wrapper"><pre><code class="hljs go">n.AddControllerServiceCapabilities([]csi.ControllerServiceCapability_RPC_Type&#123;csi.ControllerServiceCapability_RPC_CREATE_DELETE_VOLUME,csi.ControllerServiceCapability_RPC_CREATE_DELETE_SNAPSHOT,&#125;)</code></pre></div><p><code>ControllerServiceCapability_RPC_CREATE_DELETE_VOLUME</code> 表示这个 Controller Server 支持创建/删除卷，<code>ControllerServiceCapability_RPC_CREATE_DELETE_SNAPSHOT</code> 表示支持创建/删除快照(快照功能是后来闲的没事加的)；<strong>应该明确的是我们返回了特定的 cap 那就要针对特定方法做实现，因为你一旦声明了这些 cap Kubernetes 就认为有相应请求可以让你处理(你不能吹完牛逼然后关键时刻掉链子)。</strong>针对于可以返回哪些 cscap 可以通过这些 gRPC 常量来查看:</p><div class="hljs code-wrapper"><pre><code class="hljs go"><span class="hljs-keyword">const</span> (ControllerServiceCapability_RPC_UNKNOWN                  ControllerServiceCapability_RPC_Type = <span class="hljs-number">0</span>ControllerServiceCapability_RPC_CREATE_DELETE_VOLUME     ControllerServiceCapability_RPC_Type = <span class="hljs-number">1</span>ControllerServiceCapability_RPC_PUBLISH_UNPUBLISH_VOLUME ControllerServiceCapability_RPC_Type = <span class="hljs-number">2</span>ControllerServiceCapability_RPC_LIST_VOLUMES             ControllerServiceCapability_RPC_Type = <span class="hljs-number">3</span>ControllerServiceCapability_RPC_GET_CAPACITY             ControllerServiceCapability_RPC_Type = <span class="hljs-number">4</span><span class="hljs-comment">// Currently the only way to consume a snapshot is to create</span><span class="hljs-comment">// a volume from it. Therefore plugins supporting</span><span class="hljs-comment">// CREATE_DELETE_SNAPSHOT MUST support creating volume from</span><span class="hljs-comment">// snapshot.</span>ControllerServiceCapability_RPC_CREATE_DELETE_SNAPSHOT ControllerServiceCapability_RPC_Type = <span class="hljs-number">5</span>ControllerServiceCapability_RPC_LIST_SNAPSHOTS         ControllerServiceCapability_RPC_Type = <span class="hljs-number">6</span><span class="hljs-comment">// Plugins supporting volume cloning at the storage level MAY</span><span class="hljs-comment">// report this capability. The source volume MUST be managed by</span><span class="hljs-comment">// the same plugin. Not all volume sources and parameters</span><span class="hljs-comment">// combinations MAY work.</span>ControllerServiceCapability_RPC_CLONE_VOLUME ControllerServiceCapability_RPC_Type = <span class="hljs-number">7</span><span class="hljs-comment">// Indicates the SP supports ControllerPublishVolume.readonly</span><span class="hljs-comment">// field.</span>ControllerServiceCapability_RPC_PUBLISH_READONLY ControllerServiceCapability_RPC_Type = <span class="hljs-number">8</span><span class="hljs-comment">// See VolumeExpansion for details.</span>ControllerServiceCapability_RPC_EXPAND_VOLUME ControllerServiceCapability_RPC_Type = <span class="hljs-number">9</span><span class="hljs-comment">// Indicates the SP supports the</span><span class="hljs-comment">// ListVolumesResponse.entry.published_nodes field</span>ControllerServiceCapability_RPC_LIST_VOLUMES_PUBLISHED_NODES ControllerServiceCapability_RPC_Type = <span class="hljs-number">10</span><span class="hljs-comment">// Indicates that the Controller service can report volume</span><span class="hljs-comment">// conditions.</span><span class="hljs-comment">// An SP MAY implement `VolumeCondition` in only the Controller</span><span class="hljs-comment">// Plugin, only the Node Plugin, or both.</span><span class="hljs-comment">// If `VolumeCondition` is implemented in both the Controller and</span><span class="hljs-comment">// Node Plugins, it SHALL report from different perspectives.</span><span class="hljs-comment">// If for some reason Controller and Node Plugins report</span><span class="hljs-comment">// misaligned volume conditions, CO SHALL assume the worst case</span><span class="hljs-comment">// is the truth.</span><span class="hljs-comment">// Note that, for alpha, `VolumeCondition` is intended be</span><span class="hljs-comment">// informative for humans only, not for automation.</span>ControllerServiceCapability_RPC_VOLUME_CONDITION ControllerServiceCapability_RPC_Type = <span class="hljs-number">11</span><span class="hljs-comment">// Indicates the SP supports the ControllerGetVolume RPC.</span><span class="hljs-comment">// This enables COs to, for example, fetch per volume</span><span class="hljs-comment">// condition after a volume is provisioned.</span>ControllerServiceCapability_RPC_GET_VOLUME ControllerServiceCapability_RPC_Type = <span class="hljs-number">12</span>)</code></pre></div><p>当声明了 <code>ControllerServiceCapability_RPC_CREATE_DELETE_VOLUME</code> 以后针对创建删除卷方法 <code>CreateVolume</code>、<code>DeleteVolume</code> 做实现即可；这两个方法实现就是常规的业务逻辑层面没什么技术含量，对于外部存储供应商是 NFS 来说无非就是接到一个 <code>CreateVolumeRequest</code> ，然后根据 request 给的 volume name 啥的信息自己执行一下在 NFS Server 上 <code>mkdir</code> ，删除卷处理就是反向的 <code>rm -rf dir</code>；在两个方法的处理中可能额外掺杂一些校验等其他的辅助实现。</p><p><img src="https://cdn.oss.link/markdown/jkhb6.png" alt="CreateVolume"></p><p><img src="https://cdn.oss.link/markdown/96ij8.png" alt="DeleteVolume"></p><p><strong>最后有几点需要注意的地方:</strong></p><ul><li><strong>幂等性: Kubernetes 可能由于一些其他原因会重复发出请求(比如超时重试)，此时一定要保证创建/删除卷实现的幂等性，简单地说 Kubernetes 连续两次调用同一个卷创建 CSI 插件应当实现自动去重过滤，不能调用两次返回两个新卷。</strong></li><li><strong>数据回写: 要明白的是 Controller Server 是 Provisioning and Deleting 阶段，此时还没有真正挂载到 Pod，所以就本地使用 NFS 作为存储后端来说 <code>mkdir</code> 以后要把目录、NFS Server 地址等必要信息通过 VolumeContext 返回给 Kubernetes，Kubernetes 接下来会传递给 Node Driver(Mount/Umount)用。</strong></li><li><strong>预挂载: 当然这个问题目前只存在在 NFS 作为存储后端中，问题核心在于在创建卷进行 <code>mkdir</code> 之前，NFS 应该已经确保 mount 到了 Controller Server 容器本地，所以目前的做法就是启动 Controller Server 时就执行 NFS 挂载；如果用其他的后端存储比如阿里云存储时也要考虑在创建卷之前相关的 API Client 是否可用。</strong></li></ul><h3 id="3-7、实现-Node-Server"><a href="#3-7、实现-Node-Server" class="headerlink" title="3.7、实现 Node Server"></a>3.7、实现 Node Server</h3><p>Node Server 实际上就是 Node Driver，简单地说当 Controller Server 完成一个卷的创建，并且已经 Attach 到 Node 以后(当然这里的 NFS 不需要 Attach)，Node Server 就需要实现根据给定的信息将卷 Mount 到 Pod 或者从 Pod Umount 掉卷；同样的 Node Server 也许要返回一些信息来告诉 Kubernetes 自己的详细情况，这部份由两个方法完成 <code>NodeGetInfo</code> 和 <code>NodeGetCapabilities</code></p><p><img src="https://cdn.oss.link/markdown/ts3l9.png" alt="NodeGetInfo_NodeGetCapabilities"></p><p><code>NodeGetInfo</code> 中返回节点的常规信息，比如 Node ID、最大允许的 Volume 数量、集群拓扑信息等；<code>NodeGetCapabilities</code> 返回这个 Node 的 cap，由于我们的 NFS 是真的啥也不支持，所以只好返回 <code>NodeServiceCapability_RPC_UNKNOWN</code>，至于其他的 cap 如下(含义自己看注释):</p><div class="hljs code-wrapper"><pre><code class="hljs go"><span class="hljs-keyword">const</span> (NodeServiceCapability_RPC_UNKNOWN              NodeServiceCapability_RPC_Type = <span class="hljs-number">0</span>NodeServiceCapability_RPC_STAGE_UNSTAGE_VOLUME NodeServiceCapability_RPC_Type = <span class="hljs-number">1</span><span class="hljs-comment">// If Plugin implements GET_VOLUME_STATS capability</span><span class="hljs-comment">// then it MUST implement NodeGetVolumeStats RPC</span><span class="hljs-comment">// call for fetching volume statistics.</span>NodeServiceCapability_RPC_GET_VOLUME_STATS NodeServiceCapability_RPC_Type = <span class="hljs-number">2</span><span class="hljs-comment">// See VolumeExpansion for details.</span>NodeServiceCapability_RPC_EXPAND_VOLUME NodeServiceCapability_RPC_Type = <span class="hljs-number">3</span><span class="hljs-comment">// Indicates that the Node service can report volume conditions.</span><span class="hljs-comment">// An SP MAY implement `VolumeCondition` in only the Node</span><span class="hljs-comment">// Plugin, only the Controller Plugin, or both.</span><span class="hljs-comment">// If `VolumeCondition` is implemented in both the Node and</span><span class="hljs-comment">// Controller Plugins, it SHALL report from different</span><span class="hljs-comment">// perspectives.</span><span class="hljs-comment">// If for some reason Node and Controller Plugins report</span><span class="hljs-comment">// misaligned volume conditions, CO SHALL assume the worst case</span><span class="hljs-comment">// is the truth.</span><span class="hljs-comment">// Note that, for alpha, `VolumeCondition` is intended to be</span><span class="hljs-comment">// informative for humans only, not for automation.</span>NodeServiceCapability_RPC_VOLUME_CONDITION NodeServiceCapability_RPC_Type = <span class="hljs-number">4</span>)</code></pre></div><p>剩下的核心方法 <code>NodePublishVolume</code> 和 <code>NodeUnpublishVolume</code> 挂载/卸载卷同 Controller Server 创建删除卷一样都是业务处理，没啥可说的，按步就班的调用一下 Mount 上就行；<strong>唯一需要注意的点就是这里也要保证幂等性，同时由于要操作 Pod 目录，所以要把宿主机的 <code>/var/lib/kubelet/pods</code> 目录挂载到 Node Server 容器里。</strong></p><h3 id="3-8、部署测试-NFS-插件"><a href="#3-8、部署测试-NFS-插件" class="headerlink" title="3.8、部署测试 NFS 插件"></a>3.8、部署测试 NFS 插件</h3><p>NFS 插件写完以后就可以实体环境做测试了，测试方法不同插件可能并不相同，本 NFS 插件可以直接使用源码项目的 <code>deploy</code> 目录创建相关容器做测试(需要根据自己的 NFS Server 修改一些参数)。针对于如何部署下面做一下简单说明:</p><p>三大阶段笼统的其实对应着三个 Sidecar Container:</p><ul><li>Provisioning and Deleting: external-provisioner</li><li>Attaching and Detaching: external-attacher</li><li>Mount and Umount: node-driver-registrar</li></ul><p><strong>我们的 NFS CSI 插件不需要 Attach，所以 external-attacher 也不需要部署；external-provisioner 只响应创建删除卷请求，所以通过 Deployment 部署足够多的复本保证 HA 就行；由于 Pod 不一定会落到那个节点上，理论上任意 Node 都可能有 Mount/Umount 行为，所以 node-driver-registrar 要以 Daemonset 方式部署保证每个节点都有一个。</strong></p><h2 id="四、其他说明"><a href="#四、其他说明" class="headerlink" title="四、其他说明"></a>四、其他说明</h2><h3 id="4-1、前期调试"><a href="#4-1、前期调试" class="headerlink" title="4.1、前期调试"></a>4.1、前期调试</h3><p>在前期代码编写时一般都是 “盲狙”，就是按照自己的理解无脑实现，这时候可能离实际部署还很远，但是只是单纯的想知道某个 Request 里面到底是什么个东西，这时候你可以利用 <code>mritd/socket2tcp</code> 容器模拟监听 socket 文件，然后将请求转发到你的 IDE 监听端口上，然后再进行 Debug。</p><p>可能有人会问: “我直接在 Sidecar Containers 里写个 tcp 地址不就行了，还转发毛线，这不是脱裤子放屁多此一举么？”，但是这里我友情提醒一下，Sidecar Containers 指定 CSI 地址时填写非 socket 类型的地址是不好使的，会直接启动失败。</p><h3 id="4-2、后期调试"><a href="#4-2、后期调试" class="headerlink" title="4.2、后期调试"></a>4.2、后期调试</h3><p>等到代码编写到后期其实就开始 “真机” 调试了，这时候其实不必使用原始的打日志调试方法，NFS CSI 的项目源码中的 <code>Dockerfile.debug</code> 提供了使用 dlv 做远程调试的样例；具体怎么配合 IDE 做远程调试请自行 Google。</p><h3 id="4-3、其他功能实现"><a href="#4-3、其他功能实现" class="headerlink" title="4.3、其他功能实现"></a>4.3、其他功能实现</h3><p>其他功能根据需要可以自己酌情实现，比如创建/删除快照功能；对于 NFS 插件来说 NFS Server 又没有 API，所以最简单最 low 的办法当然是 <code>tar -zcvf</code> 了(哈哈哈(超大声))，当然性能么就不要提了。</p><h2 id="五、总结"><a href="#五、总结" class="headerlink" title="五、总结"></a>五、总结</h2><p><strong>CSI 开发其实是针对 Kubernetes CSI Sidecar Containers 的 gRPC 开发，根据自己需求实现三大阶段中对应三大 gRPC Server 相应方法即可；相关功能要保证幂等性，cap 要看文档根据实际情况返回。</strong></p><h2 id="六、参考文档"><a href="#六、参考文档" class="headerlink" title="六、参考文档"></a>六、参考文档</h2><ul><li><a href="https://kubernetes-csi.github.io/docs/introduction.html">https://kubernetes-csi.github.io/docs/introduction.html</a></li><li><a href="https://github.com/container-storage-interface/spec">https://github.com/container-storage-interface/spec</a></li></ul>]]></content>
    
    
    <summary type="html">本篇文章详细介绍 CSI 插件，同时涉及到的源码比较多，主要倾向于使用 go 来开发 CSI 驱动。</summary>
    
    
    
    <category term="Kubernetes" scheme="https://mritd.com/categories/kubernetes/"/>
    
    
    <category term="CSI" scheme="https://mritd.com/tags/csi/"/>
    
  </entry>
  
  <entry>
    <title>树莓派4 Manjaro 系统定制</title>
    <link href="https://mritd.com/2020/08/19/make-a-custom-manjaro-image-for-rpi4/"/>
    <id>https://mritd.com/2020/08/19/make-a-custom-manjaro-image-for-rpi4/</id>
    <published>2020-08-19T06:05:00.000Z</published>
    <updated>2020-08-19T06:05:00.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="一、目前的系统现状"><a href="#一、目前的系统现状" class="headerlink" title="一、目前的系统现状"></a>一、目前的系统现状</h2><p>截止本文编写时间，树莓派4 官方系统仍然不支持 64bit；但是当我在 3b+ 上使用 arch 64bit 以后我发现 32bit 系统和 64bit 系统装在同一个树莓派上在使用时那就是两个完全不一样的树莓派…所以对于这个新的 rpi4 那么必需要用 64bit 的系统；而当前我大致查看到支持 64bit 的系统只有 Ubuntu20、Manjaro 两个，Ubuntu 对我来说太重了(虽然服务器上我一直是 Ubuntu，但是 rpi 上我选择说 “不”)，Manjaro 基于 Arch 这种非常轻量的系统非常适合树莓派这种开发板，所以最终我选择了 Manjaro。但是万万没想到的是 Manjaro 都是带 KDE 什么的图形化的，而我的树莓派只想仍在角落里跑东西，所以说图形化这东西对我来说也没啥用，最后迫于无奈只能自己通过 Manjaro 的工具自己定制了。</p><h2 id="二、manjaro-arm-tools"><a href="#二、manjaro-arm-tools" class="headerlink" title="二、manjaro-arm-tools"></a>二、manjaro-arm-tools</h2><p>经过几经查找各种 Google，发现了 Manjaro 官方提供了自定义创建 arm 镜像的工具 <a href="https://gitlab.manjaro.org/manjaro-arm/applications/manjaro-arm-tools">manjaro-arm-tools</a>，这个工具简单使用如下:</p><ul><li>首先准备一个 Manjaro 系统(虚拟机 x86 即可)</li><li>然后安装 manjaro-arm-tool 所需<a href="https://gitlab.manjaro.org/manjaro-arm/applications/manjaro-arm-tools#dependencies">依赖工具</a></li><li>添加 Manjaro 的<a href="https://gitlab.manjaro.org/manjaro-arm/applications/manjaro-arm-tools#git-version-from-manjaro-strit-repo">软件源</a></li><li>安装 manjaro-arm-tool <code>sudo pacman -Syyu manjaro-strit-keyring &amp;&amp; sudo pacman -S manjaro-arm-tools-git</code></li></ul><p>当工具都准备完成后，只需要执行 <code>sudo buildarmimg -d rpi4 -e minimal</code> 即可创建 manjaro 的 rpi4 最小镜像。</p><h2 id="三、系统定制"><a href="#三、系统定制" class="headerlink" title="三、系统定制"></a>三、系统定制</h2><p>在使用 manjaro-arm-tool 创建系统以后发现一些细微的东西需要自己调整，比如网络设置常用软件包等，而 manjaro-arm-tool 工具又没有提供太好的自定义处理的一些 hook，所以最后萌生了自己根据 manjaro-arm-tool 来创建自己的 rpi4 系统定制工具的想法。</p><h3 id="3-1、常用软件包安装"><a href="#3-1、常用软件包安装" class="headerlink" title="3.1、常用软件包安装"></a>3.1、常用软件包安装</h3><p>在查看了 manjaro-arm-tool 的源码后可以看到实际上软件安装就是利用 systemd-nspawn 进入到 arm 系统执行 pacman 安装，自己依葫芦画瓢增加一些常用的软件包安装:</p><div class="hljs code-wrapper"><pre><code class="hljs sh">systemd-nspawn -q --resolv-conf=copy-host --timezone=off -D <span class="hljs-variable">$&#123;ROOTFS_DIR&#125;</span> pacman -Syyu zsh htop vim wget <span class="hljs-built_in">which</span> git make net-tools dnsutils inetutils iproute2 sysstat nload lsof --noconfirm</code></pre></div><h3 id="3-2、pacman-镜像"><a href="#3-2、pacman-镜像" class="headerlink" title="3.2、pacman 镜像"></a>3.2、pacman 镜像</h3><p>在安装软件包时发现安装速读奇慢，研究以后发现是没有使用国内的镜像源，故增加了国内镜像源的处理:</p><div class="hljs code-wrapper"><pre><code class="hljs sh">systemd-nspawn -q --resolv-conf=copy-host --timezone=off -D <span class="hljs-variable">$&#123;ROOTFS_DIR&#125;</span> pacman-mirrors -c China</code></pre></div><h3 id="3-3、网络处理"><a href="#3-3、网络处理" class="headerlink" title="3.3、网络处理"></a>3.3、网络处理</h3><h4 id="3-3-1、有线连接"><a href="#3-3-1、有线连接" class="headerlink" title="3.3.1、有线连接"></a>3.3.1、有线连接</h4><p>默认的 manjaro-arm-tool 创建的系统网络部分采用 dhspcd 做 dhcp 处理，但是我个人感觉一切尽量精简统一还是比较好的；所以准备网络部分完全由 systemd 接管处理，即直接使用 systemd-networkd 和 systemd-resolved；systemd-networkd 处理相对简单，编写一个配置文件然后 enable systemd-networkd 服务即可:</p><p><strong>/etc/systemd/network/10-eth-dhcp.network</strong></p><div class="hljs code-wrapper"><pre><code class="hljs sh">[Match]Name=eth*[Network]DHCP=yes</code></pre></div><p><strong>让 systemd-networkd 开机自启动</strong></p><div class="hljs code-wrapper"><pre><code class="hljs sh">systemd-nspawn -q --resolv-conf=copy-host --timezone=off -D <span class="hljs-variable">$&#123;ROOTFS_DIR&#125;</span> systemctl <span class="hljs-built_in">enable</span> systemd-networkd.service</code></pre></div><p>一开始以为 systemd-resolved 同样 enable 一下就行，后来发现每次开机初始化以后 systemd-resolved 都会被莫明其妙的 disable 掉；经过几经寻找和开 issue 问作者，发现这个操作是被 manjaro-arm-oem-install 包下的脚本执行的，作者的回复意思是大部分带有图形化的版本网络管理工具都会与 systemd-resolved 冲突，所以默认关闭了，这时候我们就要针对 manjaro-arm-oem-install 单独处理一下:</p><div class="hljs code-wrapper"><pre><code class="hljs sh">systemd-nspawn -q --resolv-conf=copy-host --timezone=off -D <span class="hljs-variable">$&#123;ROOTFS_DIR&#125;</span> systemctl <span class="hljs-built_in">enable</span> systemd-resolved.servicesed -i <span class="hljs-string">&#x27;s@systemctl disable systemd-resolved.service 1&gt; /dev/null 2&gt;&amp;1@@g&#x27;</span> <span class="hljs-variable">$&#123;ROOTFS_DIR&#125;</span>/usr/share/manjaro-arm-oem-install/manjaro-arm-oem-install</code></pre></div><h4 id="3-3-2、无限连接"><a href="#3-3-2、无限连接" class="headerlink" title="3.3.2、无限连接"></a>3.3.2、无限连接</h4><p>有线连接只要 systemd-networkd 处理好就能很好的工作，而无线连接目前有很多方案，我一开始想用 <a href="https://wiki.archlinux.org/index.php/Netctl_(%E7%AE%80%E4%BD%93%E4%B8%AD%E6%96%87)">netctl</a>，后来发现这东西虽然是 Arch 亲儿子，但是在系统定制时采用 systemd-nspawn 调用不兼容(因为里面调用了 systemd 的一些命令，这些命令一般只有在开机时才可用)，而且只用 netctl 来管理 wifi 还感觉怪怪的，后来我的想法是要么用就全都用，要么就纯手动不要用这些东西，所以最后的方案是 wpa_supplicant + systemd-networkd 一把梭:</p><p><strong>/etc/systemd/network/10-wlan-dhcp.network.example</strong></p><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment"># 1. Generate wifi configuration (don&#x27;t modify the name of wpa_supplicant-wlan0.conf file)</span><span class="hljs-comment"># $ wpa_passphrase MyNetwork SuperSecretPassphrase &gt; /etc/wpa_supplicant/wpa_supplicant-wlan0.conf</span><span class="hljs-comment">#</span><span class="hljs-comment"># 2. Connect to wifi automatically after booting</span><span class="hljs-comment"># $ systemctl enable wpa_supplicant@wlan0</span><span class="hljs-comment">#</span><span class="hljs-comment"># 3.Systemd automatically makes dhcp request</span><span class="hljs-comment"># $ cp /etc/systemd/network/10-wlan-dhcp.network.example /etc/systemd/network/10-wlan-dhcp.network</span>[Match]Name=wlan*[Network]DHCP=yes</code></pre></div><h3 id="3-4、内核调整"><a href="#3-4、内核调整" class="headerlink" title="3.4、内核调整"></a>3.4、内核调整</h3><p>在上面的一些调整完成后我就启动系统实体机测试了，测试过程中发现安装 docker 以后会有两个警告，大致意思就是不支持 swap limit 和 cpu limit；查询资料以后发现是内核有两个参数没开启(<code>CONFIG_MEMCG_SWAP</code>、<code>CONFIG_CFS_BANDWIDTH</code>)…当然我这种强迫症是不能忍的，没办法就自己在 rpi4 上重新编译了内核(后来我想想还不如用 arch 32bit 然后自己编译 64bit 内核了):</p><div class="hljs code-wrapper"><pre><code class="hljs sh">git <span class="hljs-built_in">clone</span> https://github.com/mritd/linux-rpi4.git<span class="hljs-built_in">cd</span> linux-rpi4MAKEFLAGS=<span class="hljs-string">&#x27;-j4&#x27;</span> makepkg</code></pre></div><h3 id="3-5、外壳驱动"><a href="#3-5、外壳驱动" class="headerlink" title="3.5、外壳驱动"></a>3.5、外壳驱动</h3><p>由于我的 rpi4 配的是 ARGON ONE 的外壳，所以电源按钮还有风扇需要驱动才能完美工作，没办法我又编译了 ARGON ONE 外壳的驱动:</p><div class="hljs code-wrapper"><pre><code class="hljs sh">git <span class="hljs-built_in">clone</span> https://github.com/mritd/argonone.git<span class="hljs-built_in">cd</span> argononemakepkg</code></pre></div><h2 id="四、定制脚本"><a href="#四、定制脚本" class="headerlink" title="四、定制脚本"></a>四、定制脚本</h2><p>综合以上的各种修改以后，我从 manjaro-arm-tool 提取出了定制化的 rpi4 的编译脚本，该脚本目前存放在 <a href="https://github.com/mritd/manjaro-rpi4">mritd/manjaro-rpi4</a> 仓库中；目前使用此脚本编译的系统镜像默认进行了以下处理:</p><ul><li>调整 pacman mirror 为中国</li><li>安装常用软件包(zsh htop vim wget which…)</li><li>有线网络完全的 systemd-networkd 接管，resolv.conf 由 systemd-resolved 接管</li><li>无线网络由 wpa_supplicant 和 systemd-networkd 接管</li><li>安装自行编译的内核以消除 docker 警告(<strong>自编译内核不影响升级，升级/覆盖安装后自动恢复</strong>)</li></ul><p>至于 ARGON ONE 的外壳驱动只在 resources 目录下提供了安装包，并未默认安装到系统。</p>]]></content>
    
    
    <summary type="html">最近入手了新玩具 &quot;吃灰派4&quot;，这一代性能提升真的很大，所以买回来是真的没办法 &quot;吃灰&quot; 了；但是由于目前 64bit 系统比较难产，所以只能自己定义一下 Manjaro 了。</summary>
    
    
    
    <category term="Linux" scheme="https://mritd.com/categories/linux/"/>
    
    
    <category term="Linux" scheme="https://mritd.com/tags/linux/"/>
    
    <category term="Manjaro" scheme="https://mritd.com/tags/manjaro/"/>
    
  </entry>
  
  <entry>
    <title>如何在 Filebeat 端进行日志处理</title>
    <link href="https://mritd.com/2020/08/19/how-to-modify-filebeat-source-code-to-processing-logs/"/>
    <id>https://mritd.com/2020/08/19/how-to-modify-filebeat-source-code-to-processing-logs/</id>
    <published>2020-08-19T06:01:00.000Z</published>
    <updated>2020-08-19T06:01:00.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="一、起因"><a href="#一、起因" class="headerlink" title="一、起因"></a>一、起因</h2><p>目前某项目组日志需要做切割处理，针对日志信息进行分割并提取 k/v 放入 es 中方便查询。这种需求在传统 ELK 中应当由 logstash 组件完成，通过 <code>gork</code> 等操作对日志进行过滤、切割等处理。不过很尴尬的是我并不会 ruby，logstash pipeline 的一些配置我也是极其头疼，而且还不想学…更不凑巧的是我会写点 go，<strong>那么理所应当的此时的我对 filebeat 源码产生了一些想法，比如我直接在 filebeat 端完成日志处理，然后直接发 es/logstash，这样似乎更方便，而且还能分摊 logstash 的压力，我感觉这个操作并不过分😂…</strong></p><h2 id="二、需求"><a href="#二、需求" class="headerlink" title="二、需求"></a>二、需求</h2><p>目前某项目组 java 日志格式如下:</p><div class="hljs code-wrapper"><pre><code class="hljs sh">2020-04-30 21:56:30.117$<span class="hljs-variable">$api</span>-test-65c8c7cf7f-lng7h$<span class="hljs-variable">$http</span>-nio-8080-exec-3$$INFO$<span class="hljs-variable">$com</span>.example.api.common.filter.GlobalDataFilter$<span class="hljs-variable">$GlobalDataFilter</span>.java$$95$<span class="hljs-variable">$test</span>build commonData from header :&#123;<span class="hljs-string">&quot;romVersion&quot;</span>:<span class="hljs-string">&quot;W_V2.1.4&quot;</span>,<span class="hljs-string">&quot;softwareVersion&quot;</span>:<span class="hljs-string">&quot;15&quot;</span>,<span class="hljs-string">&quot;token&quot;</span>:<span class="hljs-string">&quot;aFxANNM3pnRYpohvLMSmENydgFSfsmFMgCbFWAosIE=&quot;</span>&#125;$$$$</code></pre></div><p>目前开发约定格式为日志通过 <code>$$</code> 进行分割，日志格式比较简单，但是 logstash 共用(nginx 等各种日志都会往这个 logstash 输出)，不想去折腾 logstash 配置的情况下，只需要让 filebeat 能够直接切割并设置好 k/v 对应既可。</p><h2 id="三、filebeat-module"><a href="#三、filebeat-module" class="headerlink" title="三、filebeat module"></a>三、filebeat module</h2><blockquote><p>module 部份只做简介，以为实际上依托 es 完成，意义不大。</p></blockquote><p>当然在考虑修改 filebeat 源码后，我第一想到的是 filebeat 的 module，这个 module 在官方文档中是个很神奇的东西；通过开启一个 module 就可以对某种日志直接做处理，这种东西似乎就是我想要的；比如我写一个 “项目名” module，然后 filebeat 直接开启这个 module，这个项目的日志就直接自动处理好(听起来就很 “上流”)…</p><p>针对于自定义 module，官方给出了文档: <a href="https://www.elastic.co/guide/en/beats/devguide/current/filebeat-modules-devguide.html">Creating a New Filebeat Module</a></p><p>按照文档操作如下(假设我们的项目名为 cdm):</p><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment"># 克隆源码</span>git <span class="hljs-built_in">clone</span> git@github.com:elastic/beats.git<span class="hljs-comment"># 切换到稳定分支</span><span class="hljs-built_in">cd</span> bests &amp;&amp; git checkout -b v7.6.2 v7.6.2-module<span class="hljs-comment"># 创建 module，GO111MODULE 需要设置为 off</span><span class="hljs-comment"># 在 7.6.2 版本官方尚未开始支持 go mod</span><span class="hljs-built_in">cd</span> filebeatGO111MODULE=off make create-module MODULE=cdm</code></pre></div><p>创建完成后目录结构如下</p><div class="hljs code-wrapper"><pre><code class="hljs sh">➜  filebeat git:(v7.6.2-module) ✗ tree module/cdmmodule/cdm├── _meta│   ├── config.yml│   ├── docs.asciidoc│   └── fields.yml└── module.yml1 directory, 4 files</code></pre></div><p>这几个文件具体作用<a href="https://www.elastic.co/guide/en/beats/devguide/current/filebeat-modules-devguide.html">官方文档</a>都有详细的描述；但是根据文档描述光有这几个文件是不够的，<strong>module 只是一个处理集合的定义，尚未包含任何处理，针对真正的处理需要继续创建 fileset，fileset 简单的理解就是针对具体的一组文件集合的处理；</strong>例如官方 nginx module 中包含两个 fileset: <code>access</code> 和 <code>error</code>，这两个一个针对 access 日志处理一个针对 error 日志进行处理；在 fileset 中可以设置默认文件位置、处理方式。</p><p><strong>But… 我翻了 nginx module 的样例配置才发现，module 这个东西实质上只做定义和存储处理表达式，具体的切割处理实际上交由 es 的 <a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/ingest.html">Ingest Node</a> 处理；表达式里仍需要定义 <code>grok</code> 等操作，而且这东西最终会编译到 go 静态文件里；</strong>此时的我想说一句 “MMP”，本来我是不像写 grok 啥的才来折腾 filebeat，结果这个 module 折腾一圈还是要写 grok 啥的，而且这东西直接借助 es 完成导致压力回到了 es 同时每次修改还得重新编译 filebeat… 所以折腾到这我就放弃了，这已经违背了当初的目的，有兴趣的可以参考以下文档继续折腾:</p><ul><li><a href="https://www.elastic.co/guide/en/beats/devguide/current/filebeat-modules-devguide.html">Creating a New Filebeat Module</a></li><li><a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/ingest.html">Ingest nodeedit</a></li><li><a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/ingest-apis.html">Ingest APIs</a></li><li><a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/ingest-processors.html">Processors</a></li></ul><h2 id="四、filebeat-processors"><a href="#四、filebeat-processors" class="headerlink" title="四、filebeat processors"></a>四、filebeat processors</h2><p>经历了 module 的失望以后，我把目光对准了 processors；processors 是 filebeat 一个强大的功能，顾名思义它可以对 filbeat 收集到的日志进行一些处理；从官方 <a href="https://www.elastic.co/guide/en/beats/filebeat/current/filtering-and-enhancing-data.html">Processors</a> 页面可以看到其内置了大量的 processor；这些 processor 大部份都是直接对日志进行 “写” 操作，所以理论上我们自己写一个 processor 就可以 “为所欲为+为所欲为=为所欲为”。</p><p>不过不幸的是关于 processor 的开发官方并未给出文档，官方认为这是一个 <code>high level</code> 的东西，不过也找到了一个 issue 对其做了相关回答: <a href="https://github.com/elastic/beats/issues/6760">How do I write a processor plugin by myself</a>；所以最好的办法就是直接看已有 processor 的源码抄一个。</p><p>理所应当的找了一个软柿子捏: <code>add_host_metadata</code>，add_host_metadata processor 顾名思义在每个日志事件(以下简称为 event)中加入宿主机的信息，比如 hostname 啥的；以下为 add_host_metadata processor 的文件结构(processors 代码存储在 <code>libbeat/processors</code> 目录下)。</p><p><img src="https://cdn.oss.link/markdown/axucc.jpg" alt="dir_tree"></p><p>通过阅读源码和 issue 的回答可以看出，我们自定义的 processor 只需要实现 <a href="https://godoc.org/github.com/elastic/beats/libbeat/processors#Processor">Processor interface</a> 既可，这个接口定义如下:</p><p><img src="https://cdn.oss.link/markdown/xuja6.png" alt="Processor interface"></p><p>通过查看 add_host_metadata 的源码，<code>String() string</code> 方法只需要返回这个 processor 名称既可(可以包含必要的配置信息)；<strong>而 <code>Run(event *beat.Event) (*beat.Event, error)</code> 方法表示在每一条日志被读取后都会转换为一个 event 对象，我们在方法内进行处理然后把 event 返回既可(其他 processor 可能也要处理)。</strong></p><p><img src="https://cdn.oss.link/markdown/jhtnx.png" alt="add_host_metadata source"></p><p>有了这些信息就简单得多了，毕竟作为<strong>一名合格的 CCE(Ctrl C + Ctrl V + Engineer)</strong> 抄这种操作还是很简单的，直接照猫画虎写一个就行了</p><p>config.go</p><div class="hljs code-wrapper"><pre><code class="hljs go"><span class="hljs-keyword">package</span> cmd<span class="hljs-comment">// Config for cdm processor.</span><span class="hljs-keyword">type</span> Config <span class="hljs-keyword">struct</span> &#123;Name           <span class="hljs-keyword">string</span>          <span class="hljs-string">`config:&quot;name&quot;`</span>&#125;<span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-title">defaultConfig</span><span class="hljs-params">()</span> <span class="hljs-title">Config</span></span> &#123;<span class="hljs-keyword">return</span> Config&#123;&#125;&#125;</code></pre></div><p>cdm.go</p><div class="hljs code-wrapper"><pre><code class="hljs go"><span class="hljs-keyword">package</span> cmd<span class="hljs-keyword">import</span> (<span class="hljs-string">&quot;strings&quot;</span><span class="hljs-string">&quot;github.com/elastic/beats/libbeat/logp&quot;</span><span class="hljs-string">&quot;github.com/pkg/errors&quot;</span><span class="hljs-string">&quot;github.com/elastic/beats/libbeat/beat&quot;</span><span class="hljs-string">&quot;github.com/elastic/beats/libbeat/common&quot;</span><span class="hljs-string">&quot;github.com/elastic/beats/libbeat/processors&quot;</span>jsprocessor <span class="hljs-string">&quot;github.com/elastic/beats/libbeat/processors/script/javascript/module/processor&quot;</span>)<span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-title">init</span><span class="hljs-params">()</span></span> &#123;processors.RegisterPlugin(<span class="hljs-string">&quot;cdm&quot;</span>, New)jsprocessor.RegisterPlugin(<span class="hljs-string">&quot;CDM&quot;</span>, New)&#125;<span class="hljs-keyword">type</span> cdm <span class="hljs-keyword">struct</span> &#123;config Configfields []<span class="hljs-keyword">string</span>log    *logp.Logger&#125;<span class="hljs-keyword">const</span> (processorName = <span class="hljs-string">&quot;cdm&quot;</span>logName       = <span class="hljs-string">&quot;processor.cdm&quot;</span>)<span class="hljs-comment">// New constructs a new cdm processor.</span><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-title">New</span><span class="hljs-params">(cfg *common.Config)</span> <span class="hljs-params">(processors.Processor, error)</span></span> &#123;<span class="hljs-comment">// 配置文件里就一个 Name 字段，结构体留着以后方便扩展</span>config := defaultConfig()<span class="hljs-keyword">if</span> err := cfg.Unpack(&amp;config); err != <span class="hljs-literal">nil</span> &#123;<span class="hljs-keyword">return</span> <span class="hljs-literal">nil</span>, errors.Wrapf(err, <span class="hljs-string">&quot;fail to unpack the %v configuration&quot;</span>, processorName)&#125;p := &amp;cdm&#123;config: config,<span class="hljs-comment">// 待分割的每段日志对应的 key</span>fields: []<span class="hljs-keyword">string</span>&#123;<span class="hljs-string">&quot;timestamp&quot;</span>, <span class="hljs-string">&quot;hostname&quot;</span>, <span class="hljs-string">&quot;thread&quot;</span>, <span class="hljs-string">&quot;level&quot;</span>, <span class="hljs-string">&quot;logger&quot;</span>, <span class="hljs-string">&quot;file&quot;</span>, <span class="hljs-string">&quot;line&quot;</span>, <span class="hljs-string">&quot;serviceName&quot;</span>, <span class="hljs-string">&quot;traceId&quot;</span>, <span class="hljs-string">&quot;feTraceId&quot;</span>, <span class="hljs-string">&quot;msg&quot;</span>, <span class="hljs-string">&quot;exception&quot;</span>&#125;,log:    logp.NewLogger(logName),&#125;<span class="hljs-keyword">return</span> p, <span class="hljs-literal">nil</span>&#125;<span class="hljs-comment">// 真正的日志处理逻辑</span><span class="hljs-comment">// 为了保证后面的 processor 正常处理，这里面没有 return 任何 error，只是简单的打印</span><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(p *cdm)</span> <span class="hljs-title">Run</span><span class="hljs-params">(event *beat.Event)</span> <span class="hljs-params">(*beat.Event, error)</span></span> &#123;<span class="hljs-comment">// 尝试获取 message，理论上这一步不应该出现问题</span>msg, err := event.GetValue(<span class="hljs-string">&quot;message&quot;</span>)<span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;p.log.Error(err)<span class="hljs-keyword">return</span> event, <span class="hljs-literal">nil</span>&#125;message, ok := msg.(<span class="hljs-keyword">string</span>)<span class="hljs-keyword">if</span> !ok &#123;p.log.Error(<span class="hljs-string">&quot;failed to parse message&quot;</span>)<span class="hljs-keyword">return</span> event, <span class="hljs-literal">nil</span>&#125;<span class="hljs-comment">// 分割日志</span>fieldsValue := strings.Split(message, <span class="hljs-string">&quot;$$&quot;</span>)p.log.Debugf(<span class="hljs-string">&quot;message fields: %v&quot;</span>, fieldsVaule)<span class="hljs-comment">// 为了保证不会出现数组越界需要判断一下(万一弄出个格式不正常的日志过来保证不崩)</span><span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(fieldsValue) &lt; <span class="hljs-built_in">len</span>(p.fields) &#123;p.log.Errorf(<span class="hljs-string">&quot;incorrect field length: %d, expected length: %d&quot;</span>, <span class="hljs-built_in">len</span>(fieldsValue), <span class="hljs-built_in">len</span>(p.fields))<span class="hljs-keyword">return</span> event, <span class="hljs-literal">nil</span>&#125;<span class="hljs-comment">// 这里遍历然后赛会到 event 既可</span>data := common.MapStr&#123;&#125;<span class="hljs-keyword">for</span> i, k := <span class="hljs-keyword">range</span> p.fields &#123;_, _ = event.PutValue(k, strings.TrimSpace(fieldsValue[i]))&#125;event.Fields.DeepUpdate(data)<span class="hljs-keyword">return</span> event, <span class="hljs-literal">nil</span>&#125;<span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(p *cdm)</span> <span class="hljs-title">String</span><span class="hljs-params">()</span> <span class="hljs-title">string</span></span> &#123;<span class="hljs-keyword">return</span> processorName&#125;</code></pre></div><p>写好代码以后就可以编译一个自己的 filebeat 了(开心ing)</p><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-built_in">cd</span> filebeat<span class="hljs-comment"># 如果想交叉编译 linux 需要增加 GOOS=linux 变量 </span>GO111MODULE=off make</code></pre></div><p>然后编写配置文件进行测试，日志相关字段已经成功塞到了 event 中，这样我直接发到 es 或者 logstash 就行了。</p><div class="hljs code-wrapper"><pre><code class="hljs yaml"><span class="hljs-attr">filebeat.inputs:</span><span class="hljs-bullet">-</span> <span class="hljs-attr">type:</span> <span class="hljs-string">log</span>  <span class="hljs-attr">enabled:</span> <span class="hljs-literal">true</span>  <span class="hljs-attr">paths:</span>    <span class="hljs-bullet">-</span> <span class="hljs-string">/Users/natural/tmp/cdm.log</span>  <span class="hljs-attr">processors:</span>    <span class="hljs-bullet">-</span> <span class="hljs-attr">cdm:</span> <span class="hljs-string">~</span>  <span class="hljs-attr">multiline.pattern:</span> <span class="hljs-string">^\d&#123;4&#125;-\d&#123;1,2&#125;-\d&#123;1,2&#125;</span>  <span class="hljs-attr">multiline.match:</span> <span class="hljs-string">after</span>  <span class="hljs-attr">multiline.negate:</span> <span class="hljs-literal">true</span>  <span class="hljs-attr">multiline.timeout:</span> <span class="hljs-string">5s</span></code></pre></div><h2 id="五、script-processor"><a href="#五、script-processor" class="headerlink" title="五、script processor"></a>五、script processor</h2><p>在我折腾完源码以后，反思一下其实这种方式需要自己编译 filebeat，而且每次规则修改也很不方便，唯一的好处真的就是用代码可以 “为所欲为”；反过来一想 “filebeat 有没有 processor 的扩展呢？脚本热加载那种？” 答案是使用 script processor，<strong>script processor 虽然名字上是个 processor，实际上其包含了完整的 ECMA 5.1 js 规范实现；结论就是我们可以写一些 js 脚本来处理日志，然后 filebeat 每次启动后加载这些脚本既可。</strong></p><p>script processor 的使用方式很简单，js 文件中只需要包含一个 <code>function process(event)</code> 方法既可，与自己用 go 实现的 processor 类似，每行日志也会形成一个 event 对象然后调用这个方法进行处理；目前 event 对象可用的 api 需要参考<a href="https://www.elastic.co/guide/en/beats/filebeat/current/processor-script.html#_event_api">官方文档</a>；<strong>需要注意的是 script processor 目前只支持 ECMA 5.1 语法规范，超过这个范围的语法是不被支持；</strong>实际上其根本是借助了 <a href="https://github.com/dop251/goja">https://github.com/dop251/goja</a> 这个库来实现的。同时为了方便开发调试，script processor 也增加了一些 nodejs 的兼容 module，比如 <code>console.log</code> 等方法是可用的；以下为 js 处理上面日志的逻辑:</p><div class="hljs code-wrapper"><pre><code class="hljs js"><span class="hljs-keyword">var</span> <span class="hljs-built_in">console</span> = <span class="hljs-built_in">require</span>(<span class="hljs-string">&#x27;console&#x27;</span>);<span class="hljs-keyword">var</span> fileds = <span class="hljs-keyword">new</span> <span class="hljs-built_in">Array</span>(<span class="hljs-string">&quot;timestamp&quot;</span>, <span class="hljs-string">&quot;hostname&quot;</span>, <span class="hljs-string">&quot;thread&quot;</span>, <span class="hljs-string">&quot;level&quot;</span>, <span class="hljs-string">&quot;logger&quot;</span>, <span class="hljs-string">&quot;file&quot;</span>, <span class="hljs-string">&quot;line&quot;</span>, <span class="hljs-string">&quot;serviceName&quot;</span>, <span class="hljs-string">&quot;traceId&quot;</span>, <span class="hljs-string">&quot;feTraceId&quot;</span>, <span class="hljs-string">&quot;msg&quot;</span>, <span class="hljs-string">&quot;exception&quot;</span>)<span class="hljs-function"><span class="hljs-keyword">function</span> <span class="hljs-title">process</span>(<span class="hljs-params">event</span>) </span>&#123;    <span class="hljs-keyword">var</span> message = event.Get(<span class="hljs-string">&quot;message&quot;</span>);    <span class="hljs-keyword">if</span> (message == <span class="hljs-literal">null</span> || message == <span class="hljs-literal">undefined</span> || message == <span class="hljs-string">&#x27;&#x27;</span>) &#123;        <span class="hljs-built_in">console</span>.log(<span class="hljs-string">&quot;failed to get message&quot;</span>);        <span class="hljs-keyword">return</span>    &#125;    <span class="hljs-keyword">var</span> fieldValues = message.split(<span class="hljs-string">&quot;$$&quot;</span>);    <span class="hljs-keyword">if</span> (fieldValues.length&lt;fileds.length) &#123;        <span class="hljs-built_in">console</span>.log(<span class="hljs-string">&quot;incorrect field length&quot;</span>);        <span class="hljs-keyword">return</span>    &#125;    <span class="hljs-keyword">for</span> (<span class="hljs-keyword">var</span> i = <span class="hljs-number">0</span>; i &lt; fileds.length; ++i) &#123;        event.Put(fileds[i],fieldValues[i].trim())    &#125;&#125;</code></pre></div><p>写好脚本后调整配置测试既可，如果 js 编写有问题，可以通过 <code>console.log</code> 来打印日志进行不断的调试</p><div class="hljs code-wrapper"><pre><code class="hljs yaml"><span class="hljs-attr">filebeat.inputs:</span><span class="hljs-bullet">-</span> <span class="hljs-attr">type:</span> <span class="hljs-string">log</span>  <span class="hljs-attr">enabled:</span> <span class="hljs-literal">true</span>  <span class="hljs-attr">paths:</span>    <span class="hljs-bullet">-</span> <span class="hljs-string">/Users/natural/tmp/cdm.log</span>  <span class="hljs-attr">processors:</span>    <span class="hljs-bullet">-</span> <span class="hljs-attr">script:</span>        <span class="hljs-attr">lang:</span> <span class="hljs-string">js</span>        <span class="hljs-attr">id:</span> <span class="hljs-string">cdm</span>        <span class="hljs-attr">file:</span> <span class="hljs-string">cdm.js</span>  <span class="hljs-attr">multiline.pattern:</span> <span class="hljs-string">^\d&#123;4&#125;-\d&#123;1,2&#125;-\d&#123;1,2&#125;</span>  <span class="hljs-attr">multiline.match:</span> <span class="hljs-string">after</span>  <span class="hljs-attr">multiline.negate:</span> <span class="hljs-literal">true</span>  <span class="hljs-attr">multiline.timeout:</span> <span class="hljs-string">5s</span></code></pre></div><p><strong>需要注意的是目前 <code>lang</code> 的值只能为 <code>javascript</code> 和 <code>js</code>(官方文档写的只能是 <code>javascript</code>)；根据代码来看后续 script processor 有可能支持其他脚本语言，个人认为主要取决于其他脚本语言有没有纯 go 实现的 runtime，如果有的话未来很有可能被整合到 script processor 中。</strong></p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gegg80j1gmj31nc0u0wpa.jpg" alt="script processor"></p><h2 id="六、其他-processor"><a href="#六、其他-processor" class="headerlink" title="六、其他 processor"></a>六、其他 processor</h2><p>研究完 script processor 后我顿时对其他 processor 也产生了兴趣，随着更多的查看processor 文档，我发现其实大部份过滤分割能力已经有很多 processor 进行了实现，<strong>其完善程度外加可扩展的 script processor 实际能力已经足矣替换掉 logstash 的日志分割过滤处理了。</strong>比如上面的日志切割其实使用 dissect processor 实现更加简单(这个配置并不完善，只是样例):</p><div class="hljs code-wrapper"><pre><code class="hljs yaml"><span class="hljs-attr">processors:</span>  <span class="hljs-bullet">-</span> <span class="hljs-attr">dissect:</span>      <span class="hljs-attr">field:</span> <span class="hljs-string">&quot;message&quot;</span>      <span class="hljs-attr">tokenizer:</span> <span class="hljs-string">&quot;<span class="hljs-template-variable">%&#123;timestamp&#125;</span>$$<span class="hljs-template-variable">%&#123;hostname&#125;</span>$$<span class="hljs-template-variable">%&#123;thread&#125;</span>$$<span class="hljs-template-variable">%&#123;level&#125;</span>$$<span class="hljs-template-variable">%&#123;logger&#125;</span>$$<span class="hljs-template-variable">%&#123;file&#125;</span>$$<span class="hljs-template-variable">%&#123;line&#125;</span>$$<span class="hljs-template-variable">%&#123;serviceName&#125;</span>$$<span class="hljs-template-variable">%&#123;traceId&#125;</span>$$<span class="hljs-template-variable">%&#123;feTraceId&#125;</span>$$<span class="hljs-template-variable">%&#123;msg&#125;</span>$$<span class="hljs-template-variable">%&#123;exception&#125;</span>$$&quot;</span></code></pre></div><p>除此之外还有很多 processor，例如 <code>drop_event</code>、<code>drop_fields</code>、<code>timestamp</code> 等等，感兴趣的可以自行研究。</p><h2 id="七、总结"><a href="#七、总结" class="headerlink" title="七、总结"></a>七、总结</h2><p>基本上折腾完以后做了一个总结:</p><ul><li><strong>filebeat module</strong>: 这就是个华而不实的东西，每次修改需要重新编译且扩展能力几近于零，最蛋疼的是实际逻辑通过 es 来完成；我能想到的是唯一应用场景就是官方给我们弄一些 demo 来炫耀用的，比如 nginx module；实际生产中 nginx 日志格式保持原封不动的人我相信少之又少。</li><li><strong>filebeat custom processor</strong>: 每次修改也需要重新编译且需要会 go 语言还有相关工具链，但是好处就是完全通过代码实现真正的为所欲为；扩展性取决于外部是否对特定位置做了可配置化，比如预留可以配置切割用正则表达式的变量等，最终取决于代码编写者(怎么为所欲为的问题)。</li><li><strong>filebeat script processor</strong>: 完整 ECMA 5.1 js 规范支持，代码化对日志进行为所欲为，修改不需要重新编译；普通用户我个人觉得是首选，当然同时会写 go 和 js 的就看你想用哪个了。</li><li><strong>filebeat other processor</strong>: 基本上实现了很多 logstash 的功能，简单用用很舒服，复杂场景还是得撸代码；但是一些特定的 processor 很实用，比如加入宿主机信息的 add_host_metadata processor 等。</li></ul>]]></content>
    
    
    <summary type="html">本文主要介绍在 ELK 日志系统中，日志切割处理直接在 filebeat 端实现的一些方式；其中包括 filebeat processor 的扩展以及 module 扩展等。</summary>
    
    
    
    <category term="Golang" scheme="https://mritd.com/categories/golang/"/>
    
    
    <category term="Kubernetes" scheme="https://mritd.com/tags/kubernetes/"/>
    
    <category term="Golang" scheme="https://mritd.com/tags/golang/"/>
    
  </entry>
  
  <entry>
    <title>如何不通过 docker 下载 docker image</title>
    <link href="https://mritd.com/2020/03/31/how-to-download-docker-image-without-docker/"/>
    <id>https://mritd.com/2020/03/31/how-to-download-docker-image-without-docker/</id>
    <published>2020-03-31T15:52:38.000Z</published>
    <updated>2020-03-31T15:52:38.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>这是一个比较骚的动作，但是事实上确实有这个需求，折腾半天找工具看源码，这里记录一下(不想看源码分析啥的请直接跳转到第五部份)。</p></blockquote><h2 id="一、起因"><a href="#一、起因" class="headerlink" title="一、起因"></a>一、起因</h2><p>由于最近某个爬虫业务需要抓取微信公众号的一些文章，某开发小伙伴想到了通过启动安卓虚拟机然后抓包的方式实现；经过几番寻找最终我们选择采用 docker 的方式启动安卓虚拟机，docker 里安卓虚拟机比较成熟的项目我们找到了 <a href="https://github.com/budtmo/docker-android">https://github.com/budtmo/docker-android</a> 这个项目；但是由于众所周知的原因这个 2G+ 的镜像国内拉取是非常慢的，于是我想到了通过国外 VPS 拉取然后 scp 回来… 由于贫穷的原因，当我实际操作的时候遇到了比较尴尬的问题: **VPS 磁盘空间 25G，镜像拉取后解压接近 10G，我需要 <code>docker save</code> 成 tar 包再进行打包成 <code>tar.gz</code> 格式 scp 回来，这个时候空间不够用了…**所以我当时就在想有没有办法让 docker daemon 拉取镜像时不解压？或者说自己通过 HTTP 下载镜像直接存储为 tar？</p><h2 id="二、尝试造轮子"><a href="#二、尝试造轮子" class="headerlink" title="二、尝试造轮子"></a>二、尝试造轮子</h2><p>当出现了上面的问题后，我第一反应就是:</p><ul><li>1、docker 拆分为 moby</li><li>2、moby 模块化，大部份开源到 <a href="https://github.com/containers">containers</a></li><li>3、<a href="https://github.com/containers/image">containers/image</a> 项目是镜像部份源码</li><li>4、看 <a href="https://github.com/containers/image">containers/image</a> 源码造轮子</li><li>5、不确定是否需要 <a href="https://github.com/containers/storage">containers/storage</a> 做存储</li></ul><h2 id="三、猜测源码"><a href="#三、猜测源码" class="headerlink" title="三、猜测源码"></a>三、猜测源码</h2><p>当我查看 <a href="https://github.com/containers/image">containers/image</a> README 文档时发现其提到了 <a href="https://github.com/containers/skopeo">skopeo</a> 项目，并且很明确的说了</p><blockquote><p>The containers/image project is only a library with no user interface; you can either incorporate it into your Go programs, or use the skopeo tool:<br>The skopeo tool uses the containers/image library and takes advantage of many of its features, e.g. skopeo copy exposes the containers/image/copy.Image functionality.</p></blockquote><p>那么也就是说镜像下载这块很大可能应该调用 <code>containers/image/copy.Image</code> 完成，随即就看了下源码文档</p><p><img src="https://cdn.oss.link/markdown/tv7iy.png"></p><p>很明显，<code>types.ImageReference</code>、<code>Options</code> 里面的属性啥的我完全看不懂… 😂😂😂</p><h2 id="四、看-skopeo-源码"><a href="#四、看-skopeo-源码" class="headerlink" title="四、看 skopeo 源码"></a>四、看 skopeo 源码</h2><p>当 <a href="https://github.com/containers/image">containers/image</a> 源码看不懂时，突然想到 <a href="https://github.com/containers/skopeo">skopeo</a> 调用的是这个玩意，那么依葫芦画瓢看 <a href="https://github.com/containers/skopeo">skopeo</a> 源码应该能行；接下来常规操作 clone skopeo 源码然后编译运行测试；编译后 skopeo 支持命令如下</p><div class="hljs code-wrapper"><pre><code class="hljs sh">NAME:   skopeo - Various operations with container images and container image registriesUSAGE:   skopeo [global options] <span class="hljs-built_in">command</span> [<span class="hljs-built_in">command</span> options] [arguments...]VERSION:   0.1.42-dev commit: 018a0108b103341526b41289c434b59d65783f6fCOMMANDS:   copy               Copy an IMAGE-NAME from one location to another   inspect            Inspect image IMAGE-NAME   delete             Delete image IMAGE-NAME   manifest-digest    Compute a manifest digest of a file   sync               Synchronize one or more images from one location to another   standalone-sign    Create a signature using <span class="hljs-built_in">local</span> files   standalone-verify  Verify a signature using <span class="hljs-built_in">local</span> files   list-tags          List tags <span class="hljs-keyword">in</span> the transport/repository specified by the REPOSITORY-NAME   <span class="hljs-built_in">help</span>, h            Shows a list of commands or <span class="hljs-built_in">help</span> <span class="hljs-keyword">for</span> one <span class="hljs-built_in">command</span>GLOBAL OPTIONS:   --debug                     <span class="hljs-built_in">enable</span> debug output   --policy value              Path to a trust policy file   --insecure-policy           run the tool without any policy check   --registries.d DIR          use registry configuration files <span class="hljs-keyword">in</span> DIR (e.g. <span class="hljs-keyword">for</span> container signature storage)   --override-arch ARCH        use ARCH instead of the architecture of the machine <span class="hljs-keyword">for</span> choosing images   --override-os OS            use OS instead of the running OS <span class="hljs-keyword">for</span> choosing images   --override-variant VARIANT  use VARIANT instead of the running architecture variant <span class="hljs-keyword">for</span> choosing images   --command-timeout value     timeout <span class="hljs-keyword">for</span> the <span class="hljs-built_in">command</span> execution (default: 0s)   --<span class="hljs-built_in">help</span>, -h                  show <span class="hljs-built_in">help</span>   --version, -v               <span class="hljs-built_in">print</span> the version</code></pre></div><p><strong>我掐指一算调用 copy 命令应该是我要找的那个它</strong>，所以常规操作打开源码直接看</p><p><img src="https://cdn.oss.link/markdown/urn3l.png" alt="copy_cmd"></p><p>通过继续追踪 <code>alltransports.ParseImageName</code> 方法最终可以得知 copy 命令的 <code>SOURCE-IMAGE</code> 和 <code>DESTINATION-IMAGE</code> 都支持哪些写法</p><p><img src="https://cdn.oss.link/markdown/ush4t.png" alt="tp_register"></p><p><strong>每一个 Transport 的实现都提供了 Name 方法，其名称即为 src 或 dest 镜像名称的前缀，例如 <code>docker://nginx:1.17.6</code></strong></p><p><img src="https://cdn.oss.link/markdown/7fpap.png" alt="tp_docker"></p><p><strong>经过测试不同的 Transport 格式并不完全一致(具体看源码)，比如 <code>docker://nginx:1.17.6</code> 和 <code>dir:/tmp/nginx</code>；同时这些 Transport 并非完全都适用与 src 与 dest，比如 <code>tarball:/tmp/nginx.tar</code> 支持 src 而不支持 dest；</strong>其判断核心依据为 <code>ImageReference.NewImageSource</code> 和 <code>ImageReference.NewImageDestination</code> 方法实现是否返回 error</p><p><img src="https://cdn.oss.link/markdown/jb087.png" alt="NewImageDestination"></p><p>当我看了一会各种 Transport 源码后我发现一件事: <strong>这特么不就是我要造的轮子么！😱😱😱</strong></p><h2 id="五、skopeo-copy-使用"><a href="#五、skopeo-copy-使用" class="headerlink" title="五、skopeo copy 使用"></a>五、skopeo copy 使用</h2><h3 id="5-1、不借助-docker-下载镜像"><a href="#5-1、不借助-docker-下载镜像" class="headerlink" title="5.1、不借助 docker 下载镜像"></a>5.1、不借助 docker 下载镜像</h3><div class="hljs code-wrapper"><pre><code class="hljs sh">skopeo --insecure-policy copy docker://nginx:1.17.6 docker-archive:/tmp/nginx.tar</code></pre></div><p><code>--insecure-policy</code> 选项用于忽略安全策略配置文件，该命令将会直接通过 http 下载目标镜像并存储为 <code>/tmp/nginx.tar</code>，此文件可以直接通过 <code>docker load</code> 命令导入</p><h3 id="5-2、从-docker-daemon-导出镜像"><a href="#5-2、从-docker-daemon-导出镜像" class="headerlink" title="5.2、从 docker daemon 导出镜像"></a>5.2、从 docker daemon 导出镜像</h3><div class="hljs code-wrapper"><pre><code class="hljs sh">skopeo --insecure-policy copy docker-daemon:nginx:1.17.6 docker-archive:/tmp/nginx.tar</code></pre></div><p>该命令将会从 docker daemon 导出镜像到 <code>/tmp/nginx.tar</code>；为什么不用 <code>docker save</code>？因为我是偷懒 dest 也是 docker-archive，实际上 skopeo 可以导出为其他格式比如 <code>oci</code>、<code>oci-archive</code>、<code>ostree</code> 等</p><h3 id="5-3、其他命令"><a href="#5-3、其他命令" class="headerlink" title="5.3、其他命令"></a>5.3、其他命令</h3><p>skopeo 还有一些其他的实用命令，比如 <code>sync</code> 可以在两个位置之间同步镜像(😂早知道我还写个鸡儿 gcrsync)，<code>inspect</code> 可以查看镜像信息等，迫于本人太懒，剩下的请自行查阅文档、<code>--help</code> 以及源码(没错，整篇文章都没写 skopeo 怎么安装)。</p>]]></content>
    
    
    <summary type="html">这是一个比较骚的动作，但是事实上确实有这个需求，折腾半天找工具看源码，这里记录一下(不想看源码分析啥的请直接跳转到第五部份)。</summary>
    
    
    
    <category term="Docker" scheme="https://mritd.com/categories/docker/"/>
    
    
    <category term="Linux" scheme="https://mritd.com/tags/linux/"/>
    
    <category term="Docker" scheme="https://mritd.com/tags/docker/"/>
    
  </entry>
  
  <entry>
    <title>kubeadm 证书期限调整</title>
    <link href="https://mritd.com/2020/01/21/how-to-extend-the-validity-of-your-kubeadm-certificate/"/>
    <id>https://mritd.com/2020/01/21/how-to-extend-the-validity-of-your-kubeadm-certificate/</id>
    <published>2020-01-21T04:43:36.000Z</published>
    <updated>2020-01-21T04:43:36.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="一、证书管理"><a href="#一、证书管理" class="headerlink" title="一、证书管理"></a>一、证书管理</h2><p>kubeadm 集群安装完成后，证书管理上实际上大致是两大类型:</p><ul><li>自动滚动续期</li><li>手动定期续期</li></ul><p>自动滚动续期类型的证书目前从我所阅读文档和实际测试中目前只有 kubelet 的 client 证书；kubelet client 证书自动滚动涉及到了 <a href="https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet-tls-bootstrapping/">TLS bootstrapping</a> 部份，<strong>其核心由两个 ClusterRole 完成(<code>system:certificates.k8s.io:certificatesigningrequests:nodeclient</code> 和 <code>system:certificates.k8s.io:certificatesigningrequests:selfnodeclient</code>)，针对这两个 ClusterRole kubeadm 在引导期间创建了 <a href="https://kubernetes.io/docs/reference/setup-tools/kubeadm/implementation-details/#create-a-bootstrap-token">bootstrap token</a> 来完成引导期间证书签发(该 Token 24h 失效)，后续通过预先创建的 ClusterRoleBinding(<code>kubeadm:node-autoapprove-bootstrap</code> 和 <code>kubeadm:node-autoapprove-certificate-rotation</code>) 完成自动的 node 证书续期；</strong>kubelet client 证书续期部份涉及到 TLS bootstrapping 太多了，有兴趣的可以仔细查看(最后还是友情提醒: <strong>用 kubeadm 一定要看看 <a href="https://kubernetes.io/docs/reference/setup-tools/kubeadm/implementation-details">Implementation details</a></strong>)。</p><p>手动续期的证书目前需要在到期前使用 kubeadm 命令自行续期，这些证书目前可以通过以下命令列出</p><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment"># 不要在意我的证书过期时间是 10 年，下面会说</span>k1.node ➜ kubeadm alpha certs check-expiration[check-expiration] Reading configuration from the cluster...[check-expiration] FYI: You can look at this config file with <span class="hljs-string">&#x27;kubectl -n kube-system get cm kubeadm-config -oyaml&#x27;</span>CERTIFICATE                EXPIRES                  RESIDUAL TIME   CERTIFICATE AUTHORITY   EXTERNALLY MANAGEDadmin.conf                 Dec 06, 2029 20:58 UTC   9y                                      noapiserver                  Dec 06, 2029 20:59 UTC   9y              ca                      noapiserver-kubelet-client   Dec 06, 2029 20:59 UTC   9y              ca                      nocontroller-manager.conf    Dec 06, 2029 20:59 UTC   9y                                      nofront-proxy-client         Dec 06, 2029 20:59 UTC   9y              front-proxy-ca          noscheduler.conf             Dec 06, 2029 20:59 UTC   9y                                      noCERTIFICATE AUTHORITY   EXPIRES                  RESIDUAL TIME   EXTERNALLY MANAGEDca                      Jan 13, 2030 08:45 UTC   9y              nofront-proxy-ca          Jan 13, 2030 08:45 UTC   9y              no</code></pre></div><h2 id="二、证书期限调整"><a href="#二、证书期限调整" class="headerlink" title="二、证书期限调整"></a>二、证书期限调整</h2><p>上面已经提到了，手动管理部份的证书需要自己用命令续签(<code>kubeadm alpha certs renew all</code>)，而且你会发现续签以后有效期还是 1 年；kubeadm 的初衷是 **”为快速创建 kubernetes 集群的最佳实践”**，当然最佳实践包含确保证书安全性，毕竟 Let’s Encrypt 的证书有效期只有 3 个月的情况下 kubeadm 有效期有 1 年已经很不错了；但是对于最佳实践来说，我们公司的集群安全性并不需要那么高，一年续期一次无疑在增加运维人员心智负担(它并不最佳)，所以我们迫切需要一种 “一劳永逸” 的解决方案；当然我目前能想到的就是找到证书签发时在哪设置的有效期，然后想办法改掉它。</p><h3 id="2-1、源码分析"><a href="#2-1、源码分析" class="headerlink" title="2.1、源码分析"></a>2.1、源码分析</h3><p>目前通过宏观角度看整个 kubeadm 集群搭建过程，其中涉及到证书签署大致有两大部份: init  阶段和后期 renew，下面开始分析两个阶段的源码</p><h4 id="2-1-1、init-阶段"><a href="#2-1-1、init-阶段" class="headerlink" title="2.1.1、init 阶段"></a>2.1.1、init 阶段</h4><p>由于 kubernetes 整个命令行都是通过 cobra 库构建的，那么根据这个库的习惯首先直接从 <code>cmd</code> 包开始翻，而 kubernetes 源码组织的又比较清晰进而直接定位到 kubeadm 命令包下面；接着打开 <code>app</code> 目录一眼就看到了 <code>phases</code>… <code>phases</code> 顾名思义啊，整个 init 都是通过不同的 <code>phases</code> 完成的，那么直接去 <code>phases</code> 包下面找证书阶段的源码既可</p><p><img src="https://cdn.oss.link/markdown/ssdo7.jpg" alt="init_source"></p><p>进入到这个 <code>certs.go</code> 里面，直接列出所有方法，go 的规范里只有首字母大写才会被暴露出去，那么我们直接查看这些方法名既可；从名字上很轻松的看到了这个方法…基本上就是它了</p><p><img src="https://cdn.oss.link/markdown/uoqx4.jpg" alt="certs.go"></p><p>通过这个方法的代码会发现最终还是调用了 <code>certSpec.CreateFromCA(cfg, caCert, caKey)</code>，那么接着看看这个方法</p><p><img src="https://cdn.oss.link/markdown/psrho.jpg" alt="pkiutil.NewCertAndKey"></p><p>通过这个方法继续往下翻发现调用了 <code>pkiutil.NewCertAndKey(caCert, caKey, cfg)</code>，这个方法里最终调用了 <code>NewSignedCert(config, key, caCert, caKey)</code></p><p><img src="https://cdn.oss.link/markdown/nel5u.jpg" alt="NewSignedCert"></p><p>从 <code>NewSignedCert</code> 方法里看到证书有效期实际上是个常量，<strong>那也就意味着我改了这个常量 init 阶段的证书有效期八九不离十的就变了，再通过包名看这个是个 <code>pkiutil</code>… <code>xxxxxutil</code> 明显是公共的，所以推测改了它 renew 阶段应该也会变</strong></p><p><img src="https://cdn.oss.link/markdown/t3amy.jpg" alt="CertificateValidity"></p><h4 id="2-1-2、renew-阶段"><a href="#2-1-2、renew-阶段" class="headerlink" title="2.1.2、renew 阶段"></a>2.1.2、renew 阶段</h4><p>renew 阶段也是老套路，不过稳妥点先从 cmd 找起来，所以先看 <code>alpha</code> 包下的 <code>certs.go</code>；这时候方法名语义清晰就很有好处，一下就能找到 <code>newCmdCertsRenewal</code> 方法</p><p><img src="https://cdn.oss.link/markdown/amupo.jpg" alt="alpha_certs.go"></p><p>而这个 <code>newCmdCertsRenewal</code> 方法实际上没啥实现，所以目测实现是从 <code>getRenewSubCommands</code> 实现的</p><p><img src="https://cdn.oss.link/markdown/8c38y.jpg" alt="getRenewSubCommands"></p><p>看了 <code>getRenewSubCommands</code> 以后发现上面全是命令行库、配置文件参数啥的处理，核心在 <code>renewCert</code> 上，从这个方法里发现还有意外收获: <strong>renew 时实际上分两种情况处理，一种是使用了 <code>--use-api</code> 选项，另一种是未使用</strong>；当然根据上面的命令来说我们没使用，那么看 else 部份就行了(没看源码之前我特么居然没看 <code>--help</code> 不知道有这个选项)</p><p><img src="https://cdn.oss.link/markdown/9zsgp.jpg" alt="renewCert"></p><p>else 部份源码最终还是调用了 <code>RenewUsingLocalCA</code> 方法，这个方法一直往下跟会有一个 <code>Renew</code> 方法</p><p><img src="https://cdn.oss.link/markdown/s3a5c.jpg" alt="Renew"></p><p>这个方法一点进去… <strong>我上面的想法是对的</strong></p><p><img src="https://cdn.oss.link/markdown/08cnb.jpg" alt="FileRenewer_Renew"></p><h4 id="2-1-3、其他推测"><a href="#2-1-3、其他推测" class="headerlink" title="2.1.3、其他推测"></a>2.1.3、其他推测</h4><p>根据刚刚查看代码可以看到在 renew 阶段判断了 <code>--use-api</code> 选项是否使用，通过跟踪源码发现最终会调用到 <code>RenewUsingCSRAPI</code> 方法上，<code>RenewUsingCSRAPI</code> 会调用集群 CSR Api 执行证书签署</p><p><img src="https://cdn.oss.link/markdown/xivs9.png" alt="RenewUsingCSRAPI"></p><p>有了这个发现后基本上可以推测出这一步通过集群完成，那么按理说是应该受到 <code>kube-controller-manager</code> 组件的 <code>--experimental-cluster-signing-duration</code> 影响。</p><h3 id="2-2、测试验证"><a href="#2-2、测试验证" class="headerlink" title="2.2、测试验证"></a>2.2、测试验证</h3><h4 id="2-2-1、验证修改源码"><a href="#2-2-1、验证修改源码" class="headerlink" title="2.2.1、验证修改源码"></a>2.2.1、验证修改源码</h4><p>想验证修改源码是否有效只需要修改源码重新 build 出 kubeadm 命令，然后使用这个特定版本的 kubeadm renew 证书测试既可，源码调整的位置如下</p><p><img src="https://cdn.oss.link/markdown/qaavr.png" alt="update_source"></p><p>然后命令行下执行 <code>make cross</code> 进行跨平台交叉编译(如果过你在 linux amd64 平台下则直接 <code>make</code> 既可)</p><div class="hljs code-wrapper"><pre><code class="hljs sh">➜  kubernetes git:(v1.17.4) ✗ make crossgrep: /proc/meminfo: No such file or directorygrep: /proc/meminfo: No such file or directory+++ [0116 23:43:19] Multiple platforms requested and available 64G &gt;= threshold 40G, building platforms <span class="hljs-keyword">in</span> parallel+++ [0116 23:43:19] Building go targets <span class="hljs-keyword">for</span> &#123;linux/amd64 linux/arm linux/arm64 linux/s390x linux/ppc64le&#125; <span class="hljs-keyword">in</span> parallel (output will appear <span class="hljs-keyword">in</span> a burst when complete):    cmd/kube-proxy    cmd/kube-apiserver    cmd/kube-controller-manager    cmd/kubelet    cmd/kubeadm    cmd/kube-scheduler    vendor/k8s.io/apiextensions-apiserver    cluster/gce/gci/mounter+++ [0116 23:43:19] linux/amd64: build started+++ [0116 23:47:24] linux/amd64: build finished+++ [0116 23:43:19] linux/arm: build started+++ [0116 23:47:23] linux/arm: build finished+++ [0116 23:43:19] linux/arm64: build started+++ [0116 23:47:23] linux/arm64: build finished+++ [0116 23:43:19] linux/s390x: build started+++ [0116 23:47:24] linux/s390x: build finished+++ [0116 23:43:19] linux/ppc64le: build started+++ [0116 23:47:24] linux/ppc64le: build finishedgrep: /proc/meminfo: No such file or directorygrep: /proc/meminfo: No such file or directory+++ [0116 23:47:52] Multiple platforms requested and available 64G &gt;= threshold 40G, building platforms <span class="hljs-keyword">in</span> parallel+++ [0116 23:47:52] Building go targets <span class="hljs-keyword">for</span> &#123;linux/amd64 linux/arm<span class="hljs-comment"># ... 省略编译日志</span></code></pre></div><p>编译完成后能够在 <code>_output/local/bin/linux/amd64</code> 下找到刚刚编译成功的 <code>kubeadm</code> 文件，将编译好的 kubeadm scp 到已经存在集群上执行 renew，然后查看证书时间</p><p><img src="https://cdn.oss.link/markdown/i3laa.png" alt="kubeadm_renew"></p><p><strong>经过测试后确认源码修改方式有效</strong></p><h4 id="2-2-2、验证调整-CSR-API"><a href="#2-2-2、验证调整-CSR-API" class="headerlink" title="2.2.2、验证调整 CSR API"></a>2.2.2、验证调整 CSR API</h4><p>根据推测当使用 <code>--use-api</code> 会受到 <code>kube-controller-manager</code> 组件的 <code>--experimental-cluster-signing-duration</code> 影响，从而从集群中下发证书；所以首先在启动集群时需要将 <code>--experimental-cluster-signing-duration</code> 调整为 10 年，然后再进行测试</p><div class="hljs code-wrapper"><pre><code class="hljs yaml"><span class="hljs-attr">controllerManager:</span>  <span class="hljs-attr">extraArgs:</span>    <span class="hljs-attr">v:</span> <span class="hljs-string">&quot;4&quot;</span>    <span class="hljs-attr">node-cidr-mask-size:</span> <span class="hljs-string">&quot;19&quot;</span>    <span class="hljs-attr">deployment-controller-sync-period:</span> <span class="hljs-string">&quot;10s&quot;</span>    <span class="hljs-comment"># 在 kubeadm 配置文件中设置证书有效期为 10 年</span>    <span class="hljs-attr">experimental-cluster-signing-duration:</span> <span class="hljs-string">&quot;86700h&quot;</span>    <span class="hljs-attr">node-monitor-grace-period:</span> <span class="hljs-string">&quot;20s&quot;</span>    <span class="hljs-attr">pod-eviction-timeout:</span> <span class="hljs-string">&quot;2m&quot;</span>    <span class="hljs-attr">terminated-pod-gc-threshold:</span> <span class="hljs-string">&quot;30&quot;</span></code></pre></div><p>然后使用 <code>--use-api</code> 选项进行 renew</p><div class="hljs code-wrapper"><pre><code class="hljs sh">kubeadm alpha certs renew all --use-api</code></pre></div><p>此时会发现日志中打印出 <code>[certs] Certificate request &quot;kubeadm-cert-kubernetes-admin-648w4&quot; created</code> 字样，接下来从 <code>kube-system</code> 的 namespace 中能够看到相关 csr</p><p><img src="https://cdn.oss.link/markdown/54awl.png" alt="list_csr"></p><p>这时我们开始手动批准证书，每次批准完成一个 csr，紧接着 kubeadm 会创建另一个 csr</p><p><img src="https://cdn.oss.link/markdown/tdde7.png" alt="approve_csr"></p><p>当所有 csr 被批准后，再次查看集群证书发现证书期限确实被调整了</p><p><img src="https://cdn.oss.link/markdown/081qe.png" alt="success"></p><h2 id="三、总结"><a href="#三、总结" class="headerlink" title="三、总结"></a>三、总结</h2><p>总结一下，调整 kubeadm 证书期限有两种方案；第一种直接修改源码，耗时耗力还得会 go，最后还要跑跨平台编译(很耗时)；第二种在启动集群时调整 <code>kube-controller-manager</code> 组件的 <code>--experimental-cluster-signing-duration</code> 参数，集群创建好后手动 renew 一下并批准相关 csr。</p><p>两种方案各有利弊，修改源码方式意味着在 client 端签发处理，不会对集群产生永久性影响，也就是说哪天你想 “反悔了” 你不需要修改集群什么配置，直接用官方 kubeadm renew 一下就会变回一年期限的证书；改集群参数实现的方式意味着你不需要懂 go 代码，只需要常规的集群配置既可实现，同时你也不需要跑几十分钟的交叉编译，不需要为编译过程中的网络问题而烦恼；所以最后使用哪种方案因人因情况而定吧。</p>]]></content>
    
    
    <summary type="html">最近 kubeadm HA 的集群折腾完了，发现集群证书始终是 1 年有效期，然后自己还有点子担心；无奈只能研究一下源码一探究竟了...</summary>
    
    
    
    <category term="Kubernetes" scheme="https://mritd.com/categories/kubernetes/"/>
    
    
    <category term="Kubernetes" scheme="https://mritd.com/tags/kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>kubeadm 集群升级</title>
    <link href="https://mritd.com/2020/01/21/how-to-upgrade-kubeadm-cluster/"/>
    <id>https://mritd.com/2020/01/21/how-to-upgrade-kubeadm-cluster/</id>
    <published>2020-01-21T04:41:46.000Z</published>
    <updated>2020-01-21T04:41:46.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="一、升级前准备"><a href="#一、升级前准备" class="headerlink" title="一、升级前准备"></a>一、升级前准备</h2><ul><li>确保你的集群是 kubeadm 搭建的(等同于废话)</li><li>确保当前集群已经完成 HA(多个 master 节点)</li><li>确保在夜深人静的时候(无大量业务流量)</li><li>确保集群版本大于 v1.16.0</li><li>确保已经仔细阅读了目标版本 CHANGELOG</li><li>确保做好了完整地集群备份</li></ul><h2 id="二、升级注意事项"><a href="#二、升级注意事项" class="headerlink" title="二、升级注意事项"></a>二、升级注意事项</h2><ul><li>升级后所有集群组件 Pod 会重启(hash 变更)</li><li><strong>升级时 <code>kubeadm</code> 版本必须大于或等于目标版本</strong></li><li><strong>升级期间所有 <code>kube-proxy</code> 组件会有一次全节点滚动更新</strong></li><li><strong>升级只支持顺次进行，不支持跨版本升级(You only can upgrade from one MINOR version to the next MINOR version, or between PATCH versions of the same MINOR. That is, you cannot skip MINOR versions when you upgrade. For example, you can upgrade from 1.y to 1.y+1, but not from 1.y to 1.y+2.)</strong></li></ul><p>关于升级版本问题…虽然是这么说的，但是官方文档样例代码里是从 <code>v1.16.0</code> 升级到 <code>v1.17.0</code>；可能是我理解有误，跨大版本升级好像官方没提，具体啥后果不清楚…</p><h2 id="三、升级-Master"><a href="#三、升级-Master" class="headerlink" title="三、升级 Master"></a>三、升级 Master</h2><blockquote><p>事实上所有升级工作主要是针对 master 节点做的，所以整个升级流程中最重要的是如何把 master 升级好。</p></blockquote><h3 id="3-1、升级-kubeadm、kubectl"><a href="#3-1、升级-kubeadm、kubectl" class="headerlink" title="3.1、升级 kubeadm、kubectl"></a>3.1、升级 kubeadm、kubectl</h3><p>首先由于升级限制，必须先将 <code>kubeadm</code> 和 <code>kubectl</code> 升级到大于等于目标版本</p><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment"># replace x in 1.17.x-00 with the latest patch version</span>apt-mark unhold kubeadm kubectlapt-get updateapt-get install -y kubeadm=1.17.x-00 kubectl=1.17.x-00apt-mark hold kubeadm kubectl</code></pre></div><p>当然如果你之前没有 <code>hold</code> 住这几个软件包的版本，那么就不需要 <code>unhold</code>；我的做法可能比较极端…一般为了防止后面的误升级安装完成后我会直接 <code>rename</code> 掉相关软件包的 <code>apt source</code> 配置(从根本上防止手贱)。</p><h3 id="3-2、升级前准备"><a href="#3-2、升级前准备" class="headerlink" title="3.2、升级前准备"></a>3.2、升级前准备</h3><h4 id="3-2-1、配置修改"><a href="#3-2-1、配置修改" class="headerlink" title="3.2.1、配置修改"></a>3.2.1、配置修改</h4><p>对于高级玩家一般安装集群时都会自定义很多组件参数，此时不可避免的会采用配置文件；所以安装完新版本的 <code>kubeadm</code> 后就要着手修改配置文件中的 <code>kubernetesVersion</code> 字段为目标集群版本，当然有其他变更也可以一起修改。</p><h4 id="3-2-2、节点驱逐"><a href="#3-2-2、节点驱逐" class="headerlink" title="3.2.2、节点驱逐"></a>3.2.2、节点驱逐</h4><p>如果你的 master 节点也当作 node 在跑一些工作负载，则需要执行以下命令驱逐这些 pod 并使节点进入维护模式(禁止调度)。</p><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment"># 将 NODE_NAME 换成 Master 节点名称</span>kubectl drain NODE_NAME --ignore-daemonsets</code></pre></div><h4 id="3-2-3、查看升级计划"><a href="#3-2-3、查看升级计划" class="headerlink" title="3.2.3、查看升级计划"></a>3.2.3、查看升级计划</h4><p>完成节点驱逐以后，可以通过以下命令查看升级计划；<strong>升级计划中列出了升级期间要执行的所有步骤以及相关警告，一定要仔细查看。</strong></p><div class="hljs code-wrapper"><pre><code class="hljs sh">k8s16.node ➜  ~ kubeadm upgrade plan --config /etc/kubernetes/kubeadm.yamlW0115 10:59:52.586204     983 validation.go:28] Cannot validate kube-proxy config - no validator is availableW0115 10:59:52.586241     983 validation.go:28] Cannot validate kubelet config - no validator is available[upgrade/config] Making sure the configuration is correct:W0115 10:59:52.605458     983 common.go:94] WARNING: Usage of the --config flag <span class="hljs-keyword">for</span> reconfiguring the cluster during upgrade is not recommended!W0115 10:59:52.607258     983 validation.go:28] Cannot validate kube-proxy config - no validator is availableW0115 10:59:52.607274     983 validation.go:28] Cannot validate kubelet config - no validator is available[preflight] Running pre-flight checks.[upgrade] Making sure the cluster is healthy:[upgrade] Fetching available versions to upgrade to[upgrade/versions] Cluster version: v1.17.0[upgrade/versions] kubeadm version: v1.17.1External components that should be upgraded manually before you upgrade the control plane with <span class="hljs-string">&#x27;kubeadm upgrade apply&#x27;</span>:COMPONENT   CURRENT   AVAILABLEEtcd        3.3.18    3.4.3-0Components that must be upgraded manually after you have upgraded the control plane with <span class="hljs-string">&#x27;kubeadm upgrade apply&#x27;</span>:COMPONENT   CURRENT       AVAILABLEKubelet     5 x v1.17.0   v1.17.1Upgrade to the latest version <span class="hljs-keyword">in</span> the v1.17 series:COMPONENT            CURRENT   AVAILABLEAPI Server           v1.17.0   v1.17.1Controller Manager   v1.17.0   v1.17.1Scheduler            v1.17.0   v1.17.1Kube Proxy           v1.17.0   v1.17.1CoreDNS              1.6.5     1.6.5You can now apply the upgrade by executing the following <span class="hljs-built_in">command</span>:        kubeadm upgrade apply v1.17.1_____________________________________________________________________</code></pre></div><h3 id="3-3、执行升级"><a href="#3-3、执行升级" class="headerlink" title="3.3、执行升级"></a>3.3、执行升级</h3><p>确认好升级计划以后，只需要一条命令既可将当前 master 节点升级到目标版本</p><div class="hljs code-wrapper"><pre><code class="hljs sh">kubeadm upgrade apply v1.17.1 --config /etc/kubernetes/kubeadm.yaml</code></pre></div><p>升级期间会打印很详细的日志，在日志中可以实时观察到升级流程，建议仔细关注升级流程；<strong>在最后一步会有一条日志 <code>[addons] Applied essential addon: kube-proxy</code>，这意味着集群开始更新 <code>kube-proxy</code> 组件，该组件目前是通过 <code>daemonset</code> 方式启动的；这会意味着此时会造成全节点的 <code>kube-proxy</code> 更新；</strong>理论上不会有很大影响，但是升级是还是需要注意一下这一步操作，在我的观察中似乎 <code>kube-proxy</code> 也是通过滚动更新完成的，所以问题应该不大。</p><h3 id="3-4、升级-kubelet"><a href="#3-4、升级-kubelet" class="headerlink" title="3.4、升级 kubelet"></a>3.4、升级 kubelet</h3><p>在单个 master 上升级完成后，<strong>只会升级本节点的 master 相关组件和全节点的 <code>kube-proxy</code> 组件；</strong>由于 kubelet 是在宿主机安装的，所以需要通过包管理器手动升级 kubelet</p><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment"># replace x in 1.17.x-00 with the latest patch version</span>apt-mark unhold kubeletapt-get install -y kubelet=1.17.x-00apt-mark hold kubelet</code></pre></div><p>更新完成后执行 <code>systemctl restart kubelet</code> 重启，并等待启动成功既可；最后不要忘记解除当前节点的维护模式(<code>uncordon</code>)。</p><h3 id="3-5、升级其他-Master"><a href="#3-5、升级其他-Master" class="headerlink" title="3.5、升级其他 Master"></a>3.5、升级其他 Master</h3><p>当其中一个 master 节点升级完成后，其他的 master 升级就会相对简单的多；<strong>首先国际惯例升级一下 <code>kubeadm</code> 和 <code>kubectl</code> 软件包，然后直接在其他 master 节点执行 <code>kubeadm upgrade node</code> 既可。</strong>由于 apiserver 等组件配置已经在升级第一个 master 时上传到了集群的 configMap 中，所以事实上其他 master 节点只是正常拉取然后重启相关组件既可；这一步同样会输出详细日志，可以仔细观察进度，<strong>最后不要忘记升级之前先进入维护模式，升级完成后重新安装 <code>kubelet</code> 并关闭节点维护模式。</strong></p><h2 id="四、升级-Node"><a href="#四、升级-Node" class="headerlink" title="四、升级 Node"></a>四、升级 Node</h2><p>node 节点的升级实际上在升级完 master 节点以后不需要什么特殊操作，node 节点唯一需要升级的就是 <code>kubelet</code> 组件；<strong>首先在 node 节点执行 <code>kubeadm upgrade node</code> 命令，该命令会拉取集群内的 <code>kubelet</code> 配置文件，然后重新安装 <code>kubelet</code> 重启既可；</strong>同样升级 node 节点时不要忘记开启维护模式。针对于 CNI 组件请按需手动升级，并且确认好 CNI 组件的兼容版本。</p><h2 id="五、验证集群"><a href="#五、验证集群" class="headerlink" title="五、验证集群"></a>五、验证集群</h2><p>所有组件升级完成后，可以通过 <code>kubectl describe POD_NAME</code> 的方式验证 master 组件是否都升级到了最新版本；通过 <code>kuebctl version</code> 命令验证 api 相关信息(HA rr 轮训模式下可以多执行几遍)；还有就是通过 <code>kubectl get node -o wide</code> 查看相关 node 的信息，确保 <code>kubelet</code> 都升级成功，同时全部节点维护模式都已经关闭，其他细节可以参考<a href="https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade">官方文档</a>。</p>]]></content>
    
    
    <summary type="html">真是不巧，刚折腾完 kubeadm 搭建集群(v1.17.0)，第二天早上醒来特么的 v1.17.1 发布了；这我能忍么，肯定不能忍，然后就开始了集群升级之路...</summary>
    
    
    
    <category term="Kubernetes" scheme="https://mritd.com/categories/kubernetes/"/>
    
    
    <category term="Kubernetes" scheme="https://mritd.com/tags/kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>kubeadm 搭建 HA kubernetes 集群</title>
    <link href="https://mritd.com/2020/01/21/set-up-kubernetes-ha-cluster-by-kubeadm/"/>
    <id>https://mritd.com/2020/01/21/set-up-kubernetes-ha-cluster-by-kubeadm/</id>
    <published>2020-01-21T04:40:36.000Z</published>
    <updated>2020-01-21T04:40:36.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="一、环境准备"><a href="#一、环境准备" class="headerlink" title="一、环境准备"></a>一、环境准备</h2><p>搭建环境为 5 台虚拟机，每台虚拟机配置为 4 核心 8G 内存，虚拟机 IP 范围为 <code>172.16.10.21~25</code>，其他软件配置如下</p><ul><li>os version: ubuntu 18.04</li><li>kubeadm version: 1.17.0</li><li>kubernetes version: 1.17.0</li><li>etcd version: 3.3.18</li><li>docker version: 19.03.5</li></ul><h2 id="二、HA-方案"><a href="#二、HA-方案" class="headerlink" title="二、HA 方案"></a>二、HA 方案</h2><p>目前的 HA 方案与官方的不同，官方 HA 方案推荐使用类似 haproxy 等工具进行 4 层代理 apiserver，但是同样会有一个问题就是我们还需要对这个 haproxy 做 HA；由于目前我们实际生产环境都是多个独立的小集群，所以单独弄 2 台 haproxy + keeplived 去维持这个 apiserver LB 的 HA 有点不划算；所以还是准备延续老的 HA 方案，将外部 apiserver 的 4 层 LB 前置到每个 node 节点上；<strong>目前是采用在每个 node 节点上部署 nginx 4 层代理所有 apiserver，nginx 本身资源消耗低而且请求量不大，综合来说对宿主机影响很小；</strong>以下为 HA 的大致方案图</p><p><img src="https://cdn.oss.link/markdown/mktld.png" alt="ha"></p><h2 id="三、环境初始化"><a href="#三、环境初始化" class="headerlink" title="三、环境初始化"></a>三、环境初始化</h2><h3 id="3-1、系统环境"><a href="#3-1、系统环境" class="headerlink" title="3.1、系统环境"></a>3.1、系统环境</h3><p>由于个人操作习惯原因，目前已经将常用的初始化环境整理到一个小脚本里了，脚本具体参见 <a href="https://github.com/mritd/shell_scripts/blob/master/init_ubuntu.sh">mritd/shell_scripts</a> 仓库，基本上常用的初始化内容为: </p><ul><li>设置 locale(en_US.UTF-8)</li><li>设置时区(Asia/Shanghai)</li><li>更新所有系统软件包(system update)</li><li>配置 vim(vim8 + 常用插件、配色)</li><li>ohmyzsh(别跟我说不兼容 bash 脚本，我就是喜欢)</li><li>docker</li><li>ctop(一个 docker 的辅助工具)</li><li>docker-compose</li></ul><p><strong>在以上初始化中，实际对 kubernetes 安装产生影响的主要有三个地方:</strong></p><ul><li><strong>docker 的 cgroup driver 调整为 systemd，具体参考 <a href="https://github.com/mritd/config/blob/master/docker/docker.service">docker.service</a></strong></li><li><strong>docker 一定要限制 conatiner 日志大小，防止 apiserver 等日志大量输出导致磁盘占用过大</strong></li><li><strong>安装 <code>conntrack</code> 和 <code>ipvsadm</code>，后面可能需要借助其排查问题</strong></li></ul><h3 id="3-2、配置-ipvs"><a href="#3-2、配置-ipvs" class="headerlink" title="3.2、配置 ipvs"></a>3.2、配置 ipvs</h3><p>由于后面 kube-proxy 需要使用 ipvs 模式，所以需要对内核参数、模块做一些调整，调整命令如下:</p><div class="hljs code-wrapper"><pre><code class="hljs sh">cat &gt;&gt; /etc/sysctl.conf &lt;&lt;<span class="hljs-string">EOF</span><span class="hljs-string">net.ipv4.ip_forward=1</span><span class="hljs-string">net.bridge.bridge-nf-call-iptables=1</span><span class="hljs-string">net.bridge.bridge-nf-call-ip6tables=1</span><span class="hljs-string">EOF</span>sysctl -pcat &gt;&gt; /etc/modules &lt;&lt;<span class="hljs-string">EOF</span><span class="hljs-string">ip_vs</span><span class="hljs-string">ip_vs_lc</span><span class="hljs-string">ip_vs_wlc</span><span class="hljs-string">ip_vs_rr</span><span class="hljs-string">ip_vs_wrr</span><span class="hljs-string">ip_vs_lblc</span><span class="hljs-string">ip_vs_lblcr</span><span class="hljs-string">ip_vs_dh</span><span class="hljs-string">ip_vs_sh</span><span class="hljs-string">ip_vs_fo</span><span class="hljs-string">ip_vs_nq</span><span class="hljs-string">ip_vs_sed</span><span class="hljs-string">ip_vs_ftp</span><span class="hljs-string">EOF</span></code></pre></div><p><strong>配置完成后切记需要重启，重启完成后使用 <code>lsmod | grep ip_vs</code> 验证相关 ipvs 模块加载是否正常，本文将主要使用 <code>ip_vs_wrr</code>，所以目前只关注这个模块既可。</strong></p><p><img src="https://cdn.oss.link/markdown/4irz1.png" alt="ipvs_mode"></p><h2 id="四、安装-Etcd"><a href="#四、安装-Etcd" class="headerlink" title="四、安装 Etcd"></a>四、安装 Etcd</h2><h3 id="4-1、方案选择"><a href="#4-1、方案选择" class="headerlink" title="4.1、方案选择"></a>4.1、方案选择</h3><p>官方对于集群 HA 给出了两种有关于 Etcd 的部署方案: </p><ul><li>一种是深度耦合到 <code>control plane</code> 上，即每个 <code>control plane</code> 一个 etcd</li><li>另一种是使用外部的 Etcd 集群，通过在配置中指定外部集群让 apiserver 等组件连接</li></ul><p>在测试深度耦合 <code>control plane</code> 方案后，发现一些比较恶心的问题；比如说开始创建第二个 <code>control plane</code> 时配置写错了需要重建，此时你一旦删除第二个 <code>control plane</code> 会导致第一个 <code>control plane</code> 也会失败，原因是<strong>创建第二个 <code>control plane</code> 时 kubeadm 已经自动完成了 etcd 的集群模式，当删除第二个 <code>control plane</code> 的时候由于集群可用原因会导致第一个 <code>control plane</code> 下的 etcd 发现节点失联从而也不提供服务；</strong>所以综合考虑到后续迁移、灾备等因素，这里选择了将 etcd 放置在外部集群中；同样也方便我以后各种折腾应对一些极端情况啥的。</p><h3 id="4-2、部署-Etcd"><a href="#4-2、部署-Etcd" class="headerlink" title="4.2、部署 Etcd"></a>4.2、部署 Etcd</h3><p>确定了需要在外部部署 etcd 集群后，只需要开干就完事了；查了一下 ubuntu 官方源已经有了 etcd 安装包，但是版本比较老，测试了一下 golang 的 build 版本是 1.10；所以我还是选择了从官方 release 下载最新的版本安装；当然最后还是因为懒，我自己打了一个 deb 包… deb 包可以从这个项目 <a href="https://github.com/mritd/etcd-deb/releases">mritd/etcd-deb</a> 下载，担心安全性的可以利用项目脚本自己打包，以下是安装过程:</p><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment"># 下载软件包</span>wget https://github.com/mritd/etcd-deb/releases/download/v3.3.18/etcd_3.3.18_amd64.debwget https://github.com/mritd/etcd-deb/releases/download/v3.3.18/cfssl_1.4.1_amd64.deb<span class="hljs-comment"># 安装 etcd(至少在 3 台节点上执行)</span>dpkg -i etcd_3.3.18_amd64.deb cfssl_1.4.1_amd64.deb</code></pre></div><p><strong>既然自己部署 etcd，那么证书签署啥的还得自己来了，证书签署这里借助 cfssl 工具，cfssl 目前提供了 deb 的 make target，但是没找到 deb 包，所以也自己 build 了(担心安全性的可自行去官方下载)；</strong>接着编辑一下 <code>/etc/etcd/cfssl/etcd-csr.json</code> 文件，用 <code>/etc/etcd/cfssl/create.sh</code> 脚本创建证书，并将证书复制到指定目录</p><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment"># 创建证书</span><span class="hljs-built_in">cd</span> /etc/etcd/cfssl &amp;&amp; ./create.sh<span class="hljs-comment"># 复制证书</span>mv /etc/etcd/cfssl/*.pem /etc/etcd/ssl</code></pre></div><p>最后在 3 台节点上修改配置，并将刚刚创建的证书同步到其他两台节点启动既可；下面是单台节点的配置样例</p><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment"># /etc/etcd/etcd.conf</span><span class="hljs-comment"># [member]</span>ETCD_NAME=etcd1ETCD_DATA_DIR=<span class="hljs-string">&quot;/var/lib/etcd/data&quot;</span>ETCD_WAL_DIR=<span class="hljs-string">&quot;/var/lib/etcd/wal&quot;</span>ETCD_SNAPSHOT_COUNT=<span class="hljs-string">&quot;100&quot;</span>ETCD_HEARTBEAT_INTERVAL=<span class="hljs-string">&quot;100&quot;</span>ETCD_ELECTION_TIMEOUT=<span class="hljs-string">&quot;1000&quot;</span>ETCD_LISTEN_PEER_URLS=<span class="hljs-string">&quot;https://172.16.10.21:2380&quot;</span>ETCD_LISTEN_CLIENT_URLS=<span class="hljs-string">&quot;https://172.16.10.21:2379,http://127.0.0.1:2379&quot;</span>ETCD_MAX_SNAPSHOTS=<span class="hljs-string">&quot;5&quot;</span>ETCD_MAX_WALS=<span class="hljs-string">&quot;5&quot;</span><span class="hljs-comment">#ETCD_CORS=&quot;&quot;</span><span class="hljs-comment"># [cluster]</span>ETCD_INITIAL_ADVERTISE_PEER_URLS=<span class="hljs-string">&quot;https://172.16.10.21:2380&quot;</span><span class="hljs-comment"># if you use different ETCD_NAME (e.g. test), set ETCD_INITIAL_CLUSTER value for this name, i.e. &quot;test=http://...&quot;</span>ETCD_INITIAL_CLUSTER=<span class="hljs-string">&quot;etcd1=https://172.16.10.21:2380,etcd2=https://172.16.10.22:2380,etcd3=https://172.16.10.23:2380&quot;</span>ETCD_INITIAL_CLUSTER_STATE=<span class="hljs-string">&quot;new&quot;</span>ETCD_INITIAL_CLUSTER_TOKEN=<span class="hljs-string">&quot;etcd-cluster&quot;</span>ETCD_ADVERTISE_CLIENT_URLS=<span class="hljs-string">&quot;https://172.16.10.21:2379&quot;</span><span class="hljs-comment">#ETCD_DISCOVERY=&quot;&quot;</span><span class="hljs-comment">#ETCD_DISCOVERY_SRV=&quot;&quot;</span><span class="hljs-comment">#ETCD_DISCOVERY_FALLBACK=&quot;proxy&quot;</span><span class="hljs-comment">#ETCD_DISCOVERY_PROXY=&quot;&quot;</span><span class="hljs-comment">#ETCD_STRICT_RECONFIG_CHECK=&quot;false&quot;</span>ETCD_AUTO_COMPACTION_RETENTION=<span class="hljs-string">&quot;24&quot;</span><span class="hljs-comment"># [proxy]</span><span class="hljs-comment">#ETCD_PROXY=&quot;off&quot;</span><span class="hljs-comment">#ETCD_PROXY_FAILURE_WAIT=&quot;5000&quot;</span><span class="hljs-comment">#ETCD_PROXY_REFRESH_INTERVAL=&quot;30000&quot;</span><span class="hljs-comment">#ETCD_PROXY_DIAL_TIMEOUT=&quot;1000&quot;</span><span class="hljs-comment">#ETCD_PROXY_WRITE_TIMEOUT=&quot;5000&quot;</span><span class="hljs-comment">#ETCD_PROXY_READ_TIMEOUT=&quot;0&quot;</span><span class="hljs-comment"># [security]</span>ETCD_CERT_FILE=<span class="hljs-string">&quot;/etc/etcd/ssl/etcd.pem&quot;</span>ETCD_KEY_FILE=<span class="hljs-string">&quot;/etc/etcd/ssl/etcd-key.pem&quot;</span>ETCD_CLIENT_CERT_AUTH=<span class="hljs-string">&quot;true&quot;</span>ETCD_TRUSTED_CA_FILE=<span class="hljs-string">&quot;/etc/etcd/ssl/etcd-root-ca.pem&quot;</span>ETCD_AUTO_TLS=<span class="hljs-string">&quot;true&quot;</span>ETCD_PEER_CERT_FILE=<span class="hljs-string">&quot;/etc/etcd/ssl/etcd.pem&quot;</span>ETCD_PEER_KEY_FILE=<span class="hljs-string">&quot;/etc/etcd/ssl/etcd-key.pem&quot;</span>ETCD_PEER_CLIENT_CERT_AUTH=<span class="hljs-string">&quot;true&quot;</span>ETCD_PEER_TRUSTED_CA_FILE=<span class="hljs-string">&quot;/etc/etcd/ssl/etcd-root-ca.pem&quot;</span>ETCD_PEER_AUTO_TLS=<span class="hljs-string">&quot;true&quot;</span><span class="hljs-comment"># [logging]</span><span class="hljs-comment">#ETCD_DEBUG=&quot;false&quot;</span><span class="hljs-comment"># examples for -log-package-levels etcdserver=WARNING,security=DEBUG</span><span class="hljs-comment">#ETCD_LOG_PACKAGE_LEVELS=&quot;&quot;</span><span class="hljs-comment"># [performance]</span>ETCD_QUOTA_BACKEND_BYTES=<span class="hljs-string">&quot;5368709120&quot;</span>ETCD_AUTO_COMPACTION_RETENTION=<span class="hljs-string">&quot;3&quot;</span></code></pre></div><p><strong>注意: 其他两台节点请调整 <code>ETCD_NAME</code> 为不重复的其他名称，调整 <code>ETCD_LISTEN_PEER_URLS</code>、<code>ETCD_LISTEN_CLIENT_URLS</code>、<code>ETCD_INITIAL_ADVERTISE_PEER_URLS</code>、<code>ETCD_ADVERTISE_CLIENT_URLS</code> 为其他节点对应的 IP；同时生产环境请将 <code>ETCD_INITIAL_CLUSTER_TOKEN</code> 替换为复杂的 token</strong></p><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment"># 同步证书</span>scp -r /etc/etcd/ssl 172.16.10.22:/etc/etcd/sslscp -r /etc/etcd/ssl 172.16.10.23:/etc/etcd/ssl<span class="hljs-comment"># 修复权限(3台节点都要执行)</span>chown -R etcd:etcd /etc/etcd<span class="hljs-comment"># 最后每个节点依次启动既可</span>systemctl start etcd</code></pre></div><p>启动完成后可以通过以下命令测试是否正常</p><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment"># 查看集群成员</span>k1.node ➜ etcdctl member list3cbbaf77904c6153, started, etcd2, https://172.16.10.22:2380, https://172.16.10.22:23798eb7652b6bd99c30, started, etcd1, https://172.16.10.21:2380, https://172.16.10.21:237991f4e10726460d8c, started, etcd3, https://172.16.10.23:2380, https://172.16.10.23:2379<span class="hljs-comment"># 检测集群健康状态</span>k1.node ➜ etcdctl endpoint health --cacert /etc/etcd/ssl/etcd-root-ca.pem --cert /etc/etcd/ssl/etcd.pem --key /etc/etcd/ssl/etcd-key.pem --endpoints https://172.16.10.21:2379,https://172.16.10.22:2379,https://172.16.10.23:2379https://172.16.10.21:2379 is healthy: successfully committed proposal: took = 16.632246mshttps://172.16.10.23:2379 is healthy: successfully committed proposal: took = 21.122603mshttps://172.16.10.22:2379 is healthy: successfully committed proposal: took = 22.592005ms</code></pre></div><h2 id="五、部署-Kubernetes"><a href="#五、部署-Kubernetes" class="headerlink" title="五、部署 Kubernetes"></a>五、部署 Kubernetes</h2><h3 id="5-1、安装-kueadm"><a href="#5-1、安装-kueadm" class="headerlink" title="5.1、安装 kueadm"></a>5.1、安装 kueadm</h3><p>安装 kubeadm 没什么好说的，国内被墙用阿里的源既可</p><div class="hljs code-wrapper"><pre><code class="hljs sh">apt-get install -y apt-transport-httpscurl https://mirrors.aliyun.com/kubernetes/apt/doc/apt-key.gpg | apt-key add -cat &lt;&lt;<span class="hljs-string">EOF &gt;/etc/apt/sources.list.d/kubernetes.list</span><span class="hljs-string">deb https://mirrors.aliyun.com/kubernetes/apt/ kubernetes-xenial main</span><span class="hljs-string">EOF</span>apt update<span class="hljs-comment"># ebtables、ethtool kubelet 可能会用，具体忘了，反正从官方文档上看到的</span>apt install kubelet kubeadm kubectl ebtables ethtool -y</code></pre></div><h3 id="5-2、部署-Nginx"><a href="#5-2、部署-Nginx" class="headerlink" title="5.2、部署 Nginx"></a>5.2、部署 Nginx</h3><p>从上面的 HA 架构图上可以看到，为了维持 apiserver 的 HA，需要在每个机器上部署一个 nginx 做 4 层的 LB；为保证后续的 node 节点正常加入，需要首先行部署 nginx；nginx 安装同样喜欢偷懒，直接 docker 跑了…毕竟都开始 kubeadm 了，那么也没必要去纠结 docker 是否稳定的问题了；以下为 nginx 相关配置</p><p><strong>apiserver-proxy.conf</strong></p><div class="hljs code-wrapper"><pre><code class="hljs sh">error_log stderr notice;worker_processes auto;events &#123;multi_accept on;use epoll;worker_connections 1024;&#125;stream &#123;    upstream kube_apiserver &#123;        least_conn;        <span class="hljs-comment"># 后端为三台 master 节点的 apiserver 地址</span>        server 172.16.10.21:5443;        server 172.16.10.22:5443;        server 172.16.10.23:5443;    &#125;        server &#123;        listen        0.0.0.0:6443;        proxy_pass    kube_apiserver;        proxy_timeout 10m;        proxy_connect_timeout 1s;    &#125;&#125;</code></pre></div><p><strong>kube-apiserver-proxy.service</strong></p><div class="hljs code-wrapper"><pre><code class="hljs sh">[Unit]Description=kubernetes apiserver docker wrapperWants=docker.socketAfter=docker.service[Service]User=rootPermissionsStartOnly=<span class="hljs-literal">true</span>ExecStart=/usr/bin/docker run -p 6443:6443 \                          -v /etc/kubernetes/apiserver-proxy.conf:/etc/nginx/nginx.conf \                          --name kube-apiserver-proxy \                          --net=host \                          --restart=on-failure:5 \                          --memory=512M \                          nginx:1.17.6-alpineExecStartPre=-/usr/bin/docker rm -f kube-apiserver-proxyExecStop=/usr/bin/docker rm -rf kube-apiserver-proxyRestart=alwaysRestartSec=15sTimeoutStartSec=30s[Install]WantedBy=multi-user.target</code></pre></div><p>启动 nginx 代理(每台机器都要启动，包括 master 节点)</p><div class="hljs code-wrapper"><pre><code class="hljs sh">cp apiserver-proxy.conf /etc/kubernetescp kube-apiserver-proxy.service /lib/systemd/systemsystemctl daemon-reloadsystemctl <span class="hljs-built_in">enable</span> kube-apiserver-proxy.service &amp;&amp; systemctl start kube-apiserver-proxy.service</code></pre></div><h3 id="5-3、启动-control-plane"><a href="#5-3、启动-control-plane" class="headerlink" title="5.3、启动 control plane"></a>5.3、启动 control plane</h3><h4 id="5-3-1、关于-Swap"><a href="#5-3-1、关于-Swap" class="headerlink" title="5.3.1、关于 Swap"></a>5.3.1、关于 Swap</h4><p>目前 kubelet 为了保证内存 limit，需要在每个节点上关闭 swap；但是说实话我看了这篇文章 <a href="https://chrisdown.name/2018/01/02/in-defence-of-swap.html">In defence of swap: common misconceptions</a> 以后还是不想关闭 swap；更确切的说其实我们生产环境比较 “富”，pod 都不 limit 内存，所以下面的部署我忽略了 swap 错误检测</p><h4 id="5-3-2、kubeadm-配置"><a href="#5-3-2、kubeadm-配置" class="headerlink" title="5.3.2、kubeadm 配置"></a>5.3.2、kubeadm 配置</h4><p>当前版本的 kubeadm 已经支持了完善的配置管理(当然细节部分还有待支持)，以下为我目前使用的配置，相关位置已经做了注释，更具体的配置自行查阅官方文档</p><p><strong>kubeadm.yaml</strong></p><div class="hljs code-wrapper"><pre><code class="hljs yaml"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">kubeadm.k8s.io/v1beta2</span><span class="hljs-attr">kind:</span> <span class="hljs-string">InitConfiguration</span><span class="hljs-attr">localAPIEndpoint:</span>  <span class="hljs-comment"># 第一个 master 节点 IP</span>  <span class="hljs-attr">advertiseAddress:</span> <span class="hljs-string">&quot;172.16.10.21&quot;</span>  <span class="hljs-comment"># 6443 留给了 nginx，apiserver 换到 5443</span>  <span class="hljs-attr">bindPort:</span> <span class="hljs-number">5443</span><span class="hljs-comment"># 这个 token 使用以下命令生成</span><span class="hljs-comment"># kubeadm alpha certs certificate-key</span><span class="hljs-attr">certificateKey:</span> <span class="hljs-string">7373f829c733b46fb78f0069f90185e0f00254381641d8d5a7c5984b2cf17cd3</span> <span class="hljs-meta">---</span><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">kubeadm.k8s.io/v1beta2</span><span class="hljs-attr">kind:</span> <span class="hljs-string">ClusterConfiguration</span><span class="hljs-comment"># 使用外部 etcd 配置</span><span class="hljs-attr">etcd:</span>  <span class="hljs-attr">external:</span>    <span class="hljs-attr">endpoints:</span>    <span class="hljs-bullet">-</span> <span class="hljs-string">&quot;https://172.16.10.21:2379&quot;</span>    <span class="hljs-bullet">-</span> <span class="hljs-string">&quot;https://172.16.10.22:2379&quot;</span>    <span class="hljs-bullet">-</span> <span class="hljs-string">&quot;https://172.16.10.23:2379&quot;</span>    <span class="hljs-attr">caFile:</span> <span class="hljs-string">&quot;/etc/etcd/ssl/etcd-root-ca.pem&quot;</span>    <span class="hljs-attr">certFile:</span> <span class="hljs-string">&quot;/etc/etcd/ssl/etcd.pem&quot;</span>    <span class="hljs-attr">keyFile:</span> <span class="hljs-string">&quot;/etc/etcd/ssl/etcd-key.pem&quot;</span><span class="hljs-comment"># 网络配置</span><span class="hljs-attr">networking:</span>  <span class="hljs-attr">serviceSubnet:</span> <span class="hljs-string">&quot;10.25.0.0/16&quot;</span>  <span class="hljs-attr">podSubnet:</span> <span class="hljs-string">&quot;10.30.0.1/16&quot;</span>  <span class="hljs-attr">dnsDomain:</span> <span class="hljs-string">&quot;cluster.local&quot;</span><span class="hljs-attr">kubernetesVersion:</span> <span class="hljs-string">&quot;v1.17.0&quot;</span><span class="hljs-comment"># 全局 apiserver LB 地址，由于采用了 nginx 负载，所以直接指向本地既可</span><span class="hljs-attr">controlPlaneEndpoint:</span> <span class="hljs-string">&quot;127.0.0.1:6443&quot;</span><span class="hljs-attr">apiServer:</span>  <span class="hljs-comment"># apiserver 的自定义扩展参数</span>  <span class="hljs-attr">extraArgs:</span>    <span class="hljs-attr">v:</span> <span class="hljs-string">&quot;4&quot;</span>    <span class="hljs-attr">alsologtostderr:</span> <span class="hljs-string">&quot;true&quot;</span>    <span class="hljs-comment"># 审计日志相关配置</span>    <span class="hljs-attr">audit-log-maxage:</span> <span class="hljs-string">&quot;20&quot;</span>    <span class="hljs-attr">audit-log-maxbackup:</span> <span class="hljs-string">&quot;10&quot;</span>    <span class="hljs-attr">audit-log-maxsize:</span> <span class="hljs-string">&quot;100&quot;</span>    <span class="hljs-attr">audit-log-path:</span> <span class="hljs-string">&quot;/var/log/kube-audit/audit.log&quot;</span>    <span class="hljs-attr">audit-policy-file:</span> <span class="hljs-string">&quot;/etc/kubernetes/audit-policy.yaml&quot;</span>    <span class="hljs-attr">authorization-mode:</span> <span class="hljs-string">&quot;Node,RBAC&quot;</span>    <span class="hljs-attr">event-ttl:</span> <span class="hljs-string">&quot;720h&quot;</span>    <span class="hljs-attr">runtime-config:</span> <span class="hljs-string">&quot;api/all=true&quot;</span>    <span class="hljs-attr">service-node-port-range:</span> <span class="hljs-string">&quot;30000-50000&quot;</span>    <span class="hljs-attr">service-cluster-ip-range:</span> <span class="hljs-string">&quot;10.25.0.0/16&quot;</span>  <span class="hljs-comment"># 由于自行定义了审计日志配置，所以需要将宿主机上的审计配置</span>  <span class="hljs-comment"># 挂载到 kube-apiserver 的 pod 容器中</span>  <span class="hljs-attr">extraVolumes:</span>  <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">&quot;audit-config&quot;</span>    <span class="hljs-attr">hostPath:</span> <span class="hljs-string">&quot;/etc/kubernetes/audit-policy.yaml&quot;</span>    <span class="hljs-attr">mountPath:</span> <span class="hljs-string">&quot;/etc/kubernetes/audit-policy.yaml&quot;</span>    <span class="hljs-attr">readOnly:</span> <span class="hljs-literal">true</span>    <span class="hljs-attr">pathType:</span> <span class="hljs-string">&quot;File&quot;</span>  <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">&quot;audit-log&quot;</span>    <span class="hljs-attr">hostPath:</span> <span class="hljs-string">&quot;/var/log/kube-audit&quot;</span>    <span class="hljs-attr">mountPath:</span> <span class="hljs-string">&quot;/var/log/kube-audit&quot;</span>    <span class="hljs-attr">pathType:</span> <span class="hljs-string">&quot;DirectoryOrCreate&quot;</span>  <span class="hljs-comment"># 这里是 apiserver 的证书地址配置</span>  <span class="hljs-comment"># 为了防止以后出特殊情况，我增加了一个泛域名</span>  <span class="hljs-attr">certSANs:</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">&quot;*.kubernetes.node&quot;</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">&quot;172.16.10.21&quot;</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">&quot;172.16.10.22&quot;</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">&quot;172.16.10.23&quot;</span>  <span class="hljs-attr">timeoutForControlPlane:</span> <span class="hljs-string">5m</span><span class="hljs-attr">controllerManager:</span>  <span class="hljs-attr">extraArgs:</span>    <span class="hljs-attr">v:</span> <span class="hljs-string">&quot;4&quot;</span>    <span class="hljs-comment"># 宿主机 ip 掩码</span>    <span class="hljs-attr">node-cidr-mask-size:</span> <span class="hljs-string">&quot;19&quot;</span>    <span class="hljs-attr">deployment-controller-sync-period:</span> <span class="hljs-string">&quot;10s&quot;</span>    <span class="hljs-attr">experimental-cluster-signing-duration:</span> <span class="hljs-string">&quot;87600h&quot;</span>    <span class="hljs-attr">node-monitor-grace-period:</span> <span class="hljs-string">&quot;20s&quot;</span>    <span class="hljs-attr">pod-eviction-timeout:</span> <span class="hljs-string">&quot;2m&quot;</span>    <span class="hljs-attr">terminated-pod-gc-threshold:</span> <span class="hljs-string">&quot;30&quot;</span><span class="hljs-attr">scheduler:</span>  <span class="hljs-attr">extraArgs:</span>    <span class="hljs-attr">v:</span> <span class="hljs-string">&quot;4&quot;</span><span class="hljs-attr">certificatesDir:</span> <span class="hljs-string">&quot;/etc/kubernetes/pki&quot;</span><span class="hljs-comment"># gcr.io 被墙，换成微软的镜像地址</span><span class="hljs-attr">imageRepository:</span> <span class="hljs-string">&quot;gcr.azk8s.cn/google_containers&quot;</span><span class="hljs-attr">clusterName:</span> <span class="hljs-string">&quot;kuberentes&quot;</span><span class="hljs-meta">---</span><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">kubelet.config.k8s.io/v1beta1</span><span class="hljs-attr">kind:</span> <span class="hljs-string">KubeletConfiguration</span><span class="hljs-comment"># kubelet specific options here</span><span class="hljs-comment"># 此配置保证了 kubelet 能在 swap 开启的情况下启动</span><span class="hljs-attr">failSwapOn:</span> <span class="hljs-literal">false</span><span class="hljs-attr">nodeStatusUpdateFrequency:</span> <span class="hljs-string">5s</span><span class="hljs-comment"># 一些驱逐阀值，具体自行查文档修改</span><span class="hljs-attr">evictionSoft:</span>  <span class="hljs-attr">&quot;imagefs.available&quot;:</span> <span class="hljs-string">&quot;15%&quot;</span>  <span class="hljs-attr">&quot;memory.available&quot;:</span> <span class="hljs-string">&quot;512Mi&quot;</span>  <span class="hljs-attr">&quot;nodefs.available&quot;:</span> <span class="hljs-string">&quot;15%&quot;</span>  <span class="hljs-attr">&quot;nodefs.inodesFree&quot;:</span> <span class="hljs-string">&quot;10%&quot;</span><span class="hljs-attr">evictionSoftGracePeriod:</span>  <span class="hljs-attr">&quot;imagefs.available&quot;:</span> <span class="hljs-string">&quot;3m&quot;</span>  <span class="hljs-attr">&quot;memory.available&quot;:</span> <span class="hljs-string">&quot;1m&quot;</span>  <span class="hljs-attr">&quot;nodefs.available&quot;:</span> <span class="hljs-string">&quot;3m&quot;</span>  <span class="hljs-attr">&quot;nodefs.inodesFree&quot;:</span> <span class="hljs-string">&quot;1m&quot;</span><span class="hljs-attr">evictionHard:</span>  <span class="hljs-attr">&quot;imagefs.available&quot;:</span> <span class="hljs-string">&quot;10%&quot;</span>  <span class="hljs-attr">&quot;memory.available&quot;:</span> <span class="hljs-string">&quot;256Mi&quot;</span>  <span class="hljs-attr">&quot;nodefs.available&quot;:</span> <span class="hljs-string">&quot;10%&quot;</span>  <span class="hljs-attr">&quot;nodefs.inodesFree&quot;:</span> <span class="hljs-string">&quot;5%&quot;</span><span class="hljs-attr">evictionMaxPodGracePeriod:</span> <span class="hljs-number">30</span><span class="hljs-attr">imageGCLowThresholdPercent:</span> <span class="hljs-number">70</span><span class="hljs-attr">imageGCHighThresholdPercent:</span> <span class="hljs-number">80</span><span class="hljs-attr">kubeReserved:</span>  <span class="hljs-attr">&quot;cpu&quot;:</span> <span class="hljs-string">&quot;500m&quot;</span>  <span class="hljs-attr">&quot;memory&quot;:</span> <span class="hljs-string">&quot;512Mi&quot;</span>  <span class="hljs-attr">&quot;ephemeral-storage&quot;:</span> <span class="hljs-string">&quot;1Gi&quot;</span><span class="hljs-attr">rotateCertificates:</span> <span class="hljs-literal">true</span><span class="hljs-meta">---</span><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">kubeproxy.config.k8s.io/v1alpha1</span><span class="hljs-attr">kind:</span> <span class="hljs-string">KubeProxyConfiguration</span><span class="hljs-comment"># kube-proxy specific options here</span><span class="hljs-attr">clusterCIDR:</span> <span class="hljs-string">&quot;10.30.0.1/16&quot;</span><span class="hljs-comment"># 启用 ipvs 模式</span><span class="hljs-attr">mode:</span> <span class="hljs-string">&quot;ipvs&quot;</span><span class="hljs-attr">ipvs:</span>  <span class="hljs-attr">minSyncPeriod:</span> <span class="hljs-string">5s</span>  <span class="hljs-attr">syncPeriod:</span> <span class="hljs-string">5s</span>  <span class="hljs-comment"># ipvs 负载策略</span>  <span class="hljs-attr">scheduler:</span> <span class="hljs-string">&quot;wrr&quot;</span></code></pre></div><p><strong>关于这个配置配置文件的文档还是很不完善，对于不懂 golang 的人来说很难知道具体怎么配置，以下做一下简要说明(请确保你已经拉取了 kubernetes 源码和安装了 Goland)</strong></p><p><strong>kubeadm 配置中每个配置段都会有个 <code>kind</code> 字段，<code>kind</code> 实际上对应了 go 代码中的 <code>struct</code> 结构体；同时从 <code>apiVersion</code> 字段中能够看到具体的版本，比如 <code>v1alpha1</code> 等；有了这两个信息事实上你就可以直接在源码中去找到对应的结构体</strong></p><p><img src="https://cdn.oss.link/markdown/dwo5h.png" alt="struct_search"></p><p>在结构体中所有的配置便可以一目了然</p><p><img src="https://cdn.oss.link/markdown/0jc9b.png" alt="struct_detail"></p><p>关于数据类型，如果是 <code>string</code> 的类型，那么意味着你要在 yaml 里写 <code>&quot;xxxx&quot;</code> 带引号这种，当然有些时候不写能兼容，有些时候不行比如 <code>extraArgs</code> 字段是一个 <code>map[string]string</code> 如果 value 不带引号就报错；<strong>如果数据类型为 <code>metav1.Duration</code>(实际上就是 <code>time.Duration</code>)，那么你看着它是个 <code>int64</code> 但实际上你要写 <code>1h2m3s</code> 这种人类可读的格式，这是 go 的特色…</strong></p><p><strong>audit-policy.yaml</strong></p><div class="hljs code-wrapper"><pre><code class="hljs yaml"><span class="hljs-comment"># Log all requests at the Metadata level.</span><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">audit.k8s.io/v1</span><span class="hljs-attr">kind:</span> <span class="hljs-string">Policy</span><span class="hljs-attr">rules:</span><span class="hljs-bullet">-</span> <span class="hljs-attr">level:</span> <span class="hljs-string">Metadata</span></code></pre></div><p>可能 <code>Metadata</code> 级别的审计日志比较多，想自行调整审计日志级别的可以参考<a href="https://kubernetes.io/docs/tasks/debug-application-cluster/audit/#audit-policy">官方文档</a></p><h4 id="5-3-3、拉起-control-plane"><a href="#5-3-3、拉起-control-plane" class="headerlink" title="5.3.3、拉起 control plane"></a>5.3.3、拉起 control plane</h4><p>有了完整的 <code>kubeadm.yaml</code> 和 <code>audit-policy.yaml</code> 配置后，直接一条命令拉起 control plane 既可</p><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment"># 先将审计配置放到目标位置(3 台 master 都要执行)</span>cp audit-policy.yaml /etc/kubernetes<span class="hljs-comment"># 拉起 control plane</span>kubeadm init --config kubeadm.yaml --upload-certs --ignore-preflight-errors=Swap</code></pre></div><p><strong>control plane 拉起以后注意要保存屏幕输出，方便后续添加其他集群节点</strong></p><div class="hljs code-wrapper"><pre><code class="hljs sh">Your Kubernetes control-plane has initialized successfully!To start using your cluster, you need to run the following as a regular user:  mkdir -p <span class="hljs-variable">$HOME</span>/.kube  sudo cp -i /etc/kubernetes/admin.conf <span class="hljs-variable">$HOME</span>/.kube/config  sudo chown $(id -u):$(id -g) <span class="hljs-variable">$HOME</span>/.kube/configYou should now deploy a pod network to the cluster.Run <span class="hljs-string">&quot;kubectl apply -f [podnetwork].yaml&quot;</span> with one of the options listed at:  https://kubernetes.io/docs/concepts/cluster-administration/addons/You can now join any number of the control-plane node running the following <span class="hljs-built_in">command</span> on each as root:  kubeadm join 127.0.0.1:6443 --token r4t3l3.14mmuivm7xbtaeoj \    --discovery-token-ca-cert-hash sha256:06f49f1f29d08b797fbf04d87b9b0fd6095a4693e9b1d59c429745cfa082b31d \    --control-plane --certificate-key 7373f829c733b46fb78f0069f90185e0f00254381641d8d5a7c5984b2cf17cd3Please note that the certificate-key gives access to cluster sensitive data, keep it secret!As a safeguard, uploaded-certs will be deleted <span class="hljs-keyword">in</span> two hours; If necessary, you can use<span class="hljs-string">&quot;kubeadm init phase upload-certs --upload-certs&quot;</span> to reload certs afterward.Then you can join any number of worker nodes by running the following on each as root:kubeadm join 127.0.0.1:6443 --token r4t3l3.14mmuivm7xbtaeoj \    --discovery-token-ca-cert-hash sha256:06f49f1f29d08b797fbf04d87b9b0fd6095a4693e9b1d59c429745cfa082b31d</code></pre></div><p><strong>根据屏幕提示配置 kubectl</strong></p><div class="hljs code-wrapper"><pre><code class="hljs sh">mkdir -p <span class="hljs-variable">$HOME</span>/.kubesudo cp -i /etc/kubernetes/admin.conf <span class="hljs-variable">$HOME</span>/.kube/configsudo chown $(id -u):$(id -g) <span class="hljs-variable">$HOME</span>/.kube/config</code></pre></div><h3 id="5-4、部署-CNI"><a href="#5-4、部署-CNI" class="headerlink" title="5.4、部署 CNI"></a>5.4、部署 CNI</h3><p>关于网络插件的选择，以前一直喜欢 Calico，因为其性能确实好；到后来 flannel 出了 <code>host-gw</code> 以后现在两者性能也差不多了；但是 <strong>flannel 好处是一个工具通吃所有环境(云环境+裸机2层直通)，坏处是 flannel 缺乏比较好的策略管理(当然可以使用两者结合的 Canal)；</strong>后来思来想去其实我们生产倒是很少需要策略管理，所以这回怂回到 flannel 了(逃…)</p><p>Flannel 部署非常简单，根据官方文档下载配置，根据情况调整 <code>backend</code> 和 pod 的 CIDR，然后 apply 一下既可</p><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment"># 下载配置文件</span>wget https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml<span class="hljs-comment"># 调整 backend 为 host-gw(测试环境 2 层直连)</span>k1.node ➜  grep -A 35 ConfigMap kube-flannel.ymlkind: ConfigMapapiVersion: v1metadata:  name: kube-flannel-cfg  namespace: kube-system  labels:    tier: node    app: flanneldata:  cni-conf.json: |    &#123;      <span class="hljs-string">&quot;name&quot;</span>: <span class="hljs-string">&quot;cbr0&quot;</span>,      <span class="hljs-string">&quot;cniVersion&quot;</span>: <span class="hljs-string">&quot;0.3.1&quot;</span>,      <span class="hljs-string">&quot;plugins&quot;</span>: [        &#123;          <span class="hljs-string">&quot;type&quot;</span>: <span class="hljs-string">&quot;flannel&quot;</span>,          <span class="hljs-string">&quot;delegate&quot;</span>: &#123;            <span class="hljs-string">&quot;hairpinMode&quot;</span>: <span class="hljs-literal">true</span>,            <span class="hljs-string">&quot;isDefaultGateway&quot;</span>: <span class="hljs-literal">true</span>          &#125;        &#125;,        &#123;          <span class="hljs-string">&quot;type&quot;</span>: <span class="hljs-string">&quot;portmap&quot;</span>,          <span class="hljs-string">&quot;capabilities&quot;</span>: &#123;            <span class="hljs-string">&quot;portMappings&quot;</span>: <span class="hljs-literal">true</span>          &#125;        &#125;      ]    &#125;  net-conf.json: |    &#123;      <span class="hljs-string">&quot;Network&quot;</span>: <span class="hljs-string">&quot;10.30.0.0/16&quot;</span>,      <span class="hljs-string">&quot;Backend&quot;</span>: &#123;        <span class="hljs-string">&quot;Type&quot;</span>: <span class="hljs-string">&quot;host-gw&quot;</span>      &#125;    &#125;<span class="hljs-comment"># 调整完成后 apply 一下</span>kubectl apply -f kube-flannel.yml</code></pre></div><h3 id="5-5、启动其他-control-plane"><a href="#5-5、启动其他-control-plane" class="headerlink" title="5.5、启动其他 control plane"></a>5.5、启动其他 control plane</h3><p>为了保证 HA 架构，还需要在另外两台 master 上启动 control plane；<strong>在启动之前请确保另外两台 master 节点节点上 <code>/etc/kubernetes/audit-policy.yaml</code> 审计配置已经分发完成，确保 <code>127.0.0.1:6443</code> 上监听的 4 层 LB 工作正常(可尝试使用 <code>curl -k https://127.0.0.1:6443</code> 测试)；</strong>根据第一个 control plane 终端输出，其他 control plane 加入命令如下</p><div class="hljs code-wrapper"><pre><code class="hljs sh">kubeadm join 127.0.0.1:6443 --token r4t3l3.14mmuivm7xbtaeoj \    --discovery-token-ca-cert-hash sha256:06f49f1f29d08b797fbf04d87b9b0fd6095a4693e9b1d59c429745cfa082b31d \    --control-plane --certificate-key 7373f829c733b46fb78f0069f90185e0f00254381641d8d5a7c5984b2cf17cd3</code></pre></div><p><strong>由于在使用 <code>kubeadm join</code> 时相关选项(<code>--discovery-token-ca-cert-hash</code>、<code>--control-plane</code>)无法与 <code>--config</code> 一起使用，这也就意味着我们必须增加一些附加指令来提供 <code>kubeadm.yaml</code> 配置文件中的一些属性</strong>；最终完整的 control plane 加入命令如下，在其他 master 直接执行既可(<strong><code>--apiserver-advertise-address</code> 的 IP 地址是目标 master 的 IP</strong>)</p><div class="hljs code-wrapper"><pre><code class="hljs sh">kubeadm join 127.0.0.1:6443 --token r4t3l3.14mmuivm7xbtaeoj \    --discovery-token-ca-cert-hash sha256:06f49f1f29d08b797fbf04d87b9b0fd6095a4693e9b1d59c429745cfa082b31d \    --control-plane --certificate-key 7373f829c733b46fb78f0069f90185e0f00254381641d8d5a7c5984b2cf17cd3 \    --apiserver-advertise-address 172.16.10.22 \    --apiserver-bind-port 5443 \    --ignore-preflight-errors=Swap</code></pre></div><p><strong>所有 control plane 启动完成后应当通过在每个节点上运行 <code>kubectl get cs</code> 验证各个组件运行状态</strong></p><div class="hljs code-wrapper"><pre><code class="hljs sh">k2.node ➜ kubectl get csNAME                 STATUS    MESSAGE             ERRORscheduler            Healthy   okcontroller-manager   Healthy   oketcd-1               Healthy   &#123;<span class="hljs-string">&quot;health&quot;</span>:<span class="hljs-string">&quot;true&quot;</span>&#125;etcd-0               Healthy   &#123;<span class="hljs-string">&quot;health&quot;</span>:<span class="hljs-string">&quot;true&quot;</span>&#125;etcd-2               Healthy   &#123;<span class="hljs-string">&quot;health&quot;</span>:<span class="hljs-string">&quot;true&quot;</span>&#125;k2.node ➜ kubectl get node -o wideNAME      STATUS   ROLES    AGE   VERSION   INTERNAL-IP    EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION      CONTAINER-RUNTIMEk1.node   Ready    master   28m   v1.17.0   172.16.10.21   &lt;none&gt;        Ubuntu 18.04.3 LTS   4.15.0-74-generic   docker://19.3.5k2.node   Ready    master   10m   v1.17.0   172.16.10.22   &lt;none&gt;        Ubuntu 18.04.3 LTS   4.15.0-74-generic   docker://19.3.5k3.node   Ready    master   3m    v1.17.0   172.16.10.23   &lt;none&gt;        Ubuntu 18.04.3 LTS   4.15.0-74-generic   docker://19.3.5</code></pre></div><h3 id="5-6、启动-Node"><a href="#5-6、启动-Node" class="headerlink" title="5.6、启动 Node"></a>5.6、启动 Node</h3><p>node 节点的启动相较于 master 来说要简单得多，只需要增加一个防止 <code>swap</code> 开启拒绝启动的参数既可</p><div class="hljs code-wrapper"><pre><code class="hljs sh">kubeadm join 127.0.0.1:6443 --token r4t3l3.14mmuivm7xbtaeoj \    --discovery-token-ca-cert-hash sha256:06f49f1f29d08b797fbf04d87b9b0fd6095a4693e9b1d59c429745cfa082b31d \    --ignore-preflight-errors=Swap</code></pre></div><p>启动成功后在 master 上可以看到所有 node 信息</p><div class="hljs code-wrapper"><pre><code class="hljs sh">k1.node ➜ kubectl get node -o wideNAME      STATUS   ROLES    AGE     VERSION   INTERNAL-IP    EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION      CONTAINER-RUNTIMEk1.node   Ready    master   32m     v1.17.0   172.16.10.21   &lt;none&gt;        Ubuntu 18.04.3 LTS   4.15.0-74-generic   docker://19.3.5k2.node   Ready    master   14m     v1.17.0   172.16.10.22   &lt;none&gt;        Ubuntu 18.04.3 LTS   4.15.0-74-generic   docker://19.3.5k3.node   Ready    master   6m35s   v1.17.0   172.16.10.23   &lt;none&gt;        Ubuntu 18.04.3 LTS   4.15.0-74-generic   docker://19.3.5k4.node   Ready    &lt;none&gt;   72s     v1.17.0   172.16.10.24   &lt;none&gt;        Ubuntu 18.04.3 LTS   4.15.0-74-generic   docker://19.3.5k5.node   Ready    &lt;none&gt;   66s     v1.17.0   172.16.10.25   &lt;none&gt;        Ubuntu 18.04.3 LTS   4.15.0-74-generic   docker://19.3.5</code></pre></div><h3 id="5-7、调整及测试"><a href="#5-7、调整及测试" class="headerlink" title="5.7、调整及测试"></a>5.7、调整及测试</h3><p>集群搭建好以后，如果想让 master 节点也参与调度任务，需要在任意一台 master 节点执行以下命令</p><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment"># node 节点报错属于正常情况</span>k1.node ➜ kubectl taint nodes --all node-role.kubernetes.io/master-node/k1.node untaintednode/k2.node untaintednode/k3.node untaintedtaint <span class="hljs-string">&quot;node-role.kubernetes.io/master&quot;</span> not foundtaint <span class="hljs-string">&quot;node-role.kubernetes.io/master&quot;</span> not found</code></pre></div><p>最后创建一个 deployment 和一个 service，并在不同主机上 ping pod IP 测试网络联通性，在 pod 内直接 curl service 名称测试 dns 解析既可</p><p><strong>test-nginx.deploy.yaml</strong></p><div class="hljs code-wrapper"><pre><code class="hljs yaml"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">apps/v1</span><span class="hljs-attr">kind:</span> <span class="hljs-string">Deployment</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">test-nginx</span>  <span class="hljs-attr">labels:</span>    <span class="hljs-attr">app:</span> <span class="hljs-string">test-nginx</span><span class="hljs-attr">spec:</span>  <span class="hljs-attr">replicas:</span> <span class="hljs-number">3</span>  <span class="hljs-attr">selector:</span>    <span class="hljs-attr">matchLabels:</span>      <span class="hljs-attr">app:</span> <span class="hljs-string">test-nginx</span>  <span class="hljs-attr">template:</span>    <span class="hljs-attr">metadata:</span>      <span class="hljs-attr">labels:</span>        <span class="hljs-attr">app:</span> <span class="hljs-string">test-nginx</span>    <span class="hljs-attr">spec:</span>      <span class="hljs-attr">containers:</span>      <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">test-nginx</span>        <span class="hljs-attr">image:</span> <span class="hljs-string">nginx:1.17.6-alpine</span>        <span class="hljs-attr">ports:</span>        <span class="hljs-bullet">-</span> <span class="hljs-attr">containerPort:</span> <span class="hljs-number">80</span></code></pre></div><p><strong>test-nginx.svc.yaml</strong></p><div class="hljs code-wrapper"><pre><code class="hljs yaml"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span><span class="hljs-attr">kind:</span> <span class="hljs-string">Service</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">test-nginx</span><span class="hljs-attr">spec:</span>  <span class="hljs-attr">selector:</span>    <span class="hljs-attr">app:</span> <span class="hljs-string">test-nginx</span>  <span class="hljs-attr">ports:</span>    <span class="hljs-bullet">-</span> <span class="hljs-attr">protocol:</span> <span class="hljs-string">TCP</span>      <span class="hljs-attr">port:</span> <span class="hljs-number">80</span>      <span class="hljs-attr">targetPort:</span> <span class="hljs-number">80</span></code></pre></div><h2 id="六、后续处理"><a href="#六、后续处理" class="headerlink" title="六、后续处理"></a>六、后续处理</h2><blockquote><p>说实话使用 kubeadm 后，我更关注的是集群后续的扩展性调整是否能达到目标；搭建其实很简单，大部份时间都在测试后续调整上</p></blockquote><h3 id="6-1、Etcd-迁移"><a href="#6-1、Etcd-迁移" class="headerlink" title="6.1、Etcd 迁移"></a>6.1、Etcd 迁移</h3><p>由于我们采用的是外部的 Etcd，所以迁移起来比较简单怎么折腾都行；需要注意的是换 IP 的时候注意保证老的 3 个节点至少有一个可用，否则可能导致集群崩溃；调整完成后记得分发相关 Etcd 节点的证书，重启时顺序一个一个重启，不要并行操作</p><h3 id="6-2、Master-配置修改"><a href="#6-2、Master-配置修改" class="headerlink" title="6.2、Master 配置修改"></a>6.2、Master 配置修改</h3><p>如果需要修改 conrol plane 上 apiserver、scheduler 等配置，直接修改 <code>kubeadm.yaml</code> 配置文件(<strong>所以集群搭建好后务必保存好</strong>)，然后执行 <code>kubeadm upgrade apply --config kubeadm.yaml</code> 升级集群既可，升级前一定作好相关备份工作；我只在测试环境测试这个命令工作还可以，生产环境还是需要谨慎</p><h3 id="6-3、证书续期"><a href="#6-3、证书续期" class="headerlink" title="6.3、证书续期"></a>6.3、证书续期</h3><p>目前根据我测试的结果，controller manager 的 <strong>experimental-cluster-signing-duration</strong> 参数在 init 的签发证书阶段似乎并未生效；<strong>目前根据文档描述 <code>kubelet</code> client 的证书会自动滚动，其他证书默认 1 年有效期，需要自己使用命令续签；</strong>续签命令如下</p><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment"># 查看证书过期时间</span>k1.node ➜ kubeadm alpha certs check-expiration[check-expiration] Reading configuration from the cluster...[check-expiration] FYI: You can look at this config file with <span class="hljs-string">&#x27;kubectl -n kube-system get cm kubeadm-config -oyaml&#x27;</span>CERTIFICATE                EXPIRES                  RESIDUAL TIME   CERTIFICATE AUTHORITY   EXTERNALLY MANAGEDadmin.conf                 Jan 11, 2021 10:06 UTC   364d                                    noapiserver                  Jan 11, 2021 10:06 UTC   364d            ca                      noapiserver-kubelet-client   Jan 11, 2021 10:06 UTC   364d            ca                      nocontroller-manager.conf    Jan 11, 2021 10:06 UTC   364d                                    nofront-proxy-client         Jan 11, 2021 10:06 UTC   364d            front-proxy-ca          noscheduler.conf             Jan 11, 2021 10:06 UTC   364d                                    noCERTIFICATE AUTHORITY   EXPIRES                  RESIDUAL TIME   EXTERNALLY MANAGEDca                      Jan 09, 2030 10:06 UTC   9y              nofront-proxy-ca          Jan 09, 2030 10:06 UTC   9y              no<span class="hljs-comment"># 续签证书</span>k1.node ➜ kubeadm alpha certs renew all[renew] Reading configuration from the cluster...[renew] FYI: You can look at this config file with <span class="hljs-string">&#x27;kubectl -n kube-system get cm kubeadm-config -oyaml&#x27;</span>certificate embedded <span class="hljs-keyword">in</span> the kubeconfig file <span class="hljs-keyword">for</span> the admin to use and <span class="hljs-keyword">for</span> kubeadm itself renewedcertificate <span class="hljs-keyword">for</span> serving the Kubernetes API renewedcertificate <span class="hljs-keyword">for</span> the API server to connect to kubelet renewedcertificate embedded <span class="hljs-keyword">in</span> the kubeconfig file <span class="hljs-keyword">for</span> the controller manager to use renewedcertificate <span class="hljs-keyword">for</span> the front proxy client renewedcertificate embedded <span class="hljs-keyword">in</span> the kubeconfig file <span class="hljs-keyword">for</span> the scheduler manager to use renewed</code></pre></div><h3 id="6-4、Node-重加入"><a href="#6-4、Node-重加入" class="headerlink" title="6.4、Node 重加入"></a>6.4、Node 重加入</h3><p>默认的 bootstrap token 会在 24h 后失效，所以后续增加新节点需要重新创建 token，重新创建 token 可以通过以下命令完成</p><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment"># 列出 token</span>k1.node ➜ kubeadm token listTOKEN                     TTL         EXPIRES                     USAGES                   DESCRIPTION                                                EXTRA GROUPSr4t3l3.14mmuivm7xbtaeoj   22h         2020-01-13T18:06:54+08:00   authentication,signing   &lt;none&gt;                                                     system:bootstrappers:kubeadm:default-node-tokenzady4i.57f9i2o6zl9vf9hy   45m         2020-01-12T20:06:53+08:00   &lt;none&gt;                   Proxy <span class="hljs-keyword">for</span> managing TTL <span class="hljs-keyword">for</span> the kubeadm-certs secret        &lt;none&gt;<span class="hljs-comment"># 创建新 token</span>k1.node ➜ kubeadm token create --print-join-commandW0112 19:21:15.174765   26626 validation.go:28] Cannot validate kube-proxy config - no validator is availableW0112 19:21:15.174836   26626 validation.go:28] Cannot validate kubelet config - no validator is availablekubeadm join 127.0.0.1:6443 --token 2dz4dc.mobzgjbvu0bkxz7j     --discovery-token-ca-cert-hash sha256:06f49f1f29d08b797fbf04d87b9b0fd6095a4693e9b1d59c429745cfa082b31d</code></pre></div><p>如果忘记了 certificate-key 可以通过一下命令重新 upload 并查看</p><div class="hljs code-wrapper"><pre><code class="hljs sh">k1.node ➜ kubeadm init --config kubeadm.yaml phase upload-certs --upload-certsW0112 19:23:06.466711   28637 validation.go:28] Cannot validate kubelet config - no validator is availableW0112 19:23:06.466778   28637 validation.go:28] Cannot validate kube-proxy config - no validator is available[upload-certs] Storing the certificates <span class="hljs-keyword">in</span> Secret <span class="hljs-string">&quot;kubeadm-certs&quot;</span> <span class="hljs-keyword">in</span> the <span class="hljs-string">&quot;kube-system&quot;</span> Namespace[upload-certs] Using certificate key:7373f829c733b46fb78f0069f90185e0f00254381641d8d5a7c5984b2cf17cd3</code></pre></div><h3 id="6-5、调整-kubelet"><a href="#6-5、调整-kubelet" class="headerlink" title="6.5、调整 kubelet"></a>6.5、调整 kubelet</h3><p>node 节点一旦启动完成后，kubelet 配置便不可再修改；如果想要修改 kubelet 配置，可以通过调整 <code>/etc/systemd/system/kubelet.service.d/10-kubeadm.conf</code> 配置文件完成</p><h2 id="七、其他"><a href="#七、其他" class="headerlink" title="七、其他"></a>七、其他</h2><p>本文参考了许多官方文档，以下是一些个人认为比较有价值并且在使用 kubeadm 后应该阅读的文档</p><ul><li><a href="https://kubernetes.io/docs/reference/setup-tools/kubeadm/implementation-details">Implementation details</a></li><li><a href="https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/kubelet-integration/">Configuring each kubelet in your cluster using kubeadm</a></li><li><a href="https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/control-plane-flags/">Customizing control plane configuration with kubeadm</a></li><li><a href="https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/high-availability/">Creating Highly Available clusters with kubeadm</a></li><li><a href="https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-certs/">Certificate Management with kubeadm</a></li><li><a href="https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade/">Upgrading kubeadm clusters</a></li><li><a href="https://kubernetes.io/docs/tasks/administer-cluster/reconfigure-kubelet/">Reconfigure a Node’s Kubelet in a Live Cluster</a></li></ul>]]></content>
    
    
    <summary type="html">距离上一次折腾 kubeadm 大约已经一两年了(记不太清了)，在很久一段时间内一直采用二进制部署的方式来部署 kubernetes 集群，随着 kubeadm 的不断稳定，目前终于可以重新试试这个不错的工具了</summary>
    
    
    
    <category term="Kubernetes" scheme="https://mritd.com/categories/kubernetes/"/>
    
    
    <category term="Kubernetes" scheme="https://mritd.com/tags/kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>云服务器下 Ubuntu 18 正确的 DNS 修改</title>
    <link href="https://mritd.com/2020/01/21/how-to-modify-dns-on-ubuntu18-server/"/>
    <id>https://mritd.com/2020/01/21/how-to-modify-dns-on-ubuntu18-server/</id>
    <published>2020-01-21T04:35:46.000Z</published>
    <updated>2020-01-21T04:35:46.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="一、起因"><a href="#一、起因" class="headerlink" title="一、起因"></a>一、起因</h2><p>Netflix DNS 分流实际上我目前的方案是通过 CoreDNS 作为主 DNS Server，然后在 CoreDNS 上针对 Netflix 全部域名解析 forward 到一台国外可以解锁 Netflix 机器上；如果直接将 CoreDNS 暴露在公网，那么无疑是在作死，为 DNS 反射 DDos 提供肉鸡；所以想到的方案是自己编写一个不可描述的工具，本地 Client 到 Server 端以后，Server 端再去设置到 CoreDNS 做分流；其中不可避免的需要调整 Server 端默认 DNS。</p><h2 id="二、已废弃修改方式"><a href="#二、已废弃修改方式" class="headerlink" title="二、已废弃修改方式"></a>二、已废弃修改方式</h2><p>目前大部份人还是习惯修改 <code>/etc/resolv.conf</code> 配置文件，这个配置文件上面已经明确标注了不要去修改它；<strong>因为自 Systemd 一统江山以后，系统 DNS 已经被 <code>systemd-resolved</code> 服务接管；一但修改了 <code>/etc/resolv.conf</code>，机器重启后就会被恢复；</strong>所以根源解决方案还是需要修改 <code>systemd-resolved</code> 的配置。</p><h2 id="三、netplan-的调整"><a href="#三、netplan-的调整" class="headerlink" title="三、netplan 的调整"></a>三、netplan 的调整</h2><p>在调整完 <code>systemd-resolved</code> 配置后其实有些地方仍然是不生效的；<strong>原因是 Ubuntu 18 开始网络已经被 netplan 接管，所以问题又回到了如何修改 netplan；</strong>由于云服务器初始化全部是由 cloud-init 完成的，netplan 配置里 IP 全部是由 DHCP 完成；那么直接修改 netplan 为 static IP 理论上可行，但是事实上还是不够优雅；后来研究了一下其实更优雅的方式是覆盖掉 DHCP 的某些配置，比如 DNS 配置；在阿里云上配置如下(<code>/etc/netplan/99-netcfg.yaml</code>)</p><div class="hljs code-wrapper"><pre><code class="hljs yaml"><span class="hljs-attr">network:</span>  <span class="hljs-attr">version:</span> <span class="hljs-number">2</span>  <span class="hljs-attr">renderer:</span> <span class="hljs-string">networkd</span>  <span class="hljs-attr">ethernets:</span>    <span class="hljs-attr">eth0:</span>      <span class="hljs-attr">dhcp4:</span> <span class="hljs-literal">yes</span>      <span class="hljs-attr">dhcp4-overrides:</span>        <span class="hljs-attr">use-dns:</span> <span class="hljs-literal">no</span>      <span class="hljs-attr">dhcp6:</span> <span class="hljs-literal">no</span>      <span class="hljs-attr">nameservers:</span>        <span class="hljs-attr">search:</span> [<span class="hljs-string">local</span>,<span class="hljs-string">node</span>]        <span class="hljs-comment"># 我自己的 CoreDNS 服务器</span>        <span class="hljs-attr">addresses:</span> [<span class="hljs-number">172.17</span><span class="hljs-number">.3</span><span class="hljs-number">.17</span>]</code></pre></div><p>修改完成后执行 <code>netplan try</code> 等待几秒钟，如果屏幕的读秒倒计时一直在动，说明修改没问题，接着回车既可(尽量不要 <code>netplan apply</code>，一旦修改错误你就再也连不上了…)</p><h2 id="四、DNS-分流"><a href="#四、DNS-分流" class="headerlink" title="四、DNS 分流"></a>四、DNS 分流</h2><p>顺便贴一下 CoreDNS 配置吧，可能有些人也需要；第一部分的域名是目前我整理的 Netflix 全部访问域名，针对这些域名的流量转发到自己其他可解锁 Netflix 的机器既可</p><div class="hljs code-wrapper"><pre><code class="hljs sh">netflix.com nflxext.com nflximg.net nflxso.net nflxvideo.net &#123;    <span class="hljs-built_in">bind</span> 172.17.3.17    cache 30 . &#123;        success 4096    &#125;    forward . 158.1.1.1 &#123;        max_fails 2        prefer_udp        expire 20s        policy random        health_check 0.2s    &#125;    errors    <span class="hljs-built_in">log</span> . <span class="hljs-string">&quot;&#123;remote&#125;:&#123;port&#125; - &#123;&gt;id&#125; \&quot;&#123;type&#125; &#123;class&#125; &#123;name&#125; &#123;proto&#125; &#123;size&#125; &#123;&gt;do&#125; &#123;&gt;bufsize&#125;\&quot; &#123;rcode&#125; &#123;&gt;rflags&#125; &#123;rsize&#125; &#123;duration&#125;&quot;</span>&#125;.:53 &#123;    <span class="hljs-built_in">bind</span> 172.17.3.17    cache 30 . &#123;        success 4096    &#125;    forward . 8.8.8.8 1.1.1.1 &#123;        except netflix.com nflxext.com nflximg.net nflxso.net nflxvideo.net        max_fails 2        expire 20s        policy random        health_check 0.2s    &#125;    errors    <span class="hljs-built_in">log</span> . <span class="hljs-string">&quot;&#123;remote&#125;:&#123;port&#125; - &#123;&gt;id&#125; \&quot;&#123;type&#125; &#123;class&#125; &#123;name&#125; &#123;proto&#125; &#123;size&#125; &#123;&gt;do&#125; &#123;&gt;bufsize&#125;\&quot; &#123;rcode&#125; &#123;&gt;rflags&#125; &#123;rsize&#125; &#123;duration&#125;&quot;</span></code></pre></div><h2 id="五、关于-docker"><a href="#五、关于-docker" class="headerlink" title="五、关于 docker"></a>五、关于 docker</h2><p>当 netplan 修改完成后，只需要重启 docker 既可保证 docker 内所有容器 DNS 请求全部发送到自己定义的 DNS 服务器上；<strong>请不要尝试将自己的 CoreDNS 监听到 <code>127.*</code> 或者 <code>::1</code> 上，这两个地址会导致 docker 中的 DNS 无效</strong>，因为在 <a href="https://github.com/docker/libnetwork/blob/fec6476dfa21380bf8ee4d74048515d968c1ee63/resolvconf/resolvconf.go#L148">libnetwork</a> 中针对这两个地址做了过滤，并且 <code>FilterResolvDNS</code> 方法在剔除这两种地址时不会给予任何警告日志</p>]]></content>
    
    
    <summary type="html">博客服务器换成了阿里云香港，个人还偶尔看美剧，所以做了一下 Netflix 分流；分流过程主要是做 DNS 解析 SNI 代理，调了半天记录一下</summary>
    
    
    
    <category term="Linux" scheme="https://mritd.com/categories/linux/"/>
    
    
    <category term="Linux" scheme="https://mritd.com/tags/linux/"/>
    
  </entry>
  
  <entry>
    <title>Percona MySQL 搭建</title>
    <link href="https://mritd.com/2020/01/20/set-up-percona-server/"/>
    <id>https://mritd.com/2020/01/20/set-up-percona-server/</id>
    <published>2020-01-20T11:45:46.000Z</published>
    <updated>2020-01-20T11:45:46.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="一、版本信息"><a href="#一、版本信息" class="headerlink" title="一、版本信息"></a>一、版本信息</h2><p>目前采用 MySQL fork 版本 Percona Server 5.7.28，监控方面选择 Percona Monitoring and Management 2.1.0，对应监控 Client 版本为 2.1.0</p><h2 id="二、Percona-Server-安装"><a href="#二、Percona-Server-安装" class="headerlink" title="二、Percona Server 安装"></a>二、Percona Server 安装</h2><p>为保证兼容以及稳定性，MySQL 宿主机系统选择 CentOS 7，Percona Server 安装方式为 rpm 包，安装后由 Systemd 守护</p><h3 id="2-1、下载安装包"><a href="#2-1、下载安装包" class="headerlink" title="2.1、下载安装包"></a>2.1、下载安装包</h3><p>安装包下载地址为 <a href="https://www.percona.com/downloads/Percona-Server-5.7/LATEST/">https://www.percona.com/downloads/Percona-Server-5.7/LATEST/</a>，下载时选择 <code>Download All Packages Together</code>，下载后是所有组件全量的压缩 tar 包。</p><h3 id="2-2、安装前准备"><a href="#2-2、安装前准备" class="headerlink" title="2.2、安装前准备"></a>2.2、安装前准备</h3><p>针对 CentOS 7 系统，安装前升级所有系统组件库，执行 <code>yum update</code> 既可；大部份 <strong>CentOS 7 安装后可能会附带 <code>mariadb-libs</code> 包，这个包会默认创建一些配置文件，导致后面的 Percona Server 无法覆盖它(例如 <code>/etc/my.cnf</code>)，所以安装 Percona Server 之前需要卸载它 <code>yum remove mariadb-libs</code></strong></p><p>针对于数据存储硬盘，目前统一为 SSD 硬盘，挂载点为 <code>/data</code>，挂载方式可以采用 <code>fstab</code>、<code>systemd-mount</code>，分区格式目前采用 <code>xfs</code> 格式。</p><p><strong>SSD 优化有待补充…</strong></p><h3 id="2-3、安装-Percona-Server"><a href="#2-3、安装-Percona-Server" class="headerlink" title="2.3、安装 Percona Server"></a>2.3、安装 Percona Server</h3><p>Percona Server tar 包解压后会有 9 个 rpm 包，实际安装时只需要安装其中 4 个既可</p><div class="hljs code-wrapper"><pre><code class="hljs sh">yum install Percona-Server-client-57-5.7.28-31.1.el7.x86_64.rpm Percona-Server-server-57-5.7.28-31.1.el7.x86_64.rpm Percona-Server-shared-57-5.7.28-31.1.el7.x86_64.rpm Percona-Server-shared-compat-57-5.7.28-31.1.el7.x86_64.rpm</code></pre></div><h3 id="2-4、安装后调整"><a href="#2-4、安装后调整" class="headerlink" title="2.4、安装后调整"></a>2.4、安装后调整</h3><h4 id="2-4-1、硬盘调整"><a href="#2-4-1、硬盘调整" class="headerlink" title="2.4.1、硬盘调整"></a>2.4.1、硬盘调整</h4><p>目前 MySQL 数据会统一存放到 <code>/data</code> 目录下，所以需要将单独的数据盘挂载到 <code>/data</code> 目录；<strong>如果是 SSD 硬盘还需要调整系统 I/O 调度器等其他优化。</strong></p><h4 id="2-4-2、目录预创建"><a href="#2-4-2、目录预创建" class="headerlink" title="2.4.2、目录预创建"></a>2.4.2、目录预创建</h4><p>Percona Server 安装完成后，由于配置调整原因，还会用到一些其他的数据目录，这些目录可以预先创建并授权</p><div class="hljs code-wrapper"><pre><code class="hljs sh">mkdir -p /var/<span class="hljs-built_in">log</span>/mysql /data/mysql_tmpchown -R mysql:mysql /var/<span class="hljs-built_in">log</span>/mysql /data/mysql_tmp</code></pre></div><p><code>/var/log/mysql</code> 目录用来存放 MySQL 相关的日志(不包括 binlog)，<code>/data/mysql_tmp</code> 用来存放 MySQL 运行时产生的缓存文件。</p><h4 id="2-4-3、文件描述符调整"><a href="#2-4-3、文件描述符调整" class="headerlink" title="2.4.3、文件描述符调整"></a>2.4.3、文件描述符调整</h4><p>由于 rpm 安装的 Percona Server 会采用 Systemd 守护，所以如果想修改文件描述符配置应当调整 Systemd 配置文件</p><div class="hljs code-wrapper"><pre><code class="hljs sh">vim /usr/lib/systemd/system/mysqld.service<span class="hljs-comment"># Sets open_files_limit</span><span class="hljs-comment"># 注意 infinity = 65536</span>LimitCORE = infinityLimitNOFILE = infinityLimitNPROC = infinity</code></pre></div><p>然后执行 <code>systemctl daemon-reload</code> 重载既可。</p><h4 id="2-4-4、配置文件调整"><a href="#2-4-4、配置文件调整" class="headerlink" title="2.4.4、配置文件调整"></a>2.4.4、配置文件调整</h4><p>Percona Server 安装完成后也会生成 <code>/etc/my.cnf</code> 配置文件，不过不建议直接修改该文件；修改配置文件需要进入到 <code>/etc/percona-server.conf.d/</code> 目录调整相应配置；以下为配置样例(<strong>生产环境 mysqld 配置需要优化调整</strong>)</p><p><strong>mysql.cnf</strong></p><div class="hljs code-wrapper"><pre><code class="hljs ini"><span class="hljs-section">[mysql]</span>auto-rehash<span class="hljs-attr">default_character_set</span>=utf8mb4</code></pre></div><p><strong>mysqld.cnf</strong></p><div class="hljs code-wrapper"><pre><code class="hljs ini"><span class="hljs-comment"># Percona Server template configuration</span><span class="hljs-section">[mysqld]</span><span class="hljs-comment">#</span><span class="hljs-comment"># Remove leading # and set to the amount of RAM for the most important data</span><span class="hljs-comment"># cache in MySQL. Start at 70% of total RAM for dedicated server, else 10%.</span><span class="hljs-comment"># innodb_buffer_pool_size = 128M</span><span class="hljs-comment">#</span><span class="hljs-comment"># Remove leading # to turn on a very important data integrity option: logging</span><span class="hljs-comment"># changes to the binary log between backups.</span><span class="hljs-comment"># log_bin</span><span class="hljs-comment">#</span><span class="hljs-comment"># Remove leading # to set options mainly useful for reporting servers.</span><span class="hljs-comment"># The server defaults are faster for transactions and fast SELECTs.</span><span class="hljs-comment"># Adjust sizes as needed, experiment to find the optimal values.</span><span class="hljs-comment"># join_buffer_size = 128M</span><span class="hljs-comment"># sort_buffer_size = 2M</span><span class="hljs-comment"># read_rnd_buffer_size = 2M</span><span class="hljs-attr">port</span>=<span class="hljs-number">3306</span><span class="hljs-attr">datadir</span>=/data/mysql<span class="hljs-attr">socket</span>=/data/mysql/mysql.sock<span class="hljs-attr">pid_file</span>=/data/mysql/mysqld.pid<span class="hljs-comment"># 服务端编码</span><span class="hljs-attr">character_set_server</span>=utf8mb4<span class="hljs-comment"># 服务端排序</span><span class="hljs-attr">collation_server</span>=utf8mb4_general_ci<span class="hljs-comment"># 强制使用 utf8mb4 编码集，忽略客户端设置</span><span class="hljs-attr">skip_character_set_client_handshake</span>=<span class="hljs-number">1</span><span class="hljs-comment"># 日志输出到文件</span><span class="hljs-attr">log_output</span>=FILE<span class="hljs-comment"># 开启常规日志输出</span><span class="hljs-attr">general_log</span>=<span class="hljs-number">1</span><span class="hljs-comment"># 常规日志输出文件位置</span><span class="hljs-attr">general_log_file</span>=/var/log/mysql/mysqld.log<span class="hljs-comment"># 错误日志位置</span><span class="hljs-attr">log_error</span>=/var/log/mysql/mysqld-error.log<span class="hljs-comment"># 记录慢查询</span><span class="hljs-attr">slow_query_log</span>=<span class="hljs-number">1</span><span class="hljs-comment"># 慢查询时间(大于 1s 被视为慢查询)</span><span class="hljs-attr">long_query_time</span>=<span class="hljs-number">1</span><span class="hljs-comment"># 慢查询日志文件位置</span><span class="hljs-attr">slow_query_log_file</span>=/var/log/mysql/mysqld-slow.log<span class="hljs-comment"># 临时文件位置</span><span class="hljs-attr">tmpdir</span>=/data/mysql_tmp<span class="hljs-comment"># 线程池缓存(refs https://my.oschina.net/realfighter/blog/363853)</span><span class="hljs-attr">thread_cache_size</span>=<span class="hljs-number">30</span><span class="hljs-comment"># The number of open tables for all threads.(refs https://dev.mysql.com/doc/refman/5.7/en/server-system-variables.html#sysvar_table_open_cache)</span><span class="hljs-attr">table_open_cache</span>=<span class="hljs-number">16384</span><span class="hljs-comment"># 文件描述符(此处修改不生效，请修改 systemd service 配置) </span><span class="hljs-comment"># refs https://www.percona.com/blog/2017/10/12/open_files_limit-mystery/</span><span class="hljs-comment"># refs https://www.cnblogs.com/wxxjianchi/p/10370419.html</span><span class="hljs-comment">#open_files_limit=65535</span><span class="hljs-comment"># 表定义缓存(5.7 以后自动调整)</span><span class="hljs-comment"># refs https://dev.mysql.com/doc/refman/5.6/en/server-system-variables.html#sysvar_table_definition_cache</span><span class="hljs-comment"># refs http://mysql.taobao.org/monthly/2015/08/10/</span><span class="hljs-comment">#table_definition_cache=16384</span><span class="hljs-attr">sort_buffer_size</span>=<span class="hljs-number">1</span>M<span class="hljs-attr">join_buffer_size</span>=<span class="hljs-number">1</span>M<span class="hljs-comment"># MyiSAM 引擎专用(内部临时磁盘表可能会用)</span><span class="hljs-attr">read_buffer_size</span>=<span class="hljs-number">1</span>M<span class="hljs-attr">read_rnd_buffer_size</span>=<span class="hljs-number">1</span>M<span class="hljs-comment"># MyiSAM 引擎专用(内部临时磁盘表可能会用)</span><span class="hljs-attr">key_buffer_size</span>=<span class="hljs-number">32</span>M<span class="hljs-comment"># MyiSAM 引擎专用(内部临时磁盘表可能会用)</span><span class="hljs-attr">bulk_insert_buffer_size</span>=<span class="hljs-number">16</span>M<span class="hljs-comment"># myisam_sort_buffer_size 与 sort_buffer_size 区别请参考(https://stackoverflow.com/questions/7871027/myisam-sort-buffer-size-vs-sort-buffer-size)</span><span class="hljs-attr">myisam_sort_buffer_size</span>=<span class="hljs-number">64</span>M<span class="hljs-comment"># 内部内存临时表大小</span><span class="hljs-attr">tmp_table_size</span>=<span class="hljs-number">32</span>M<span class="hljs-comment"># 用户创建的 MEMORY 表最大大小(tmp_table_size 受此值影响)</span><span class="hljs-attr">max_heap_table_size</span>=<span class="hljs-number">32</span>M<span class="hljs-comment"># 开启查询缓存</span><span class="hljs-attr">query_cache_type</span>=<span class="hljs-number">1</span><span class="hljs-comment"># 查询缓存大小</span><span class="hljs-attr">query_cache_size</span>=<span class="hljs-number">32</span>M<span class="hljs-comment"># sql mode</span><span class="hljs-attr">sql_mode</span>=<span class="hljs-string">&#x27;STRICT_TRANS_TABLES,NO_ZERO_IN_DATE,NO_ZERO_DATE,ERROR_FOR_DIVISION_BY_ZERO,NO_AUTO_CREATE_USER,NO_ENGINE_SUBSTITUTION&#x27;</span><span class="hljs-comment">########### Network ###########</span><span class="hljs-comment"># 最大连接数(该参数受到最大文件描述符影响，如果不生效请检查最大文件描述符设置)</span><span class="hljs-comment"># refs https://stackoverflow.com/questions/39976756/the-max-connections-in-mysql-5-7</span><span class="hljs-attr">max_connections</span>=<span class="hljs-number">1500</span><span class="hljs-comment"># mysql 堆栈内暂存的链接数量</span><span class="hljs-comment"># 当短时间内链接数量超过 max_connections 时，部分链接会存储在堆栈内，存储数量受此参数控制</span><span class="hljs-attr">back_log</span>=<span class="hljs-number">256</span><span class="hljs-comment"># 最大链接错误，针对于 client 主机，超过此数量的链接错误将会导致 mysql server 针对此主机执行锁定(禁止链接 ERROR 1129 )</span><span class="hljs-comment"># 此错误计数仅在 mysql 链接握手失败才会计算，一般出现问题时都是网络故障</span><span class="hljs-comment"># refs https://www.cnblogs.com/kerrycode/p/8405862.html</span><span class="hljs-attr">max_connect_errors</span>=<span class="hljs-number">100000</span><span class="hljs-comment"># mysql server 允许的最大数据包大小</span><span class="hljs-attr">max_allowed_packet</span>=<span class="hljs-number">64</span>M<span class="hljs-comment"># 交互式客户端链接超时(30分钟自动断开)</span><span class="hljs-attr">interactive_timeout</span>=<span class="hljs-number">1800</span><span class="hljs-comment"># 非交互式链接超时时间(10分钟)</span><span class="hljs-comment"># 如果客户端有连接池，则需要协商此参数(refs https://database.51cto.com/art/201909/603519.htm)</span><span class="hljs-attr">wait_timeout</span>=<span class="hljs-number">600</span><span class="hljs-comment"># 跳过外部文件系统锁定</span><span class="hljs-comment"># If you run multiple servers that use the same database directory (not recommended), </span><span class="hljs-comment"># each server must have external locking enabled.</span><span class="hljs-comment"># refs https://dev.mysql.com/doc/refman/5.7/en/external-locking.html</span><span class="hljs-attr">skip_external_locking</span>=<span class="hljs-number">1</span><span class="hljs-comment"># 跳过链接的域名解析(开启此选项后 mysql 用户授权的 host 方式失效)</span><span class="hljs-attr">skip_name_resolve</span>=<span class="hljs-number">0</span><span class="hljs-comment"># 禁用主机名缓存，每次都会走 DNS</span><span class="hljs-attr">host_cache_size</span>=<span class="hljs-number">0</span><span class="hljs-comment">########### REPL ###########</span><span class="hljs-comment"># 开启 binlog</span><span class="hljs-attr">log_bin</span>=mysql-bin<span class="hljs-comment"># 作为从库时，同步信息依然写入 binlog，方便此从库再作为其他从库的主库</span><span class="hljs-attr">log_slave_updates</span>=<span class="hljs-number">1</span><span class="hljs-comment"># server id，默认为 ipv4 地址去除第一段</span><span class="hljs-comment"># eg: 172.16.10.11 =&gt; 161011</span><span class="hljs-attr">server_id</span>=<span class="hljs-number">161011</span><span class="hljs-comment"># 每次次事务 binlog 刷新到磁盘</span><span class="hljs-comment"># refs http://liyangliang.me/posts/2014/03/innodb_flush_log_at_trx_commit-and-sync_binlog/</span><span class="hljs-attr">sync_binlog</span>=<span class="hljs-number">100</span><span class="hljs-comment"># binlog 格式(refs https://zhuanlan.zhihu.com/p/33504555)</span><span class="hljs-attr">binlog_format</span>=row<span class="hljs-comment"># binlog 自动清理时间</span><span class="hljs-attr">expire_logs_days</span>=<span class="hljs-number">10</span><span class="hljs-comment"># 开启 relay-log，一般作为 slave 时开启</span><span class="hljs-attr">relay_log</span>=mysql-replay<span class="hljs-comment"># 主从复制时跳过 test 库</span><span class="hljs-attr">replicate_ignore_db</span>=test<span class="hljs-comment"># 每个 session binlog 缓存</span><span class="hljs-attr">binlog_cache_size</span>=<span class="hljs-number">4</span>M<span class="hljs-comment"># binlog 滚动大小</span><span class="hljs-attr">max_binlog_size</span>=<span class="hljs-number">1024</span>M<span class="hljs-comment"># GTID 相关(refs https://keithlan.github.io/2016/06/23/gtid/)</span><span class="hljs-comment">#gtid_mode=1</span><span class="hljs-comment">#enforce_gtid_consistency=1</span><span class="hljs-comment">########### InnoDB ###########</span><span class="hljs-comment"># 永久表默认存储引擎</span><span class="hljs-attr">default_storage_engine</span>=InnoDB<span class="hljs-comment"># 系统表空间数据文件大小(初始化为 1G，并且自动增长)</span><span class="hljs-attr">innodb_data_file_path</span>=ibdata1:<span class="hljs-number">1</span>G:autoextend<span class="hljs-comment"># InnoDB 缓存池大小</span><span class="hljs-comment"># innodb_buffer_pool_size 必须等于 innodb_buffer_pool_chunk_size*innodb_buffer_pool_instances，或者是其整数倍</span><span class="hljs-comment"># refs https://dev.mysql.com/doc/refman/5.7/en/innodb-buffer-pool-resize.html</span><span class="hljs-comment"># refs https://zhuanlan.zhihu.com/p/60089484</span><span class="hljs-attr">innodb_buffer_pool_size</span>=<span class="hljs-number">7680</span>M<span class="hljs-attr">innodb_buffer_pool_instances</span>=<span class="hljs-number">10</span><span class="hljs-attr">innodb_buffer_pool_chunk_size</span>=<span class="hljs-number">128</span>M<span class="hljs-comment"># InnoDB 强制恢复(refs https://www.askmaclean.com/archives/mysql-innodb-innodb_force_recovery.html)</span><span class="hljs-attr">innodb_force_recovery</span>=<span class="hljs-number">0</span><span class="hljs-comment"># InnoDB buffer 预热(refs http://www.dbhelp.net/2017/01/12/mysql-innodb-buffer-pool-warmup.html)</span><span class="hljs-attr">innodb_buffer_pool_dump_at_shutdown</span>=<span class="hljs-number">1</span><span class="hljs-attr">innodb_buffer_pool_load_at_startup</span>=<span class="hljs-number">1</span><span class="hljs-comment"># InnoDB 日志组中的日志文件数</span><span class="hljs-attr">innodb_log_files_in_group</span>=<span class="hljs-number">2</span><span class="hljs-comment"># InnoDB redo 日志大小</span><span class="hljs-comment"># refs https://www.percona.com/blog/2017/10/18/chose-mysql-innodb_log_file_size/</span><span class="hljs-attr">innodb_log_file_size</span>=<span class="hljs-number">256</span>MB<span class="hljs-comment"># 缓存还未提交的事务的缓冲区大小</span><span class="hljs-attr">innodb_log_buffer_size</span>=<span class="hljs-number">16</span>M<span class="hljs-comment"># InnoDB 在事务提交后的日志写入频率</span><span class="hljs-comment"># refs http://liyangliang.me/posts/2014/03/innodb_flush_log_at_trx_commit-and-sync_binlog/</span><span class="hljs-attr">innodb_flush_log_at_trx_commit</span>=<span class="hljs-number">2</span><span class="hljs-comment"># InnoDB DML 操作行级锁等待时间</span><span class="hljs-comment"># 超时返回 ERROR 1205 (HY000): Lock wait timeout exceeded; try restarting transaction</span><span class="hljs-comment"># refs https://ningyu1.github.io/site/post/75-mysql-lock-wait-timeout-exceeded/</span><span class="hljs-attr">innodb_lock_wait_timeout</span>=<span class="hljs-number">30</span><span class="hljs-comment"># InnoDB 行级锁超时是否回滚整个事务，默认为 OFF 仅回滚上一条语句</span><span class="hljs-comment"># 此时应用程序可以接受到错误后选择是否继续提交事务(并没有违反 ACID 原子性)</span><span class="hljs-comment"># refs https://www.cnblogs.com/hustcat/archive/2012/11/18/2775487.html</span><span class="hljs-comment">#innodb_rollback_on_timeout=ON</span><span class="hljs-comment"># InnoDB 数据写入磁盘的方式，具体见博客文章</span><span class="hljs-comment"># refs https://www.cnblogs.com/gomysql/p/3595806.html</span><span class="hljs-attr">innodb_flush_method</span>=O_DIRECT<span class="hljs-comment"># InnoDB 缓冲池脏页刷新百分比</span><span class="hljs-comment"># refs https://dbarobin.com/2015/08/29/mysql-optimization-under-ssd</span><span class="hljs-attr">innodb_max_dirty_pages_pct</span>=<span class="hljs-number">50</span><span class="hljs-comment"># InnoDB 每秒执行的写IO量</span><span class="hljs-comment"># refs https://www.centos.bz/2016/11/mysql-performance-tuning-15-config-item/#10.INNODB_IO_CAPACITY,%20INNODB_IO_CAPACITY_MAX</span><span class="hljs-attr">innodb_io_capacity</span>=<span class="hljs-number">500</span><span class="hljs-attr">innodb_io_capacity_max</span>=<span class="hljs-number">1000</span><span class="hljs-comment"># 请求并发 InnoDB 线程数</span><span class="hljs-comment"># refs https://www.cnblogs.com/xinysu/p/6439715.html#_lab2_1_0</span><span class="hljs-attr">innodb_thread_concurrency</span>=<span class="hljs-number">60</span><span class="hljs-comment"># 再使用多个 InnoDB 表空间时，允许打开的最大 &quot;.ibd&quot; 文件个数，不设置默认 300，</span><span class="hljs-comment"># 并且取与 table_open_cache 相比较大的一个，此选项独立于 open_files_limit</span><span class="hljs-comment"># refs https://dev.mysql.com/doc/refman/5.7/en/innodb-parameters.html#sysvar_innodb_open_files</span><span class="hljs-attr">innodb_open_files</span>=<span class="hljs-number">65535</span><span class="hljs-comment"># 每个 InnoDB 表都存储在独立的表空间(.ibd)中</span><span class="hljs-attr">innodb_file_per_table</span>=<span class="hljs-number">1</span><span class="hljs-comment"># 事务级别(可重复读，会出幻读)</span><span class="hljs-attr">transaction_isolation</span>=REPEATABLE-READ<span class="hljs-comment"># 是否在搜索和索引扫描中使用间隙锁(gap locking)，不建议使用未来将删除</span><span class="hljs-attr">innodb_locks_unsafe_for_binlog</span>=<span class="hljs-number">0</span><span class="hljs-comment"># InnoDB 后台清理线程数，更大的值有助于 DML 执行性能，&gt;= 5.7.8 默认为 4</span><span class="hljs-attr">innodb_purge_threads</span>=<span class="hljs-number">4</span></code></pre></div><p><strong>mysqld_safe.cnf</strong></p><div class="hljs code-wrapper"><pre><code class="hljs ini"><span class="hljs-comment">#</span><span class="hljs-comment"># The Percona Server 5.7 configuration file.</span><span class="hljs-comment">#</span><span class="hljs-comment"># One can use all long options that the program supports.</span><span class="hljs-comment"># Run program with --help to get a list of available options and with</span><span class="hljs-comment"># --print-defaults to see which it would actually understand and use.</span><span class="hljs-comment">#</span><span class="hljs-comment"># For explanations see</span><span class="hljs-comment"># http://dev.mysql.com/doc/mysql/en/server-system-variables.html</span><span class="hljs-section">[mysqld_safe]</span><span class="hljs-attr">pid-file</span> = /var/run/mysqld/mysqld.pid<span class="hljs-attr">socket</span>   = /var/run/mysqld/mysqld.sock<span class="hljs-attr">nice</span>     = <span class="hljs-number">0</span></code></pre></div><p><strong>mysqldump.cnf</strong></p><div class="hljs code-wrapper"><pre><code class="hljs ini"><span class="hljs-section">[mysqldump]</span>quick<span class="hljs-attr">default-character-set</span>=utf8mb4<span class="hljs-attr">max_allowed_packet</span>=<span class="hljs-number">256</span>M</code></pre></div><h3 id="2-5、启动"><a href="#2-5、启动" class="headerlink" title="2.5、启动"></a>2.5、启动</h3><p>配置文件调整完成后启动既可</p><div class="hljs code-wrapper"><pre><code class="hljs sh">systemctl start mysqld</code></pre></div><p>启动完成后默认 root 密码会自动生成，通过 <code>grep &#39;temporary password&#39; /var/log/mysql/*</code> 查看默认密码；获得默认密码后可以通过 <code>mysqladmin -S /data/mysql/mysql.sock -u root -p password</code> 修改 root 密码。</p><h2 id="三、Percona-Monitoring-and-Management"><a href="#三、Percona-Monitoring-and-Management" class="headerlink" title="三、Percona Monitoring and Management"></a>三、Percona Monitoring and Management</h2><p>数据库创建成功后需要增加 pmm 监控，后续将会通过监控信息来调优数据库，所以数据库监控必不可少。</p><h3 id="3-1、安装前准备"><a href="#3-1、安装前准备" class="headerlink" title="3.1、安装前准备"></a>3.1、安装前准备</h3><p>pmm 监控需要使用特定用户来监控数据信息，所以需要预先为 pmm 创建用户</p><div class="hljs code-wrapper"><pre><code class="hljs sql">USE mysql;<span class="hljs-keyword">GRANT</span> <span class="hljs-keyword">ALL</span> PRIVILEGES <span class="hljs-keyword">ON</span> <span class="hljs-operator">*</span>.<span class="hljs-operator">*</span> <span class="hljs-keyword">TO</span> <span class="hljs-string">&#x27;pmm&#x27;</span>@<span class="hljs-string">&#x27;%&#x27;</span> IDENTIFIED <span class="hljs-keyword">BY</span> <span class="hljs-string">&#x27;pmm12345&#x27;</span> <span class="hljs-keyword">WITH</span> <span class="hljs-keyword">GRANT</span> OPTION;FLUSH PRIVILEGES;</code></pre></div><h3 id="3-2、安装-PMM-Server"><a href="#3-2、安装-PMM-Server" class="headerlink" title="3.2、安装 PMM Server"></a>3.2、安装 PMM Server</h3><p>pmm server 端推荐直接使用 docker 启动，以下为样例 docker compose</p><div class="hljs code-wrapper"><pre><code class="hljs yaml"><span class="hljs-attr">version:</span> <span class="hljs-string">&#x27;3.7&#x27;</span><span class="hljs-attr">services:</span>  <span class="hljs-attr">pmm:</span>    <span class="hljs-attr">image:</span> <span class="hljs-string">percona/pmm-server:2.1.0</span>    <span class="hljs-attr">container_name:</span> <span class="hljs-string">pmm</span>    <span class="hljs-attr">restart:</span> <span class="hljs-string">always</span>    <span class="hljs-attr">volumes:</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">data:/srv</span>    <span class="hljs-attr">ports:</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">&quot;80:80&quot;</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">&quot;443:443&quot;</span><span class="hljs-attr">volumes:</span>  <span class="hljs-attr">data:</span></code></pre></div><p><strong>如果想要自定义证书，请将证书复制到 volume 内的 nginx 目录下，自定义证书需要以下证书文件</strong></p><div class="hljs code-wrapper"><pre><code class="hljs sh">pmmserver.node ➜ tree.├── ca-certs.pem├── certificate.conf  <span class="hljs-comment"># 此文件是 pmm 默认生成自签证书的配置文件，不需要关注</span>├── certificate.crt├── certificate.key└── dhparam.pem</code></pre></div><p><strong>pmm server 启动后访问 <code>http(s)://IP_ADDRESS</code> 既可进入 granafa 面板，默认账户名和密码都是 <code>admin</code></strong></p><h3 id="3-3、安装-PMM-Client"><a href="#3-3、安装-PMM-Client" class="headerlink" title="3.3、安装 PMM Client"></a>3.3、安装 PMM Client</h3><p>PMM Client 同样采用 rpm 安装，下载地址 <a href="https://www.percona.com/downloads/pmm2/">https://www.percona.com/downloads/pmm2/</a>，当前采用最新的 2.1.0 版本；rpm 下载完成后直接 <code>yum install</code> 既可。</p><p>rpm 安装完成后使用 <code>pmm-admin</code> 命令配置服务端地址，并添加当前 mysql 实例监控</p><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment"># 配置服务端地址</span>pmm-admin config --server-url https://admin:admin@pmm.mysql.node 172.16.0.11 generic mysql<span class="hljs-comment"># 配置当前 mysql 实例</span>pmm-admin add mysql --username=pmm --password=pmm12345 mysql 172.16.0.11:3306</code></pre></div><p>完成后稍等片刻既可在 pmm server 端的 granafa 中看到相关数据。</p><h2 id="四、数据导入"><a href="#四、数据导入" class="headerlink" title="四、数据导入"></a>四、数据导入</h2><p>从原始数据库 dump 相关库，并导入到新数据库既可</p><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment"># dump</span>mysqldump -h 172.16.1.10 -u root -p --master-data=2 --routines --triggers --single_transaction --databases DATABASE_NAME &gt; dump.sql<span class="hljs-comment"># load</span>mysql -S /data/mysql/mysql.sock -u root -p &lt; dump.sql</code></pre></div><p>数据导入后重建业务用户既可</p><div class="hljs code-wrapper"><pre><code class="hljs sql">USE mysql;<span class="hljs-keyword">GRANT</span> <span class="hljs-keyword">ALL</span> PRIVILEGES <span class="hljs-keyword">ON</span> <span class="hljs-operator">*</span>.<span class="hljs-operator">*</span> <span class="hljs-keyword">TO</span> <span class="hljs-string">&#x27;test_user&#x27;</span>@<span class="hljs-string">&#x27;%&#x27;</span> IDENTIFIED <span class="hljs-keyword">BY</span> <span class="hljs-string">&#x27;test_user&#x27;</span> <span class="hljs-keyword">WITH</span> <span class="hljs-keyword">GRANT</span> OPTION;FLUSH PRIVILEGES;</code></pre></div><h2 id="五、数据备份"><a href="#五、数据备份" class="headerlink" title="五、数据备份"></a>五、数据备份</h2><h3 id="5-1、安装-xtrabackup"><a href="#5-1、安装-xtrabackup" class="headerlink" title="5.1、安装 xtrabackup"></a>5.1、安装 xtrabackup</h3><p>目前数据备份采用 Perconra xtrabackup 工具，xtrabackup 可以实现高速、压缩带增量的备份；xtrabackup 安装同样采用 rpm 方式，下载地址为 <a href="https://www.percona.com/downloads/Percona-XtraBackup-2.4/LATEST/">https://www.percona.com/downloads/Percona-XtraBackup-2.4/LATEST/</a>，下载完成后执行 <code>yum install</code> 既可</p><h3 id="5-2、备份工具"><a href="#5-2、备份工具" class="headerlink" title="5.2、备份工具"></a>5.2、备份工具</h3><p>目前备份工具开源在 <a href="https://github.com/gozap/mybak">GitHub</a> 上，每次全量备份会写入 <code>.full-backup</code> 文件，增量备份会写入 <code>.inc-backup</code> 文件</p><h3 id="5-3、配置-systemd"><a href="#5-3、配置-systemd" class="headerlink" title="5.3、配置 systemd"></a>5.3、配置 systemd</h3><p>为了使备份自动运行，目前将定时任务配置到 systemd 中，由 systemd 调度并执行；以下为相关 systemd 配置文件</p><p><strong>mysql-backup-full.service</strong></p><div class="hljs code-wrapper"><pre><code class="hljs sh">[Unit]Description=mysql full backupAfter=network.target[Service]Type=simpleRestart=on-failureExecStart=/usr/<span class="hljs-built_in">local</span>/bin/mybak --backup-dir /data/mysql_backup --prefix mysql full[Install]WantedBy=multi-user.target</code></pre></div><p><strong>mysql-backup-inc.service</strong></p><div class="hljs code-wrapper"><pre><code class="hljs sh">[Unit]Description=mysql incremental backupAfter=network.target[Service]Type=simpleRestart=on-failureExecStart=/usr/<span class="hljs-built_in">local</span>/bin/mybak --backup-dir /data/mysql_backup --prefix mysql inc[Install]WantedBy=multi-user.target</code></pre></div><p><strong>mysql-backup-compress.service</strong></p><div class="hljs code-wrapper"><pre><code class="hljs sh">[Unit]Description=mysql backup compressAfter=network.target[Service]Type=simpleRestart=on-failureExecStart=/usr/<span class="hljs-built_in">local</span>/bin/mybak --backup-dir /data/mysql_backup --prefix mysql compress --clean[Install]WantedBy=multi-user.target</code></pre></div><p><strong>mysql-backup-full.timer</strong></p><div class="hljs code-wrapper"><pre><code class="hljs sh">[Unit]Description=mysql weekly full backup<span class="hljs-comment"># 备份之前依赖相关目录的挂载</span>After=data.mountAfter=data-mysql_backup.mount[Timer]<span class="hljs-comment"># 目前每周日一个全量备份</span>OnCalendar=Sun *-*-* 3:00Persistent=<span class="hljs-literal">true</span>[Install]WantedBy=timers.target</code></pre></div><p><strong>mysql-backup-inc.timer</strong></p><div class="hljs code-wrapper"><pre><code class="hljs sh">[Unit]Description=mysql weekly full backupAfter=data.mountAfter=data-mysql_backup.mount[Timer]<span class="hljs-comment"># 每天三个增量备份</span>OnCalendar=*-*-* 9:00OnCalendar=*-*-* 13:00OnCalendar=*-*-* 18:00Persistent=<span class="hljs-literal">true</span>[Install]WantedBy=timers.target</code></pre></div><p><strong>mysql-backup-compress.timer</strong></p><div class="hljs code-wrapper"><pre><code class="hljs sh">[Unit]Description=mysql weekly backup compress<span class="hljs-comment"># 备份之前依赖相关目录的挂载</span>After=data.mountAfter=data-mysql_backup.mount[Timer]<span class="hljs-comment"># 目前每周日一个全量备份，自动压缩后同时完成清理</span>OnCalendar=Sun *-*-* 5:00Persistent=<span class="hljs-literal">true</span>[Install]WantedBy=timers.target</code></pre></div><p>创建好相关文件后启动相关定时器既可</p><div class="hljs code-wrapper"><pre><code class="hljs sh">cp *.timer *.service /lib/systemd/systemsystemctl daemon-reloadsystemctl start mysql-backup-full.timer mysql-backup-inc.timer mysql-backup-compress.timersystemctl <span class="hljs-built_in">enable</span> mysql-backup-full.timer mysql-backup-inc.timer mysql-backup-compress.timer</code></pre></div><h2 id="六、数据恢复"><a href="#六、数据恢复" class="headerlink" title="六、数据恢复"></a>六、数据恢复</h2><h3 id="6-1、全量备份恢复"><a href="#6-1、全量备份恢复" class="headerlink" title="6.1、全量备份恢复"></a>6.1、全量备份恢复</h3><p>针对于全量备份，只需要按照官方文档的还原顺序进行还原既可</p><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment"># 由于备份时进行了压缩，所以先解压备份文件</span>xtrabackup --decompress --parallel 4 --target-dir /data/mysql_backup/mysql-20191205230502<span class="hljs-comment"># 执行预处理</span>xtrabackup --prepare --target-dir /data/mysql_backup/mysql-20191205230502<span class="hljs-comment"># 执行恢复(恢复时自动根据 my.cnf 将数据覆盖到 data 数据目录)</span>xtrabackup --copy-back --target-dir /data/mysql_backup/mysql-20191205230502<span class="hljs-comment"># 修复数据目录权限</span>chown -R mysql:mysql /data/mysql<span class="hljs-comment"># 启动 mysql</span>systemctl start mysqld</code></pre></div><h3 id="6-2、增量备份恢复"><a href="#6-2、增量备份恢复" class="headerlink" title="6.2、增量备份恢复"></a>6.2、增量备份恢复</h3><p>对于增量备份恢复，其与全量备份恢复的根本区别在于: <strong>对于非最后一个增量文件的预处理必须使用 <code>--apply-log-only</code> 选项防止运行回滚阶段的处理</strong></p><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment"># 对所有备份文件进行解压处理</span><span class="hljs-keyword">for</span> dir <span class="hljs-keyword">in</span> `ls`; <span class="hljs-keyword">do</span> xtrabackup --decompress --parallel 4 --target-dir <span class="hljs-variable">$dir</span>; <span class="hljs-keyword">done</span><span class="hljs-comment"># 对全量备份文件执行预处理(注意增加 --apply-log-only 选项)</span>xtrabackup --prepare --apply-log-only --target-dir /data/mysql_backup/mysql-20191205230502<span class="hljs-comment"># 对非最后一个增量备份执行预处理</span>xtrabackup --prepare --apply-log-only --target-dir /data/mysql_backup/mysql-20191205230502 --incremental-dir /data/mysql_backup/mysql-inc-20191206230802<span class="hljs-comment"># 对最后一个增量备份执行预处理(不需要 --apply-log-only)</span>xtrabackup --prepare --target-dir /data/mysql_backup/mysql-20191205230502 --incremental-dir /data/mysql_backup/mysql-inc-20191207031005<span class="hljs-comment"># 执行恢复(恢复时自动根据 my.cnf 将数据覆盖到 data 数据目录)</span>xtrabackup --copy-back --target-dir /data/mysql_backup/mysql-20191205230502<span class="hljs-comment"># 修复数据目录权限</span>chown -R mysql:mysql /data/mysql<span class="hljs-comment"># 启动 mysql</span>systemctl start mysqld</code></pre></div><h3 id="6-3、创建-slave"><a href="#6-3、创建-slave" class="headerlink" title="6.3、创建 slave"></a>6.3、创建 slave</h3><p>针对 xtrabackup 备份的数据可以直接恢复成 slave 节点，具体步骤如下:</p><p>首先将备份文件复制到目标机器，然后执行解压(默认备份工具采用 lz4 压缩)</p><div class="hljs code-wrapper"><pre><code class="hljs sh">xtrabackup --decompress --target-dir=xxxxxx</code></pre></div><p>解压完成后执行预处理操作(<strong>在执行预处理之前请确保 slave 机器上相关配置文件与 master 相同，并且处理好数据目录存放等</strong>)</p><div class="hljs code-wrapper"><pre><code class="hljs sh">xtrabackup --user=root --password=xxxxxxx --prepare --target-dir=xxxx</code></pre></div><p>预处理成功后便可执行恢复，以下命令将自动读取 <code>my.cnf</code> 配置，自动识别数据目录位置并将数据文件移动到该位置</p><div class="hljs code-wrapper"><pre><code class="hljs sh">xtrabackup --move-back --target-dir=xxxxx</code></pre></div><p>所由准备就绪后需要进行权限修复</p><div class="hljs code-wrapper"><pre><code class="hljs sh">chown -R mysql:mysql MYSQL_DATA_DIR</code></pre></div><p>最后在 mysql 内启动 slave 既可，slave 信息可通过从数据备份目录的 <code>xtrabackup_binlog_info</code> 中获取</p><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment"># 获取备份 POS 信息</span>cat xxxxxx/xtrabackup_binlog_info<span class="hljs-comment"># 创建 slave 节点</span>CHANGE MASTER TO    MASTER_HOST=<span class="hljs-string">&#x27;192.168.2.48&#x27;</span>,    MASTER_USER=<span class="hljs-string">&#x27;repl&#x27;</span>,    MASTER_PASSWORD=<span class="hljs-string">&#x27;xxxxxxx&#x27;</span>,    MASTER_LOG_FILE=<span class="hljs-string">&#x27;mysql-bin.000005&#x27;</span>,    MASTER_LOG_POS=52500595;<span class="hljs-comment"># 启动 slave</span>start slave;show slave status \G;</code></pre></div><h2 id="七、生产处理"><a href="#七、生产处理" class="headerlink" title="七、生产处理"></a>七、生产处理</h2><h3 id="7-1、数据目录"><a href="#7-1、数据目录" class="headerlink" title="7.1、数据目录"></a>7.1、数据目录</h3><p>目前生产环境数据目录位置调整到 <code>/home/mysql</code>，所以目录权限处理也要做对应调整</p><div class="hljs code-wrapper"><pre><code class="hljs sh">mkdir -p /var/<span class="hljs-built_in">log</span>/mysql /home/mysql_tmpchown -R mysql:mysql /var/<span class="hljs-built_in">log</span>/mysql /home/mysql_tmp</code></pre></div><h3 id="7-2、配置文件"><a href="#7-2、配置文件" class="headerlink" title="7.2、配置文件"></a>7.2、配置文件</h3><p>生产环境目前节点配置如下</p><ul><li>CPU: <code>Intel(R) Xeon(R) CPU E5-2620 v4 @ 2.10GHz</code></li><li>RAM: <code>128G</code></li></ul><p>所以配置文件也需要做相应的优化调整</p><p><strong>mysql.cnf</strong></p><div class="hljs code-wrapper"><pre><code class="hljs ini"><span class="hljs-section">[mysql]</span>auto-rehash<span class="hljs-attr">default_character_set</span>=utf8mb4</code></pre></div><p><strong>mysqld.cnf</strong></p><div class="hljs code-wrapper"><pre><code class="hljs ini"><span class="hljs-comment"># Percona Server template configuration</span><span class="hljs-section">[mysqld]</span><span class="hljs-comment">#</span><span class="hljs-comment"># Remove leading # and set to the amount of RAM for the most important data</span><span class="hljs-comment"># cache in MySQL. Start at 70% of total RAM for dedicated server, else 10%.</span><span class="hljs-comment"># innodb_buffer_pool_size = 128M</span><span class="hljs-comment">#</span><span class="hljs-comment"># Remove leading # to turn on a very important data integrity option: logging</span><span class="hljs-comment"># changes to the binary log between backups.</span><span class="hljs-comment"># log_bin</span><span class="hljs-comment">#</span><span class="hljs-comment"># Remove leading # to set options mainly useful for reporting servers.</span><span class="hljs-comment"># The server defaults are faster for transactions and fast SELECTs.</span><span class="hljs-comment"># Adjust sizes as needed, experiment to find the optimal values.</span><span class="hljs-comment"># join_buffer_size = 128M</span><span class="hljs-comment"># sort_buffer_size = 2M</span><span class="hljs-comment"># read_rnd_buffer_size = 2M</span><span class="hljs-attr">port</span>=<span class="hljs-number">3306</span><span class="hljs-attr">datadir</span>=/home/mysql/mysql<span class="hljs-attr">socket</span>=/home/mysql/mysql/mysql.sock<span class="hljs-attr">pid_file</span>=/home/mysql/mysql/mysqld.pid<span class="hljs-comment"># 服务端编码</span><span class="hljs-attr">character_set_server</span>=utf8mb4<span class="hljs-comment"># 服务端排序</span><span class="hljs-attr">collation_server</span>=utf8mb4_general_ci<span class="hljs-comment"># 强制使用 utf8mb4 编码集，忽略客户端设置</span><span class="hljs-attr">skip_character_set_client_handshake</span>=<span class="hljs-number">1</span><span class="hljs-comment"># 日志输出到文件</span><span class="hljs-attr">log_output</span>=FILE<span class="hljs-comment"># 开启常规日志输出</span><span class="hljs-attr">general_log</span>=<span class="hljs-number">1</span><span class="hljs-comment"># 常规日志输出文件位置</span><span class="hljs-attr">general_log_file</span>=/var/log/mysql/mysqld.log<span class="hljs-comment"># 错误日志位置</span><span class="hljs-attr">log_error</span>=/var/log/mysql/mysqld-error.log<span class="hljs-comment"># 记录慢查询</span><span class="hljs-attr">slow_query_log</span>=<span class="hljs-number">1</span><span class="hljs-comment"># 慢查询时间(大于 1s 被视为慢查询)</span><span class="hljs-attr">long_query_time</span>=<span class="hljs-number">1</span><span class="hljs-comment"># 慢查询日志文件位置</span><span class="hljs-attr">slow_query_log_file</span>=/var/log/mysql/mysqld-slow.log<span class="hljs-comment"># 临时文件位置</span><span class="hljs-attr">tmpdir</span>=/home/mysql/mysql_tmp<span class="hljs-comment"># The number of open tables for all threads.(refs https://dev.mysql.com/doc/refman/5.7/en/server-system-variables.html#sysvar_table_open_cache)</span><span class="hljs-attr">table_open_cache</span>=<span class="hljs-number">16384</span><span class="hljs-comment"># 文件描述符(此处修改不生效，请修改 systemd service 配置) </span><span class="hljs-comment"># refs https://www.percona.com/blog/2017/10/12/open_files_limit-mystery/</span><span class="hljs-comment"># refs https://www.cnblogs.com/wxxjianchi/p/10370419.html</span><span class="hljs-comment">#open_files_limit=65535</span><span class="hljs-comment"># 表定义缓存(5.7 以后自动调整)</span><span class="hljs-comment"># refs https://dev.mysql.com/doc/refman/5.6/en/server-system-variables.html#sysvar_table_definition_cache</span><span class="hljs-comment"># refs http://mysql.taobao.org/monthly/2015/08/10/</span><span class="hljs-comment">#table_definition_cache=16384</span><span class="hljs-attr">sort_buffer_size</span>=<span class="hljs-number">1</span>M<span class="hljs-attr">join_buffer_size</span>=<span class="hljs-number">1</span>M<span class="hljs-comment"># MyiSAM 引擎专用(内部临时磁盘表可能会用)</span><span class="hljs-attr">read_buffer_size</span>=<span class="hljs-number">1</span>M<span class="hljs-attr">read_rnd_buffer_size</span>=<span class="hljs-number">1</span>M<span class="hljs-comment"># MyiSAM 引擎专用(内部临时磁盘表可能会用)</span><span class="hljs-attr">key_buffer_size</span>=<span class="hljs-number">32</span>M<span class="hljs-comment"># MyiSAM 引擎专用(内部临时磁盘表可能会用)</span><span class="hljs-attr">bulk_insert_buffer_size</span>=<span class="hljs-number">16</span>M<span class="hljs-comment"># myisam_sort_buffer_size 与 sort_buffer_size 区别请参考(https://stackoverflow.com/questions/7871027/myisam-sort-buffer-size-vs-sort-buffer-size)</span><span class="hljs-attr">myisam_sort_buffer_size</span>=<span class="hljs-number">64</span>M<span class="hljs-comment"># 内部内存临时表大小</span><span class="hljs-attr">tmp_table_size</span>=<span class="hljs-number">32</span>M<span class="hljs-comment"># 用户创建的 MEMORY 表最大大小(tmp_table_size 受此值影响)</span><span class="hljs-attr">max_heap_table_size</span>=<span class="hljs-number">32</span>M<span class="hljs-comment"># 开启查询缓存</span><span class="hljs-attr">query_cache_type</span>=<span class="hljs-number">1</span><span class="hljs-comment"># 查询缓存大小</span><span class="hljs-attr">query_cache_size</span>=<span class="hljs-number">32</span>M<span class="hljs-comment"># sql mode</span><span class="hljs-attr">sql_mode</span>=<span class="hljs-string">&#x27;STRICT_TRANS_TABLES,NO_ZERO_IN_DATE,NO_ZERO_DATE,ERROR_FOR_DIVISION_BY_ZERO,NO_AUTO_CREATE_USER,NO_ENGINE_SUBSTITUTION&#x27;</span><span class="hljs-comment">########### Network ###########</span><span class="hljs-comment"># 最大连接数(该参数受到最大文件描述符影响，如果不生效请检查最大文件描述符设置)</span><span class="hljs-comment"># refs https://stackoverflow.com/questions/39976756/the-max-connections-in-mysql-5-7</span><span class="hljs-attr">max_connections</span>=<span class="hljs-number">1500</span><span class="hljs-comment"># mysql 堆栈内暂存的链接数量</span><span class="hljs-comment"># 当短时间内链接数量超过 max_connections 时，部分链接会存储在堆栈内，存储数量受此参数控制</span><span class="hljs-attr">back_log</span>=<span class="hljs-number">256</span><span class="hljs-comment"># 最大链接错误，针对于 client 主机，超过此数量的链接错误将会导致 mysql server 针对此主机执行锁定(禁止链接 ERROR 1129 )</span><span class="hljs-comment"># 此错误计数仅在 mysql 链接握手失败才会计算，一般出现问题时都是网络故障</span><span class="hljs-comment"># refs https://www.cnblogs.com/kerrycode/p/8405862.html</span><span class="hljs-attr">max_connect_errors</span>=<span class="hljs-number">100000</span><span class="hljs-comment"># mysql server 允许的最大数据包大小</span><span class="hljs-attr">max_allowed_packet</span>=<span class="hljs-number">64</span>M<span class="hljs-comment"># 交互式客户端链接超时(30分钟自动断开)</span><span class="hljs-attr">interactive_timeout</span>=<span class="hljs-number">1800</span><span class="hljs-comment"># 非交互式链接超时时间(10分钟)</span><span class="hljs-comment"># 如果客户端有连接池，则需要协商此参数(refs https://database.51cto.com/art/201909/603519.htm)</span><span class="hljs-attr">wait_timeout</span>=<span class="hljs-number">28800</span><span class="hljs-comment"># 跳过外部文件系统锁定</span><span class="hljs-comment"># If you run multiple servers that use the same database directory (not recommended), </span><span class="hljs-comment"># each server must have external locking enabled.</span><span class="hljs-comment"># refs https://dev.mysql.com/doc/refman/5.7/en/external-locking.html</span><span class="hljs-attr">skip_external_locking</span>=<span class="hljs-number">1</span><span class="hljs-comment"># 跳过链接的域名解析(开启此选项后 mysql 用户授权的 host 方式失效)</span><span class="hljs-attr">skip_name_resolve</span>=<span class="hljs-number">0</span><span class="hljs-comment"># 禁用主机名缓存，每次都会走 DNS</span><span class="hljs-attr">host_cache_size</span>=<span class="hljs-number">0</span><span class="hljs-comment">########### REPL ###########</span><span class="hljs-comment"># 开启 binlog</span><span class="hljs-attr">log_bin</span>=mysql-bin<span class="hljs-comment"># 作为从库时，同步信息依然写入 binlog，方便此从库再作为其他从库的主库</span><span class="hljs-attr">log_slave_updates</span>=<span class="hljs-number">1</span><span class="hljs-comment"># server id，默认为 ipv4 地址去除第一段</span><span class="hljs-comment"># eg: 192.168.2.48 =&gt; 168248</span><span class="hljs-attr">server_id</span>=<span class="hljs-number">168248</span><span class="hljs-comment"># 每 n 次事务 binlog 刷新到磁盘</span><span class="hljs-comment"># refs http://liyangliang.me/posts/2014/03/innodb_flush_log_at_trx_commit-and-sync_binlog/</span><span class="hljs-attr">sync_binlog</span>=<span class="hljs-number">100</span><span class="hljs-comment"># binlog 格式(refs https://zhuanlan.zhihu.com/p/33504555)</span><span class="hljs-attr">binlog_format</span>=row<span class="hljs-comment"># binlog 自动清理时间</span><span class="hljs-attr">expire_logs_days</span>=<span class="hljs-number">20</span><span class="hljs-comment"># 开启 relay-log，一般作为 slave 时开启</span><span class="hljs-attr">relay_log</span>=mysql-replay<span class="hljs-comment"># 主从复制时跳过 test 库</span><span class="hljs-attr">replicate_ignore_db</span>=test<span class="hljs-comment"># 每个 session binlog 缓存</span><span class="hljs-attr">binlog_cache_size</span>=<span class="hljs-number">4</span>M<span class="hljs-comment"># binlog 滚动大小</span><span class="hljs-attr">max_binlog_size</span>=<span class="hljs-number">1024</span>M<span class="hljs-comment"># GTID 相关(refs https://keithlan.github.io/2016/06/23/gtid/)</span><span class="hljs-comment">#gtid_mode=1</span><span class="hljs-comment">#enforce_gtid_consistency=1</span><span class="hljs-comment">########### InnoDB ###########</span><span class="hljs-comment"># 永久表默认存储引擎</span><span class="hljs-attr">default_storage_engine</span>=InnoDB<span class="hljs-comment"># 系统表空间数据文件大小(初始化为 1G，并且自动增长)</span><span class="hljs-attr">innodb_data_file_path</span>=ibdata1:<span class="hljs-number">1</span>G:autoextend<span class="hljs-comment"># InnoDB 缓存池大小(资源充足，为所欲为)</span><span class="hljs-comment"># innodb_buffer_pool_size 必须等于 innodb_buffer_pool_chunk_size*innodb_buffer_pool_instances，或者是其整数倍</span><span class="hljs-comment"># refs https://dev.mysql.com/doc/refman/5.7/en/innodb-buffer-pool-resize.html</span><span class="hljs-comment"># refs https://zhuanlan.zhihu.com/p/60089484</span><span class="hljs-attr">innodb_buffer_pool_size</span>=<span class="hljs-number">61440</span>M<span class="hljs-attr">innodb_buffer_pool_instances</span>=<span class="hljs-number">16</span><span class="hljs-comment"># 默认 128M</span><span class="hljs-attr">innodb_buffer_pool_chunk_size</span>=<span class="hljs-number">128</span>M<span class="hljs-comment"># InnoDB 强制恢复(refs https://www.askmaclean.com/archives/mysql-innodb-innodb_force_recovery.html)</span><span class="hljs-attr">innodb_force_recovery</span>=<span class="hljs-number">0</span><span class="hljs-comment"># InnoDB buffer 预热(refs http://www.dbhelp.net/2017/01/12/mysql-innodb-buffer-pool-warmup.html)</span><span class="hljs-attr">innodb_buffer_pool_dump_at_shutdown</span>=<span class="hljs-number">1</span><span class="hljs-attr">innodb_buffer_pool_load_at_startup</span>=<span class="hljs-number">1</span><span class="hljs-comment"># InnoDB 日志组中的日志文件数</span><span class="hljs-attr">innodb_log_files_in_group</span>=<span class="hljs-number">2</span><span class="hljs-comment"># InnoDB redo 日志大小</span><span class="hljs-comment"># refs https://www.percona.com/blog/2017/10/18/chose-mysql-innodb_log_file_size/</span><span class="hljs-attr">innodb_log_file_size</span>=<span class="hljs-number">256</span>MB<span class="hljs-comment"># 缓存还未提交的事务的缓冲区大小</span><span class="hljs-attr">innodb_log_buffer_size</span>=<span class="hljs-number">16</span>M<span class="hljs-comment"># InnoDB 在事务提交后的日志写入频率</span><span class="hljs-comment"># refs http://liyangliang.me/posts/2014/03/innodb_flush_log_at_trx_commit-and-sync_binlog/</span><span class="hljs-attr">innodb_flush_log_at_trx_commit</span>=<span class="hljs-number">2</span><span class="hljs-comment"># InnoDB DML 操作行级锁等待时间</span><span class="hljs-comment"># 超时返回 ERROR 1205 (HY000): Lock wait timeout exceeded; try restarting transaction</span><span class="hljs-comment"># refs https://ningyu1.github.io/site/post/75-mysql-lock-wait-timeout-exceeded/</span><span class="hljs-attr">innodb_lock_wait_timeout</span>=<span class="hljs-number">30</span><span class="hljs-comment"># InnoDB 行级锁超时是否回滚整个事务，默认为 OFF 仅回滚上一条语句</span><span class="hljs-comment"># 此时应用程序可以接受到错误后选择是否继续提交事务(并没有违反 ACID 原子性)</span><span class="hljs-comment"># refs https://www.cnblogs.com/hustcat/archive/2012/11/18/2775487.html</span><span class="hljs-comment">#innodb_rollback_on_timeout=ON</span><span class="hljs-comment"># InnoDB 数据写入磁盘的方式，具体见博客文章</span><span class="hljs-comment"># refs https://www.cnblogs.com/gomysql/p/3595806.html</span><span class="hljs-attr">innodb_flush_method</span>=O_DIRECT<span class="hljs-comment"># InnoDB 缓冲池脏页刷新百分比</span><span class="hljs-comment"># refs https://dbarobin.com/2015/08/29/mysql-optimization-under-ssd</span><span class="hljs-attr">innodb_max_dirty_pages_pct</span>=<span class="hljs-number">50</span><span class="hljs-comment"># InnoDB 每秒执行的写IO量</span><span class="hljs-comment"># refs https://www.centos.bz/2016/11/mysql-performance-tuning-15-config-item/#10.INNODB_IO_CAPACITY,%20INNODB_IO_CAPACITY_MAX</span><span class="hljs-comment"># refs https://www.alibabacloud.com/blog/testing-io-performance-with-sysbench_594709</span><span class="hljs-attr">innodb_io_capacity</span>=<span class="hljs-number">8000</span><span class="hljs-attr">innodb_io_capacity_max</span>=<span class="hljs-number">16000</span><span class="hljs-comment"># 请求并发 InnoDB 线程数</span><span class="hljs-comment"># refs https://www.cnblogs.com/xinysu/p/6439715.html#_lab2_1_0</span><span class="hljs-attr">innodb_thread_concurrency</span>=<span class="hljs-number">0</span><span class="hljs-comment"># 再使用多个 InnoDB 表空间时，允许打开的最大 &quot;.ibd&quot; 文件个数，不设置默认 300，</span><span class="hljs-comment"># 并且取与 table_open_cache 相比较大的一个，此选项独立于 open_files_limit</span><span class="hljs-comment"># refs https://dev.mysql.com/doc/refman/5.7/en/innodb-parameters.html#sysvar_innodb_open_files</span><span class="hljs-attr">innodb_open_files</span>=<span class="hljs-number">65535</span><span class="hljs-comment"># 每个 InnoDB 表都存储在独立的表空间(.ibd)中</span><span class="hljs-attr">innodb_file_per_table</span>=<span class="hljs-number">1</span><span class="hljs-comment"># 事务级别(可重复读，会出幻读)</span><span class="hljs-attr">transaction_isolation</span>=REPEATABLE-READ<span class="hljs-comment"># 是否在搜索和索引扫描中使用间隙锁(gap locking)，不建议使用未来将删除</span><span class="hljs-attr">innodb_locks_unsafe_for_binlog</span>=<span class="hljs-number">0</span><span class="hljs-comment"># InnoDB 后台清理线程数，更大的值有助于 DML 执行性能，&gt;= 5.7.8 默认为 4</span><span class="hljs-attr">innodb_purge_threads</span>=<span class="hljs-number">4</span></code></pre></div><p><strong>mysqld_safe.cnf</strong></p><div class="hljs code-wrapper"><pre><code class="hljs ini"><span class="hljs-comment">#</span><span class="hljs-comment"># The Percona Server 5.7 configuration file.</span><span class="hljs-comment">#</span><span class="hljs-comment"># One can use all long options that the program supports.</span><span class="hljs-comment"># Run program with --help to get a list of available options and with</span><span class="hljs-comment"># --print-defaults to see which it would actually understand and use.</span><span class="hljs-comment">#</span><span class="hljs-comment"># For explanations see</span><span class="hljs-comment"># http://dev.mysql.com/doc/mysql/en/server-system-variables.html</span><span class="hljs-section">[mysqld_safe]</span><span class="hljs-attr">pid-file</span> = /var/run/mysqld/mysqld.pid<span class="hljs-attr">socket</span>   = /var/run/mysqld/mysqld.sock<span class="hljs-attr">nice</span>     = <span class="hljs-number">0</span></code></pre></div><p><strong>mysqldump.cnf</strong></p><div class="hljs code-wrapper"><pre><code class="hljs ini"><span class="hljs-section">[mysqldump]</span>quick<span class="hljs-attr">default-character-set</span>=utf8mb4<span class="hljs-attr">max_allowed_packet</span>=<span class="hljs-number">256</span>M</code></pre></div><h2 id="八、常用诊断"><a href="#八、常用诊断" class="headerlink" title="八、常用诊断"></a>八、常用诊断</h2><h3 id="8-1、动态配置-diff"><a href="#8-1、动态配置-diff" class="headerlink" title="8.1、动态配置 diff"></a>8.1、动态配置 diff</h3><p>mysql 默认允许在实例运行后使用 <code>set global VARIABLES=VALUE</code> 的方式动态调整一些配置，这可能导致在运行一段时间后(运维动态修改)实例运行配置和配置文件中配置不一致；所以<strong>建议定期 diff 运行时配置与配置文件配置差异，防制特殊情况下 mysql 重启后运行期配置丢失</strong></p><div class="hljs code-wrapper"><pre><code class="hljs sh">pt-config-diff /etc/percona-server.conf.d/mysqld.cnf h=127.0.0.1 --user root --ask-pass --report-width 100Enter MySQL password:2 config differencesVariable                  /etc/percona-server.conf.d/mysqld.cnf mysql47.test.com========================= ===================================== ==================innodb_max_dirty_pages... 50                                    50.000000skip_name_resolve         0                                     ON</code></pre></div><h3 id="8-2、配置优化建议"><a href="#8-2、配置优化建议" class="headerlink" title="8.2、配置优化建议"></a>8.2、配置优化建议</h3><p>Percona Toolkit 提供了一个诊断工具，用于对 mysql 内的配置进行扫描并给出优化建议，在初始化时可以使用此工具评估 mysql 当前配置的具体情况</p><div class="hljs code-wrapper"><pre><code class="hljs sh">pt-variable-advisor 127.0.0.1 --user root --ask-pass | grep -v <span class="hljs-string">&#x27;^$&#x27;</span>Enter password: <span class="hljs-comment"># WARN delay_key_write: MyISAM index blocks are never flushed until necessary.</span><span class="hljs-comment"># WARN innodb_flush_log_at_trx_commit-1: InnoDB is not configured in strictly ACID mode.</span><span class="hljs-comment"># NOTE innodb_max_dirty_pages_pct: The innodb_max_dirty_pages_pct is lower than the default.</span><span class="hljs-comment"># WARN max_connections: If the server ever really has more than a thousand threads running, then the system is likely to spend more time scheduling threads than really doing useful work.</span><span class="hljs-comment"># NOTE read_buffer_size-1: The read_buffer_size variable should generally be left at its default unless an expert determines it is necessary to change it.</span><span class="hljs-comment"># NOTE read_rnd_buffer_size-1: The read_rnd_buffer_size variable should generally be left at its default unless an expert determines it is necessary to change it.</span><span class="hljs-comment"># NOTE sort_buffer_size-1: The sort_buffer_size variable should generally be left at its default unless an expert determines it is necessary to change it.</span><span class="hljs-comment"># NOTE innodb_data_file_path: Auto-extending InnoDB files can consume a lot of disk space that is very difficult to reclaim later.</span><span class="hljs-comment"># WARN myisam_recover_options: myisam_recover_options should be set to some value such as BACKUP,FORCE to ensure that table corruption is noticed.</span><span class="hljs-comment"># WARN sync_binlog: Binary logging is enabled, but sync_binlog isn&#x27;t configured so that every transaction is flushed to the binary log for durability.</span></code></pre></div><h3 id="8-3、死锁诊断"><a href="#8-3、死锁诊断" class="headerlink" title="8.3、死锁诊断"></a>8.3、死锁诊断</h3><p>使用 pt-deadlock-logger 工具可以诊断当前的死锁状态，以下为对死锁检测的测试</p><p>首先创建测试数据库和表</p><div class="hljs code-wrapper"><pre><code class="hljs sql"># 创建测试库<span class="hljs-keyword">CREATE</span> DATABASE dbatest <span class="hljs-type">CHARACTER</span> <span class="hljs-keyword">SET</span> utf8mb4 <span class="hljs-keyword">COLLATE</span> utf8mb4_unicode_ci;# 切换到测试库并建立测试表USE dbatest;<span class="hljs-keyword">CREATE</span> <span class="hljs-keyword">TABLE</span> IF <span class="hljs-keyword">NOT</span> <span class="hljs-keyword">EXISTS</span> test (id <span class="hljs-type">INT</span> AUTO_INCREMENT <span class="hljs-keyword">PRIMARY</span> KEY, <span class="hljs-keyword">value</span> <span class="hljs-type">VARCHAR</span>(<span class="hljs-number">255</span>), createtime <span class="hljs-type">TIMESTAMP</span> <span class="hljs-keyword">DEFAULT</span> <span class="hljs-built_in">CURRENT_TIMESTAMP</span>) ENGINE<span class="hljs-operator">=</span>INNODB;</code></pre></div><p>在一个其他终端上开启 pt-deadlock-logger 检测</p><div class="hljs code-wrapper"><pre><code class="hljs sh">pt-deadlock-logger 127.0.0.1 --user root --ask-pass --tab</code></pre></div><p>检测开启后进行死锁测试</p><div class="hljs code-wrapper"><pre><code class="hljs sql"># 插入两条测试数据<span class="hljs-keyword">INSERT</span> <span class="hljs-keyword">INTO</span> test(<span class="hljs-keyword">value</span>) <span class="hljs-keyword">VALUES</span>(<span class="hljs-string">&#x27;test1&#x27;</span>);<span class="hljs-keyword">INSERT</span> <span class="hljs-keyword">INTO</span> test(<span class="hljs-keyword">value</span>) <span class="hljs-keyword">VALUES</span>(<span class="hljs-string">&#x27;test2&#x27;</span>);# 在两个终端下进行交叉事务# 统一关闭自动提交terminal_1 # <span class="hljs-keyword">SET</span> AUTOCOMMIT <span class="hljs-operator">=</span> <span class="hljs-number">0</span>;terminal_2 # <span class="hljs-keyword">SET</span> AUTOCOMMIT <span class="hljs-operator">=</span> <span class="hljs-number">0</span>;# 交叉事务，终端 <span class="hljs-number">1</span> 先更新第一条数据，终端 <span class="hljs-number">2</span> 先更新第二条数据terminal_1 # <span class="hljs-keyword">BEGIN</span>;terminal_1 # UPDATE test <span class="hljs-keyword">set</span> <span class="hljs-keyword">value</span><span class="hljs-operator">=</span><span class="hljs-string">&#x27;x1&#x27;</span> <span class="hljs-keyword">where</span> id<span class="hljs-operator">=</span><span class="hljs-number">1</span>;terminal_2 # <span class="hljs-keyword">BEGIN</span>;terminal_2 # UPDATE test <span class="hljs-keyword">set</span> <span class="hljs-keyword">value</span><span class="hljs-operator">=</span><span class="hljs-string">&#x27;x2&#x27;</span> <span class="hljs-keyword">where</span> id<span class="hljs-operator">=</span><span class="hljs-number">2</span>;# 此后终端 <span class="hljs-number">1</span> 再尝试更新第二条数据，终端 <span class="hljs-number">2</span> 再尝试更新第一条数据；造成等待互向释放锁的死锁terminal_1 # UPDATE test <span class="hljs-keyword">set</span> <span class="hljs-keyword">value</span><span class="hljs-operator">=</span><span class="hljs-string">&#x27;lock2&#x27;</span> <span class="hljs-keyword">where</span> id<span class="hljs-operator">=</span><span class="hljs-number">2</span>;terminal_2 # UPDATE test <span class="hljs-keyword">set</span> <span class="hljs-keyword">value</span><span class="hljs-operator">=</span><span class="hljs-string">&#x27;lock1&#x27;</span> <span class="hljs-keyword">where</span> id<span class="hljs-operator">=</span><span class="hljs-number">1</span>;# 此时由于开启了 mysql innodb 的死锁自动检测机制，会导致终端 <span class="hljs-number">2</span> 弹出错误ERROR <span class="hljs-number">1213</span> (<span class="hljs-number">40001</span>): Deadlock found <span class="hljs-keyword">when</span> trying <span class="hljs-keyword">to</span> <span class="hljs-keyword">get</span> lock; try restarting transaction# 同时 pt<span class="hljs-operator">-</span>deadlock<span class="hljs-operator">-</span>logger 有日志输出server  ts      thread  txn_id  txn_time        <span class="hljs-keyword">user</span>    hostname    ip      db      tbl     idx     lock_type       lock_mode       wait_hold       victim  query<span class="hljs-number">127.0</span><span class="hljs-number">.0</span><span class="hljs-number">.1</span>       <span class="hljs-number">2019</span><span class="hljs-number">-12</span><span class="hljs-number">-24</span>T14:<span class="hljs-number">57</span>:<span class="hljs-number">10</span>     <span class="hljs-number">87</span>      <span class="hljs-number">0</span>       <span class="hljs-number">52</span>      root            <span class="hljs-number">127.0</span><span class="hljs-number">.0</span><span class="hljs-number">.1</span>       dbatest test    <span class="hljs-keyword">PRIMARY</span> RECORD  X       w       <span class="hljs-number">0</span>       UPDATE test <span class="hljs-keyword">set</span> <span class="hljs-keyword">value</span><span class="hljs-operator">=</span><span class="hljs-string">&#x27;lock2&#x27;</span> <span class="hljs-keyword">where</span> id<span class="hljs-operator">=</span><span class="hljs-number">2</span><span class="hljs-number">127.0</span><span class="hljs-number">.0</span><span class="hljs-number">.1</span>       <span class="hljs-number">2019</span><span class="hljs-number">-12</span><span class="hljs-number">-24</span>T14:<span class="hljs-number">57</span>:<span class="hljs-number">10</span>     <span class="hljs-number">89</span>      <span class="hljs-number">0</span>       <span class="hljs-number">41</span>      root            <span class="hljs-number">127.0</span><span class="hljs-number">.0</span><span class="hljs-number">.1</span>       dbatest test    <span class="hljs-keyword">PRIMARY</span> RECORD  X       w       <span class="hljs-number">1</span>       UPDATE test <span class="hljs-keyword">set</span> <span class="hljs-keyword">value</span><span class="hljs-operator">=</span><span class="hljs-string">&#x27;lock1&#x27;</span> <span class="hljs-keyword">where</span> id<span class="hljs-operator">=</span><span class="hljs-number">1</span></code></pre></div><h3 id="8-4、查看-IO-详情"><a href="#8-4、查看-IO-详情" class="headerlink" title="8.4、查看 IO 详情"></a>8.4、查看 IO 详情</h3><p>不同于 <code>iostat</code>，<code>pt-diskstats</code> 提供了更加详细的 IO 详情统计，并且据有交互式处理，执行一下命令将会实时检测 IO 状态</p><div class="hljs code-wrapper"><pre><code class="hljs sh">pt-diskstats --show-timestamps</code></pre></div><p>其中几个关键值含义如下(更详细的请参考官方文档 <a href="https://www.percona.com/doc/percona-toolkit/LATEST/pt-diskstats.html#output">https://www.percona.com/doc/percona-toolkit/LATEST/pt-diskstats.html#output</a>)</p><ul><li>rd_s: 每秒平均读取次数。这是发送到基础设备的 IO 请求数。通常，此数量少于应用程序发出的逻辑IO请求的数量。更多请求可能已排队到块设备，但是其中一些请求通常在发送到磁盘之前先进行合并。</li><li>rd_avkb: 读取的平均大小，以千字节为单位。</li><li>rd_mb_s: 每秒读取的平均兆字节数。</li><li>rd_mrg: 在发送到物理设备之前在队列调度程序中合并在一起的读取请求的百分比。</li><li>rd_rt: 读取操作的平均响应时间(以毫秒为单位)；这是端到端响应时间，包括在队列中花费的时间。这是发出 IO 请求的应用程序看到的响应时间，而不是块设备下的物理磁盘的响应时间。</li><li>busy: 设备至少有一个请求 wall-clock 时间的比例；等同于 <code>iostat</code> 的 <code>％util</code>。</li><li>in_prg: 正在进行的请求数。与读写并发是从可靠数字中生成的平均值不同，该数字是一个时样本，您可以看到它可能表示请求峰值，而不是真正的长期平均值。如果此数字很大，则从本质上讲意味着设备高负载运行。</li><li>ios_s: 物理设备的平均吞吐量，以每秒 IO 操作(IOPS)为单位。此列显示基础设备正在处理的总 IOPS；它是 rd_s 和 wr_s 的总和。</li><li>qtime: 平均排队时间；也就是说，请求在发送到物理设备之前在设备调度程序队列中花费的时间。</li><li>stime: 平均服务时间；也就是说，请求完成在队列中的等待之后，物理设备处理请求的时间。</li></ul><h3 id="8-5、重复索引优化"><a href="#8-5、重复索引优化" class="headerlink" title="8.5、重复索引优化"></a>8.5、重复索引优化</h3><p>pt-duplicate-key-checker 工具提供了对数据库重复索引和外键的自动查找功能，工具使用如下</p><div class="hljs code-wrapper"><pre><code class="hljs sh">pt-duplicate-key-checker 127.0.0.1 --user root --ask-passEnter password:<span class="hljs-comment"># A software update is available:</span><span class="hljs-comment"># ########################################################################</span><span class="hljs-comment"># aaaaaa.aaaaaa_audit</span><span class="hljs-comment"># ########################################################################</span><span class="hljs-comment"># index_linkId is a duplicate of unique_linkId</span><span class="hljs-comment"># Key definitions:</span><span class="hljs-comment">#   KEY `index_linkId` (`link_id`)</span><span class="hljs-comment">#   UNIQUE KEY `unique_linkId` (`link_id`),</span><span class="hljs-comment"># Column types:</span><span class="hljs-comment">#         `link_id` bigint(20) not null comment &#x27;bdid&#x27;</span><span class="hljs-comment"># To remove this duplicate index, execute:</span>ALTER TABLE `aaaaaa.aaaaaa_audit` DROP INDEX `index_linkId`;<span class="hljs-comment"># ########################################################################</span><span class="hljs-comment"># Summary of indexes</span><span class="hljs-comment"># ########################################################################</span><span class="hljs-comment"># Size Duplicate Indexes   927420</span><span class="hljs-comment"># Total Duplicate Indexes  3</span><span class="hljs-comment"># Total Indexes            847</span></code></pre></div><h3 id="8-6、表统计"><a href="#8-6、表统计" class="headerlink" title="8.6、表统计"></a>8.6、表统计</h3><p>pt-find 是一个很方便的表查找统计工具，默认的一些选项可以实现批量查找符合条件的表，甚至执行一些 SQL 处理命令</p><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment"># 批量查找大于 5G 的表，并排序</span>pt-find --host 127.0.0.1 --user root --ask-pass --tablesize +5G | sort -rnEnter password: `rss_service`.`test_feed_news``db_log_history`.`test_mobile_click_201912``db_log_history`.`test_mobile_click_201911``db_log_history`.`test_mobile_click_201910``test_dix`.`test_user_messages``test_dix`.`test_user_link_history``test_dix`.`test_mobile_click``test_dix`.`test_message``test_dix`.`test_link_votes``test_dix`.`test_links_mobile_content``test_dix`.`test_links``test_dix`.`test_comment_votes``test_dix`.`test_comments`</code></pre></div><p>如果想要定制输出可以采用 <code>--printf</code> 选项</p><div class="hljs code-wrapper"><pre><code class="hljs sh">pt-find --host 127.0.0.1 --user root --ask-pass --tablesize +5G --<span class="hljs-built_in">printf</span> <span class="hljs-string">&quot;%T\t%D.%N\n&quot;</span> | sort -rnEnter password: 13918404608     `test_dix`.`test_links_mobile_content`13735231488     `test_dix`.`test_comment_votes`12633227264     `test_dix`.`test_user_messages`12610174976     `test_dix`.`test_user_link_history`10506305536     `test_dix`.`test_links`9686745088      `test_dix`.`test_message`9603907584      `rss_service`.`test_feed_news`9004122112      `db_log_history`.`test_mobile_click_201910`8919007232      `test_dix`.`test_comments`8045707264      `db_log_history`.`test_mobile_click_201912`7855915008      `db_log_history`.`test_mobile_click_201911`6099566592      `test_dix`.`test_mobile_click`5892898816      `test_dix`.`test_link_votes`</code></pre></div><p><strong>遗憾的是目前 <code>printf</code> 格式来源与 Perl 的 <code>sprintf</code> 函数，所以支持格式有限，不过简单的格式定制已经基本实现，复杂的建议通过 awk 处理</strong>；其他的可选参数具体参考官方文档 <a href="https://www.percona.com/doc/percona-toolkit/LATEST/pt-find.html">https://www.percona.com/doc/percona-toolkit/LATEST/pt-find.html</a></p><h3 id="8-7、其他命令"><a href="#8-7、其他命令" class="headerlink" title="8.7、其他命令"></a>8.7、其他命令</h3><p>迫于篇幅，其他更多的高级命令请自行查阅官方文档 <a href="https://www.percona.com/doc/percona-toolkit/LATEST/index.html">https://www.percona.com/doc/percona-toolkit/LATEST/index.html</a></p>]]></content>
    
    
    <summary type="html">最近被拉去折腾 MySQL 了，Kuberntes 相关的文章停更了好久... MySQL 折腾完了顺便记录一下折腾过程，值得注意的是本篇文章从实际生产环境文档中摘录，部分日志和数据库敏感信息已被胡乱替换，所以不要盲目复制粘贴。</summary>
    
    
    
    <category term="Linux" scheme="https://mritd.com/categories/linux/"/>
    
    <category term="Database" scheme="https://mritd.com/categories/linux/database/"/>
    
    
    <category term="Linux" scheme="https://mritd.com/tags/linux/"/>
    
    <category term="MySQL" scheme="https://mritd.com/tags/mysql/"/>
    
    <category term="Percona" scheme="https://mritd.com/tags/percona/"/>
    
  </entry>
  
  <entry>
    <title>Writing Plugin for Coredns</title>
    <link href="https://mritd.com/2019/11/05/writing-plugin-for-coredns/"/>
    <id>https://mritd.com/2019/11/05/writing-plugin-for-coredns/</id>
    <published>2019-11-05T12:57:41.000Z</published>
    <updated>2019-11-05T12:57:41.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>目前测试环境中有很多个 DNS 服务器，不同项目组使用的 DNS 服务器不同，但是不可避免的他们会访问一些公共域名；老的 DNS 服务器都是 dnsmasq，改起来很麻烦，最近研究了一下 CoreDNS，通过编写插件的方式可以实现让多个 CoreDNS 实例实现分布式的统一控制，以下记录了插件编写过程</p></blockquote><h2 id="一、CoreDNS-简介"><a href="#一、CoreDNS-简介" class="headerlink" title="一、CoreDNS 简介"></a>一、CoreDNS 简介</h2><p>CoreDNS 目前是 CNCF 旗下的项目(已毕业)，为 Kubernetes 等云原生环境提供可靠的 DNS 服务发现等功能；官网的描述只有一句话: <strong>CoreDNS: DNS and Service Discovery</strong>，而实际上分析源码以后发现 CoreDNS 实际上是基于 Caddy (一个现代化的负载均衡器)而开发的，通过插件式注入，并监听 TCP/UDP 端口提供 DNS 服务；<strong>得益于 Caddy 的插件机制，CoreDNS 支持自行编写插件，拦截 DNS 请求然后处理，</strong>通过这个插件机制你可以在 CoreDNS 上实现各种功能，比如构建分布式一致性的 DNS 集群、动态的 DNS 负载均衡等等</p><h2 id="二、CoreDNS-插件规范"><a href="#二、CoreDNS-插件规范" class="headerlink" title="二、CoreDNS 插件规范"></a>二、CoreDNS 插件规范</h2><h3 id="2-1、插件模式"><a href="#2-1、插件模式" class="headerlink" title="2.1、插件模式"></a>2.1、插件模式</h3><p>CoreDNS 插件编写目前有两种方式:</p><ul><li>深度耦合 CoreDNS，使用 Go 编写插件，直接编译进 CoreDNS 二进制文件</li><li>通过 GRPC 解耦，任意语言编写 GRPC 接口实现，CoreDNS 通过 GRPC 与插件交互</li></ul><p>由于 GRPC 链接实际上借助于 CoreDNS 的 GRPC 插件，同时 GRPC 会有网络开销，TCP 链接不稳定可能造成 DNS 响应过慢等问题，所以本文只介绍如何使用 Go 编写 CoreDNS 的插件，这种插件将直接编译进 CoreDNS 二进制文件中</p><h3 id="2-2、插件注册"><a href="#2-2、插件注册" class="headerlink" title="2.2、插件注册"></a>2.2、插件注册</h3><p>在通常情况下，插件中应当包含一个 <code>setup.go</code> 文件，这个文件的 <code>init</code> 方法调用插件注册，类似这样</p><div class="hljs code-wrapper"><pre><code class="hljs go"><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-title">init</span><span class="hljs-params">()</span></span> &#123;     plugin.Register(<span class="hljs-string">&quot;gdns&quot;</span>, setup) &#125;</code></pre></div><p>注册方法的第一个参数是插件名称，第二个是一个 func，func 签名如下</p><div class="hljs code-wrapper"><pre><code class="hljs go"><span class="hljs-comment">// SetupFunc is used to set up a plugin, or in other words,</span><span class="hljs-comment">// execute a directive. It will be called once per key for</span><span class="hljs-comment">// each server block it appears in.</span><span class="hljs-keyword">type</span> SetupFunc <span class="hljs-function"><span class="hljs-keyword">func</span><span class="hljs-params">(c *Controller)</span> <span class="hljs-title">error</span></span></code></pre></div><p><strong>在这个 SetupFunc 中，插件编写者应当通过 <code>*Controller</code> 拿到 CoreDNS 的配置并解析它，从而完成自己插件的初始化配置；</strong>比如你的插件需要连接 Etcd，那么在这个方法里你要通过 <code>*Controller</code> 遍历配置，拿到 Etcd 的地址、证书、用户名密码配置等信息；</p><p>如果配置信息没有问题，该插件应当初始化完成；如果有问题就报错退出，然后整个 CoreDNS 启动失败；如果插件初始化完成，最后不要忘记将自己的插件加入到整个插件链路中(CoreDNS 根据情况逐个调用)</p><div class="hljs code-wrapper"><pre><code class="hljs go"><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-title">setup</span><span class="hljs-params">(c *caddy.Controller)</span> <span class="hljs-title">error</span></span> &#123;e, err := etcdParse(c)<span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;<span class="hljs-keyword">return</span> plugin.Error(<span class="hljs-string">&quot;gdns&quot;</span>, err)&#125;dnsserver.GetConfig(c).AddPlugin(<span class="hljs-function"><span class="hljs-keyword">func</span><span class="hljs-params">(next plugin.Handler)</span> <span class="hljs-title">plugin</span>.<span class="hljs-title">Handler</span></span> &#123;e.Next = next<span class="hljs-keyword">return</span> e&#125;)<span class="hljs-keyword">return</span> <span class="hljs-literal">nil</span>&#125;</code></pre></div><h3 id="2-3、插件结构体"><a href="#2-3、插件结构体" class="headerlink" title="2.3、插件结构体"></a>2.3、插件结构体</h3><p>一般来说，每一个插件都会定义一个结构体，<strong>结构体中包含必要的 CoreDNS 内置属性，以及当前插件特性的相关配置；</strong>一个样例的插件结构体如下所示</p><div class="hljs code-wrapper"><pre><code class="hljs go"><span class="hljs-keyword">type</span> GDNS <span class="hljs-keyword">struct</span> &#123;  <span class="hljs-comment">// Next 属性在 Setup 之后会被设置到下一个插件的引用，以便在本插件解析失败后可以交由下面的插件继续解析</span>Next       plugin.Handler<span class="hljs-comment">// Fall 列表用来控制哪些域名的请求解析失败后可以继续穿透到下一个插件重新处理</span>Fall       fall.F<span class="hljs-comment">// Zones 表示当前插件应该 case 哪些域名的 DNS 请求</span>Zones      []<span class="hljs-keyword">string</span><span class="hljs-comment">// PathPrefix 和 Client 就是插件本身的业务属性了，由于插件要连 Etcd</span><span class="hljs-comment">// PathPrefix 就是 Etcd 目录前缀，Client 是一个 Etcd 的 client</span><span class="hljs-comment">// endpoints 是 Etcd api 端点的地址</span>PathPrefix <span class="hljs-keyword">string</span>Client     *etcdcv3.Clientendpoints []<span class="hljs-keyword">string</span> <span class="hljs-comment">// Stored here as well, to aid in testing.</span>&#125;</code></pre></div><h3 id="2-4、插件接口"><a href="#2-4、插件接口" class="headerlink" title="2.4、插件接口"></a>2.4、插件接口</h3><p>一个 Go 编写的 CoreDNS 插件实际上只需要实现一个 <code>Handler</code> 接口既可，接口定义如下</p><div class="hljs code-wrapper"><pre><code class="hljs go"><span class="hljs-comment">// Handler is like dns.Handler except ServeDNS may return an rcode</span><span class="hljs-comment">// and/or error.</span><span class="hljs-comment">//</span><span class="hljs-comment">// If ServeDNS writes to the response body, it should return a status</span><span class="hljs-comment">// code. CoreDNS assumes *no* reply has yet been written if the status</span><span class="hljs-comment">// code is one of the following:</span><span class="hljs-comment">//</span><span class="hljs-comment">// * SERVFAIL (dns.RcodeServerFailure)</span><span class="hljs-comment">//</span><span class="hljs-comment">// * REFUSED (dns.RecodeRefused)</span><span class="hljs-comment">//</span><span class="hljs-comment">// * FORMERR (dns.RcodeFormatError)</span><span class="hljs-comment">//</span><span class="hljs-comment">// * NOTIMP (dns.RcodeNotImplemented)</span><span class="hljs-comment">//</span><span class="hljs-comment">// All other response codes signal other handlers above it that the</span><span class="hljs-comment">// response message is already written, and that they should not write</span><span class="hljs-comment">// to it also.</span><span class="hljs-comment">//</span><span class="hljs-comment">// If ServeDNS encounters an error, it should return the error value</span><span class="hljs-comment">// so it can be logged by designated error-handling plugin.</span><span class="hljs-comment">//</span><span class="hljs-comment">// If writing a response after calling another ServeDNS method, the</span><span class="hljs-comment">// returned rcode SHOULD be used when writing the response.</span><span class="hljs-comment">//</span><span class="hljs-comment">// If handling errors after calling another ServeDNS method, the</span><span class="hljs-comment">// returned error value SHOULD be logged or handled accordingly.</span><span class="hljs-comment">//</span><span class="hljs-comment">// Otherwise, return values should be propagated down the plugin</span><span class="hljs-comment">// chain by returning them unchanged.</span>Handler <span class="hljs-keyword">interface</span> &#123;ServeDNS(context.Context, dns.ResponseWriter, *dns.Msg) (<span class="hljs-keyword">int</span>, error)Name() <span class="hljs-keyword">string</span>&#125;</code></pre></div><ul><li><code>ServeDNS</code> 方法是插件需要实现的主要逻辑方法，DNS 请求接受后会从这个方法传入，插件编写者需要实现查询并返回结果</li><li><code>Name</code> 方法只返回一个插件名称标识，具体作用记不太清楚，好像是为了判断插件命名唯一性然后做链式顺序调用的，原则只要你不跟系统插件重名就行</li></ul><p><strong>基本逻辑就是在 setup 阶段通过配置文件创建你的插件结构体对象；然后插件结构体实现这个 <code>Handler</code> 接口，运行期 CoreDNS 会调用接口的 <code>ServeDNS</code> 方法来向插件查询 DNS 请求</strong></p><h3 id="2-5、ServeDNS-方法"><a href="#2-5、ServeDNS-方法" class="headerlink" title="2.5、ServeDNS 方法"></a>2.5、ServeDNS 方法</h3><p>ServeDNS 方法入参有 3 个:</p><ul><li><code>context.Context</code> 用来控制超时等情况的 context</li><li><code>dns.ResponseWriter</code> 插件通过这个对象写入对 Client DNS 请求的响应结果</li><li><code>*dns.Msg</code> 这个是 Client 发起的 DNS 请求，插件负责处理它，比如当你发现请求类型是 <code>AAAA</code> 而你的插件又不想去支持时要如何返回结果</li></ul><p>对于返回结果，插件编写者应当通过 <code>dns.ResponseWriter.WriteMsg</code> 方法写入返回结果，基本代码如下</p><div class="hljs code-wrapper"><pre><code class="hljs go"><span class="hljs-comment">// ServeDNS implements the plugin.Handler interface.</span><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(gDNS *GDNS)</span> <span class="hljs-title">ServeDNS</span><span class="hljs-params">(ctx context.Context, w dns.ResponseWriter, r *dns.Msg)</span> <span class="hljs-params">(<span class="hljs-keyword">int</span>, error)</span></span> &#123;<span class="hljs-comment">// ...... 这里应当实现你的业务逻辑，查找相应的 DNS 记录</span><span class="hljs-comment">// 最后通过 new 一个 dns.Msg 作为返回结果</span>resp := <span class="hljs-built_in">new</span>(dns.Msg)resp.SetReply(r)resp.Authoritative = <span class="hljs-literal">true</span><span class="hljs-comment">// records 是真正的记录结果，应当在业务逻辑区准备好</span>resp.Answer = <span class="hljs-built_in">append</span>(resp.Answer, records...)<span class="hljs-comment">// 返回结果</span>err = w.WriteMsg(resp)<span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;log.Error(err)&#125;   <span class="hljs-comment">// 告诉 CoreDNS 是否处理成功</span><span class="hljs-keyword">return</span> dns.RcodeSuccess, <span class="hljs-literal">nil</span>&#125;</code></pre></div><p><strong>需要注意的是，无论根据业务逻辑是否查询到 DNS 记录，都要返回响应结果(没有就返回空)，错误或者未返回将会导致 Client 端查询 DNS 超时，然后不断重试，最终可能导致 Client 端服务故障</strong></p><h3 id="2-6、Name-方法"><a href="#2-6、Name-方法" class="headerlink" title="2.6、Name 方法"></a>2.6、Name 方法</h3><p><code>Name</code> 方法非常简单，只需要返回当前插件名称既可；该方法的作用是为了其他插件判断本插件是否加载等情况</p><div class="hljs code-wrapper"><pre><code class="hljs go"><span class="hljs-comment">// Name implements the Handler interface.</span><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(gDNS *GDNS)</span> <span class="hljs-title">Name</span><span class="hljs-params">()</span> <span class="hljs-title">string</span></span> &#123; <span class="hljs-keyword">return</span> <span class="hljs-string">&quot;gdns&quot;</span> &#125;</code></pre></div><h2 id="三、CoreDNS-插件处理"><a href="#三、CoreDNS-插件处理" class="headerlink" title="三、CoreDNS 插件处理"></a>三、CoreDNS 插件处理</h2><p>对于实际的业务处理，可以通过 <code>case</code> 请求 <code>QType</code> 来做具体的业务实现</p><div class="hljs code-wrapper"><pre><code class="hljs go"><span class="hljs-comment">// ServeDNS implements the plugin.Handler interface.</span><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(gDNS *GDNS)</span> <span class="hljs-title">ServeDNS</span><span class="hljs-params">(ctx context.Context, w dns.ResponseWriter, r *dns.Msg)</span> <span class="hljs-params">(<span class="hljs-keyword">int</span>, error)</span></span> &#123;state := request.Request&#123;W: w, Req: r&#125;zone := plugin.Zones(gDNS.Zones).Matches(state.Name())<span class="hljs-keyword">if</span> zone == <span class="hljs-string">&quot;&quot;</span> &#123;<span class="hljs-keyword">return</span> plugin.NextOrFailure(gDNS.Name(), gDNS.Next, ctx, w, r)&#125;<span class="hljs-comment">// ...业务处理</span><span class="hljs-keyword">switch</span> state.QType() &#123;<span class="hljs-keyword">case</span> dns.TypeA:<span class="hljs-comment">// A 记录查询业务逻辑</span><span class="hljs-keyword">case</span> dns.TypeAAAA:<span class="hljs-comment">// AAAA 记录查询业务逻辑</span><span class="hljs-keyword">default</span>:<span class="hljs-keyword">return</span> <span class="hljs-literal">false</span>resp := <span class="hljs-built_in">new</span>(dns.Msg)resp.SetReply(r)resp.Authoritative = <span class="hljs-literal">true</span>resp.Answer = <span class="hljs-built_in">append</span>(resp.Answer, records...)err = w.WriteMsg(resp)<span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;log.Error(err)&#125;<span class="hljs-keyword">return</span> dns.RcodeSuccess, <span class="hljs-literal">nil</span>&#125;</code></pre></div><h2 id="四、插件编译及测试"><a href="#四、插件编译及测试" class="headerlink" title="四、插件编译及测试"></a>四、插件编译及测试</h2><h3 id="4-1、官方标准操作"><a href="#4-1、官方标准操作" class="headerlink" title="4.1、官方标准操作"></a>4.1、官方标准操作</h3><p>根据官方文档的描述，当你编写好插件以后，<strong>你的插件应当提交到一个 Git 仓库中，可以使 Github 等(保证可以 <code>go get</code> 拉取就行)，然后修改 <code>plugin.cfg</code>，最后执行 <code>make</code> 既可</strong>；具体修改如下所示</p><p><img src="https://cdn.oss.link/markdown/vey4u.png" alt="plugin.cfg"></p><p><strong>值得注意的是: 插件配置在 <code>plugin.cfg</code> 内的顺序决定了插件的执行顺序；通俗的讲，如果 Client 的一个 DNS 请求进来，CoreDNS 根据你在 <code>plugin.cfg</code> 内书写的顺序依次调用，而并非 <code>Corefile</code> 内的配置顺序</strong></p><p>配置好以后直接执行 <code>make</code> 既可编译成功一个包含自定义插件的 CoreDNS 二进制文件(编译过程的 <code>go mod</code> 下载加速问题不在本文讨论范围内)；你可以直接通过这个二进制测试插件的处理情况，当然这种测试不够直观，而且频繁修改由于 <code>go mod</code> 缓存等原因并不一定能保证每次编译的都包含最新插件代码，所以另一种方式请看下一章节</p><h3 id="4-2、经验性的操作"><a href="#4-2、经验性的操作" class="headerlink" title="4.2、经验性的操作"></a>4.2、经验性的操作</h3><p>根据个人测试以及对源码的分析，在修改 <code>plugin.cfg</code> 然后执行 <code>make</code> 命令后，实际上是进行了代码生成；当你通过 git 命令查看相关修改文件时，整个插件加载体系便没什么秘密可言了；<strong>在整个插件体系中，插件加载是通过 <code>init</code> 方法注册的，那么既然用 go 写插件，那么应该清楚 <code>init</code> 方法只有在包引用之后才会执行，所以整个插件体系实际上是这样事儿的:</strong></p><p>首先 <code>make</code> 以后会修改 <code>core/plugin/zplugin.go</code> 文件，这个文件啥也不干，就是 <code>import</code> 来实现调用对应包的 <code>init</code> 方法</p><p><img src="https://cdn.oss.link/markdown/ny1rz.png" alt="zplugin.go"></p><p>当 <code>init</code> 执行后你去追源码，实际上就是 Caddy 维护了一个 <code>map[string]Plugin</code>，<code>init</code> 会把你的插件 func 塞进去然后后面再调用，实现一个懒加载或者说延迟初始化</p><p><img src="https://cdn.oss.link/markdown/idno4.png" alt="caddy_plugin"></p><p>接着修改了一下 <code>core/dnsserver/zdirectives.go</code>，这个里面也没啥，就是一个 <code>[]string</code>，<strong>但是 <code>[]string</code> 这玩意有顺序啊，这就是为什么你在 <code>plugin.cfg</code> 里写的顺序决定了插件处理顺序的原因(因为生成的这个切片有顺序)</strong></p><p><img src="https://cdn.oss.link/markdown/bixos.png" alt="zdirectives.go"></p><p>综上所述，实际上 <code>make</code> 命令一共修改了两个文件，如果想在 IDE 内直接 debug CoreDNS + Plugin 源码，那么只需要这样做:</p><p>复制自己编写的插件目录到 <code>plugin</code> 目录，类似这样</p><p><img src="https://cdn.oss.link/markdown/whwuy.png" alt="gdns"></p><p>手动修改 <code>core/plugin/zplugin.go</code>，加入自己插件的 <code>import</code>(此时你直接复制系统其他插件，改一下目录名既可)</p><p><img src="https://cdn.oss.link/markdown/g7wp0.png" alt="update_zplugin"></p><p>手动修改 <code>core/dnsserver/zdirectives.go</code> 把自己插件名称写进去(自己控制顺序)，然后 debug 启动 <code>coredns.go</code> 里面的 main 方法测试既可</p><p><img src="https://cdn.oss.link/markdown/4ucqg.png" alt="coredns.go"></p><h2 id="五、本文参考"><a href="#五、本文参考" class="headerlink" title="五、本文参考"></a>五、本文参考</h2><ul><li>Writing Plugins for CoreDNS: <a href="https://coredns.io/2016/12/19/writing-plugins-for-coredns">https://coredns.io/2016/12/19/writing-plugins-for-coredns</a></li><li>how-to-add-plugins.md: <a href="https://github.com/coredns/coredns.io/blob/master/content/blog/how-to-add-plugins.md">https://github.com/coredns/coredns.io/blob/master/content/blog/how-to-add-plugins.md</a></li><li>example plugin: <a href="https://github.com/coredns/example">https://github.com/coredns/example</a></li></ul>]]></content>
    
    
    <summary type="html">目前测试环境中有很多个 DNS 服务器，不同项目组使用的 DNS 服务器不同，但是不可避免的他们会访问一些公共域名；老的 DNS 服务器都是 dnsmasq，改起来很麻烦，最近研究了一下 CoreDNS，通过编写插件的方式可以实现让多个 CoreDNS 实例实现分布式的统一控制，以下记录了插件编写过程</summary>
    
    
    
    <category term="Golang" scheme="https://mritd.com/categories/golang/"/>
    
    
    <category term="Kubernetes" scheme="https://mritd.com/tags/kubernetes/"/>
    
    <category term="Golang" scheme="https://mritd.com/tags/golang/"/>
    
    <category term="CoreDNS" scheme="https://mritd.com/tags/coredns/"/>
    
  </entry>
  
  <entry>
    <title>Golang Etcd Client Example</title>
    <link href="https://mritd.com/2019/10/15/golang-etcd-client-example/"/>
    <id>https://mritd.com/2019/10/15/golang-etcd-client-example/</id>
    <published>2019-10-15T04:21:07.000Z</published>
    <updated>2019-10-15T04:21:07.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>准备开发点东西，需要用到 Etcd，由于生产 Etcd 全部开启了 TLS 加密，所以客户端需要相应修改，以下为 Golang 链接 Etcd 并且使用客户端证书验证的样例代码</p></blockquote><h2 id="API-V2"><a href="#API-V2" class="headerlink" title="API V2"></a>API V2</h2><div class="hljs code-wrapper"><pre><code class="hljs go"><span class="hljs-keyword">package</span> main<span class="hljs-keyword">import</span> (<span class="hljs-string">&quot;context&quot;</span><span class="hljs-string">&quot;crypto/tls&quot;</span><span class="hljs-string">&quot;crypto/x509&quot;</span><span class="hljs-string">&quot;io/ioutil&quot;</span><span class="hljs-string">&quot;log&quot;</span><span class="hljs-string">&quot;net&quot;</span><span class="hljs-string">&quot;net/http&quot;</span><span class="hljs-string">&quot;time&quot;</span><span class="hljs-string">&quot;go.etcd.io/etcd/client&quot;</span>)<span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-title">main</span><span class="hljs-params">()</span></span> &#123;<span class="hljs-comment">// 为了保证 HTTPS 链接可信，需要预先加载目标证书签发机构的 CA 根证书</span>etcdCA, err := ioutil.ReadFile(<span class="hljs-string">&quot;/Users/mritd/tmp/etcd_ssl/etcd-root-ca.pem&quot;</span>)<span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;log.Fatal(err)&#125;<span class="hljs-comment">// etcd 启用了双向 TLS 认证，所以客户端证书同样需要加载</span>etcdClientCert, err := tls.LoadX509KeyPair(<span class="hljs-string">&quot;/Users/mritd/tmp/etcd_ssl/etcd.pem&quot;</span>, <span class="hljs-string">&quot;/Users/mritd/tmp/etcd_ssl/etcd-key.pem&quot;</span>)<span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;log.Fatal(err)&#125;<span class="hljs-comment">// 创建一个空的 CA Pool</span><span class="hljs-comment">// 因为后续只会链接 Etcd 的 api 端点，所以此处选择使用空的 CA Pool，然后只加入 Etcd CA 既可</span><span class="hljs-comment">// 如果期望链接其他 TLS 端点，那么最好使用 x509.SystemCertPool() 方法先 copy 一份系统根 CA</span><span class="hljs-comment">// 然后再向这个 Pool 中添加自定义 CA</span>rootCertPool := x509.NewCertPool()rootCertPool.AppendCertsFromPEM(etcdCA)cfg := client.Config&#123;<span class="hljs-comment">// Etcd HTTPS api 端点</span>Endpoints: []<span class="hljs-keyword">string</span>&#123;<span class="hljs-string">&quot;https://172.16.14.114:2379&quot;</span>&#125;,<span class="hljs-comment">// 自定义 Transport 实现自签 CA 加载以及 Client Cert 加载</span><span class="hljs-comment">// 其他参数最好从 client.DefaultTranspor copy，以保证与默认 client 相同的行为</span>Transport: &amp;http.Transport&#123;Proxy: http.ProxyFromEnvironment,<span class="hljs-comment">// Dial 方法已被启用，采用新的 DialContext 设置超时</span>DialContext: (&amp;net.Dialer&#123;KeepAlive: <span class="hljs-number">30</span> * time.Second,Timeout:   <span class="hljs-number">30</span> * time.Second,&#125;).DialContext,<span class="hljs-comment">// 自定义 CA 及 Client Cert 配置</span>TLSClientConfig: &amp;tls.Config&#123;RootCAs:      rootCertPool,Certificates: []tls.Certificate&#123;etcdClientCert&#125;,&#125;,TLSHandshakeTimeout: <span class="hljs-number">10</span> * time.Second,&#125;,<span class="hljs-comment">// set timeout per request to fail fast when the target endpoint is unavailable</span>HeaderTimeoutPerRequest: time.Second,&#125;c, err := client.New(cfg)<span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;log.Fatal(err)&#125;kapi := client.NewKeysAPI(c)<span class="hljs-comment">// set &quot;/foo&quot; key with &quot;bar&quot; value</span>log.Print(<span class="hljs-string">&quot;Setting &#x27;/foo&#x27; key with &#x27;bar&#x27; value&quot;</span>)resp, err := kapi.Set(context.Background(), <span class="hljs-string">&quot;/foo&quot;</span>, <span class="hljs-string">&quot;bar&quot;</span>, <span class="hljs-literal">nil</span>)<span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;log.Fatal(err)&#125; <span class="hljs-keyword">else</span> &#123;<span class="hljs-comment">// print common key info</span>log.Printf(<span class="hljs-string">&quot;Set is done. Metadata is %q\n&quot;</span>, resp)&#125;<span class="hljs-comment">// get &quot;/foo&quot; key&#x27;s value</span>log.Print(<span class="hljs-string">&quot;Getting &#x27;/foo&#x27; key value&quot;</span>)resp, err = kapi.Get(context.Background(), <span class="hljs-string">&quot;/foo&quot;</span>, <span class="hljs-literal">nil</span>)<span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;log.Fatal(err)&#125; <span class="hljs-keyword">else</span> &#123;<span class="hljs-comment">// print common key info</span>log.Printf(<span class="hljs-string">&quot;Get is done. Metadata is %q\n&quot;</span>, resp)<span class="hljs-comment">// print value</span>log.Printf(<span class="hljs-string">&quot;%q key has %q value\n&quot;</span>, resp.Node.Key, resp.Node.Value)&#125;&#125;</code></pre></div><h2 id="API-V3"><a href="#API-V3" class="headerlink" title="API V3"></a>API V3</h2><div class="hljs code-wrapper"><pre><code class="hljs go"><span class="hljs-keyword">package</span> main<span class="hljs-keyword">import</span> (<span class="hljs-string">&quot;context&quot;</span><span class="hljs-string">&quot;crypto/tls&quot;</span><span class="hljs-string">&quot;crypto/x509&quot;</span><span class="hljs-string">&quot;io/ioutil&quot;</span><span class="hljs-string">&quot;log&quot;</span><span class="hljs-string">&quot;time&quot;</span><span class="hljs-string">&quot;go.etcd.io/etcd/clientv3&quot;</span>)<span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-title">main</span><span class="hljs-params">()</span></span> &#123;<span class="hljs-comment">// 为了保证 HTTPS 链接可信，需要预先加载目标证书签发机构的 CA 根证书</span>etcdCA, err := ioutil.ReadFile(<span class="hljs-string">&quot;/Users/mritd/tmp/etcd_ssl/etcd-root-ca.pem&quot;</span>)<span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;log.Fatal(err)&#125;<span class="hljs-comment">// etcd 启用了双向 TLS 认证，所以客户端证书同样需要加载</span>etcdClientCert, err := tls.LoadX509KeyPair(<span class="hljs-string">&quot;/Users/mritd/tmp/etcd_ssl/etcd.pem&quot;</span>, <span class="hljs-string">&quot;/Users/mritd/tmp/etcd_ssl/etcd-key.pem&quot;</span>)<span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;log.Fatal(err)&#125;<span class="hljs-comment">// 创建一个空的 CA Pool</span><span class="hljs-comment">// 因为后续只会链接 Etcd 的 api 端点，所以此处选择使用空的 CA Pool，然后只加入 Etcd CA 既可</span><span class="hljs-comment">// 如果期望链接其他 TLS 端点，那么最好使用 x509.SystemCertPool() 方法先 copy 一份系统根 CA</span><span class="hljs-comment">// 然后再向这个 Pool 中添加自定义 CA</span>rootCertPool := x509.NewCertPool()rootCertPool.AppendCertsFromPEM(etcdCA)<span class="hljs-comment">// 创建 api v3 的 client</span>cli, err := clientv3.New(clientv3.Config&#123;<span class="hljs-comment">// etcd https api 端点</span>Endpoints:   []<span class="hljs-keyword">string</span>&#123;<span class="hljs-string">&quot;https://172.16.14.114:2379&quot;</span>&#125;,DialTimeout: <span class="hljs-number">5</span> * time.Second,<span class="hljs-comment">// 自定义 CA 及 Client Cert 配置</span>TLS: &amp;tls.Config&#123;RootCAs:      rootCertPool,Certificates: []tls.Certificate&#123;etcdClientCert&#125;,&#125;,&#125;)<span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;log.Fatal(err)&#125;<span class="hljs-keyword">defer</span> <span class="hljs-function"><span class="hljs-keyword">func</span><span class="hljs-params">()</span></span> &#123; _ = cli.Close() &#125;()ctx, cancel := context.WithTimeout(context.Background(), <span class="hljs-number">3</span>*time.Second)putResp, err := cli.Put(ctx, <span class="hljs-string">&quot;sample_key&quot;</span>, <span class="hljs-string">&quot;sample_value&quot;</span>)<span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;log.Fatal(err)&#125; <span class="hljs-keyword">else</span> &#123;log.Println(putResp)&#125;cancel()ctx, cancel = context.WithTimeout(context.Background(), <span class="hljs-number">3</span>*time.Second)delResp, err := cli.Delete(ctx, <span class="hljs-string">&quot;sample_key&quot;</span>)<span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;log.Fatal(err)&#125; <span class="hljs-keyword">else</span> &#123;log.Println(delResp)&#125;cancel()&#125;</code></pre></div>]]></content>
    
    
    <summary type="html">准备开发点东西，需要用到 Etcd，由于生产 Etcd 全部开启了 TLS 加密，所以客户端需要相应修改，以下为 Golang 链接 Etcd 并且使用客户端证书验证的样例代码</summary>
    
    
    
    <category term="Golang" scheme="https://mritd.com/categories/golang/"/>
    
    
    <category term="Golang" scheme="https://mritd.com/tags/golang/"/>
    
    <category term="etcd" scheme="https://mritd.com/tags/etcd/"/>
    
  </entry>
  
  <entry>
    <title>Podman 初试 - 容器发展史</title>
    <link href="https://mritd.com/2019/06/26/podman-history-of-container/"/>
    <id>https://mritd.com/2019/06/26/podman-history-of-container/</id>
    <published>2019-06-26T15:22:49.000Z</published>
    <updated>2019-06-26T15:22:49.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>这是一篇纯介绍性文章，本文不包含任何技术层面的操作，本文仅作为后续 Podman 文章铺垫；本文细节部份并未阐述，很多地方并不详实(一家只谈，不可轻信)。</p></blockquote><h2 id="一、缘起"><a href="#一、缘起" class="headerlink" title="一、缘起"></a>一、缘起</h2><h3 id="1-1、鸿蒙"><a href="#1-1、鸿蒙" class="headerlink" title="1.1、鸿蒙"></a>1.1、鸿蒙</h3><p>在上古时期，天地初开，一群称之为 “运维” 的人们每天在一种叫作 “服务器” 的神秘盒子中创造属于他们的世界；他们在这个世界中每日劳作，一遍又一遍的写入他们的历史，比如搭建一个 nginx、布署一个 java web 应用…</p><p>大多数人其实并没有那么聪明，他们所 “创造” 的事实上可能是有人已经创造过的东西，他们可能每天都在做着重复的劳动；久而久之，一些人厌倦了、疲惫了…又过了一段时间，一些功力深厚的老前辈创造了一些批量布署工具来帮助人们做一些重复性的劳动，这些工具被起名为 “Asible”、”Chef”、”Puppet” 等等…</p><p>而随着时代的发展，”世界” 变得越来越复杂，运维们需要处理的事情越来越多，比如各种网络、磁盘环境的隔离，各种应用服务的高可用…在时代的洪流下，运维们急需要一种简单高效的布署工具，既能有一定的隔离性，又能方便使用，并且最大程度降低重复劳动来提升效率。</p><h3 id="1-2、创世"><a href="#1-2、创世" class="headerlink" title="1.2、创世"></a>1.2、创世</h3><p>在时代洪流的冲击下，一位名为 “Solomon Hykes” 的人异军突起，他创造了一个称之为 Docker 的工具，Docker 被创造以后就以灭世之威向运维们展示了它的强大；一个战斗力只有 5 的运维只需要学习 Docker 很短时间就可以完成资深运维们才能完成的事情，在某些情况下以前需要 1 天才能完成的工作使用 Docker 后几分钟就可以完成；此时运维们已经意识到 “新的时代” 开启了，接下来 Docker 开源并被整个运维界人们使用，Docker 也不断地完善增加各种各样的功能，此后世界正式进入 “容器纪元”。</p><h2 id="二、纷争"><a href="#二、纷争" class="headerlink" title="二、纷争"></a>二、纷争</h2><h3 id="2-1、发展"><a href="#2-1、发展" class="headerlink" title="2.1、发展"></a>2.1、发展</h3><p>随着 Docker 的日益成熟，一些人开始在 Docker 之上创造更加强大的工具，一些人开始在 Docker 之下为其提供更稳定的运行环境…</p><p>其中一个叫作 Google 的公司在 Docker 之上创建了名为 “Kubernetes” 的工具，Kubernetes 操纵 Docker 完成更加复杂的任务；Kubernetes 的出现更加印证了 Docker 的强大，以及 “容器纪元” 的发展正确性。</p><h3 id="2-2、野心"><a href="#2-2、野心" class="headerlink" title="2.2、野心"></a>2.2、野心</h3><p>当然这是一个充满利益的世界，Google 公司创造 Kubernetes 是可以为他们带来利益的，比如他们可以让 Kubernetes 深度适配他们的云平台，以此来增加云平台的销量等；此时 Docker 创始人也成立了一个公司，提供 Docker 的付费服务以及深度定制等；不过值得一提的是 Docker 公司提供的付费服务始终没有 Kubernetes 为 Google 公司带来的利益高，所以在利益的驱使下，Docker 公司开始动起了歪心思: <strong>创造一个 Kubernetes 的替代品，利用用户粘度复制 Kubernetes 的成功，从 Google 嘴里抢下这块蛋糕！</strong>此时 Docker 公司只想把蛋糕抢过来，但是他们根本没有在意到暗中一群人创造了一个叫 “rkt” 的东西也在妄图夺走他们嘴里的蛋糕。</p><h3 id="2-3、冲突"><a href="#2-3、冲突" class="headerlink" title="2.3、冲突"></a>2.3、冲突</h3><p>在一段时间的沉默后，Docker 公司又创造了 “Swarm” 这个工具，妄图夺走 Google 公司利用 Kubernetes 赢来的蛋糕；当然，Google 这个公司极其庞大，人数众多，而且在这个社会有很大的影响地位…</p><p>终于，巨人苏醒了，Google 联合了 Redhat、Microsoft、IBM、Intel、Cisco 等公司决定对这个爱动歪脑筋的 Docker 公司进行制裁；当然制裁的手段不能过于暴力，那样会让别人落下把柄，成为别人的笑料，被人所不耻；<strong>最总他们决定制订规范，成立组织，明确规定 Docker 的角色，以及它应当拥有的能力，这些规范包括但不限于 <code>CRI</code>、<code>CNI</code> 等；自此之后各大公司宣布他们容器相关的工具只兼容 CRI 等相关标准，无论是 Docker 还是 rkt 等工具，只要实现了这些标准，就可以配合这些容器工具进行使用</strong>。</p><h2 id="三、成败"><a href="#三、成败" class="headerlink" title="三、成败"></a>三、成败</h2><p>自此之后，Docker 跌下神坛，各路大神纷纷创造满足 CRI 等规范的工具用来取代 Docker，Docker 丢失了往日一家独大的场面，最终为了顺应时代发展，拆分自己成为模块化组件；这些模块化组件被放置在 <a href="https://mobyproject.org/">mobyproject</a> 中方便其他人重复利用。</p><p>时至今日，虽然 Docker 已经不负以前，但是仍然是容器化首选工具，因为 Docker 是一个完整的产品，它可以提供除了满足 CRI 等标准以外更加方便的功能；但是制裁并非没有结果，Google 公司借此创造了 cri-o 用来满足 CRI 标准，其他公司也相应创建了对应的 CRI 实现；<strong>为了进一步分化 Docker 势力，一个叫作 Podman 的工具被创建，它以 cri-o 为基础，兼容大部份 Docker 命令的方式开始抢夺 Dcoker 用户</strong>；到目前为止 Podman 已经可以在大部份功能上替代 Docker。</p>]]></content>
    
    
    <summary type="html">这是一篇纯介绍性文章，本文不包含任何技术层面的操作，本文仅作为后续 Podman 文章铺垫；本文细节部份并未阐述，很多地方并不详实(一家只谈，不可轻信)。</summary>
    
    
    
    <category term="Docker" scheme="https://mritd.com/categories/docker/"/>
    
    
    <category term="Docker" scheme="https://mritd.com/tags/docker/"/>
    
    <category term="Podman" scheme="https://mritd.com/tags/podman/"/>
    
  </entry>
  
  <entry>
    <title>Calico 3.6 转发外部流量到集群 Pod</title>
    <link href="https://mritd.com/2019/06/18/calico-3.6-forward-network-traffic/"/>
    <id>https://mritd.com/2019/06/18/calico-3.6-forward-network-traffic/</id>
    <published>2019-06-18T14:20:54.000Z</published>
    <updated>2019-06-18T14:20:54.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>由于开发有部份服务使用 GRPC 进行通讯，同时采用 Consul 进行服务发现；在微服务架构下可能会导致一些访问问题，目前解决方案就是打通开发环境网络与测试环境 Kubernetes 内部 Pod 网络；翻了好多资料发现都是 2.x 的，而目前测试集群 Calico 版本为 3.6.3，很多文档都不适用只能自己折腾，目前折腾完了这里记录一下</p></blockquote><p><strong>本文默认为读者已经存在一个运行正常的 Kubernetes 集群，并且采用 Calico 作为 CNI 组件，且 Calico 工作正常；同时应当在某个节点完成了 calicoctl 命令行工具的配置</strong></p><h2 id="一、问题描述"><a href="#一、问题描述" class="headerlink" title="一、问题描述"></a>一、问题描述</h2><p>在微服务架构下，由于服务组件很多，开发在本地机器想测试应用需要启动整套服务，这对开发机器的性能确实是个考验；但如果直接连接测试环境的服务，由于服务发现问题最终得到的具体服务 IP 是 Kubernetes Pod IP，此 IP 由集群内部 Calico 维护与分配，外部不可访问；最终目标为打通开发环境与集群内部网络，实现开发网络下直连 Pod IP，这或许在以后对生产服务暴露负载均衡有一定帮助意义；目前网络环境如下:</p><p>开发网段: <code>10.10.0.0/24</code><br>测试网段: <code>172.16.0.0/24</code><br>Kubernetes Pod 网段: <code>10.20.0.0/16</code></p><h2 id="二、打通网络"><a href="#二、打通网络" class="headerlink" title="二、打通网络"></a>二、打通网络</h2><p>首先面临的第一个问题是 Calico 处理，因为<strong>如果想要让数据包能从开发网络到达 Pod 网络，那么必然需要测试环境宿主机上的 Calico Node 帮忙转发</strong>；因为 Pod 网络由 Calico 维护，只要 Calico Node 帮忙转发那么数据一定可以到达 Pod IP 上；</p><p>一开始我很天真的认为这就是个 <code>ip route add 10.20.0.0/16 via 172.16.0.13</code> 的问题… 后来发现</p><p><img src="https://cdn.oss.link/markdown/hwp9s.jpg" alt="没那么简单"></p><p>经过翻文档、issue、blog 等最终发现需要进行以下步骤</p><h3 id="2-1、关闭全互联模式"><a href="#2-1、关闭全互联模式" class="headerlink" title="2.1、关闭全互联模式"></a>2.1、关闭全互联模式</h3><p><strong>注意: 关闭全互联时可能导致网络暂时中断，请在夜深人静时操作</strong></p><p>首先执行以下命令查看是否存在默认的 BGP 配置</p><div class="hljs code-wrapper"><pre><code class="hljs sh">calicoctl get bgpconfig default</code></pre></div><p>如果存在则将其保存为配置文件</p><div class="hljs code-wrapper"><pre><code class="hljs sh">calicoctl get bgpconfig default -o yaml &gt; bgp.yaml</code></pre></div><p>修改其中的 <code>spec.nodeToNodeMeshEnabled</code> 为 <code>false</code>，然后进行替换</p><div class="hljs code-wrapper"><pre><code class="hljs sh">calicoctl apply -f bgp.yaml</code></pre></div><p>如果不存在则手动创建一个配置，然后应用</p><div class="hljs code-wrapper"><pre><code class="hljs sh"> cat &lt;&lt; <span class="hljs-string">EOF | calicoctl create -f -</span><span class="hljs-string"> apiVersion: projectcalico.org/v3</span><span class="hljs-string"> kind: BGPConfiguration</span><span class="hljs-string"> metadata:</span><span class="hljs-string">   name: default</span><span class="hljs-string"> spec:</span><span class="hljs-string">   logSeverityScreen: Info</span><span class="hljs-string">   nodeToNodeMeshEnabled: false</span><span class="hljs-string">   asNumber: 63400</span><span class="hljs-string">EOF</span></code></pre></div><p>本部分参考: </p><ul><li><a href="https://docs.projectcalico.org/v3.6/networking/bgp">Disabling the full node-to-node BGP mesh</a></li></ul><h3 id="2-2、开启集群内-RR-模式"><a href="#2-2、开启集群内-RR-模式" class="headerlink" title="2.2、开启集群内 RR 模式"></a>2.2、开启集群内 RR 模式</h3><p>在 Calico 3.3 后支持了集群内节点的 RR 模式，即将某个集群内的 Calico Node 转变为 RR 节点；将某个节点设置为 RR 节点只需要增加 <code>routeReflectorClusterID</code> 既可，为了后面方便配置同时增加了一个 lable 字段 <code>route-reflector: &quot;true&quot;</code></p><div class="hljs code-wrapper"><pre><code class="hljs sh">calicoctl get node CALICO_NODE_NAME -o yaml &gt; node.yaml</code></pre></div><p>然后增加 <code>routeReflectorClusterID</code> 字段，样例如下</p><div class="hljs code-wrapper"><pre><code class="hljs yaml"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">projectcalico.org/v3</span><span class="hljs-attr">kind:</span> <span class="hljs-string">Node</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">annotations:</span>    <span class="hljs-attr">projectcalico.org/kube-labels:</span> <span class="hljs-string">&#x27;&#123;&quot;beta.kubernetes.io/arch&quot;:&quot;amd64&quot;,&quot;beta.kubernetes.io/os&quot;:&quot;linux&quot;,&quot;kubernetes.io/hostname&quot;:&quot;d13.node&quot;,&quot;node-role.kubernetes.io/k8s-master&quot;:&quot;true&quot;&#125;&#x27;</span>  <span class="hljs-attr">creationTimestamp:</span> <span class="hljs-number">2019-06-17T13:55:44Z</span>  <span class="hljs-attr">labels:</span>    <span class="hljs-attr">beta.kubernetes.io/arch:</span> <span class="hljs-string">amd64</span>    <span class="hljs-attr">beta.kubernetes.io/os:</span> <span class="hljs-string">linux</span>    <span class="hljs-attr">kubernetes.io/hostname:</span> <span class="hljs-string">d13.node</span>    <span class="hljs-attr">node-role.kubernetes.io/k8s-master:</span> <span class="hljs-string">&quot;true&quot;</span>    <span class="hljs-attr">route-reflector:</span> <span class="hljs-string">&quot;true&quot;</span>  <span class="hljs-comment"># 增加 lable</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">d13.node</span>  <span class="hljs-attr">resourceVersion:</span> <span class="hljs-string">&quot;61822269&quot;</span>  <span class="hljs-attr">uid:</span> <span class="hljs-string">9a1897e0-9107-11e9-bc1c-90b11c53d1e3</span><span class="hljs-attr">spec:</span>  <span class="hljs-attr">bgp:</span>    <span class="hljs-attr">ipv4Address:</span> <span class="hljs-number">172.16</span><span class="hljs-number">.0</span><span class="hljs-number">.13</span><span class="hljs-string">/19</span>    <span class="hljs-attr">ipv4IPIPTunnelAddr:</span> <span class="hljs-number">10.20</span><span class="hljs-number">.73</span><span class="hljs-number">.82</span>    <span class="hljs-attr">routeReflectorClusterID:</span> <span class="hljs-number">172.16</span><span class="hljs-number">.20</span><span class="hljs-number">.1</span> <span class="hljs-comment"># 添加集群 ID</span>  <span class="hljs-attr">orchRefs:</span>  <span class="hljs-bullet">-</span> <span class="hljs-attr">nodeName:</span> <span class="hljs-string">d13.node</span>    <span class="hljs-attr">orchestrator:</span> <span class="hljs-string">k8s</span></code></pre></div><p><strong>事实上我们应当导出多个 Calico Node 的配置，并将其配置为 RR 节点以进行冗余；对于 <code>routeReflectorClusterID</code> 目前测试只是作为一个 ID(至少在本文是这样的)，所以理论上可以是任何 IP，个人猜测最好在同一集群网络下采用相同的 IP，由于这是真正的测试环境我没有对 ID 做过多的测试(怕玩挂)</strong></p><p>修改完成后只需要应用一下就行</p><div class="hljs code-wrapper"><pre><code class="hljs sh">calicoctl apply -f node.yaml</code></pre></div><p>接下来需要创建对等规则，规则文件如下</p><div class="hljs code-wrapper"><pre><code class="hljs yaml"><span class="hljs-attr">kind:</span> <span class="hljs-string">BGPPeer</span><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">projectcalico.org/v3</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">peer-to-rrs</span><span class="hljs-attr">spec:</span>  <span class="hljs-attr">nodeSelector:</span> <span class="hljs-string">&quot;!has(route-reflector)&quot;</span>  <span class="hljs-attr">peerSelector:</span> <span class="hljs-string">has(route-reflector)</span><span class="hljs-meta">---</span><span class="hljs-attr">kind:</span> <span class="hljs-string">BGPPeer</span><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">projectcalico.org/v3</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">rr-mesh</span><span class="hljs-attr">spec:</span>  <span class="hljs-attr">nodeSelector:</span> <span class="hljs-string">has(route-reflector)</span>  <span class="hljs-attr">peerSelector:</span> <span class="hljs-string">has(route-reflector)</span></code></pre></div><p>假定规则文件名称为 <code>rr.yaml</code>，则创建命令为 <code>calicoctl create -f rr.yaml</code>；此时在 RR 节点上使用 <code>calicoctl node status</code> 应该能看到类似如下输出</p><div class="hljs code-wrapper"><pre><code class="hljs sh">Calico process is running.IPv4 BGP status+--------------+---------------+-------+----------+-------------+| PEER ADDRESS |   PEER TYPE   | STATE |  SINCE   |    INFO     |+--------------+---------------+-------+----------+-------------+| 172.16.0.19  | node specific | up    | 05:43:51 | Established || 172.16.0.16  | node specific | up    | 05:43:51 | Established || 172.16.0.17  | node specific | up    | 05:43:51 | Established || 172.16.0.13  | node specific | up    | 13:01:17 | Established |+--------------+---------------+-------+----------+-------------+IPv6 BGP statusNo IPv6 peers found.</code></pre></div><p><strong><code>PEER ADDRESS</code> 应当包含所有非 RR 节点 IP(由于真实测试环境，以上输出已人为修改)</strong></p><p>同时在非 RR 节点上使用 <code>calicoctl node status</code> 应该能看到以下输出</p><div class="hljs code-wrapper"><pre><code class="hljs sh">Calico process is running.IPv4 BGP status+--------------+---------------+-------+----------+-------------+| PEER ADDRESS |   PEER TYPE   | STATE |  SINCE   |    INFO     |+--------------+---------------+-------+----------+-------------+| 172.16.0.10  | node specific | up    | 05:43:51 | Established || 172.16.0.13  | node specific | up    | 13:01:20 | Established |+--------------+---------------+-------+----------+-------------+IPv6 BGP statusNo IPv6 peers found.</code></pre></div><p><strong><code>PEER ADDRESS</code> 应当包含所有 RR 节点 IP，此时原本的 Pod 网络连接应当已经恢复</strong></p><p>本部分参考:</p><ul><li><a href="https://www.projectcalico.org/how-does-in-cluster-route-reflection-work/">In-cluster Route Reflection</a></li><li><a href="https://docs.projectcalico.org/v3.6/networking/bgp">Configuring in-cluster route reflectors</a></li></ul><h3 id="2-3、调整-IPIP-规则"><a href="#2-3、调整-IPIP-规则" class="headerlink" title="2.3、调整 IPIP 规则"></a>2.3、调整 IPIP 规则</h3><p>先说一下 Calico IPIP 模式的三个可选项:</p><ul><li><code>Always</code>: 永远进行 IPIP 封装(默认)</li><li><code>CrossSubnet</code>: 只在跨网段时才进行 IPIP 封装，适合有 Kubernetes 节点在其他网段的情况，属于中肯友好方案</li><li><code>Never</code>: 从不进行 IPIP 封装，适合确认所有 Kubernetes 节点都在同一个网段下的情况</li></ul><p>在默认情况下，默认的 ipPool 启用了 IPIP 封装(至少通过官方安装文档安装的 Calico 是这样)，并且封装模式为 <code>Always</code>；这也就意味着任何时候都会在原报文上封装新 IP 地址，<strong>在这种情况下将外部流量路由到 RR 节点，RR 节点再转发进行 IPIP 封装时，可能出现网络无法联通的情况(没仔细追查，网络渣，猜测是 Pod 那边得到的源 IP 不对导致的)；</strong>此时我们应当调整 IPIP 封装策略为 <code>CrossSubnet</code></p><p>导出 ipPool 配置</p><div class="hljs code-wrapper"><pre><code class="hljs sh">calicoctl get ippool default-ipv4-ippool -o yaml &gt; ippool.yaml</code></pre></div><p>修改 <code>ipipMode</code> 值为 <code>CrossSubnet</code></p><div class="hljs code-wrapper"><pre><code class="hljs yaml"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">projectcalico.org/v3</span><span class="hljs-attr">kind:</span> <span class="hljs-string">IPPool</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">creationTimestamp:</span> <span class="hljs-number">2019-06-17T13:55:44Z</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">default-ipv4-ippool</span>  <span class="hljs-attr">resourceVersion:</span> <span class="hljs-string">&quot;61858741&quot;</span>  <span class="hljs-attr">uid:</span> <span class="hljs-string">99a82055-9107-11e9-815b-b82a72dffa9f</span><span class="hljs-attr">spec:</span>  <span class="hljs-attr">blockSize:</span> <span class="hljs-number">26</span>  <span class="hljs-attr">cidr:</span> <span class="hljs-number">10.20</span><span class="hljs-number">.0</span><span class="hljs-number">.0</span><span class="hljs-string">/16</span>  <span class="hljs-attr">ipipMode:</span> <span class="hljs-string">CrossSubnet</span>  <span class="hljs-attr">natOutgoing:</span> <span class="hljs-literal">true</span>  <span class="hljs-attr">nodeSelector:</span> <span class="hljs-string">all()</span></code></pre></div><p>重新使用 <code>calicoctl apply -f ippool.yaml</code> 应用既可</p><p>本部分参考:</p><ul><li><a href="https://docs.projectcalico.org/v3.6/networking/ip-in-ip">Configuring IP-in-IP</a></li><li><a href="https://docs.projectcalico.org/v3.6/reference/calicoctl/resources/ippool">IP pool resource</a></li></ul><h3 id="2-4、增加路由联通网络"><a href="#2-4、增加路由联通网络" class="headerlink" title="2.4、增加路由联通网络"></a>2.4、增加路由联通网络</h3><p>万事俱备只欠东风，最后只需要在开发机器添加路由既可</p><p>将 Pod IP <code>10.20.0.0/16</code> 和 Service IP <code>10.254.0.0/16</code> 路由到 RR 节点 <code>172.16.0.13</code></p><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment"># Pod IP</span>ip route add 10.20.0.0/16 via 172.16.0.13<span class="hljs-comment"># Service IP</span>ip route add 10.254.0.0/16 via 172.16.0.13</code></pre></div><p>当然最方便的肯定是将这一步在开发网络的路由上做，设置完成后开发网络就可以直连集群内的 Pod IP 和 Service IP 了；至于想直接访问 Service Name 只需要调整上游 DNS 解析既可</p>]]></content>
    
    
    <summary type="html">由于开发有部份服务使用 GRPC 进行通讯，同时采用 Consul 进行服务发现；在微服务架构下可能会导致一些访问问题，目前解决方案就是打通开发环境网络与测试环境 Kubernetes 内部 Pod 网络；翻了好多资料发现都是 2.x 的，而目前测试集群 Calico 版本为 3.6.3，很多文档都不适用只能自己折腾，目前折腾完了这里记录一下</summary>
    
    
    
    <category term="Kubernetes" scheme="https://mritd.com/categories/kubernetes/"/>
    
    
    <category term="Kubernetes" scheme="https://mritd.com/tags/kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>Dockerfile 目前可扩展的语法</title>
    <link href="https://mritd.com/2019/05/13/dockerfile-extended-syntax/"/>
    <id>https://mritd.com/2019/05/13/dockerfile-extended-syntax/</id>
    <published>2019-05-13T14:57:07.000Z</published>
    <updated>2019-05-13T14:57:07.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>最近在调整公司项目的 CI，目前主要使用 GitLab CI，在尝试多阶段构建中踩了点坑，然后发现了一些有意思的玩意</p></blockquote><p>本文参考:</p><ul><li><a href="https://github.com/moby/buildkit/blob/master/frontend/dockerfile/docs/experimental.md">Dockerfile frontend experimental syntaxes</a></li><li><a href="https://medium.com/@tonistiigi/advanced-multi-stage-build-patterns-6f741b852fae">Advanced multi-stage build patterns</a></li><li><a href="https://docs.docker.com/engine/reference/commandline/build/">docker build Document</a></li></ul><h2 id="一、起因"><a href="#一、起因" class="headerlink" title="一、起因"></a>一、起因</h2><p>公司目前主要使用 GitLab CI 作为主力 CI 构建工具，而且由于机器有限，我们对一些包管理器的本地 cache 直接持久化到了本机；比如 maven 的 <code>.m2</code> 目录，nodejs 的 <code>.npm</code> 目录等；虽然我们创建了对应的私服，但是在 build 时毕竟会下载，所以当时索性调整 GitLab Runner 在每个由 GitLab Runner 启动的容器中挂载这些缓存目录(GitLab CI 在 build 时会新启动容器运行 build 任务)；今天调整 nodejs 项目浪了一下，直接采用 Dockerfile 的 multi-stage build 功能进行 “Build =&gt; Package(docker image)” 的实现，基本 Dockerfile 如下</p><div class="hljs code-wrapper"><pre><code class="hljs sh">FROM gozap/build as builderCOPY . /xxxxWORKDIR /xxxxRUN <span class="hljs-built_in">source</span> ~/.bashrc \    &amp;&amp; cnpm install \    &amp;&amp; cnpm run buildFROM gozap/nginx-react:v1.0.0LABEL maintainer=<span class="hljs-string">&quot;mritd &lt;mritd@linux.com&gt;&quot;</span>COPY --from=builder /xxxx/public /usr/share/nginx/htmlEXPOSE 80STOPSIGNAL SIGTERMCMD [<span class="hljs-string">&quot;nginx&quot;</span>, <span class="hljs-string">&quot;-g&quot;</span>, <span class="hljs-string">&quot;daemon off;&quot;</span>]</code></pre></div><p>本来这个 <code>cnpm</code> 命令是带有 cache 的(<a href="https://github.com/Gozap/dockerfile/blob/master/build/cnpm">见这里</a>)，不过运行完 build 以后发现很慢，检查宿主机 cache 目录发现根本没有 cache…然后突然感觉</p><p><img src="https://cdn.oss.link/markdown/6ieh4.jpg" alt="事情并没有这么简单"></p><p>仔细想想，情况应该是这样事儿的…</p><div class="hljs code-wrapper"><pre><code class="hljs sh">+------------+                +-------------+            +----------------+|            |                |             |            |                ||            |                |    build    |            |   Multi-stage  ||   Runner   +---------------&gt;+  conatiner  +-----------&gt;+     Build      ||            |                |             |            |                ||            |                |             |            |                |+------------+                +------+------+            +----------------+                                     ^                                     |                                     |                                     |                                     |                              +------+------+                              |             |                              |    Cache    |                              |             |                              +-------------+</code></pre></div><p><img src="https://cdn.oss.link/markdown/9ov8m.jpg" alt="挂载不管用"></p><p>后来经过查阅文档，发现 Dockerfile 是有扩展语法的(当然最终我还是没用)，具体请见<del>下篇文章</del>(我怕被打死)下面，<strong>先说好，下面的内容无法完美的解决上面的问题，目前只是支持了一部分功能，当然未来很可能支持类似 <code>IF ELSE</code> 语法、直接挂载宿主机目录等功能</strong></p><h2 id="二、开启-Dockerfile-扩展语法"><a href="#二、开启-Dockerfile-扩展语法" class="headerlink" title="二、开启 Dockerfile 扩展语法"></a>二、开启 Dockerfile 扩展语法</h2><h3 id="2-1、开启实验性功能"><a href="#2-1、开启实验性功能" class="headerlink" title="2.1、开启实验性功能"></a>2.1、开启实验性功能</h3><p>目前这个扩展语法还处于实验性功能，所以需要配置 dockerd 守护进程，修改如下</p><div class="hljs code-wrapper"><pre><code class="hljs sh">ExecStart=/usr/bin/dockerd  -H unix:// \                            --init \                            --live-restore \                            --data-root=/data/docker \                            --experimental \                            --log-driver json-file \                            --log-opt max-size=30m \                            --log-opt max-file=3</code></pre></div><p>主要是 <code>--experimental</code> 参数，参考<a href="https://docs.docker.com/engine/reference/commandline/dockerd/#description">官方文档</a>；<strong>同时在 build 前声明 <code>export DOCKER_BUILDKIT=1</code> 变量</strong></p><h3 id="2-2、修改-Dockerfile"><a href="#2-2、修改-Dockerfile" class="headerlink" title="2.2、修改 Dockerfile"></a>2.2、修改 Dockerfile</h3><p>开启实验性功能后，只需要在 Dockerfile 头部增加 <code># syntax=docker/dockerfile:experimental</code> 既可；为了保证稳定性，你也可以指定具体的版本号，类似这样</p><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment"># syntax=docker/dockerfile:1.1.1-experimental</span>FROM tomcat</code></pre></div><h3 id="2-3、可用的扩展语法"><a href="#2-3、可用的扩展语法" class="headerlink" title="2.3、可用的扩展语法"></a>2.3、可用的扩展语法</h3><ul><li><code>RUN --mount=type=bind</code></li></ul><p>这个是默认的挂载模式，这个允许将上下文或者镜像以可都可写/只读模式挂载到 build 容器中，可选参数如下(不翻译了)</p><table><thead><tr><th>Option</th><th>Description</th></tr></thead><tbody><tr><td><code>target</code> (required)</td><td>Mount path.</td></tr><tr><td><code>source</code></td><td>Source path in the <code>from</code>. Defaults to the root of the <code>from</code>.</td></tr><tr><td><code>from</code></td><td>Build stage or image name for the root of the source. Defaults to the build context.</td></tr><tr><td><code>rw</code>,<code>readwrite</code></td><td>Allow writes on the mount. Written data will be discarded.</td></tr></tbody></table><ul><li><code>RUN --mount=type=cache</code></li></ul><p>专用于作为 cache 的挂载位置，一般用于 cache 包管理器的下载等</p><table><thead><tr><th>Option</th><th>Description</th></tr></thead><tbody><tr><td><code>id</code></td><td>Optional ID to identify separate/different caches</td></tr><tr><td><code>target</code> (required)</td><td>Mount path.</td></tr><tr><td><code>ro</code>,<code>readonly</code></td><td>Read-only if set.</td></tr><tr><td><code>sharing</code></td><td>One of <code>shared</code>, <code>private</code>, or <code>locked</code>. Defaults to <code>shared</code>. A <code>shared</code> cache mount can be used concurrently by multiple writers. <code>private</code> creates a new mount if there are multiple writers. <code>locked</code> pauses the second writer until the first one releases the mount.</td></tr><tr><td><code>from</code></td><td>Build stage to use as a base of the cache mount. Defaults to empty directory.</td></tr><tr><td><code>source</code></td><td>Subpath in the <code>from</code> to mount. Defaults to the root of the <code>from</code>.</td></tr></tbody></table><p><strong>Example: cache Go packages</strong></p><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment"># syntax = docker/dockerfile:experimental</span>FROM golang...RUN --mount=<span class="hljs-built_in">type</span>=cache,target=/root/.cache/go-build go build ...</code></pre></div><p><strong>Example: cache apt packages</strong></p><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment"># syntax = docker/dockerfile:experimental</span>FROM ubuntuRUN rm -f /etc/apt/apt.conf.d/docker-clean; <span class="hljs-built_in">echo</span> <span class="hljs-string">&#x27;Binary::apt::APT::Keep-Downloaded-Packages &quot;true&quot;;&#x27;</span> &gt; /etc/apt/apt.conf.d/keep-cacheRUN --mount=<span class="hljs-built_in">type</span>=cache,target=/var/cache/apt --mount=<span class="hljs-built_in">type</span>=cache,target=/var/lib/apt \  apt update &amp;&amp; apt install -y gcc</code></pre></div><ul><li><code>RUN --mount=type=tmpfs</code></li></ul><p>专用于挂载 tmpfs 的选项</p><table><thead><tr><th>Option</th><th>Description</th></tr></thead><tbody><tr><td><code>target</code> (required)</td><td>Mount path.</td></tr></tbody></table><ul><li><code>RUN --mount=type=secret</code></li></ul><p>这个类似 k8s 的 secret，用来挂载一些不想打入镜像，但是构建时想使用的密钥等，例如 docker 的 <code>config.json</code>，S3 的 <code>credentials</code></p><table><thead><tr><th>Option</th><th>Description</th></tr></thead><tbody><tr><td><code>id</code></td><td>ID of the secret. Defaults to basename of the target path.</td></tr><tr><td><code>target</code></td><td>Mount path. Defaults to <code>/run/secrets/</code> + <code>id</code>.</td></tr><tr><td><code>required</code></td><td>If set to <code>true</code>, the instruction errors out when the secret is unavailable. Defaults to <code>false</code>.</td></tr><tr><td><code>mode</code></td><td>File mode for secret file in octal. Default 0400.</td></tr><tr><td><code>uid</code></td><td>User ID for secret file. Default 0.</td></tr><tr><td><code>gid</code></td><td>Group ID for secret file. Default 0.</td></tr></tbody></table><p><strong>Example: access to S3</strong></p><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment"># syntax = docker/dockerfile:experimental</span>FROM python:3RUN pip install awscliRUN --mount=<span class="hljs-built_in">type</span>=secret,id=aws,target=/root/.aws/credentials aws s3 cp s3://... ...</code></pre></div><p><strong>注意: <code>buildctl</code> 是 BuildKit 的命令，你要测试的话自己换成 <code>docker build</code> 相关参数</strong></p><div class="hljs code-wrapper"><pre><code class="hljs console"><span class="hljs-meta">$</span><span class="bash"> buildctl build --frontend=dockerfile.v0 --<span class="hljs-built_in">local</span> context=. --<span class="hljs-built_in">local</span> dockerfile=. \</span><span class="bash">  --secret id=aws,src=<span class="hljs-variable">$HOME</span>/.aws/credentials</span></code></pre></div><ul><li><code>RUN --mount=type=ssh</code></li></ul><p>允许 build 容器通过 SSH agent 访问 SSH key，并且支持 <code>passphrases</code></p><table><thead><tr><th>Option</th><th>Description</th></tr></thead><tbody><tr><td><code>id</code></td><td>ID of SSH agent socket or key. Defaults to “default”.</td></tr><tr><td><code>target</code></td><td>SSH agent socket path. Defaults to <code>/run/buildkit/ssh_agent.$&#123;N&#125;</code>.</td></tr><tr><td><code>required</code></td><td>If set to <code>true</code>, the instruction errors out when the key is unavailable. Defaults to <code>false</code>.</td></tr><tr><td><code>mode</code></td><td>File mode for socket in octal. Default 0600.</td></tr><tr><td><code>uid</code></td><td>User ID for socket. Default 0.</td></tr><tr><td><code>gid</code></td><td>Group ID for socket. Default 0.</td></tr></tbody></table><p><strong>Example: access to Gitlab</strong></p><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment"># syntax = docker/dockerfile:experimental</span>FROM alpineRUN apk add --no-cache openssh-clientRUN mkdir -p -m 0700 ~/.ssh &amp;&amp; ssh-keyscan gitlab.com &gt;&gt; ~/.ssh/known_hostsRUN --mount=<span class="hljs-built_in">type</span>=ssh ssh -q -T git@gitlab.com 2&gt;&amp;1 | tee /hello<span class="hljs-comment"># &quot;Welcome to GitLab, @GITLAB_USERNAME_ASSOCIATED_WITH_SSHKEY&quot; should be printed here</span><span class="hljs-comment"># with the type of build progress is defined as `plain`.</span></code></pre></div><div class="hljs code-wrapper"><pre><code class="hljs sh">$ <span class="hljs-built_in">eval</span> $(ssh-agent)$ ssh-add ~/.ssh/id_rsa(Input your passphrase here)$ buildctl build --frontend=dockerfile.v0 --<span class="hljs-built_in">local</span> context=. --<span class="hljs-built_in">local</span> dockerfile=. \  --ssh default=<span class="hljs-variable">$SSH_AUTH_SOCK</span></code></pre></div><p>你也可以直接使用宿主机目录的 pem 文件，但是带有密码的 pem 目前不支持</p><p><strong>目前根据文档测试，当前的挂载类型比如 <code>cache</code> 类型，仅用于 multi-stage 内的挂载，比如你有 2+ 个构建步骤，<code>cache</code> 挂载类型能帮你在各个阶段内共享文件；但是它目前无法解决直接将宿主机目录挂载到 multi-stage 的问题(可以采取些曲线救国方案，但是很不优雅)；但是未来还是很有展望的，可以关注一下</strong></p>]]></content>
    
    
    <summary type="html">最近在调整公司项目的 CI，目前主要使用 GitLab CI，在尝试多阶段构建中踩了点坑，然后发现了一些有意思的玩意</summary>
    
    
    
    <category term="Docker" scheme="https://mritd.com/categories/docker/"/>
    
    
    <category term="Docker" scheme="https://mritd.com/tags/docker/"/>
    
  </entry>
  
  <entry>
    <title>Mac 下调校 Rime</title>
    <link href="https://mritd.com/2019/03/23/oh-my-rime/"/>
    <id>https://mritd.com/2019/03/23/oh-my-rime/</id>
    <published>2019-03-23T13:03:43.000Z</published>
    <updated>2019-03-23T13:03:43.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>由于对国内输入法隐私问题的担忧，决定放弃搜狗等输入法；为了更加 Geek 一些，最终决定了折腾 Rime(鼠须管) 输入法，以下为一些折腾的过程</p></blockquote><p><strong>国际惯例先放点图压压惊</strong></p><p><img src="https://cdn.oss.link/markdown/t3otb.jpg" alt="example1"><br><img src="https://cdn.oss.link/markdown/ep8sl.jpg" alt="example2"><br><img src="https://cdn.oss.link/markdown/wth6n.jpg" alt="example3"><br><img src="https://cdn.oss.link/markdown/5b85o.jpg" alt="example4"></p><h2 id="一、安装"><a href="#一、安装" class="headerlink" title="一、安装"></a>一、安装</h2><p>安装 Rime 没啥好说的，直接从<a href="https://rime.im/">官网</a>下载最新版本的安装包既可；安装完成后配置文件位于 <code>~/Library/Rime</code> 位置；在进行后续折腾之前我建议还是先 <code>cp -r ~/Library/Rime ~/Library/Rime.bak</code> 备份一下配置文件，以防制后续折腾挂了还可以还原；安装完成以后按 <code>⌘ + 反引号(~)</code> 切换到 <code>朙月拼音-简化字</code> 既可开启简体中文输入</p><h2 id="二、乱码解决"><a href="#二、乱码解决" class="headerlink" title="二、乱码解决"></a>二、乱码解决</h2><p>安装完成后在打字时可能出现乱码情况(俗称豆腐块)，这是由于 Rime 默认 UTF-8 字符集比较大，预选词内会出现生僻字，而 mac 字体内又不包含这些字体，从而导致乱码；解决方案很简单，下载 <a href="https://github.com/mritd/rime/tree/master/fonts">花园明朝</a> A、B 两款字体安装既可，安装后重启一下就不会出现乱码了</p><p><img src="https://cdn.oss.link/markdown/yfbis.png" alt="fonts"></p><h2 id="三、配置文件"><a href="#三、配置文件" class="headerlink" title="三、配置文件"></a>三、配置文件</h2><p>官方并不建议直接修改原始的配置文件，因为输入法更新时会重新覆盖默认配置，可能导致某些自定义配置丢失；推荐作法是创建一系列的 patch 配置，通过类似打补丁替换这种方式来实现无感的增加自定义配置；</p><p>由于使用的是 <code>朙月拼音-简化字</code> 输入方案，所以需要创建 <code>luna_pinyin_simp.custom.yaml</code> 等配置文件，后面就是查文档 + 各种 Google 一顿魔改了；目前我将我自己用的配置放在了 <a href="https://github.com/mritd/rime">Github</a> 上，有需要的可以直接 clone 下来，用里面的配置文件直接覆盖 <code>~/Library/Rime</code> 下的文件，然后重新部署既可，关于具体配置细节在下面写</p><h2 id="四、自定义配色"><a href="#四、自定义配色" class="headerlink" title="四、自定义配色"></a>四、自定义配色</h2><p>皮肤配色配置方案位于 <code>squirrel.custom.yaml</code> 配置文件中，我的配置目前是参考搜狗输入法皮肤自己调试的；官方也提供了一些皮肤外观配置，详见 <a href="https://gist.github.com/lotem/2290714">Gist</a>；想要切换皮肤配色只需要修改 <code>style/color_scheme</code> 为相应的皮肤配色名称既可</p><div class="hljs code-wrapper"><pre><code class="hljs yaml"><span class="hljs-attr">patch:</span>  <span class="hljs-attr">show_notifications_when:</span> <span class="hljs-string">appropriate</span>          <span class="hljs-comment"># 状态通知，适当，也可设为全开（always）全关（never）</span>  <span class="hljs-attr">style/color_scheme:</span> <span class="hljs-string">mritd_dark</span>                <span class="hljs-comment"># 方案命名，不能有空格</span>  <span class="hljs-attr">preset_color_schemes:</span>    <span class="hljs-attr">mritd_dark:</span>      <span class="hljs-attr">name:</span> <span class="hljs-string">漠然／mritd</span> <span class="hljs-string">dark</span>      <span class="hljs-attr">author:</span> <span class="hljs-string">mritd</span> <span class="hljs-string">&lt;mritd1234@gmail.com&gt;</span>      <span class="hljs-attr">horizontal:</span> <span class="hljs-literal">true</span>                          <span class="hljs-comment"># 水平排列</span>      <span class="hljs-attr">inline_preedit:</span> <span class="hljs-literal">true</span>                      <span class="hljs-comment"># 单行显示，false双行显示</span>      <span class="hljs-attr">candidate_format:</span> <span class="hljs-string">&quot;%c\u2005%@&quot;</span>            <span class="hljs-comment"># 用 1/6 em 空格 U+2005 来控制编号 %c 和候选词 %@ 前后的空间。</span>      <span class="hljs-attr">corner_radius:</span> <span class="hljs-number">5</span>                          <span class="hljs-comment"># 候选条圆角</span>      <span class="hljs-attr">hilited_corner_radius:</span> <span class="hljs-number">3</span>                  <span class="hljs-comment"># 高亮圆角</span>      <span class="hljs-attr">border_height:</span> <span class="hljs-number">6</span>                          <span class="hljs-comment"># 窗口边界高度，大于圆角半径才生效</span>      <span class="hljs-attr">border_width:</span> <span class="hljs-number">6</span>                           <span class="hljs-comment"># 窗口边界宽度，大于圆角半径才生效</span>      <span class="hljs-attr">border_color_width:</span> <span class="hljs-number">0</span>      <span class="hljs-comment">#font_face: &quot;PingFangSC&quot;                   # 候选词字体</span>      <span class="hljs-attr">font_point:</span> <span class="hljs-number">16</span>                            <span class="hljs-comment"># 候选字词大小</span>      <span class="hljs-attr">label_font_point:</span> <span class="hljs-number">14</span>                      <span class="hljs-comment"># 候选编号大小</span>      <span class="hljs-attr">text_color:</span> <span class="hljs-number">0xdedddd</span>                      <span class="hljs-comment"># 拼音行文字颜色，24位色值，16进制，BGR顺序</span>      <span class="hljs-attr">back_color:</span> <span class="hljs-number">0x4b4b4b</span>                      <span class="hljs-comment"># 候选条背景色</span>      <span class="hljs-attr">label_color:</span> <span class="hljs-number">0x888785</span>                     <span class="hljs-comment"># 预选栏编号颜色</span>      <span class="hljs-attr">border_color:</span> <span class="hljs-number">0x4b4b4b</span>                    <span class="hljs-comment"># 边框色</span>      <span class="hljs-attr">candidate_text_color:</span> <span class="hljs-number">0xffffff</span>            <span class="hljs-comment"># 预选项文字颜色</span>      <span class="hljs-attr">hilited_text_color:</span> <span class="hljs-number">0xdedddd</span>              <span class="hljs-comment"># 高亮拼音 (需要开启内嵌编码)</span>      <span class="hljs-attr">hilited_back_color:</span> <span class="hljs-number">0x252320</span>              <span class="hljs-comment"># 高亮拼音 (需要开启内嵌编码)</span>      <span class="hljs-attr">hilited_candidate_text_color:</span> <span class="hljs-number">0xFFE696</span>    <span class="hljs-comment"># 第一候选项文字颜色</span>      <span class="hljs-attr">hilited_candidate_back_color:</span> <span class="hljs-number">0x4b4b4b</span>    <span class="hljs-comment"># 第一候选项背景背景色</span>      <span class="hljs-attr">hilited_candidate_label_color:</span> <span class="hljs-number">0xffffff</span>   <span class="hljs-comment"># 第一候选项编号颜色</span>      <span class="hljs-attr">comment_text_color:</span> <span class="hljs-number">0xdedddd</span>              <span class="hljs-comment"># 拼音等提示文字颜色</span></code></pre></div><h2 id="五、增加自定义快捷字符"><a href="#五、增加自定义快捷字符" class="headerlink" title="五、增加自定义快捷字符"></a>五、增加自定义快捷字符</h2><p>快捷字符例如在中文输入法状态下可以直接输入 <code>/dn</code> 来调出特殊符号输入；这些配置位于 <code>luna_pinyin_simp.custom.yaml</code> 的 <code>punctuator</code> 配置中，我目前自行定义了一些，有需要的可以依葫芦画瓢直接修改</p><div class="hljs code-wrapper"><pre><code class="hljs yaml"><span class="hljs-attr">punctuator:</span>    <span class="hljs-attr">import_preset:</span> <span class="hljs-string">symbols</span>    <span class="hljs-attr">symbols:</span>      <span class="hljs-string">&quot;/fs&quot;</span><span class="hljs-string">:</span> [<span class="hljs-string">½</span>,<span class="hljs-string">‰</span>,<span class="hljs-string">¼</span>,<span class="hljs-string">⅓</span>,<span class="hljs-string">⅔</span>,<span class="hljs-string">¾</span>,<span class="hljs-string">⅒</span>]      <span class="hljs-string">&quot;/dq&quot;</span><span class="hljs-string">:</span> [<span class="hljs-string">🌍</span>,<span class="hljs-string">🌎</span>,<span class="hljs-string">🌏</span>,<span class="hljs-string">🌐</span>,<span class="hljs-string">🌑</span>,<span class="hljs-string">🌒</span>,<span class="hljs-string">🌓</span>,<span class="hljs-string">🌔</span>,<span class="hljs-string">🌕</span>,<span class="hljs-string">🌖</span>,<span class="hljs-string">🌗</span>,<span class="hljs-string">🌘</span>,<span class="hljs-string">🌙</span>,<span class="hljs-string">🌚</span>,<span class="hljs-string">🌛</span>,<span class="hljs-string">🌜</span>,<span class="hljs-string">🌝</span>,<span class="hljs-string">🌞</span>,<span class="hljs-string">⭐</span>,<span class="hljs-string">🌟</span>,<span class="hljs-string">🌠</span>,<span class="hljs-string">⛅</span>,<span class="hljs-string">⚡</span>,<span class="hljs-string">❄</span>,<span class="hljs-string">🔥</span>,<span class="hljs-string">💧</span>,<span class="hljs-string">🌊</span>]      <span class="hljs-string">&quot;/jt&quot;</span><span class="hljs-string">:</span> [<span class="hljs-string">⬆</span>,<span class="hljs-string">↗</span>,<span class="hljs-string">➡</span>,<span class="hljs-string">↘</span>,<span class="hljs-string">⬇</span>,<span class="hljs-string">↙</span>,<span class="hljs-string">⬅</span>,<span class="hljs-string">↖</span>,<span class="hljs-string">↕</span>,<span class="hljs-string">↔</span>,<span class="hljs-string">↩</span>,<span class="hljs-string">↪</span>,<span class="hljs-string">⤴</span>,<span class="hljs-string">⤵</span>,<span class="hljs-string">🔃</span>,<span class="hljs-string">🔄</span>,<span class="hljs-string">🔙</span>,<span class="hljs-string">🔚</span>,<span class="hljs-string">🔛</span>,<span class="hljs-string">🔜</span>,<span class="hljs-string">🔝</span>]      <span class="hljs-string">&quot;/sg&quot;</span><span class="hljs-string">:</span> [<span class="hljs-string">🍇</span>,<span class="hljs-string">🍈</span>,<span class="hljs-string">🍉</span>,<span class="hljs-string">🍊</span>,<span class="hljs-string">🍋</span>,<span class="hljs-string">🍌</span>,<span class="hljs-string">🍍</span>,<span class="hljs-string">🍎</span>,<span class="hljs-string">🍏</span>,<span class="hljs-string">🍐</span>,<span class="hljs-string">🍑</span>,<span class="hljs-string">🍒</span>,<span class="hljs-string">🍓</span>,<span class="hljs-string">🍅</span>,<span class="hljs-string">🍆</span>,<span class="hljs-string">🌽</span>,<span class="hljs-string">🍄</span>,<span class="hljs-string">🌰</span>,<span class="hljs-string">🍞</span>,<span class="hljs-string">🍖</span>,<span class="hljs-string">🍗</span>,<span class="hljs-string">🍔</span>,<span class="hljs-string">🍟</span>,<span class="hljs-string">🍕</span>,<span class="hljs-string">🍳</span>,<span class="hljs-string">🍲</span>,<span class="hljs-string">🍱</span>,<span class="hljs-string">🍘</span>,<span class="hljs-string">🍙</span>,<span class="hljs-string">🍚</span>,<span class="hljs-string">🍛</span>,<span class="hljs-string">🍜</span>,<span class="hljs-string">🍝</span>,<span class="hljs-string">🍠</span>,<span class="hljs-string">🍢</span>,<span class="hljs-string">🍣</span>,<span class="hljs-string">🍤</span>,<span class="hljs-string">🍥</span>,<span class="hljs-string">🍡</span>,<span class="hljs-string">🍦</span>,<span class="hljs-string">🍧</span>,<span class="hljs-string">🍨</span>,<span class="hljs-string">🍩</span>,<span class="hljs-string">🍪</span>,<span class="hljs-string">🎂</span>,<span class="hljs-string">🍰</span>,<span class="hljs-string">🍫</span>,<span class="hljs-string">🍬</span>,<span class="hljs-string">🍭</span>,<span class="hljs-string">🍮</span>,<span class="hljs-string">🍯</span>,<span class="hljs-string">🍼</span>,<span class="hljs-string">🍵</span>,<span class="hljs-string">🍶</span>,<span class="hljs-string">🍷</span>,<span class="hljs-string">🍸</span>,<span class="hljs-string">🍹</span>,<span class="hljs-string">🍺</span>,<span class="hljs-string">🍻</span>,<span class="hljs-string">🍴</span>]      <span class="hljs-string">&quot;/dw&quot;</span><span class="hljs-string">:</span> [<span class="hljs-string">🙈</span>,<span class="hljs-string">🙉</span>,<span class="hljs-string">🙊</span>,<span class="hljs-string">🐵</span>,<span class="hljs-string">🐒</span>,<span class="hljs-string">🐶</span>,<span class="hljs-string">🐕</span>,<span class="hljs-string">🐩</span>,<span class="hljs-string">🐺</span>,<span class="hljs-string">🐱</span>,<span class="hljs-string">😺</span>,<span class="hljs-string">😸</span>,<span class="hljs-string">😹</span>,<span class="hljs-string">😻</span>,<span class="hljs-string">😼</span>,<span class="hljs-string">😽</span>,<span class="hljs-string">🙀</span>,<span class="hljs-string">😿</span>,<span class="hljs-string">😾</span>,<span class="hljs-string">🐈</span>,<span class="hljs-string">🐯</span>,<span class="hljs-string">🐅</span>,<span class="hljs-string">🐆</span>,<span class="hljs-string">🐴</span>,<span class="hljs-string">🐎</span>,<span class="hljs-string">🐮</span>,<span class="hljs-string">🐂</span>,<span class="hljs-string">🐃</span>,<span class="hljs-string">🐄</span>,<span class="hljs-string">🐷</span>,<span class="hljs-string">🐖</span>,<span class="hljs-string">🐗</span>,<span class="hljs-string">🐽</span>,<span class="hljs-string">🐏</span>,<span class="hljs-string">🐑</span>,<span class="hljs-string">🐐</span>,<span class="hljs-string">🐪</span>,<span class="hljs-string">🐫</span>,<span class="hljs-string">🐘</span>,<span class="hljs-string">🐭</span>,<span class="hljs-string">🐁</span>,<span class="hljs-string">🐀</span>,<span class="hljs-string">🐹</span>,<span class="hljs-string">🐰</span>,<span class="hljs-string">🐇</span>,<span class="hljs-string">🐻</span>,<span class="hljs-string">🐨</span>,<span class="hljs-string">🐼</span>,<span class="hljs-string">🐾</span>,<span class="hljs-string">🐔</span>,<span class="hljs-string">🐓</span>,<span class="hljs-string">🐣</span>,<span class="hljs-string">🐤</span>,<span class="hljs-string">🐥</span>,<span class="hljs-string">🐦</span>,<span class="hljs-string">🐧</span>,<span class="hljs-string">🐸</span>,<span class="hljs-string">🐊</span>,<span class="hljs-string">🐢</span>,<span class="hljs-string">🐍</span>,<span class="hljs-string">🐲</span>,<span class="hljs-string">🐉</span>,<span class="hljs-string">🐳</span>,<span class="hljs-string">🐋</span>,<span class="hljs-string">🐬</span>,<span class="hljs-string">🐟</span>,<span class="hljs-string">🐠</span>,<span class="hljs-string">🐡</span>,<span class="hljs-string">🐙</span>,<span class="hljs-string">🐚</span>,<span class="hljs-string">🐌</span>,<span class="hljs-string">🐛</span>,<span class="hljs-string">🐜</span>,<span class="hljs-string">🐝</span>,<span class="hljs-string">🐞</span>,<span class="hljs-string">🦋</span>]      <span class="hljs-string">&quot;/bq&quot;</span><span class="hljs-string">:</span> [<span class="hljs-string">😀</span>,<span class="hljs-string">😁</span>,<span class="hljs-string">😂</span>,<span class="hljs-string">😃</span>,<span class="hljs-string">😄</span>,<span class="hljs-string">😅</span>,<span class="hljs-string">😆</span>,<span class="hljs-string">😉</span>,<span class="hljs-string">😊</span>,<span class="hljs-string">😋</span>,<span class="hljs-string">😎</span>,<span class="hljs-string">😍</span>,<span class="hljs-string">😘</span>,<span class="hljs-string">😗</span>,<span class="hljs-string">😙</span>,<span class="hljs-string">😚</span>,<span class="hljs-string">😇</span>,<span class="hljs-string">😐</span>,<span class="hljs-string">😑</span>,<span class="hljs-string">😶</span>,<span class="hljs-string">😏</span>,<span class="hljs-string">😣</span>,<span class="hljs-string">😥</span>,<span class="hljs-string">😮</span>,<span class="hljs-string">😯</span>,<span class="hljs-string">😪</span>,<span class="hljs-string">😫</span>,<span class="hljs-string">😴</span>,<span class="hljs-string">😌</span>,<span class="hljs-string">😛</span>,<span class="hljs-string">😜</span>,<span class="hljs-string">😝</span>,<span class="hljs-string">😒</span>,<span class="hljs-string">😓</span>,<span class="hljs-string">😔</span>,<span class="hljs-string">😕</span>,<span class="hljs-string">😲</span>,<span class="hljs-string">😷</span>,<span class="hljs-string">😖</span>,<span class="hljs-string">😞</span>,<span class="hljs-string">😟</span>,<span class="hljs-string">😤</span>,<span class="hljs-string">😢</span>,<span class="hljs-string">😭</span>,<span class="hljs-string">😦</span>,<span class="hljs-string">😧</span>,<span class="hljs-string">😨</span>,<span class="hljs-string">😬</span>,<span class="hljs-string">😰</span>,<span class="hljs-string">😱</span>,<span class="hljs-string">😳</span>,<span class="hljs-string">😵</span>,<span class="hljs-string">😡</span>,<span class="hljs-string">😠</span>]      <span class="hljs-string">&quot;/ss&quot;</span><span class="hljs-string">:</span> [<span class="hljs-string">💪</span>,<span class="hljs-string">👈</span>,<span class="hljs-string">👉</span>,<span class="hljs-string">👆</span>,<span class="hljs-string">👇</span>,<span class="hljs-string">✋</span>,<span class="hljs-string">👌</span>,<span class="hljs-string">👍</span>,<span class="hljs-string">👎</span>,<span class="hljs-string">✊</span>,<span class="hljs-string">👊</span>,<span class="hljs-string">👋</span>,<span class="hljs-string">👏</span>,<span class="hljs-string">👐</span>]      <span class="hljs-string">&quot;/dn&quot;</span><span class="hljs-string">:</span> [<span class="hljs-string">⌘</span>, <span class="hljs-string">⌥</span>, <span class="hljs-string">⇧</span>, <span class="hljs-string">⌃</span>, <span class="hljs-string">⎋</span>, <span class="hljs-string">⇪</span>, <span class="hljs-string"></span>, <span class="hljs-string">⌫</span>, <span class="hljs-string">⌦</span>, <span class="hljs-string">↩︎</span>, <span class="hljs-string">⏎</span>, <span class="hljs-string">↑</span>, <span class="hljs-string">↓</span>, <span class="hljs-string">←</span>, <span class="hljs-string">→</span>, <span class="hljs-string">↖</span>, <span class="hljs-string">↘</span>, <span class="hljs-string">⇟</span>, <span class="hljs-string">⇞</span>]      <span class="hljs-string">&quot;/fh&quot;</span><span class="hljs-string">:</span> [<span class="hljs-string">©</span>,<span class="hljs-string">®</span>,<span class="hljs-string">℗</span>,<span class="hljs-string">ⓘ</span>,<span class="hljs-string">℠</span>,<span class="hljs-string">™</span>,<span class="hljs-string">℡</span>,<span class="hljs-string">␡</span>,<span class="hljs-string">♂</span>,<span class="hljs-string">♀</span>,<span class="hljs-string">☉</span>,<span class="hljs-string">☊</span>,<span class="hljs-string">☋</span>,<span class="hljs-string">☌</span>,<span class="hljs-string">☍</span>,<span class="hljs-string">☑︎</span>,<span class="hljs-string">☒</span>,<span class="hljs-string">☜</span>,<span class="hljs-string">☝</span>,<span class="hljs-string">☞</span>,<span class="hljs-string">☟</span>,<span class="hljs-string">✎</span>,<span class="hljs-string">✄</span>,<span class="hljs-string">♻</span>,<span class="hljs-string">⚐</span>,<span class="hljs-string">⚑</span>,<span class="hljs-string">⚠</span>]      <span class="hljs-string">&quot;/xh&quot;</span><span class="hljs-string">:</span> [<span class="hljs-string">＊</span>,<span class="hljs-string">×</span>,<span class="hljs-string">✱</span>,<span class="hljs-string">★</span>,<span class="hljs-string">☆</span>,<span class="hljs-string">✩</span>,<span class="hljs-string">✧</span>,<span class="hljs-string">❋</span>,<span class="hljs-string">❊</span>,<span class="hljs-string">❉</span>,<span class="hljs-string">❈</span>,<span class="hljs-string">❅</span>,<span class="hljs-string">✿</span>,<span class="hljs-string">✲</span>]</code></pre></div><h2 id="六、设置输入方案"><a href="#六、设置输入方案" class="headerlink" title="六、设置输入方案"></a>六、设置输入方案</h2><p>在第一次按 <code>⌘ + 反引号(~)</code> 设置输入法时实际上我们可以看到很多的输入方案，而事实上很多方案我们根本用不上；想要删除和修改方案可以调整 <code>default.custom.yaml</code> 中的 <code>schema_list</code> 字段</p><div class="hljs code-wrapper"><pre><code class="hljs yaml"><span class="hljs-attr">patch:</span>  <span class="hljs-attr">menu:</span>    <span class="hljs-attr">page_size:</span> <span class="hljs-number">8</span>  <span class="hljs-attr">schema_list:</span>  <span class="hljs-bullet">-</span> <span class="hljs-attr">schema:</span> <span class="hljs-string">luna_pinyin_simp</span>      <span class="hljs-comment"># 朙月拼音 简化字</span>  <span class="hljs-bullet">-</span> <span class="hljs-attr">schema:</span> <span class="hljs-string">luna_pinyin</span>           <span class="hljs-comment"># 朙月拼音</span>  <span class="hljs-bullet">-</span> <span class="hljs-attr">schema:</span> <span class="hljs-string">luna_pinyin_fluency</span>   <span class="hljs-comment"># 语句流</span><span class="hljs-comment">#  - schema: double_pinyin         # 自然碼雙拼</span><span class="hljs-comment">#  - schema: double_pinyin_flypy   # 小鹤雙拼</span><span class="hljs-comment">#  - schema: double_pinyin_pyjj    # 拼音加加双拼</span><span class="hljs-comment">#  - schema: wubi_pinyin           # 五笔拼音混合輸入</span></code></pre></div><p><strong>实际上我只能用上第一个…毕竟写了好几年代码还得看键盘的人也只能这样了…</strong></p><h2 id="七、调整特殊键行为"><a href="#七、调整特殊键行为" class="headerlink" title="七、调整特殊键行为"></a>七、调整特殊键行为</h2><p>在刚安装完以后发现在中文输入法状态下输入英文，按 <code>shift</code> 键后字符上屏，然后还得回车一下，这就很让我难受…最后找到了这篇 <a href="https://gist.github.com/lotem/2981316">Gist</a>，目前将大写锁定、<code>shift</code> 键调整为了跟搜狗一致的配置，有需要调整的可以自行编辑 <code>default.custom.yaml</code> 中的 <code>ascii_composer/switch_key</code> 部分</p><div class="hljs code-wrapper"><pre><code class="hljs yaml"><span class="hljs-comment"># capslock 键切换英文并输出大写</span><span class="hljs-attr">ascii_composer/good_old_caps_lock:</span> <span class="hljs-literal">true</span><span class="hljs-comment"># 输入法中英文状态快捷键</span><span class="hljs-attr">ascii_composer/switch_key:</span>  <span class="hljs-attr">Caps_Lock:</span> <span class="hljs-string">commit_code</span>  <span class="hljs-attr">Control_L:</span> <span class="hljs-string">noop</span>  <span class="hljs-attr">Control_R:</span> <span class="hljs-string">noop</span>  <span class="hljs-comment"># 按下左 shift 英文字符直接上屏，不需要再次回车，输入法保持英文状态</span>  <span class="hljs-attr">Shift_L:</span> <span class="hljs-string">commit_code</span>  <span class="hljs-attr">Shift_R:</span> <span class="hljs-string">noop</span></code></pre></div><h2 id="八、自定义词库"><a href="#八、自定义词库" class="headerlink" title="八、自定义词库"></a>八、自定义词库</h2><p>Rime 默认的词库稍为有点弱，我们可以下载一些搜狗词库来进行扩展；不过搜狗词库格式默认是无法解析的，好在有人开发了工具可以方便的将搜狗细胞词库转化为 Rime 的格式(工具<a href="https://github.com/studyzy/imewlconverter/releases">点击这里</a>下载)；目前该工具只支持 Windows(也有些别人写的 py 脚本啥的，但是我没用)，所以词库转换这种操作还得需要一个 Windows 虚拟机；</p><p>转换过程很简单，先从<a href="https://pinyin.sogou.com/dict/">搜狗词库</a>下载一系列的 <code>scel</code> 文件，然后批量选中，接着调整一下输入和输出格式点击转换，最后保存成一个 <code>txt</code> 文本</p><p><img src="https://cdn.oss.link/markdown/jtv97.png" alt="input-setting"></p><p><img src="https://cdn.oss.link/markdown/p7qha.png" alt="convert"></p><p>光有这个文本还不够，我们要将它塞到词库的 <code>yaml</code> 配置里，所以新建一个词库配置文件 <code>luna_pinyin.sougou.dict.yaml</code>，然后写上头部说明(<strong>注意最后三个点后面加一个换行</strong>)</p><div class="hljs code-wrapper"><pre><code class="hljs yaml"><span class="hljs-comment"># Rime dictionary</span><span class="hljs-comment"># encoding: utf-8</span><span class="hljs-comment"># 搜狗词库 目前包含如下:</span><span class="hljs-comment"># IT计算机 实用IT词汇 亲戚称呼 化学品名 数字时间 数学词汇 淘宝词库 编程语言 软件专业 颜色名称 程序猿词库 开发专用词库 搜狗标准词库</span><span class="hljs-comment"># 摄影专业名词 计算机专业词库 计算机词汇大全 保险词汇 最详细的全国地名大全 饮食大全 常见花卉名称 房地产词汇大全 中国传统节日大全 财经金融词汇大全</span><span class="hljs-meta">---</span><span class="hljs-attr">name:</span> <span class="hljs-string">luna_pinyin.sougou</span><span class="hljs-attr">version:</span> <span class="hljs-string">&quot;1.0&quot;</span><span class="hljs-attr">sort:</span> <span class="hljs-string">by_weight</span><span class="hljs-attr">use_preset_vocabulary:</span> <span class="hljs-literal">true</span><span class="hljs-string">...</span></code></pre></div><p>接着只需要把生成好的词库 <code>txt</code> 文件内容粘贴到三个点下面既可；但是词库太多的话你会发现这个文本有好几十 M，一般编辑器打开都会卡死，解决这种情况只需要用命令行 <code>cat</code> 一下就行</p><div class="hljs code-wrapper"><pre><code class="hljs sh">cat sougou.txt &gt;&gt; luna_pinyin.sougou.dict.yaml</code></pre></div><p>最后修改 <code>luna_pinyin.extended.dict.yaml</code> 中的 <code>import_tables</code> 字段，加入刚刚新建的词库既可</p><div class="hljs code-wrapper"><pre><code class="hljs yaml"><span class="hljs-meta">---</span><span class="hljs-attr">name:</span> <span class="hljs-string">luna_pinyin.extended</span><span class="hljs-attr">version:</span> <span class="hljs-string">&quot;2016.06.26&quot;</span><span class="hljs-attr">sort:</span> <span class="hljs-string">by_weight</span>  <span class="hljs-comment">#字典初始排序，可選original或by_weight</span><span class="hljs-attr">use_preset_vocabulary:</span> <span class="hljs-literal">true</span><span class="hljs-comment">#此處爲明月拼音擴充詞庫（基本）默認鏈接載入的詞庫，有朙月拼音官方詞庫、明月拼音擴充詞庫（漢語大詞典）、明月拼音擴充詞庫（詩詞）、明月拼音擴充詞庫（含西文的詞彙）。如果不需要加載某个詞庫請將其用「#」註釋掉。</span><span class="hljs-comment">#雙拼不支持 luna_pinyin.cn_en 詞庫，請用戶手動禁用。</span><span class="hljs-attr">import_tables:</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">luna_pinyin</span>  <span class="hljs-comment"># 加入搜狗词库</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">luna_pinyin.sougou</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">luna_pinyin.poetry</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">luna_pinyin.cn_en</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">luna_pinyin.kaomoji</span></code></pre></div><h2 id="九、定制特殊单词"><a href="#九、定制特殊单词" class="headerlink" title="九、定制特殊单词"></a>九、定制特殊单词</h2><p>由于长期撸码，24 小时离不开命令行，偶尔在中文输入法下输入了一些命令导致汉字直接出现在 terminal 上就很尴尬…这时候我们可以在 <code>luna_pinyin.cn_en.dict.yaml</code> 加入一些我们自己的专属词库，比如这样</p><div class="hljs code-wrapper"><pre><code class="hljs yaml"><span class="hljs-meta">---</span><span class="hljs-attr">name:</span> <span class="hljs-string">luna_pinyin.cn_en</span><span class="hljs-attr">version:</span> <span class="hljs-string">&quot;2017.9.13&quot;</span><span class="hljs-attr">sort:</span> <span class="hljs-string">by_weight</span><span class="hljs-attr">use_preset_vocabulary:</span> <span class="hljs-literal">true</span><span class="hljs-string">...</span><span class="hljs-string">git</span><span class="hljs-string">git</span><span class="hljs-string">ls</span><span class="hljs-string">ls</span><span class="hljs-string">cd</span><span class="hljs-string">cd</span><span class="hljs-string">pwd</span><span class="hljs-string">pwd</span><span class="hljs-string">git</span> <span class="hljs-string">ps</span><span class="hljs-string">gitps</span><span class="hljs-string">kubernetes</span><span class="hljs-string">kubernetes</span><span class="hljs-string">kubernetes</span><span class="hljs-string">kuber</span><span class="hljs-string">kubectl</span><span class="hljs-string">kubectl</span><span class="hljs-string">kubectl</span><span class="hljs-string">kubec</span><span class="hljs-string">docker</span><span class="hljs-string">docker</span><span class="hljs-string">docker</span><span class="hljs-string">dock</span><span class="hljs-string">ipvs</span><span class="hljs-string">ipvs</span><span class="hljs-string">ps</span><span class="hljs-string">ps</span><span class="hljs-string">bash</span><span class="hljs-string">bash</span><span class="hljs-string">source</span><span class="hljs-string">source</span><span class="hljs-string">source</span><span class="hljs-string">sou</span><span class="hljs-string">rm</span><span class="hljs-string">rm</span></code></pre></div><p>配置后如果我在中文输入法下输入 git 则会自动匹配 git 这个单词，避免错误的键入中文字符；<strong>需要注意的是第一列代表上屏的字符，第二列代表输入的单词，即 “当输入第二列时候选词为第一列”；两列之间要用 tag 制表符隔开，记住不是空格</strong></p>]]></content>
    
    
    <summary type="html">由于对国内输入法隐私问题的担忧，决定放弃搜狗等输入法；为了更加 Geek 一些，最终决定了折腾 Rime(鼠须管) 输入法，以下为一些折腾的过程</summary>
    
    
    
    <category term="Mac" scheme="https://mritd.com/categories/mac/"/>
    
    
    <category term="Mac" scheme="https://mritd.com/tags/mac/"/>
    
    <category term="Rime" scheme="https://mritd.com/tags/rime/"/>
    
  </entry>
  
  <entry>
    <title>Ubuntu 设置多个源</title>
    <link href="https://mritd.com/2019/03/19/how-to-set-multiple-apt-mirrors-for-ubuntu/"/>
    <id>https://mritd.com/2019/03/19/how-to-set-multiple-apt-mirrors-for-ubuntu/</id>
    <published>2019-03-19T13:43:23.000Z</published>
    <updated>2019-03-19T13:43:23.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="一、源起"><a href="#一、源起" class="headerlink" title="一、源起"></a>一、源起</h2><p>使用 Ubuntu 作为生产容器系统好久了，但是 apt 源问题一致有点困扰: **由于众所周知的原因，官方源执行 <code>apt update</code> 等命令会非常慢；而国内有很多镜像服务，但是某些偶尔也会抽风(比如清华大源)，最后的结果就是日常修改 apt 源…**Google 查了了好久发现事实上 apt 源是支持 <code>mirror</code> 协议的，从而自动选择可用的一个</p><h2 id="二、使用-mirror-协议"><a href="#二、使用-mirror-协议" class="headerlink" title="二、使用 mirror 协议"></a>二、使用 mirror 协议</h2><p>废话不说多直接上代码，编辑 <code>/etc/apt/sources.list</code>，替换为如下内容</p><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment">#------------------------------------------------------------------------------#</span><span class="hljs-comment">#                            OFFICIAL UBUNTU REPOS                             #</span><span class="hljs-comment">#------------------------------------------------------------------------------#</span><span class="hljs-comment">###### Ubuntu Main Repos</span>deb mirror://mirrors.ubuntu.com/mirrors.txt bionic main restricted universe multiversedeb-src mirror://mirrors.ubuntu.com/mirrors.txt bionic main restricted universe multiverse<span class="hljs-comment">###### Ubuntu Update Repos</span>deb mirror://mirrors.ubuntu.com/mirrors.txt bionic-security main restricted universe multiversedeb mirror://mirrors.ubuntu.com/mirrors.txt bionic-updates main restricted universe multiversedeb mirror://mirrors.ubuntu.com/mirrors.txt bionic-backports main restricted universe multiversedeb-src mirror://mirrors.ubuntu.com/mirrors.txt bionic-security main restricted universe multiversedeb-src mirror://mirrors.ubuntu.com/mirrors.txt bionic-updates main restricted universe multiversedeb-src mirror://mirrors.ubuntu.com/mirrors.txt bionic-backports main restricted universe multiverse</code></pre></div><p>当使用 <code>mirror</code> 协议后，执行 <code>apt update</code> 时会首先<strong>通过 http 访问</strong> <code>mirrors.ubuntu.com/mirrors.txt</code> 文本；文本内容实际上就是当前可用的镜像源列表，如下所示</p><div class="hljs code-wrapper"><pre><code class="hljs sh">http://ftp.sjtu.edu.cn/ubuntu/http://mirrors.nju.edu.cn/ubuntu/http://mirrors.nwafu.edu.cn/ubuntu/http://mirrors.sohu.com/ubuntu/http://mirrors.aliyun.com/ubuntu/http://mirrors.shu.edu.cn/ubuntu/http://mirrors.cqu.edu.cn/ubuntu/http://mirrors.huaweicloud.com/repository/ubuntu/http://mirrors.cn99.com/ubuntu/http://mirrors.yun-idc.com/ubuntu/http://mirrors.tuna.tsinghua.edu.cn/ubuntu/http://mirrors.ustc.edu.cn/ubuntu/http://mirrors.njupt.edu.cn/ubuntu/http://mirror.lzu.edu.cn/ubuntu/http://archive.ubuntu.com/ubuntu/</code></pre></div><p>得到列表后 apt 会自动选择一个(选择规则暂不清楚，国外有文章说是选择最快的，但是不清楚这个最快是延迟还是网速)进行下载；**同时根据地区不通，官方也提供指定国家的 <code>mirror.txt</code>**，比如中国的实际上可以设置为 <code>mirrors.ubuntu.com/CN.txt</code>(我测试跟官方一样，推测可能是使用了类似 DNS 选优的策略)</p><h2 id="三、自定义-mirror-地址"><a href="#三、自定义-mirror-地址" class="headerlink" title="三、自定义 mirror 地址"></a>三、自定义 mirror 地址</h2><p>现在已经解决了能同时使用多个源的问题，但是有些时候你会发现源的可用性检测并不是很精准，比如某个源只有 40k 的下载速度…不巧你某个下载还命中了，这就很尴尬；<strong>所以有时候我们可能需要自定义 <code>mirror.txt</code> 这个源列表</strong>，经过测试证明**只需要开启一个标准的 <code>http server</code> 能返回一个文本即可，不过需要注意只能是 <code>http</code>，而不是 <code>https</code>**；所以我们首先下载一下这个文本，把不想要的删掉；然后弄个 nginx，甚至 <code>python -m http.server</code> 把文本文件暴露出去就可以；我比较懒…扔 CDN 上了: <a href="http://oss.link/config/apt-mirrors.txt">http://oss.link/config/apt-mirrors.txt</a></p><p>关于源的精简，我建议将一些 <code>edu</code> 的删掉，因为敏感时期他们很不稳定；优选阿里云、网易、华为这种大公司的，比较有名的清华大的什么的可以留着，其他的可以考虑都删掉</p>]]></content>
    
    
    <summary type="html">介绍通过 mirror 方式来设置 Ubuntu 源，从而实现自动切换 apt 源下载</summary>
    
    
    
    <category term="Linux" scheme="https://mritd.com/categories/linux/"/>
    
    
    <category term="Linux" scheme="https://mritd.com/tags/linux/"/>
    
  </entry>
  
  <entry>
    <title>Kubernetes 1.13.4 搭建</title>
    <link href="https://mritd.com/2019/03/16/set-up-kubernetes-1.13.4-cluster/"/>
    <id>https://mritd.com/2019/03/16/set-up-kubernetes-1.13.4-cluster/</id>
    <published>2019-03-16T09:36:52.000Z</published>
    <updated>2019-03-16T09:36:52.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>年后回来有点懒，也有点忙；1.13 出来好久了，周末还是决定折腾一下吧</p></blockquote><h2 id="一、环境准备"><a href="#一、环境准备" class="headerlink" title="一、环境准备"></a>一、环境准备</h2><p>老样子，安装环境为 5 台 Ubuntu 18.04.2 LTS 虚拟机，其他详细信息如下</p><table><thead><tr><th>System OS</th><th>IP Address</th><th>Docker</th><th>Kernel</th><th>Application</th></tr></thead><tbody><tr><td>Ubuntu 18.04.2 LTS</td><td>192.168.1.51</td><td>18.09.2</td><td>4.15.0-46-generic</td><td>k8s-master、etcd</td></tr><tr><td>Ubuntu 18.04.2 LTS</td><td>192.168.1.52</td><td>18.09.2</td><td>4.15.0-46-generic</td><td>k8s-master、etcd</td></tr><tr><td>Ubuntu 18.04.2 LTS</td><td>192.168.1.53</td><td>18.09.2</td><td>4.15.0-46-generic</td><td>k8s-master、etcd</td></tr><tr><td>Ubuntu 18.04.2 LTS</td><td>192.168.1.54</td><td>18.09.2</td><td>4.15.0-46-generic</td><td>k8s-node</td></tr><tr><td>Ubuntu 18.04.2 LTS</td><td>192.168.1.55</td><td>18.09.2</td><td>4.15.0-46-generic</td><td>k8s-node</td></tr></tbody></table><p><strong>所有配置生成将在第一个节点上完成，第一个节点与其他节点 root 用户免密码登录，用于分发文件；为了方便搭建弄了一点小脚本，仓库地址 <a href="https://github.com/mritd/ktool">ktool</a>，本文后续所有脚本、配置都可以在此仓库找到；关于 <a href="https://github.com/cloudflare/cfssl">cfssl</a> 等基本工具使用，本文不再阐述</strong></p><h2 id="二、安装-Etcd"><a href="#二、安装-Etcd" class="headerlink" title="二、安装 Etcd"></a>二、安装 Etcd</h2><h3 id="2-1、生成证书"><a href="#2-1、生成证书" class="headerlink" title="2.1、生成证书"></a>2.1、生成证书</h3><p>Etcd 仍然开启 TLS 认证，所以先使用 cfssl 生成相关证书</p><ul><li>etcd-root-ca-csr.json</li></ul><div class="hljs code-wrapper"><pre><code class="hljs json">&#123;    <span class="hljs-attr">&quot;CN&quot;</span>: <span class="hljs-string">&quot;etcd-root-ca&quot;</span>,    <span class="hljs-attr">&quot;key&quot;</span>: &#123;        <span class="hljs-attr">&quot;algo&quot;</span>: <span class="hljs-string">&quot;rsa&quot;</span>,        <span class="hljs-attr">&quot;size&quot;</span>: <span class="hljs-number">4096</span>    &#125;,    <span class="hljs-attr">&quot;names&quot;</span>: [        &#123;            <span class="hljs-attr">&quot;O&quot;</span>: <span class="hljs-string">&quot;etcd&quot;</span>,            <span class="hljs-attr">&quot;OU&quot;</span>: <span class="hljs-string">&quot;etcd Security&quot;</span>,            <span class="hljs-attr">&quot;L&quot;</span>: <span class="hljs-string">&quot;Beijing&quot;</span>,            <span class="hljs-attr">&quot;ST&quot;</span>: <span class="hljs-string">&quot;Beijing&quot;</span>,            <span class="hljs-attr">&quot;C&quot;</span>: <span class="hljs-string">&quot;CN&quot;</span>        &#125;    ],    <span class="hljs-attr">&quot;ca&quot;</span>: &#123;        <span class="hljs-attr">&quot;expiry&quot;</span>: <span class="hljs-string">&quot;87600h&quot;</span>    &#125;&#125;</code></pre></div><ul><li>etcd-gencert.json</li></ul><div class="hljs code-wrapper"><pre><code class="hljs json">&#123;  <span class="hljs-attr">&quot;signing&quot;</span>: &#123;    <span class="hljs-attr">&quot;default&quot;</span>: &#123;        <span class="hljs-attr">&quot;usages&quot;</span>: [          <span class="hljs-string">&quot;signing&quot;</span>,          <span class="hljs-string">&quot;key encipherment&quot;</span>,          <span class="hljs-string">&quot;server auth&quot;</span>,          <span class="hljs-string">&quot;client auth&quot;</span>        ],        <span class="hljs-attr">&quot;expiry&quot;</span>: <span class="hljs-string">&quot;87600h&quot;</span>    &#125;  &#125;&#125;</code></pre></div><ul><li>etcd-csr.json</li></ul><div class="hljs code-wrapper"><pre><code class="hljs json">&#123;    <span class="hljs-attr">&quot;key&quot;</span>: &#123;        <span class="hljs-attr">&quot;algo&quot;</span>: <span class="hljs-string">&quot;rsa&quot;</span>,        <span class="hljs-attr">&quot;size&quot;</span>: <span class="hljs-number">2048</span>    &#125;,    <span class="hljs-attr">&quot;names&quot;</span>: [        &#123;            <span class="hljs-attr">&quot;O&quot;</span>: <span class="hljs-string">&quot;etcd&quot;</span>,            <span class="hljs-attr">&quot;OU&quot;</span>: <span class="hljs-string">&quot;etcd Security&quot;</span>,            <span class="hljs-attr">&quot;L&quot;</span>: <span class="hljs-string">&quot;Beijing&quot;</span>,            <span class="hljs-attr">&quot;ST&quot;</span>: <span class="hljs-string">&quot;Beijing&quot;</span>,            <span class="hljs-attr">&quot;C&quot;</span>: <span class="hljs-string">&quot;CN&quot;</span>        &#125;    ],    <span class="hljs-attr">&quot;CN&quot;</span>: <span class="hljs-string">&quot;etcd&quot;</span>,    <span class="hljs-attr">&quot;hosts&quot;</span>: [        <span class="hljs-string">&quot;127.0.0.1&quot;</span>,        <span class="hljs-string">&quot;localhost&quot;</span>,        <span class="hljs-string">&quot;192.168.1.51&quot;</span>,        <span class="hljs-string">&quot;192.168.1.52&quot;</span>,        <span class="hljs-string">&quot;192.168.1.53&quot;</span>    ]&#125;</code></pre></div><p>接下来执行生成即可；<strong>我建议在生产环境在证书内预留几个 IP，已防止意外故障迁移时还需要重新生成证书；证书默认期限为 10 年(包括 CA 证书)，有需要加强安全性的可以适当减小</strong></p><div class="hljs code-wrapper"><pre><code class="hljs sh">cfssl gencert --initca=<span class="hljs-literal">true</span> etcd-root-ca-csr.json | cfssljson --bare etcd-root-cacfssl gencert --ca etcd-root-ca.pem --ca-key etcd-root-ca-key.pem --config etcd-gencert.json etcd-csr.json | cfssljson --bare etcd</code></pre></div><h3 id="2-2、安装-Etcd"><a href="#2-2、安装-Etcd" class="headerlink" title="2.2、安装 Etcd"></a>2.2、安装 Etcd</h3><h4 id="2-2-1、安装脚本"><a href="#2-2-1、安装脚本" class="headerlink" title="2.2.1、安装脚本"></a>2.2.1、安装脚本</h4><p>安装 Etcd 只需要将二进制文件放在可执行目录下，然后修改配置增加 systemd service 配置文件即可；为了安全性起见最好使用单独的用户启动 Etcd</p><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-meta">#!/bin/bash</span><span class="hljs-built_in">set</span> -eETCD_DEFAULT_VERSION=<span class="hljs-string">&quot;3.3.12&quot;</span><span class="hljs-keyword">if</span> [ <span class="hljs-string">&quot;<span class="hljs-variable">$1</span>&quot;</span> != <span class="hljs-string">&quot;&quot;</span> ]; <span class="hljs-keyword">then</span>  ETCD_VERSION=<span class="hljs-variable">$1</span><span class="hljs-keyword">else</span>  <span class="hljs-built_in">echo</span> -e <span class="hljs-string">&quot;\033[33mWARNING: ETCD_VERSION is blank,use default version: <span class="hljs-variable">$&#123;ETCD_DEFAULT_VERSION&#125;</span>\033[0m&quot;</span>  ETCD_VERSION=<span class="hljs-variable">$&#123;ETCD_DEFAULT_VERSION&#125;</span><span class="hljs-keyword">fi</span><span class="hljs-comment"># 下载 Etcd 二进制文件</span><span class="hljs-keyword">function</span> <span class="hljs-function"><span class="hljs-title">download</span></span>()&#123;    <span class="hljs-keyword">if</span> [ ! -f <span class="hljs-string">&quot;etcd-v<span class="hljs-variable">$&#123;ETCD_VERSION&#125;</span>-linux-amd64.tar.gz&quot;</span> ]; <span class="hljs-keyword">then</span>        wget https://github.com/coreos/etcd/releases/download/v<span class="hljs-variable">$&#123;ETCD_VERSION&#125;</span>/etcd-v<span class="hljs-variable">$&#123;ETCD_VERSION&#125;</span>-linux-amd64.tar.gz        tar -zxvf etcd-v<span class="hljs-variable">$&#123;ETCD_VERSION&#125;</span>-linux-amd64.tar.gz    <span class="hljs-keyword">fi</span>&#125;<span class="hljs-comment"># 为 Etcd 创建单独的用户</span><span class="hljs-keyword">function</span> <span class="hljs-function"><span class="hljs-title">preinstall</span></span>()&#123;getent group etcd &gt;/dev/null || groupadd -r etcdgetent passwd etcd &gt;/dev/null || useradd -r -g etcd -d /var/lib/etcd -s /sbin/nologin -c <span class="hljs-string">&quot;etcd user&quot;</span> etcd&#125;<span class="hljs-comment"># 安装(复制文件)</span><span class="hljs-keyword">function</span> <span class="hljs-function"><span class="hljs-title">install</span></span>()&#123;    <span class="hljs-comment"># 释放 Etcd 二进制文件</span>    <span class="hljs-built_in">echo</span> -e <span class="hljs-string">&quot;\033[32mINFO: Copy etcd...\033[0m&quot;</span>    tar -zxvf etcd-v<span class="hljs-variable">$&#123;ETCD_VERSION&#125;</span>-linux-amd64.tar.gz    cp etcd-v<span class="hljs-variable">$&#123;ETCD_VERSION&#125;</span>-linux-amd64/etcd* /usr/<span class="hljs-built_in">local</span>/bin    rm -rf etcd-v<span class="hljs-variable">$&#123;ETCD_VERSION&#125;</span>-linux-amd64    <span class="hljs-comment"># 复制 配置文件 到 /etc/etcd(目录内文件结构在下面)</span>    <span class="hljs-built_in">echo</span> -e <span class="hljs-string">&quot;\033[32mINFO: Copy etcd config...\033[0m&quot;</span>    cp -r conf /etc/etcd    chown -R etcd:etcd /etc/etcd    chmod -R 755 /etc/etcd/ssl    <span class="hljs-comment"># 复制 systemd service 配置</span>    <span class="hljs-built_in">echo</span> -e <span class="hljs-string">&quot;\033[32mINFO: Copy etcd systemd config...\033[0m&quot;</span>    cp systemd/*.service /lib/systemd/system    systemctl daemon-reload&#125;<span class="hljs-comment"># 创建 Etcd 存储目录(如需要更改，请求改 /etc/etcd/etcd.conf 配置文件)</span><span class="hljs-keyword">function</span> <span class="hljs-function"><span class="hljs-title">postinstall</span></span>()&#123;    <span class="hljs-keyword">if</span> [ ! -d <span class="hljs-string">&quot;/var/lib/etcd&quot;</span> ]; <span class="hljs-keyword">then</span>        mkdir /var/lib/etcd        chown -R etcd:etcd /var/lib/etcd    <span class="hljs-keyword">fi</span>&#125;<span class="hljs-comment"># 依次执行</span>downloadpreinstallinstallpostinstall</code></pre></div><h4 id="2-2-2、配置文件"><a href="#2-2-2、配置文件" class="headerlink" title="2.2.2、配置文件"></a>2.2.2、配置文件</h4><p><strong>关于配置文件目录结构如下(请自行复制证书)</strong></p><div class="hljs code-wrapper"><pre><code class="hljs sh">conf├── etcd.conf├── etcd.conf.cluster.example├── etcd.conf.single.example└── ssl    ├── etcd-key.pem    ├── etcd.pem    ├── etcd-root-ca-key.pem    └── etcd-root-ca.pem1 directory, 7 files</code></pre></div><ul><li>etcd.conf</li></ul><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment"># [member]</span>ETCD_NAME=etcd1ETCD_DATA_DIR=<span class="hljs-string">&quot;/var/lib/etcd/data&quot;</span>ETCD_WAL_DIR=<span class="hljs-string">&quot;/var/lib/etcd/wal&quot;</span>ETCD_SNAPSHOT_COUNT=<span class="hljs-string">&quot;100&quot;</span>ETCD_HEARTBEAT_INTERVAL=<span class="hljs-string">&quot;100&quot;</span>ETCD_ELECTION_TIMEOUT=<span class="hljs-string">&quot;1000&quot;</span>ETCD_LISTEN_PEER_URLS=<span class="hljs-string">&quot;https://192.168.1.51:2380&quot;</span>ETCD_LISTEN_CLIENT_URLS=<span class="hljs-string">&quot;https://192.168.1.51:2379,http://127.0.0.1:2379&quot;</span>ETCD_MAX_SNAPSHOTS=<span class="hljs-string">&quot;5&quot;</span>ETCD_MAX_WALS=<span class="hljs-string">&quot;5&quot;</span><span class="hljs-comment">#ETCD_CORS=&quot;&quot;</span><span class="hljs-comment"># [cluster]</span>ETCD_INITIAL_ADVERTISE_PEER_URLS=<span class="hljs-string">&quot;https://192.168.1.51:2380&quot;</span><span class="hljs-comment"># if you use different ETCD_NAME (e.g. test), set ETCD_INITIAL_CLUSTER value for this name, i.e. &quot;test=http://...&quot;</span>ETCD_INITIAL_CLUSTER=<span class="hljs-string">&quot;etcd1=https://192.168.1.51:2380,etcd2=https://192.168.1.52:2380,etcd3=https://192.168.1.53:2380&quot;</span>ETCD_INITIAL_CLUSTER_STATE=<span class="hljs-string">&quot;new&quot;</span>ETCD_INITIAL_CLUSTER_TOKEN=<span class="hljs-string">&quot;etcd-cluster&quot;</span>ETCD_ADVERTISE_CLIENT_URLS=<span class="hljs-string">&quot;https://192.168.1.51:2379&quot;</span><span class="hljs-comment">#ETCD_DISCOVERY=&quot;&quot;</span><span class="hljs-comment">#ETCD_DISCOVERY_SRV=&quot;&quot;</span><span class="hljs-comment">#ETCD_DISCOVERY_FALLBACK=&quot;proxy&quot;</span><span class="hljs-comment">#ETCD_DISCOVERY_PROXY=&quot;&quot;</span><span class="hljs-comment">#ETCD_STRICT_RECONFIG_CHECK=&quot;false&quot;</span><span class="hljs-comment">#ETCD_AUTO_COMPACTION_RETENTION=&quot;0&quot;</span><span class="hljs-comment"># [proxy]</span><span class="hljs-comment">#ETCD_PROXY=&quot;off&quot;</span><span class="hljs-comment">#ETCD_PROXY_FAILURE_WAIT=&quot;5000&quot;</span><span class="hljs-comment">#ETCD_PROXY_REFRESH_INTERVAL=&quot;30000&quot;</span><span class="hljs-comment">#ETCD_PROXY_DIAL_TIMEOUT=&quot;1000&quot;</span><span class="hljs-comment">#ETCD_PROXY_WRITE_TIMEOUT=&quot;5000&quot;</span><span class="hljs-comment">#ETCD_PROXY_READ_TIMEOUT=&quot;0&quot;</span><span class="hljs-comment"># [security]</span>ETCD_CERT_FILE=<span class="hljs-string">&quot;/etc/etcd/ssl/etcd.pem&quot;</span>ETCD_KEY_FILE=<span class="hljs-string">&quot;/etc/etcd/ssl/etcd-key.pem&quot;</span>ETCD_CLIENT_CERT_AUTH=<span class="hljs-string">&quot;true&quot;</span>ETCD_TRUSTED_CA_FILE=<span class="hljs-string">&quot;/etc/etcd/ssl/etcd-root-ca.pem&quot;</span>ETCD_AUTO_TLS=<span class="hljs-string">&quot;true&quot;</span>ETCD_PEER_CERT_FILE=<span class="hljs-string">&quot;/etc/etcd/ssl/etcd.pem&quot;</span>ETCD_PEER_KEY_FILE=<span class="hljs-string">&quot;/etc/etcd/ssl/etcd-key.pem&quot;</span>ETCD_PEER_CLIENT_CERT_AUTH=<span class="hljs-string">&quot;true&quot;</span>ETCD_PEER_TRUSTED_CA_FILE=<span class="hljs-string">&quot;/etc/etcd/ssl/etcd-root-ca.pem&quot;</span>ETCD_PEER_AUTO_TLS=<span class="hljs-string">&quot;true&quot;</span><span class="hljs-comment"># [logging]</span><span class="hljs-comment">#ETCD_DEBUG=&quot;false&quot;</span><span class="hljs-comment"># examples for -log-package-levels etcdserver=WARNING,security=DEBUG</span><span class="hljs-comment">#ETCD_LOG_PACKAGE_LEVELS=&quot;&quot;</span></code></pre></div><ul><li>etcd.service</li></ul><div class="hljs code-wrapper"><pre><code class="hljs sh">[Unit]Description=Etcd ServerAfter=network.targetAfter=network-online.targetWants=network-online.target[Service]Type=notifyWorkingDirectory=/var/lib/etcd/EnvironmentFile=-/etc/etcd/etcd.confUser=etcd<span class="hljs-comment"># set GOMAXPROCS to number of processors</span>ExecStart=/bin/bash -c <span class="hljs-string">&quot;GOMAXPROCS=<span class="hljs-subst">$(nproc)</span> /usr/local/bin/etcd --name=\&quot;<span class="hljs-variable">$&#123;ETCD_NAME&#125;</span>\&quot; --data-dir=\&quot;<span class="hljs-variable">$&#123;ETCD_DATA_DIR&#125;</span>\&quot; --listen-client-urls=\&quot;<span class="hljs-variable">$&#123;ETCD_LISTEN_CLIENT_URLS&#125;</span>\&quot;&quot;</span>Restart=on-failureLimitNOFILE=65536[Install]WantedBy=multi-user.target</code></pre></div><p>最后三台机器依次修改 <code>IP</code>、<code>ETCD_NAME</code> 然后启动即可，**生产环境请不要忘记修改集群 Token 为真实随机字符串 (<code>ETCD_INITIAL_CLUSTER_TOKEN</code> 变量)**启动后可以通过以下命令测试集群联通性</p><div class="hljs code-wrapper"><pre><code class="hljs sh">docker1.node ➜  ~ <span class="hljs-built_in">export</span> ETCDCTL_API=3docker1.node ➜  ~ etcdctl member list238b72cdd26e304f, started, etcd2, https://192.168.1.52:2380, https://192.168.1.52:23798034142cf01c5d1c, started, etcd3, https://192.168.1.53:2380, https://192.168.1.53:23798da171dbef9ded69, started, etcd1, https://192.168.1.51:2380, https://192.168.1.51:2379</code></pre></div><h2 id="三、安装-Kubernetes"><a href="#三、安装-Kubernetes" class="headerlink" title="三、安装 Kubernetes"></a>三、安装 Kubernetes</h2><h3 id="3-1、生成证书及配置"><a href="#3-1、生成证书及配置" class="headerlink" title="3.1、生成证书及配置"></a>3.1、生成证书及配置</h3><h4 id="3-1-1、生成证书"><a href="#3-1-1、生成证书" class="headerlink" title="3.1.1、生成证书"></a>3.1.1、生成证书</h4><p>新版本已经越来越趋近全面 TLS + RBAC 配置，<strong>所以本次安装将会启动大部分 TLS + RBAC 配置，包括 <code>kube-controler-manager</code>、<code>kube-scheduler</code> 组件不再连接本地 <code>kube-apiserver</code> 的 8080 非认证端口，<code>kubelet</code> 等组件 API 端点关闭匿名访问，启动 RBAC 认证等</strong>；为了满足这些认证，需要签署以下证书</p><ul><li>k8s-root-ca-csr.json 集群 CA 根证书</li></ul><div class="hljs code-wrapper"><pre><code class="hljs json">&#123;    <span class="hljs-attr">&quot;CN&quot;</span>: <span class="hljs-string">&quot;kubernetes&quot;</span>,    <span class="hljs-attr">&quot;key&quot;</span>: &#123;        <span class="hljs-attr">&quot;algo&quot;</span>: <span class="hljs-string">&quot;rsa&quot;</span>,        <span class="hljs-attr">&quot;size&quot;</span>: <span class="hljs-number">4096</span>    &#125;,    <span class="hljs-attr">&quot;names&quot;</span>: [        &#123;            <span class="hljs-attr">&quot;C&quot;</span>: <span class="hljs-string">&quot;CN&quot;</span>,            <span class="hljs-attr">&quot;ST&quot;</span>: <span class="hljs-string">&quot;BeiJing&quot;</span>,            <span class="hljs-attr">&quot;L&quot;</span>: <span class="hljs-string">&quot;BeiJing&quot;</span>,            <span class="hljs-attr">&quot;O&quot;</span>: <span class="hljs-string">&quot;kubernetes&quot;</span>,            <span class="hljs-attr">&quot;OU&quot;</span>: <span class="hljs-string">&quot;System&quot;</span>        &#125;    ],    <span class="hljs-attr">&quot;ca&quot;</span>: &#123;        <span class="hljs-attr">&quot;expiry&quot;</span>: <span class="hljs-string">&quot;87600h&quot;</span>    &#125;&#125;</code></pre></div><ul><li>k8s-gencert.json 用于生成其他证书的标准配置</li></ul><div class="hljs code-wrapper"><pre><code class="hljs json">&#123;    <span class="hljs-attr">&quot;signing&quot;</span>: &#123;        <span class="hljs-attr">&quot;default&quot;</span>: &#123;            <span class="hljs-attr">&quot;expiry&quot;</span>: <span class="hljs-string">&quot;87600h&quot;</span>        &#125;,        <span class="hljs-attr">&quot;profiles&quot;</span>: &#123;            <span class="hljs-attr">&quot;kubernetes&quot;</span>: &#123;                <span class="hljs-attr">&quot;usages&quot;</span>: [                    <span class="hljs-string">&quot;signing&quot;</span>,                    <span class="hljs-string">&quot;key encipherment&quot;</span>,                    <span class="hljs-string">&quot;server auth&quot;</span>,                    <span class="hljs-string">&quot;client auth&quot;</span>                ],                <span class="hljs-attr">&quot;expiry&quot;</span>: <span class="hljs-string">&quot;87600h&quot;</span>            &#125;        &#125;    &#125;&#125;</code></pre></div><ul><li>kube-apiserver-csr.json apiserver TLS 认证端口需要的证书</li></ul><div class="hljs code-wrapper"><pre><code class="hljs json">&#123;    <span class="hljs-attr">&quot;CN&quot;</span>: <span class="hljs-string">&quot;kubernetes&quot;</span>,    <span class="hljs-attr">&quot;hosts&quot;</span>: [        <span class="hljs-string">&quot;127.0.0.1&quot;</span>,        <span class="hljs-string">&quot;10.254.0.1&quot;</span>,        <span class="hljs-string">&quot;localhost&quot;</span>,        <span class="hljs-string">&quot;*.master.kubernetes.node&quot;</span>,        <span class="hljs-string">&quot;kubernetes&quot;</span>,        <span class="hljs-string">&quot;kubernetes.default&quot;</span>,        <span class="hljs-string">&quot;kubernetes.default.svc&quot;</span>,        <span class="hljs-string">&quot;kubernetes.default.svc.cluster&quot;</span>,        <span class="hljs-string">&quot;kubernetes.default.svc.cluster.local&quot;</span>    ],    <span class="hljs-attr">&quot;key&quot;</span>: &#123;        <span class="hljs-attr">&quot;algo&quot;</span>: <span class="hljs-string">&quot;rsa&quot;</span>,        <span class="hljs-attr">&quot;size&quot;</span>: <span class="hljs-number">2048</span>    &#125;,    <span class="hljs-attr">&quot;names&quot;</span>: [        &#123;            <span class="hljs-attr">&quot;C&quot;</span>: <span class="hljs-string">&quot;CN&quot;</span>,            <span class="hljs-attr">&quot;ST&quot;</span>: <span class="hljs-string">&quot;BeiJing&quot;</span>,            <span class="hljs-attr">&quot;L&quot;</span>: <span class="hljs-string">&quot;BeiJing&quot;</span>,            <span class="hljs-attr">&quot;O&quot;</span>: <span class="hljs-string">&quot;kubernetes&quot;</span>,            <span class="hljs-attr">&quot;OU&quot;</span>: <span class="hljs-string">&quot;System&quot;</span>        &#125;    ]&#125;</code></pre></div><ul><li>kube-controller-manager-csr.json controller manager 连接 apiserver 需要使用的证书，同时本身 <code>10257</code> 端口也会使用此证书</li></ul><div class="hljs code-wrapper"><pre><code class="hljs json">&#123;  <span class="hljs-attr">&quot;CN&quot;</span>: <span class="hljs-string">&quot;system:kube-controller-manager&quot;</span>,  <span class="hljs-attr">&quot;hosts&quot;</span>: [    <span class="hljs-string">&quot;127.0.0.1&quot;</span>,    <span class="hljs-string">&quot;localhost&quot;</span>,    <span class="hljs-string">&quot;*.master.kubernetes.node&quot;</span>  ],  <span class="hljs-attr">&quot;key&quot;</span>: &#123;    <span class="hljs-attr">&quot;algo&quot;</span>: <span class="hljs-string">&quot;rsa&quot;</span>,    <span class="hljs-attr">&quot;size&quot;</span>: <span class="hljs-number">2048</span>  &#125;,  <span class="hljs-attr">&quot;names&quot;</span>: [    &#123;      <span class="hljs-attr">&quot;C&quot;</span>: <span class="hljs-string">&quot;CN&quot;</span>,      <span class="hljs-attr">&quot;ST&quot;</span>: <span class="hljs-string">&quot;BeiJing&quot;</span>,      <span class="hljs-attr">&quot;L&quot;</span>: <span class="hljs-string">&quot;BeiJing&quot;</span>,      <span class="hljs-attr">&quot;O&quot;</span>: <span class="hljs-string">&quot;system:kube-controller-manager&quot;</span>,      <span class="hljs-attr">&quot;OU&quot;</span>: <span class="hljs-string">&quot;System&quot;</span>    &#125;  ]&#125;</code></pre></div><ul><li>kube-scheduler-csr.json scheduler 连接 apiserver 需要使用的证书，同时本身 <code>10259</code> 端口也会使用此证书</li></ul><div class="hljs code-wrapper"><pre><code class="hljs json">&#123;  <span class="hljs-attr">&quot;CN&quot;</span>: <span class="hljs-string">&quot;system:kube-scheduler&quot;</span>,  <span class="hljs-attr">&quot;hosts&quot;</span>: [    <span class="hljs-string">&quot;127.0.0.1&quot;</span>,    <span class="hljs-string">&quot;localhost&quot;</span>,    <span class="hljs-string">&quot;*.master.kubernetes.node&quot;</span>  ],  <span class="hljs-attr">&quot;key&quot;</span>: &#123;    <span class="hljs-attr">&quot;algo&quot;</span>: <span class="hljs-string">&quot;rsa&quot;</span>,    <span class="hljs-attr">&quot;size&quot;</span>: <span class="hljs-number">2048</span>  &#125;,  <span class="hljs-attr">&quot;names&quot;</span>: [    &#123;      <span class="hljs-attr">&quot;C&quot;</span>: <span class="hljs-string">&quot;CN&quot;</span>,      <span class="hljs-attr">&quot;ST&quot;</span>: <span class="hljs-string">&quot;BeiJing&quot;</span>,      <span class="hljs-attr">&quot;L&quot;</span>: <span class="hljs-string">&quot;BeiJing&quot;</span>,      <span class="hljs-attr">&quot;O&quot;</span>: <span class="hljs-string">&quot;system:kube-scheduler&quot;</span>,      <span class="hljs-attr">&quot;OU&quot;</span>: <span class="hljs-string">&quot;System&quot;</span>    &#125;  ]&#125;</code></pre></div><ul><li>kube-proxy-csr.json proxy 组件连接 apiserver 需要使用的证书</li></ul><div class="hljs code-wrapper"><pre><code class="hljs json">&#123;    <span class="hljs-attr">&quot;CN&quot;</span>: <span class="hljs-string">&quot;system:kube-proxy&quot;</span>,    <span class="hljs-attr">&quot;hosts&quot;</span>: [],    <span class="hljs-attr">&quot;key&quot;</span>: &#123;        <span class="hljs-attr">&quot;algo&quot;</span>: <span class="hljs-string">&quot;rsa&quot;</span>,        <span class="hljs-attr">&quot;size&quot;</span>: <span class="hljs-number">2048</span>    &#125;,    <span class="hljs-attr">&quot;names&quot;</span>: [        &#123;            <span class="hljs-attr">&quot;C&quot;</span>: <span class="hljs-string">&quot;CN&quot;</span>,            <span class="hljs-attr">&quot;ST&quot;</span>: <span class="hljs-string">&quot;BeiJing&quot;</span>,            <span class="hljs-attr">&quot;L&quot;</span>: <span class="hljs-string">&quot;BeiJing&quot;</span>,            <span class="hljs-attr">&quot;O&quot;</span>: <span class="hljs-string">&quot;system:kube-proxy&quot;</span>,            <span class="hljs-attr">&quot;OU&quot;</span>: <span class="hljs-string">&quot;System&quot;</span>        &#125;    ]&#125;</code></pre></div><ul><li>kubelet-api-admin-csr.json apiserver 反向连接 kubelet 组件 <code>10250</code> 端口需要使用的证书(例如执行 <code>kubectl logs</code>)</li></ul><div class="hljs code-wrapper"><pre><code class="hljs json">&#123;    <span class="hljs-attr">&quot;CN&quot;</span>: <span class="hljs-string">&quot;system:kubelet-api-admin&quot;</span>,    <span class="hljs-attr">&quot;hosts&quot;</span>: [],    <span class="hljs-attr">&quot;key&quot;</span>: &#123;        <span class="hljs-attr">&quot;algo&quot;</span>: <span class="hljs-string">&quot;rsa&quot;</span>,        <span class="hljs-attr">&quot;size&quot;</span>: <span class="hljs-number">2048</span>    &#125;,    <span class="hljs-attr">&quot;names&quot;</span>: [        &#123;            <span class="hljs-attr">&quot;C&quot;</span>: <span class="hljs-string">&quot;CN&quot;</span>,            <span class="hljs-attr">&quot;ST&quot;</span>: <span class="hljs-string">&quot;BeiJing&quot;</span>,            <span class="hljs-attr">&quot;L&quot;</span>: <span class="hljs-string">&quot;BeiJing&quot;</span>,            <span class="hljs-attr">&quot;O&quot;</span>: <span class="hljs-string">&quot;system:kubelet-api-admin&quot;</span>,            <span class="hljs-attr">&quot;OU&quot;</span>: <span class="hljs-string">&quot;System&quot;</span>        &#125;    ]&#125;</code></pre></div><ul><li>admin-csr.json 集群管理员(kubectl)连接 apiserver 需要使用的证书</li></ul><div class="hljs code-wrapper"><pre><code class="hljs json">&#123;    <span class="hljs-attr">&quot;CN&quot;</span>: <span class="hljs-string">&quot;system:masters&quot;</span>,    <span class="hljs-attr">&quot;hosts&quot;</span>: [],    <span class="hljs-attr">&quot;key&quot;</span>: &#123;        <span class="hljs-attr">&quot;algo&quot;</span>: <span class="hljs-string">&quot;rsa&quot;</span>,        <span class="hljs-attr">&quot;size&quot;</span>: <span class="hljs-number">2048</span>    &#125;,    <span class="hljs-attr">&quot;names&quot;</span>: [        &#123;            <span class="hljs-attr">&quot;C&quot;</span>: <span class="hljs-string">&quot;CN&quot;</span>,            <span class="hljs-attr">&quot;ST&quot;</span>: <span class="hljs-string">&quot;BeiJing&quot;</span>,            <span class="hljs-attr">&quot;L&quot;</span>: <span class="hljs-string">&quot;BeiJing&quot;</span>,            <span class="hljs-attr">&quot;O&quot;</span>: <span class="hljs-string">&quot;system:masters&quot;</span>,            <span class="hljs-attr">&quot;OU&quot;</span>: <span class="hljs-string">&quot;System&quot;</span>        &#125;    ]&#125;</code></pre></div><p><strong>注意: 请不要修改证书配置的 <code>CN</code>、<code>O</code> 字段，这两个字段名称比较特殊，大多数为 <code>system:</code> 开头，实际上是为了匹配 RBAC 规则，具体请参考 <a href="https://kubernetes.io/docs/reference/access-authn-authz/rbac/#default-roles-and-role-bindings">Default Roles and Role Bindings</a></strong></p><p>最后使用如下命令生成即可:</p><div class="hljs code-wrapper"><pre><code class="hljs sh">cfssl gencert --initca=<span class="hljs-literal">true</span> k8s-root-ca-csr.json | cfssljson --bare k8s-root-ca<span class="hljs-keyword">for</span> targetName <span class="hljs-keyword">in</span> kube-apiserver kube-controller-manager kube-scheduler kube-proxy kubelet-api-admin admin; <span class="hljs-keyword">do</span>    cfssl gencert --ca k8s-root-ca.pem --ca-key k8s-root-ca-key.pem --config k8s-gencert.json --profile kubernetes <span class="hljs-variable">$targetName</span>-csr.json | cfssljson --bare <span class="hljs-variable">$targetName</span><span class="hljs-keyword">done</span></code></pre></div><h4 id="3-1-2、生成配置文件"><a href="#3-1-2、生成配置文件" class="headerlink" title="3.1.2、生成配置文件"></a>3.1.2、生成配置文件</h4><p>集群搭建需要预先生成一系列配置文件，生成配置需要预先安装 <code>kubectl</code> 命令，请自行根据文档安装 <a href="https://kubernetes.io/docs/tasks/tools/install-kubectl/#install-kubectl-binary-using-curl">Install kubectl binary using curl</a>；其中配置文件及其作用如下:</p><ul><li><code>bootstrap.kubeconfig</code> kubelet TLS Bootstarp 引导阶段需要使用的配置文件</li><li><code>kube-controller-manager.kubeconfig</code> controller manager 组件开启安全端口及 RBAC 认证所需配置</li><li><code>kube-scheduler.kubeconfig</code> scheduler 组件开启安全端口及 RBAC 认证所需配置</li><li><code>kube-proxy.kubeconfig</code> proxy 组件连接 apiserver 所需配置文件</li><li><code>audit-policy.yaml</code> apiserver RBAC 审计日志配置文件</li><li><code>bootstrap.secret.yaml</code> kubelet TLS Bootstarp 引导阶段使用 Bootstrap Token 方式引导，需要预先创建此 Token</li></ul><p>生成这些配置文件的脚本如下</p><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment"># 指定 apiserver 地址</span>KUBE_APISERVER=<span class="hljs-string">&quot;https://127.0.0.1:6443&quot;</span><span class="hljs-comment"># 生成 Bootstrap Token</span>BOOTSTRAP_TOKEN_ID=$(head -c 6 /dev/urandom | md5sum | head -c 6)BOOTSTRAP_TOKEN_SECRET=$(head -c 16 /dev/urandom | md5sum | head -c 16)BOOTSTRAP_TOKEN=<span class="hljs-string">&quot;<span class="hljs-variable">$&#123;BOOTSTRAP_TOKEN_ID&#125;</span>.<span class="hljs-variable">$&#123;BOOTSTRAP_TOKEN_SECRET&#125;</span>&quot;</span><span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;Bootstrap Tokne: <span class="hljs-variable">$&#123;BOOTSTRAP_TOKEN&#125;</span>&quot;</span><span class="hljs-comment"># 生成 kubelet tls bootstrap 配置</span><span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;Create kubelet bootstrapping kubeconfig...&quot;</span>kubectl config set-cluster kubernetes \  --certificate-authority=k8s-root-ca.pem \  --embed-certs=<span class="hljs-literal">true</span> \  --server=<span class="hljs-variable">$&#123;KUBE_APISERVER&#125;</span> \  --kubeconfig=bootstrap.kubeconfigkubectl config set-credentials <span class="hljs-string">&quot;system:bootstrap:<span class="hljs-variable">$&#123;BOOTSTRAP_TOKEN_ID&#125;</span>&quot;</span> \  --token=<span class="hljs-variable">$&#123;BOOTSTRAP_TOKEN&#125;</span> \  --kubeconfig=bootstrap.kubeconfigkubectl config set-context default \  --cluster=kubernetes \  --user=<span class="hljs-string">&quot;system:bootstrap:<span class="hljs-variable">$&#123;BOOTSTRAP_TOKEN_ID&#125;</span>&quot;</span> \  --kubeconfig=bootstrap.kubeconfigkubectl config use-context default --kubeconfig=bootstrap.kubeconfig<span class="hljs-comment"># 生成 kube-controller-manager 配置文件</span><span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;Create kube-controller-manager kubeconfig...&quot;</span>kubectl config set-cluster kubernetes \  --certificate-authority=k8s-root-ca.pem \  --embed-certs=<span class="hljs-literal">true</span> \  --server=<span class="hljs-variable">$&#123;KUBE_APISERVER&#125;</span> \  --kubeconfig=kube-controller-manager.kubeconfigkubectl config set-credentials <span class="hljs-string">&quot;system:kube-controller-manager&quot;</span> \  --client-certificate=kube-controller-manager.pem \  --client-key=kube-controller-manager-key.pem \  --embed-certs=<span class="hljs-literal">true</span> \  --kubeconfig=kube-controller-manager.kubeconfigkubectl config set-context default \  --cluster=kubernetes \  --user=system:kube-controller-manager \  --kubeconfig=kube-controller-manager.kubeconfigkubectl config use-context default --kubeconfig=kube-controller-manager.kubeconfig <span class="hljs-comment"># 生成 kube-scheduler 配置文件</span><span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;Create kube-scheduler kubeconfig...&quot;</span>kubectl config set-cluster kubernetes \  --certificate-authority=k8s-root-ca.pem \  --embed-certs=<span class="hljs-literal">true</span> \  --server=<span class="hljs-variable">$&#123;KUBE_APISERVER&#125;</span> \  --kubeconfig=kube-scheduler.kubeconfigkubectl config set-credentials <span class="hljs-string">&quot;system:kube-scheduler&quot;</span> \  --client-certificate=kube-scheduler.pem \  --client-key=kube-scheduler-key.pem \  --embed-certs=<span class="hljs-literal">true</span> \  --kubeconfig=kube-scheduler.kubeconfigkubectl config set-context default \  --cluster=kubernetes \  --user=system:kube-scheduler \  --kubeconfig=kube-scheduler.kubeconfigkubectl config use-context default --kubeconfig=kube-scheduler.kubeconfig <span class="hljs-comment"># 生成 kube-proxy 配置文件</span><span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;Create kube-proxy kubeconfig...&quot;</span>kubectl config set-cluster kubernetes \  --certificate-authority=k8s-root-ca.pem \  --embed-certs=<span class="hljs-literal">true</span> \  --server=<span class="hljs-variable">$&#123;KUBE_APISERVER&#125;</span> \  --kubeconfig=kube-proxy.kubeconfigkubectl config set-credentials <span class="hljs-string">&quot;system:kube-proxy&quot;</span> \  --client-certificate=kube-proxy.pem \  --client-key=kube-proxy-key.pem \  --embed-certs=<span class="hljs-literal">true</span> \  --kubeconfig=kube-proxy.kubeconfigkubectl config set-context default \  --cluster=kubernetes \  --user=system:kube-proxy \  --kubeconfig=kube-proxy.kubeconfigkubectl config use-context default --kubeconfig=kube-proxy.kubeconfig <span class="hljs-comment"># 生成 apiserver RBAC 审计配置文件 </span>cat &gt;&gt; audit-policy.yaml &lt;&lt;<span class="hljs-string">EOF</span><span class="hljs-string"># Log all requests at the Metadata level.</span><span class="hljs-string">apiVersion: audit.k8s.io/v1</span><span class="hljs-string">kind: Policy</span><span class="hljs-string">rules:</span><span class="hljs-string">- level: Metadata</span><span class="hljs-string">EOF</span><span class="hljs-comment"># 生成 tls bootstrap token secret 配置文件</span>cat &gt;&gt; bootstrap.secret.yaml &lt;&lt;<span class="hljs-string">EOF</span><span class="hljs-string">apiVersion: v1</span><span class="hljs-string">kind: Secret</span><span class="hljs-string">metadata:</span><span class="hljs-string">  # Name MUST be of form &quot;bootstrap-token-&lt;token id&gt;&quot;</span><span class="hljs-string">  name: bootstrap-token-$&#123;BOOTSTRAP_TOKEN_ID&#125;</span><span class="hljs-string">  namespace: kube-system</span><span class="hljs-string"># Type MUST be &#x27;bootstrap.kubernetes.io/token&#x27;</span><span class="hljs-string">type: bootstrap.kubernetes.io/token</span><span class="hljs-string">stringData:</span><span class="hljs-string">  # Human readable description. Optional.</span><span class="hljs-string">  description: &quot;The default bootstrap token.&quot;</span><span class="hljs-string">  # Token ID and secret. Required.</span><span class="hljs-string">  token-id: $&#123;BOOTSTRAP_TOKEN_ID&#125;</span><span class="hljs-string">  token-secret: $&#123;BOOTSTRAP_TOKEN_SECRET&#125;</span><span class="hljs-string">  # Expiration. Optional.</span><span class="hljs-string">  expiration: $(date -d&#x27;+2 day&#x27; -u +&quot;%Y-%m-%dT%H:%M:%SZ&quot;)</span><span class="hljs-string">  # Allowed usages.</span><span class="hljs-string">  usage-bootstrap-authentication: &quot;true&quot;</span><span class="hljs-string">  usage-bootstrap-signing: &quot;true&quot;</span><span class="hljs-string">  # Extra groups to authenticate the token as. Must start with &quot;system:bootstrappers:&quot;</span><span class="hljs-string">#  auth-extra-groups: system:bootstrappers:worker,system:bootstrappers:ingress</span><span class="hljs-string">EOF</span></code></pre></div><h3 id="3-2、处理-ipvs-及依赖"><a href="#3-2、处理-ipvs-及依赖" class="headerlink" title="3.2、处理 ipvs 及依赖"></a>3.2、处理 ipvs 及依赖</h3><p>新版本目前 <code>kube-proxy</code> 组件全部采用 ipvs 方式负载，所以为了 <code>kube-proxy</code> 能正常工作需要预先处理一下 ipvs 配置以及相关依赖(每台 node 都要处理)</p><div class="hljs code-wrapper"><pre><code class="hljs sh">cat &gt;&gt; /etc/sysctl.conf &lt;&lt;<span class="hljs-string">EOF</span><span class="hljs-string">net.ipv4.ip_forward=1</span><span class="hljs-string">net.bridge.bridge-nf-call-iptables=1</span><span class="hljs-string">net.bridge.bridge-nf-call-ip6tables=1</span><span class="hljs-string">EOF</span>sysctl -pcat &gt;&gt; /etc/modules &lt;&lt;<span class="hljs-string">EOF</span><span class="hljs-string">ip_vs</span><span class="hljs-string">ip_vs_lc</span><span class="hljs-string">ip_vs_wlc</span><span class="hljs-string">ip_vs_rr</span><span class="hljs-string">ip_vs_wrr</span><span class="hljs-string">ip_vs_lblc</span><span class="hljs-string">ip_vs_lblcr</span><span class="hljs-string">ip_vs_dh</span><span class="hljs-string">ip_vs_sh</span><span class="hljs-string">ip_vs_fo</span><span class="hljs-string">ip_vs_nq</span><span class="hljs-string">ip_vs_sed</span><span class="hljs-string">ip_vs_ftp</span><span class="hljs-string">EOF</span>apt install -y conntrack ipvsadm</code></pre></div><h3 id="3-3、部署-Master"><a href="#3-3、部署-Master" class="headerlink" title="3.3、部署 Master"></a>3.3、部署 Master</h3><h4 id="3-3-1、安装脚本"><a href="#3-3-1、安装脚本" class="headerlink" title="3.3.1、安装脚本"></a>3.3.1、安装脚本</h4><p>master 节点上需要三个组件: <code>kube-apiserver</code>、<code>kube-controller-manager</code>、<code>kube-scheduler</code></p><p><strong>安装流程整体为以下几步</strong></p><ul><li><strong>创建单独的 <code>kube</code> 用户</strong></li><li><strong>复制相关二进制文件到 <code>/usr/bin</code>，可以采用 <code>all in one</code> 的 <code>hyperkube</code></strong></li><li><strong>复制配置文件到 <code>/etc/kubernetes</code></strong></li><li><strong>复制证书文件到 <code>/etc/kubernetes/ssl</code></strong></li><li><strong>修改配置并启动</strong></li></ul><p>安装脚本如下所示:</p><div class="hljs code-wrapper"><pre><code class="hljs sh">KUBE_DEFAULT_VERSION=<span class="hljs-string">&quot;1.13.4&quot;</span><span class="hljs-keyword">if</span> [ <span class="hljs-string">&quot;<span class="hljs-variable">$1</span>&quot;</span> != <span class="hljs-string">&quot;&quot;</span> ]; <span class="hljs-keyword">then</span>  KUBE_VERSION=<span class="hljs-variable">$1</span><span class="hljs-keyword">else</span>  <span class="hljs-built_in">echo</span> -e <span class="hljs-string">&quot;\033[33mWARNING: KUBE_VERSION is blank,use default version: <span class="hljs-variable">$&#123;KUBE_DEFAULT_VERSION&#125;</span>\033[0m&quot;</span>  KUBE_VERSION=<span class="hljs-variable">$&#123;KUBE_DEFAULT_VERSION&#125;</span><span class="hljs-keyword">fi</span><span class="hljs-comment"># 下载 hyperkube</span><span class="hljs-keyword">function</span> <span class="hljs-function"><span class="hljs-title">download_k8s</span></span>()&#123;    <span class="hljs-keyword">if</span> [ ! -f <span class="hljs-string">&quot;hyperkube_v<span class="hljs-variable">$&#123;KUBE_VERSION&#125;</span>&quot;</span> ]; <span class="hljs-keyword">then</span>        wget https://storage.googleapis.com/kubernetes-release/release/v<span class="hljs-variable">$&#123;KUBE_VERSION&#125;</span>/bin/linux/amd64/hyperkube -O hyperkube_v<span class="hljs-variable">$&#123;KUBE_VERSION&#125;</span>        chmod +x hyperkube_v<span class="hljs-variable">$&#123;KUBE_VERSION&#125;</span>    <span class="hljs-keyword">fi</span>&#125;<span class="hljs-comment"># 创建专用用户 kube</span><span class="hljs-keyword">function</span> <span class="hljs-function"><span class="hljs-title">preinstall</span></span>()&#123;    getent group kube &gt;/dev/null || groupadd -r kube    getent passwd kube &gt;/dev/null || useradd -r -g kube -d / -s /sbin/nologin -c <span class="hljs-string">&quot;Kubernetes user&quot;</span> kube&#125;<span class="hljs-comment"># 复制可执行文件和配置以及证书</span><span class="hljs-keyword">function</span> <span class="hljs-function"><span class="hljs-title">install_k8s</span></span>()&#123;    <span class="hljs-built_in">echo</span> -e <span class="hljs-string">&quot;\033[32mINFO: Copy hyperkube...\033[0m&quot;</span>    cp hyperkube_v<span class="hljs-variable">$&#123;KUBE_VERSION&#125;</span> /usr/bin/hyperkube    <span class="hljs-built_in">echo</span> -e <span class="hljs-string">&quot;\033[32mINFO: Create symbolic link...\033[0m&quot;</span>    (<span class="hljs-built_in">cd</span> /usr/bin &amp;&amp; hyperkube --make-symlinks)    <span class="hljs-built_in">echo</span> -e <span class="hljs-string">&quot;\033[32mINFO: Copy kubernetes config...\033[0m&quot;</span>    cp -r conf /etc/kubernetes    <span class="hljs-keyword">if</span> [ -d <span class="hljs-string">&quot;/etc/kubernetes/ssl&quot;</span> ]; <span class="hljs-keyword">then</span>        chown -R kube:kube /etc/kubernetes/ssl    <span class="hljs-keyword">fi</span>    <span class="hljs-built_in">echo</span> -e <span class="hljs-string">&quot;\033[32mINFO: Copy kubernetes systemd config...\033[0m&quot;</span>    cp systemd/*.service /lib/systemd/system    systemctl daemon-reload&#125;<span class="hljs-comment"># 创建必要的目录并修改权限</span><span class="hljs-keyword">function</span> <span class="hljs-function"><span class="hljs-title">postinstall</span></span>()&#123;    <span class="hljs-keyword">if</span> [ ! -d <span class="hljs-string">&quot;/var/log/kube-audit&quot;</span> ]; <span class="hljs-keyword">then</span>        mkdir /var/<span class="hljs-built_in">log</span>/kube-audit    <span class="hljs-keyword">fi</span>        <span class="hljs-keyword">if</span> [ ! -d <span class="hljs-string">&quot;/var/lib/kubelet&quot;</span> ]; <span class="hljs-keyword">then</span>        mkdir /var/lib/kubelet    <span class="hljs-keyword">fi</span>    <span class="hljs-keyword">if</span> [ ! -d <span class="hljs-string">&quot;/usr/libexec&quot;</span> ]; <span class="hljs-keyword">then</span>        mkdir /usr/libexec    <span class="hljs-keyword">fi</span>    chown -R kube:kube /etc/kubernetes /var/<span class="hljs-built_in">log</span>/kube-audit /var/lib/kubelet /usr/libexec&#125;<span class="hljs-comment"># 执行</span>download_k8spreinstallinstall_k8spostinstall</code></pre></div><p><strong>hyperkube 是一个多合一的可执行文件，通过 <code>--make-symlinks</code> 会在当前目录生成 kubernetes 各个组件的软连接</strong></p><p>被复制的 conf 目录结构如下(最终被复制到 <code>/etc/kubernetes</code>)</p><div class="hljs code-wrapper"><pre><code class="hljs sh">.├── apiserver├── audit-policy.yaml├── bootstrap.kubeconfig├── bootstrap.secret.yaml├── controller-manager├── kube-controller-manager.kubeconfig├── kubelet├── kube-proxy.kubeconfig├── kube-scheduler.kubeconfig├── proxy├── scheduler└── ssl    ├── admin-key.pem    ├── admin.pem    ├── k8s-root-ca-key.pem    ├── k8s-root-ca.pem    ├── kube-apiserver-key.pem    ├── kube-apiserver.pem    ├── kube-controller-manager-key.pem    ├── kube-controller-manager.pem    ├── kubelet-api-admin-key.pem    ├── kubelet-api-admin.pem    ├── kube-proxy-key.pem    ├── kube-proxy.pem    ├── kube-scheduler-key.pem    └── kube-scheduler.pem1 directory, 25 files</code></pre></div><h4 id="3-3-2、配置文件"><a href="#3-3-2、配置文件" class="headerlink" title="3.3.2、配置文件"></a>3.3.2、配置文件</h4><p>以下为相关配置文件内容</p><p><strong>systemd 配置如下</strong></p><ul><li>kube-apiserver.service</li></ul><div class="hljs code-wrapper"><pre><code class="hljs sh">[Unit]Description=Kubernetes API ServerDocumentation=https://github.com/GoogleCloudPlatform/kubernetesAfter=network.targetAfter=etcd.service[Service]EnvironmentFile=-/etc/kubernetes/apiserverUser=kubeExecStart=/usr/bin/kube-apiserver \    <span class="hljs-variable">$KUBE_LOGTOSTDERR</span> \    <span class="hljs-variable">$KUBE_LOG_LEVEL</span> \    <span class="hljs-variable">$KUBE_ETCD_SERVERS</span> \    <span class="hljs-variable">$KUBE_API_ADDRESS</span> \    <span class="hljs-variable">$KUBE_API_PORT</span> \    <span class="hljs-variable">$KUBELET_PORT</span> \    <span class="hljs-variable">$KUBE_ALLOW_PRIV</span> \    <span class="hljs-variable">$KUBE_SERVICE_ADDRESSES</span> \    <span class="hljs-variable">$KUBE_ADMISSION_CONTROL</span> \    <span class="hljs-variable">$KUBE_API_ARGS</span>Restart=on-failureType=notifyLimitNOFILE=65536[Install]WantedBy=multi-user.target</code></pre></div><ul><li>kube-controller-manager.service</li></ul><div class="hljs code-wrapper"><pre><code class="hljs sh">[Unit]Description=Kubernetes Controller ManagerDocumentation=https://github.com/GoogleCloudPlatform/kubernetes[Service]EnvironmentFile=-/etc/kubernetes/controller-managerUser=kubeExecStart=/usr/bin/kube-controller-manager \    <span class="hljs-variable">$KUBE_LOGTOSTDERR</span> \    <span class="hljs-variable">$KUBE_LOG_LEVEL</span> \    <span class="hljs-variable">$KUBE_MASTER</span> \    <span class="hljs-variable">$KUBE_CONTROLLER_MANAGER_ARGS</span>Restart=on-failureLimitNOFILE=65536[Install]WantedBy=multi-user.target</code></pre></div><ul><li>kube-scheduler.service</li></ul><div class="hljs code-wrapper"><pre><code class="hljs sh">[Unit]Description=Kubernetes Scheduler PluginDocumentation=https://github.com/GoogleCloudPlatform/kubernetes[Service]EnvironmentFile=-/etc/kubernetes/schedulerUser=kubeExecStart=/usr/bin/kube-scheduler \    <span class="hljs-variable">$KUBE_LOGTOSTDERR</span> \    <span class="hljs-variable">$KUBE_LOG_LEVEL</span> \    <span class="hljs-variable">$KUBE_MASTER</span> \    <span class="hljs-variable">$KUBE_SCHEDULER_ARGS</span>Restart=on-failureLimitNOFILE=65536[Install]WantedBy=multi-user.target</code></pre></div><p><strong>核心配置文件</strong></p><ul><li>apiserver</li></ul><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment">###</span><span class="hljs-comment"># kubernetes system config</span><span class="hljs-comment">#</span><span class="hljs-comment"># The following values are used to configure the kube-apiserver</span><span class="hljs-comment">#</span><span class="hljs-comment"># The address on the local server to listen to.</span>KUBE_API_ADDRESS=<span class="hljs-string">&quot;--advertise-address=192.168.1.51 --bind-address=0.0.0.0&quot;</span><span class="hljs-comment"># The port on the local server to listen on.</span>KUBE_API_PORT=<span class="hljs-string">&quot;--secure-port=6443&quot;</span><span class="hljs-comment"># Port minions listen on</span><span class="hljs-comment"># KUBELET_PORT=&quot;--kubelet-port=10250&quot;</span><span class="hljs-comment"># Comma separated list of nodes in the etcd cluster</span>KUBE_ETCD_SERVERS=<span class="hljs-string">&quot;--etcd-servers=https://192.168.1.51:2379,https://192.168.1.52:2379,https://192.168.1.53:2379&quot;</span><span class="hljs-comment"># Address range to use for services</span>KUBE_SERVICE_ADDRESSES=<span class="hljs-string">&quot;--service-cluster-ip-range=10.254.0.0/16&quot;</span><span class="hljs-comment"># default admission control policies</span>KUBE_ADMISSION_CONTROL=<span class="hljs-string">&quot;--enable-admission-plugins=NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,Priority,ResourceQuota&quot;</span><span class="hljs-comment"># Add your own!</span>KUBE_API_ARGS=<span class="hljs-string">&quot; --allow-privileged=true \</span><span class="hljs-string">                --anonymous-auth=false \</span><span class="hljs-string">                --alsologtostderr \</span><span class="hljs-string">                --apiserver-count=3 \</span><span class="hljs-string">                --audit-log-maxage=30 \</span><span class="hljs-string">                --audit-log-maxbackup=3 \</span><span class="hljs-string">                --audit-log-maxsize=100 \</span><span class="hljs-string">                --audit-log-path=/var/log/kube-audit/audit.log \</span><span class="hljs-string">                --audit-policy-file=/etc/kubernetes/audit-policy.yaml \</span><span class="hljs-string">                --authorization-mode=Node,RBAC \</span><span class="hljs-string">                --client-ca-file=/etc/kubernetes/ssl/k8s-root-ca.pem \</span><span class="hljs-string">                --enable-bootstrap-token-auth \</span><span class="hljs-string">                --enable-garbage-collector \</span><span class="hljs-string">                --enable-logs-handler \</span><span class="hljs-string">                --endpoint-reconciler-type=lease \</span><span class="hljs-string">                --etcd-cafile=/etc/etcd/ssl/etcd-root-ca.pem \</span><span class="hljs-string">                --etcd-certfile=/etc/etcd/ssl/etcd.pem \</span><span class="hljs-string">                --etcd-keyfile=/etc/etcd/ssl/etcd-key.pem \</span><span class="hljs-string">                --etcd-compaction-interval=0s \</span><span class="hljs-string">                --event-ttl=168h0m0s \</span><span class="hljs-string">                --kubelet-https=true \</span><span class="hljs-string">                --kubelet-certificate-authority=/etc/kubernetes/ssl/k8s-root-ca.pem \</span><span class="hljs-string">                --kubelet-client-certificate=/etc/kubernetes/ssl/kubelet-api-admin.pem \</span><span class="hljs-string">                --kubelet-client-key=/etc/kubernetes/ssl/kubelet-api-admin-key.pem \</span><span class="hljs-string">                --kubelet-timeout=3s \</span><span class="hljs-string">                --runtime-config=api/all=true \</span><span class="hljs-string">                --service-node-port-range=30000-50000 \</span><span class="hljs-string">                --service-account-key-file=/etc/kubernetes/ssl/k8s-root-ca.pem \</span><span class="hljs-string">                --tls-cert-file=/etc/kubernetes/ssl/kube-apiserver.pem \</span><span class="hljs-string">                --tls-private-key-file=/etc/kubernetes/ssl/kube-apiserver-key.pem \</span><span class="hljs-string">                --v=2&quot;</span></code></pre></div><p>配置解释:</p><table><thead><tr><th>选项</th><th>作用</th></tr></thead><tbody><tr><td><code>--client-ca-file</code></td><td>定义客户端 CA</td></tr><tr><td><code>--endpoint-reconciler-type</code></td><td>master endpoint 策略</td></tr><tr><td><code>--kubelet-client-certificate</code>、<code>--kubelet-client-key</code></td><td>master 反向连接 kubelet 使用的证书</td></tr><tr><td><code>--service-account-key-file</code></td><td>service account 签名 key(用于有效性验证)</td></tr><tr><td><code>--tls-cert-file</code>、<code>--tls-private-key-file</code></td><td>master apiserver <code>6443</code> 端口证书</td></tr></tbody></table><ul><li>controller-manager</li></ul><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment">###</span><span class="hljs-comment"># The following values are used to configure the kubernetes controller-manager</span><span class="hljs-comment"># defaults from config and apiserver should be adequate</span><span class="hljs-comment"># Add your own!</span>KUBE_CONTROLLER_MANAGER_ARGS=<span class="hljs-string">&quot;  --address=127.0.0.1 \</span><span class="hljs-string">                                --authentication-kubeconfig=/etc/kubernetes/kube-controller-manager.kubeconfig \</span><span class="hljs-string">                                --authorization-kubeconfig=/etc/kubernetes/kube-controller-manager.kubeconfig \</span><span class="hljs-string">                                --bind-address=0.0.0.0 \</span><span class="hljs-string">                                --cluster-name=kubernetes \</span><span class="hljs-string">                                --cluster-signing-cert-file=/etc/kubernetes/ssl/k8s-root-ca.pem \</span><span class="hljs-string">                                --cluster-signing-key-file=/etc/kubernetes/ssl/k8s-root-ca-key.pem \</span><span class="hljs-string">                                --client-ca-file=/etc/kubernetes/ssl/k8s-root-ca.pem \</span><span class="hljs-string">                                --controllers=*,bootstrapsigner,tokencleaner \</span><span class="hljs-string">                                --deployment-controller-sync-period=10s \</span><span class="hljs-string">                                --experimental-cluster-signing-duration=87600h0m0s \</span><span class="hljs-string">                                --enable-garbage-collector=true \</span><span class="hljs-string">                                --kubeconfig=/etc/kubernetes/kube-controller-manager.kubeconfig \</span><span class="hljs-string">                                --leader-elect=true \</span><span class="hljs-string">                                --node-monitor-grace-period=20s \</span><span class="hljs-string">                                --node-monitor-period=5s \</span><span class="hljs-string">                                --port=10252 \</span><span class="hljs-string">                                --pod-eviction-timeout=2m0s \</span><span class="hljs-string">                                --requestheader-client-ca-file=/etc/kubernetes/ssl/k8s-root-ca.pem \</span><span class="hljs-string">                                --terminated-pod-gc-threshold=50 \</span><span class="hljs-string">                                --tls-cert-file=/etc/kubernetes/ssl/kube-controller-manager.pem \</span><span class="hljs-string">                                --tls-private-key-file=/etc/kubernetes/ssl/kube-controller-manager-key.pem \</span><span class="hljs-string">                                --root-ca-file=/etc/kubernetes/ssl/k8s-root-ca.pem \</span><span class="hljs-string">                                --secure-port=10257 \</span><span class="hljs-string">                                --service-cluster-ip-range=10.254.0.0/16 \</span><span class="hljs-string">                                --service-account-private-key-file=/etc/kubernetes/ssl/k8s-root-ca-key.pem \</span><span class="hljs-string">                                --use-service-account-credentials=true \</span><span class="hljs-string">                                --v=2&quot;</span></code></pre></div><p>controller manager 将不安全端口 <code>10252</code> 绑定到 127.0.0.1 确保 <code>kuebctl get cs</code> 有正确返回；将安全端口 <code>10257</code> 绑定到 0.0.0.0 公开，提供服务调用；<strong>由于 controller manager 开始连接 apiserver 的 <code>6443</code> 认证端口，所以需要 <code>--use-service-account-credentials</code> 选项来让 controller manager 创建单独的 service account(默认 <code>system:kube-controller-manager</code> 用户没有那么高权限)</strong></p><ul><li>scheduler</li></ul><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment">###</span><span class="hljs-comment"># kubernetes scheduler config</span><span class="hljs-comment"># default config should be adequate</span><span class="hljs-comment"># Add your own!</span>KUBE_SCHEDULER_ARGS=<span class="hljs-string">&quot;   --address=127.0.0.1 \</span><span class="hljs-string">                        --authentication-kubeconfig=/etc/kubernetes/kube-scheduler.kubeconfig \</span><span class="hljs-string">                        --authorization-kubeconfig=/etc/kubernetes/kube-scheduler.kubeconfig \</span><span class="hljs-string">                        --bind-address=0.0.0.0 \</span><span class="hljs-string">                        --client-ca-file=/etc/kubernetes/ssl/k8s-root-ca.pem \</span><span class="hljs-string">                        --kubeconfig=/etc/kubernetes/kube-scheduler.kubeconfig \</span><span class="hljs-string">                        --requestheader-client-ca-file=/etc/kubernetes/ssl/k8s-root-ca.pem \</span><span class="hljs-string">                        --secure-port=10259 \</span><span class="hljs-string">                        --leader-elect=true \</span><span class="hljs-string">                        --port=10251 \</span><span class="hljs-string">                        --tls-cert-file=/etc/kubernetes/ssl/kube-scheduler.pem \</span><span class="hljs-string">                        --tls-private-key-file=/etc/kubernetes/ssl/kube-scheduler-key.pem \</span><span class="hljs-string">                        --v=2&quot;</span></code></pre></div><p>shceduler 同  controller manager 一样将不安全端口绑定在本地，安全端口对外公开</p><p><strong>最后在三台节点上调整一下 IP 配置，启动即可</strong></p><h3 id="3-4、部署-Node"><a href="#3-4、部署-Node" class="headerlink" title="3.4、部署 Node"></a>3.4、部署 Node</h3><h4 id="3-4-1、安装脚本"><a href="#3-4-1、安装脚本" class="headerlink" title="3.4.1、安装脚本"></a>3.4.1、安装脚本</h4><p>node 安装与 master 安装过程一致，这里不再阐述</p><h4 id="3-4-2、配置文件"><a href="#3-4-2、配置文件" class="headerlink" title="3.4.2、配置文件"></a>3.4.2、配置文件</h4><p><strong>systemd 配置文件</strong></p><ul><li>kubelet.service</li></ul><div class="hljs code-wrapper"><pre><code class="hljs sh">[Unit]Description=Kubernetes Kubelet ServerDocumentation=https://github.com/GoogleCloudPlatform/kubernetesAfter=docker.serviceRequires=docker.service[Service]WorkingDirectory=/var/lib/kubeletEnvironmentFile=-/etc/kubernetes/kubeletExecStart=/usr/bin/kubelet \    <span class="hljs-variable">$KUBE_LOGTOSTDERR</span> \    <span class="hljs-variable">$KUBE_LOG_LEVEL</span> \    <span class="hljs-variable">$KUBELET_API_SERVER</span> \    <span class="hljs-variable">$KUBELET_ADDRESS</span> \    <span class="hljs-variable">$KUBELET_PORT</span> \    <span class="hljs-variable">$KUBELET_HOSTNAME</span> \    <span class="hljs-variable">$KUBE_ALLOW_PRIV</span> \    <span class="hljs-variable">$KUBELET_ARGS</span>Restart=on-failureKillMode=process[Install]WantedBy=multi-user.target</code></pre></div><ul><li>kube-proxy.service</li></ul><div class="hljs code-wrapper"><pre><code class="hljs sh">[Unit]Description=Kubernetes Kube-Proxy ServerDocumentation=https://github.com/GoogleCloudPlatform/kubernetesAfter=network.target[Service]EnvironmentFile=-/etc/kubernetes/proxyExecStart=/usr/bin/kube-proxy \    <span class="hljs-variable">$KUBE_LOGTOSTDERR</span> \    <span class="hljs-variable">$KUBE_LOG_LEVEL</span> \    <span class="hljs-variable">$KUBE_MASTER</span> \    <span class="hljs-variable">$KUBE_PROXY_ARGS</span>Restart=on-failureLimitNOFILE=65536[Install]WantedBy=multi-user.target</code></pre></div><p><strong>核心配置文件</strong></p><ul><li>kubelet</li></ul><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment">###</span><span class="hljs-comment"># kubernetes kubelet (minion) config</span><span class="hljs-comment"># The address for the info server to serve on (set to 0.0.0.0 or &quot;&quot; for all interfaces)</span>KUBELET_ADDRESS=<span class="hljs-string">&quot;--node-ip=192.168.1.54&quot;</span><span class="hljs-comment"># The port for the info server to serve on</span><span class="hljs-comment"># KUBELET_PORT=&quot;--port=10250&quot;</span><span class="hljs-comment"># You may leave this blank to use the actual hostname</span>KUBELET_HOSTNAME=<span class="hljs-string">&quot;--hostname-override=docker4.node&quot;</span><span class="hljs-comment"># location of the api-server</span><span class="hljs-comment"># KUBELET_API_SERVER=&quot;&quot;</span><span class="hljs-comment"># Add your own!</span>KUBELET_ARGS=<span class="hljs-string">&quot;  --address=0.0.0.0 \</span><span class="hljs-string">                --allow-privileged \</span><span class="hljs-string">                --anonymous-auth=false \</span><span class="hljs-string">                --authorization-mode=Webhook \</span><span class="hljs-string">                --bootstrap-kubeconfig=/etc/kubernetes/bootstrap.kubeconfig \</span><span class="hljs-string">                --client-ca-file=/etc/kubernetes/ssl/k8s-root-ca.pem \</span><span class="hljs-string">                --network-plugin=cni \</span><span class="hljs-string">                --cgroup-driver=cgroupfs \</span><span class="hljs-string">                --cert-dir=/etc/kubernetes/ssl \</span><span class="hljs-string">                --cluster-dns=10.254.0.2 \</span><span class="hljs-string">                --cluster-domain=cluster.local \</span><span class="hljs-string">                --cni-conf-dir=/etc/cni/net.d \</span><span class="hljs-string">                --eviction-soft=imagefs.available&lt;15%,memory.available&lt;512Mi,nodefs.available&lt;15%,nodefs.inodesFree&lt;10% \</span><span class="hljs-string">                --eviction-soft-grace-period=imagefs.available=3m,memory.available=1m,nodefs.available=3m,nodefs.inodesFree=1m \</span><span class="hljs-string">                --eviction-hard=imagefs.available&lt;10%,memory.available&lt;256Mi,nodefs.available&lt;10%,nodefs.inodesFree&lt;5% \</span><span class="hljs-string">                --eviction-max-pod-grace-period=30 \</span><span class="hljs-string">                --image-gc-high-threshold=80 \</span><span class="hljs-string">                --image-gc-low-threshold=70 \</span><span class="hljs-string">                --image-pull-progress-deadline=30s \</span><span class="hljs-string">                --kube-reserved=cpu=500m,memory=512Mi,ephemeral-storage=1Gi \</span><span class="hljs-string">                --kubeconfig=/etc/kubernetes/kubelet.kubeconfig \</span><span class="hljs-string">                --max-pods=100 \</span><span class="hljs-string">                --minimum-image-ttl-duration=720h0m0s \</span><span class="hljs-string">                --node-labels=node.kubernetes.io/k8s-node=true \</span><span class="hljs-string">                --pod-infra-container-image=gcr.azk8s.cn/google_containers/pause-amd64:3.1 \</span><span class="hljs-string">                --port=10250 \</span><span class="hljs-string">                --read-only-port=0 \</span><span class="hljs-string">                --rotate-certificates \</span><span class="hljs-string">                --rotate-server-certificates \</span><span class="hljs-string">                --resolv-conf=/run/systemd/resolve/resolv.conf \</span><span class="hljs-string">                --system-reserved=cpu=500m,memory=512Mi,ephemeral-storage=1Gi \</span><span class="hljs-string">                --fail-swap-on=false \</span><span class="hljs-string">                --v=2&quot;</span></code></pre></div><p><strong>当 kubelet 组件设置了 <code>--rotate-certificates</code>，<code>--rotate-server-certificates</code> 后会自动更新其使用的相关证书，同时指定 <code>--authorization-mode=Webhook</code> 后 <code>10250</code> 端口 RBAC 授权将会委托给 apiserver</strong></p><ul><li>proxy</li></ul><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment">###</span><span class="hljs-comment"># kubernetes proxy config</span><span class="hljs-comment"># default config should be adequate</span><span class="hljs-comment"># Add your own!</span>KUBE_PROXY_ARGS=<span class="hljs-string">&quot;   --bind-address=0.0.0.0 \</span><span class="hljs-string">                    --cleanup-ipvs=true \</span><span class="hljs-string">                    --cluster-cidr=10.254.0.0/16 \</span><span class="hljs-string">                    --hostname-override=docker4.node \</span><span class="hljs-string">                    --healthz-bind-address=0.0.0.0 \</span><span class="hljs-string">                    --healthz-port=10256 \</span><span class="hljs-string">                    --masquerade-all=true \</span><span class="hljs-string">                    --proxy-mode=ipvs \</span><span class="hljs-string">                    --ipvs-min-sync-period=5s \</span><span class="hljs-string">                    --ipvs-sync-period=5s \</span><span class="hljs-string">                    --ipvs-scheduler=wrr \</span><span class="hljs-string">                    --kubeconfig=/etc/kubernetes/kube-proxy.kubeconfig \</span><span class="hljs-string">                    --logtostderr=true \</span><span class="hljs-string">                    --v=2&quot;</span></code></pre></div><p>由于 <code>kubelet</code> 组件是采用 TLS Bootstrap 启动，所以需要预先创建相关配置</p><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment"># 创建用于 tls bootstrap 的 token secret</span>kubectl create -f bootstrap.secret.yaml<span class="hljs-comment"># 为了能让 kubelet 实现自动更新证书，需要配置相关 clusterrolebinding</span><span class="hljs-comment"># 允许 kubelet tls bootstrap 创建 csr 请求</span>kubectl create clusterrolebinding create-csrs-for-bootstrapping \    --clusterrole=system:node-bootstrapper \    --group=system:bootstrappers<span class="hljs-comment"># 自动批准 system:bootstrappers 组用户 TLS bootstrapping 首次申请证书的 CSR 请求</span>kubectl create clusterrolebinding auto-approve-csrs-for-group \    --clusterrole=system:certificates.k8s.io:certificatesigningrequests:nodeclient \    --group=system:bootstrappers<span class="hljs-comment"># 自动批准 system:nodes 组用户更新 kubelet 自身与 apiserver 通讯证书的 CSR 请求</span>kubectl create clusterrolebinding auto-approve-renewals-for-nodes \    --clusterrole=system:certificates.k8s.io:certificatesigningrequests:selfnodeclient \    --group=system:nodes<span class="hljs-comment"># 在 kubelet server 开启 api 认证的情况下，apiserver 反向访问 kubelet 10250 需要此授权(eg: kubectl logs)</span>kubectl create clusterrolebinding system:kubelet-api-admin \    --clusterrole=system:kubelet-api-admin \    --user=system:kubelet-api-admin</code></pre></div><h4 id="3-4-3、Nginx-代理"><a href="#3-4-3、Nginx-代理" class="headerlink" title="3.4.3、Nginx 代理"></a>3.4.3、Nginx 代理</h4><p>为了保证 apiserver 的 HA，需要在每个 node 上部署 nginx 来反向代理(tcp)所有 apiserver；然后 kubelet、kube-proxy 组件连接本地 <code>127.0.0.1:6443</code> 访问 apiserver，以确保任何 master 挂掉以后 node 都不会受到影响</p><ul><li>nginx.conf</li></ul><div class="hljs code-wrapper"><pre><code class="hljs sh">error_log stderr notice;worker_processes auto;events &#123;  multi_accept on;  use epoll;  worker_connections 1024;&#125;stream &#123;    upstream kube_apiserver &#123;        least_conn;        server 192.168.1.51:6443;        server 192.168.1.52:6443;        server 192.168.1.53:6443;    &#125;    server &#123;        listen        0.0.0.0:6443;        proxy_pass    kube_apiserver;        proxy_timeout 10m;        proxy_connect_timeout 1s;    &#125;&#125;</code></pre></div><ul><li>nginx-proxy.service</li></ul><div class="hljs code-wrapper"><pre><code class="hljs sh">[Unit]Description=kubernetes apiserver docker wrapperWants=docker.socketAfter=docker.service[Service]User=rootPermissionsStartOnly=<span class="hljs-literal">true</span>ExecStart=/usr/bin/docker run -p 127.0.0.1:6443:6443 \                              -v /etc/nginx:/etc/nginx \                              --name nginx-proxy \                              --net=host \                              --restart=on-failure:5 \                              --memory=512M \                              nginx:1.14.2-alpineExecStartPre=-/usr/bin/docker rm -f nginx-proxyExecStop=/usr/bin/docker stop nginx-proxyRestart=alwaysRestartSec=15sTimeoutStartSec=30s[Install]WantedBy=multi-user.target</code></pre></div><p>然后在每个 node 上先启动 nginx-proxy，接着启动 kubelet 与 kube-proxy 即可(master 上的 kubelet、kube-proxy 只需要修改 ip 和 node name)</p><h4 id="3-4-4、kubelet-server-证书"><a href="#3-4-4、kubelet-server-证书" class="headerlink" title="3.4.4、kubelet server 证书"></a>3.4.4、kubelet server 证书</h4><p><strong>注意: 新版本 kubelet server 的证书自动签发已经被关闭(看 issue 好像是由于安全原因)，所以对于 kubelet server 的证书仍需要手动签署</strong></p><div class="hljs code-wrapper"><pre><code class="hljs sh">docker1.node ➜  ~ kubectl get csrNAME                                                   AGE   REQUESTOR                  CONDITIONcsr-99l77                                              10s   system:node:docker4.node   Pendingnode-csr-aGwaNKorMc0MZBYOuJsJGCB8Bg8ds97rmE3oKBTV-_E   11s   system:bootstrap:5d820b    Approved,Issueddocker1.node ➜  ~ kubectl certificate approve csr-99l77certificatesigningrequest.certificates.k8s.io/csr-99l77 approved</code></pre></div><h3 id="3-5、部署-Calico"><a href="#3-5、部署-Calico" class="headerlink" title="3.5、部署 Calico"></a>3.5、部署 Calico</h3><p>当 node 全部启动后，由于网络组件(CNI)未安装会显示为 NotReady 状态；下面将部署 Calico 作为网络组件，完成跨节点网络通讯；具体安装文档可以参考 <a href="https://docs.projectcalico.org/v3.6/getting-started/kubernetes/installation/calico#installing-with-the-etcd-datastore">Installing with the etcd datastore</a></p><p>以下为 calico 的配置文件</p><ul><li>calico.yaml</li></ul><div class="hljs code-wrapper"><pre><code class="hljs yaml"><span class="hljs-meta">---</span><span class="hljs-comment"># Source: calico/templates/calico-etcd-secrets.yaml</span><span class="hljs-comment"># The following contains k8s Secrets for use with a TLS enabled etcd cluster.</span><span class="hljs-comment"># For information on populating Secrets, see http://kubernetes.io/docs/user-guide/secrets/</span><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span><span class="hljs-attr">kind:</span> <span class="hljs-string">Secret</span><span class="hljs-attr">type:</span> <span class="hljs-string">Opaque</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">calico-etcd-secrets</span>  <span class="hljs-attr">namespace:</span> <span class="hljs-string">kube-system</span><span class="hljs-attr">data:</span>  <span class="hljs-comment"># Populate the following with etcd TLS configuration if desired, but leave blank if</span>  <span class="hljs-comment"># not using TLS for etcd.</span>  <span class="hljs-comment"># The keys below should be uncommented and the values populated with the base64</span>  <span class="hljs-comment"># encoded contents of each file that would be associated with the TLS data.</span>  <span class="hljs-comment"># Example command for encoding a file contents: cat &lt;file&gt; | base64 -w 0</span>  <span class="hljs-attr">etcd-key:</span> <span class="hljs-string">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</span>  <span class="hljs-attr">etcd-cert:</span> <span class="hljs-string">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</span>  <span class="hljs-attr">etcd-ca:</span> <span class="hljs-string">LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUZyakNDQTVhZ0F3SUJBZ0lVWXVIKzIxYlNaU2hncVYxWkx3a2o4RmpZbUl3d0RRWUpLb1pJaHZjTkFRRU4KQlFBd2J6RUxNQWtHQTFVRUJoTUNRMDR4RURBT0JnTlZCQWdUQjBKbGFXcHBibWN4RURBT0JnTlZCQWNUQjBKbAphV3BwYm1jeERUQUxCZ05WQkFvVEJHVjBZMlF4RmpBVUJnTlZCQXNURFdWMFkyUWdVMlZqZFhKcGRIa3hGVEFUCkJnTlZCQU1UREdWMFkyUXRjbTl2ZEMxallUQWVGdzB4T1RBek1UWXdNelV4TURCYUZ3MHlPVEF6TVRNd016VXgKTURCYU1HOHhDekFKQmdOVkJBWVRBa05PTVJBd0RnWURWUVFJRXdkQ1pXbHFhVzVuTVJBd0RnWURWUVFIRXdkQwpaV2xxYVc1bk1RMHdDd1lEVlFRS0V3UmxkR05rTVJZd0ZBWURWUVFMRXcxbGRHTmtJRk5sWTNWeWFYUjVNUlV3CkV3WURWUVFERXd4bGRHTmtMWEp2YjNRdFkyRXdnZ0lpTUEwR0NTcUdTSWIzRFFFQkFRVUFBNElDRHdBd2dnSUsKQW9JQ0FRRGFLK0s4WStqZkdOY2pOeUloeUhXSE5adWxVZzVKZFpOVU9GOHFXbXJMa0NuY2ZWdVF3dmI4cDFwLwpSSjBFOWo0OFBhZ1RJT3U2TU81R24zejFrZGpHRk9jOVZwMlZjYWJEQzJLWWJvRzdVQ0RmTWkzR1MzUnhUejVkCnh0MG1Ya2liVkMvc01NU2RrRm1mU2FCSXBoKzAyTnMwZURyMzNtUWxTdURlTWozNHJaTkVwMzRnUUk0eElTejAKbXhXR0dWNzcwUE9ScVgrZUthTEpiclp3anFFcnpHMEtEVUlBM0ZuTFdRMnp4b0VwN3JZby9LaGRiOHdETE1kbQp6VXNOZHI0T1F4MFBVRXA4akRUU2lFODkydDQ4KytsOHJ0MW4vTHFRc1FhVncrQlQrMTRvRHdIVkFaRXZ2ZnMwCmZkZ0QvU2RINGJRdHNhT21BdFByQldseU5aMUxIZkR2djMraXFzNk83UXpWUTFCK1c5cFRxdUZ2YUxWN3R1S3UKSXNlUFlseFdjV2E2M0hGbFkxVVJ6M0owaGtrZEZ1dkhUc0dhZDVpaWVrb0dUcFdTN2dVdCtTeWVJT2FhMldHLwp4Y1NiUWE0Y2xiZThuUHV2c1ZFVDhqZ0d0NGVLT25yRVJId0hMb2VleEpsSjdUdnhHNHpOTHZsc2FOL29iRzFDClUzMXczZ2d1SXpzRk5yallsUFdSZ0hSdXdPTlE5anlkM2dqVmNYUFdHTFJISUdYbjNhUDluT3A0OE9WWDhzbXoKOGIwS0V4UVpEQWUyS0tjWEg5a1ZiUFJQSWlLeGpXelV5aDMzQlRNejlPczZHcWM0Zk05c1hxbGRhVzBGd3g4MQpJaklScWx5a3VOSXNDWGhMUzhlNmVtdUNYMTVDZGNKb0ZmdXRuTENvV1B4Umg5OEF4UUlEQVFBQm8wSXdRREFPCkJnTlZIUThCQWY4RUJBTUNBUVl3RHdZRFZSMFRBUUgvQkFVd0F3RUIvekFkQmdOVkhRNEVGZ1FVMlpVYzdHaEYKaG1PQXhzRlZ3VEEybmVkUkh2Y3dEUVlKS29aSWh2Y05BUUVOQlFBRGdnSUJBSjh3bVNJMVBsQm8zcE1RWC9DOQpRS1RrR0xvVUhGdWprdFoxM1FYeXQ1LzFSeVB2WG1lLy90N3FHR2I5RmJZSm9BYTRTd3JSZkYzZmh3UDZaS0FnCnNYSEliR2gwc014UTdqVmQwMUNMWkoxQmZFNGZtTVlaQUlEWGpTcTNqbHJXZWcxL2hWTFN2dXRuUEFWSXc1SWwKZUdXRTMyOVJ2b2d2dXV6dUsxY2xwZFpIL2p3UlZjUUFUK0xvT2xFZ3Rkd293c0xpaWx3WE95eEZLZDd1UDk3bgozTFZUekFNN3Flell4SUVMQVlUUUN5eTdpeEIxNXlJV1UrUWhreUFtWXJoNEN6VUNNUjQreDlpaGZ6UnlOQkxLCmRBRTdwcjdyUEM4WFQ0YWh2SkJCZTg1THViTVdVRmprcEF5cklQODYyYkFCOCtKSXNFdXNZVGdQakUrMGhteTkKT0NIU2x4Q25GQVdPUXcwQ05Kb3AxWGpHU0RZOXlXL1NNWS83T3B0QlBhT3VWTzVwZTg3VmVXRFFtYmlpdnc3MQo4cFhDQnN6ZWNsdjJZKzdscTRnL0FaQkViVXRvLzV4UXJCbmZGKy9hZFFOQzY4aG4yYzZWa3czYTVDR0ZMN0p2CjhWdFNmeFEzZnFUci9TdzlJbkVKVWpuc0Y3R0xINzZMWXZIU05WeldhMkhiVFNlTnQ0RUlpdlEwb2d0b2hzY0kKSHlrZlpRQ3Z6ZnBSZi9TODFiRDNnU29jQ3NzR2crdVpVU0FMdVhBRDE4RkRXNzg2LzRCckcrMzVLOVBLNktUZwpoWGN4WmRHd3V1RWx0aTRBNWx4OHNrZExPSkZ6TUJPWFJNU2Jsc0dna3pGK2JNRkMrMHV3WW1WK0VTRUdwdy9NCm93WUN1dHh2a3ltL2NOcEk1bjFhanpEcQotLS0tLUVORCBDRVJUSUZJQ0FURS0tLS0tCg==</span><span class="hljs-meta">---</span><span class="hljs-comment"># Source: calico/templates/calico-config.yaml</span><span class="hljs-comment"># This ConfigMap is used to configure a self-hosted Calico installation.</span><span class="hljs-attr">kind:</span> <span class="hljs-string">ConfigMap</span><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">calico-config</span>  <span class="hljs-attr">namespace:</span> <span class="hljs-string">kube-system</span><span class="hljs-attr">data:</span>  <span class="hljs-comment"># Configure this with the location of your etcd cluster.</span>  <span class="hljs-attr">etcd_endpoints:</span> <span class="hljs-string">&quot;https://192.168.1.51:2379,https://192.168.1.52:2379,https://192.168.1.53:2379&quot;</span>  <span class="hljs-comment"># If you&#x27;re using TLS enabled etcd uncomment the following.</span>  <span class="hljs-comment"># You must also populate the Secret below with these files.</span>  <span class="hljs-attr">etcd_ca:</span> <span class="hljs-string">&quot;/calico-secrets/etcd-ca&quot;</span>  <span class="hljs-attr">etcd_cert:</span> <span class="hljs-string">&quot;/calico-secrets/etcd-cert&quot;</span>  <span class="hljs-attr">etcd_key:</span> <span class="hljs-string">&quot;/calico-secrets/etcd-key&quot;</span>  <span class="hljs-comment"># Typha is disabled.</span>  <span class="hljs-attr">typha_service_name:</span> <span class="hljs-string">&quot;none&quot;</span>  <span class="hljs-comment"># Configure the Calico backend to use.</span>  <span class="hljs-attr">calico_backend:</span> <span class="hljs-string">&quot;bird&quot;</span>  <span class="hljs-comment"># Configure the MTU to use</span>  <span class="hljs-attr">veth_mtu:</span> <span class="hljs-string">&quot;1440&quot;</span>  <span class="hljs-comment"># The CNI network configuration to install on each node.  The special</span>  <span class="hljs-comment"># values in this config will be automatically populated.</span>  <span class="hljs-attr">cni_network_config:</span> <span class="hljs-string">|-</span>    &#123;      <span class="hljs-attr">&quot;name&quot;:</span> <span class="hljs-string">&quot;k8s-pod-network&quot;</span>,      <span class="hljs-attr">&quot;cniVersion&quot;:</span> <span class="hljs-string">&quot;0.3.0&quot;</span>,      <span class="hljs-attr">&quot;plugins&quot;:</span> [        &#123;          <span class="hljs-attr">&quot;type&quot;:</span> <span class="hljs-string">&quot;calico&quot;</span>,          <span class="hljs-attr">&quot;log_level&quot;:</span> <span class="hljs-string">&quot;info&quot;</span>,          <span class="hljs-attr">&quot;etcd_endpoints&quot;:</span> <span class="hljs-string">&quot;__ETCD_ENDPOINTS__&quot;</span>,          <span class="hljs-attr">&quot;etcd_key_file&quot;:</span> <span class="hljs-string">&quot;__ETCD_KEY_FILE__&quot;</span>,          <span class="hljs-attr">&quot;etcd_cert_file&quot;:</span> <span class="hljs-string">&quot;__ETCD_CERT_FILE__&quot;</span>,          <span class="hljs-attr">&quot;etcd_ca_cert_file&quot;:</span> <span class="hljs-string">&quot;__ETCD_CA_CERT_FILE__&quot;</span>,          <span class="hljs-attr">&quot;mtu&quot;:</span> <span class="hljs-string">__CNI_MTU__</span>,          <span class="hljs-attr">&quot;ipam&quot;:</span> &#123;              <span class="hljs-attr">&quot;type&quot;:</span> <span class="hljs-string">&quot;calico-ipam&quot;</span>          &#125;,          <span class="hljs-attr">&quot;policy&quot;:</span> &#123;              <span class="hljs-attr">&quot;type&quot;:</span> <span class="hljs-string">&quot;k8s&quot;</span>          &#125;,          <span class="hljs-attr">&quot;kubernetes&quot;:</span> &#123;              <span class="hljs-attr">&quot;kubeconfig&quot;:</span> <span class="hljs-string">&quot;__KUBECONFIG_FILEPATH__&quot;</span>          &#125;        &#125;,        &#123;          <span class="hljs-attr">&quot;type&quot;:</span> <span class="hljs-string">&quot;portmap&quot;</span>,          <span class="hljs-attr">&quot;snat&quot;:</span> <span class="hljs-literal">true</span>,          <span class="hljs-attr">&quot;capabilities&quot;:</span> &#123;<span class="hljs-attr">&quot;portMappings&quot;:</span> <span class="hljs-literal">true</span>&#125;        &#125;      ]    &#125;<span class="hljs-meta">---</span><span class="hljs-comment"># Source: calico/templates/rbac.yaml</span><span class="hljs-comment"># Include a clusterrole for the kube-controllers component,</span><span class="hljs-comment"># and bind it to the calico-kube-controllers serviceaccount.</span><span class="hljs-attr">kind:</span> <span class="hljs-string">ClusterRole</span><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">rbac.authorization.k8s.io/v1beta1</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">calico-kube-controllers</span><span class="hljs-attr">rules:</span>  <span class="hljs-comment"># Pods are monitored for changing labels.</span>  <span class="hljs-comment"># The node controller monitors Kubernetes nodes.</span>  <span class="hljs-comment"># Namespace and serviceaccount labels are used for policy.</span>  <span class="hljs-bullet">-</span> <span class="hljs-attr">apiGroups:</span> [<span class="hljs-string">&quot;&quot;</span>]    <span class="hljs-attr">resources:</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">pods</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">nodes</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">namespaces</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">serviceaccounts</span>    <span class="hljs-attr">verbs:</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">watch</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">list</span>  <span class="hljs-comment"># Watch for changes to Kubernetes NetworkPolicies.</span>  <span class="hljs-bullet">-</span> <span class="hljs-attr">apiGroups:</span> [<span class="hljs-string">&quot;networking.k8s.io&quot;</span>]    <span class="hljs-attr">resources:</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">networkpolicies</span>    <span class="hljs-attr">verbs:</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">watch</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">list</span><span class="hljs-meta">---</span><span class="hljs-attr">kind:</span> <span class="hljs-string">ClusterRoleBinding</span><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">rbac.authorization.k8s.io/v1beta1</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">calico-kube-controllers</span><span class="hljs-attr">roleRef:</span>  <span class="hljs-attr">apiGroup:</span> <span class="hljs-string">rbac.authorization.k8s.io</span>  <span class="hljs-attr">kind:</span> <span class="hljs-string">ClusterRole</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">calico-kube-controllers</span><span class="hljs-attr">subjects:</span><span class="hljs-bullet">-</span> <span class="hljs-attr">kind:</span> <span class="hljs-string">ServiceAccount</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">calico-kube-controllers</span>  <span class="hljs-attr">namespace:</span> <span class="hljs-string">kube-system</span><span class="hljs-meta">---</span><span class="hljs-comment"># Include a clusterrole for the calico-node DaemonSet,</span><span class="hljs-comment"># and bind it to the calico-node serviceaccount.</span><span class="hljs-attr">kind:</span> <span class="hljs-string">ClusterRole</span><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">rbac.authorization.k8s.io/v1beta1</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">calico-node</span><span class="hljs-attr">rules:</span>  <span class="hljs-comment"># The CNI plugin needs to get pods, nodes, and namespaces.</span>  <span class="hljs-bullet">-</span> <span class="hljs-attr">apiGroups:</span> [<span class="hljs-string">&quot;&quot;</span>]    <span class="hljs-attr">resources:</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">pods</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">nodes</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">namespaces</span>    <span class="hljs-attr">verbs:</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">get</span>  <span class="hljs-bullet">-</span> <span class="hljs-attr">apiGroups:</span> [<span class="hljs-string">&quot;&quot;</span>]    <span class="hljs-attr">resources:</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">endpoints</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">services</span>    <span class="hljs-attr">verbs:</span>      <span class="hljs-comment"># Used to discover service IPs for advertisement.</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">watch</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">list</span>  <span class="hljs-bullet">-</span> <span class="hljs-attr">apiGroups:</span> [<span class="hljs-string">&quot;&quot;</span>]    <span class="hljs-attr">resources:</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">nodes/status</span>    <span class="hljs-attr">verbs:</span>      <span class="hljs-comment"># Needed for clearing NodeNetworkUnavailable flag.</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">patch</span><span class="hljs-meta">---</span><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">rbac.authorization.k8s.io/v1beta1</span><span class="hljs-attr">kind:</span> <span class="hljs-string">ClusterRoleBinding</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">calico-node</span><span class="hljs-attr">roleRef:</span>  <span class="hljs-attr">apiGroup:</span> <span class="hljs-string">rbac.authorization.k8s.io</span>  <span class="hljs-attr">kind:</span> <span class="hljs-string">ClusterRole</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">calico-node</span><span class="hljs-attr">subjects:</span><span class="hljs-bullet">-</span> <span class="hljs-attr">kind:</span> <span class="hljs-string">ServiceAccount</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">calico-node</span>  <span class="hljs-attr">namespace:</span> <span class="hljs-string">kube-system</span><span class="hljs-meta">---</span><span class="hljs-meta">---</span><span class="hljs-comment"># Source: calico/templates/calico-node.yaml</span><span class="hljs-comment"># This manifest installs the calico/node container, as well</span><span class="hljs-comment"># as the Calico CNI plugins and network config on</span><span class="hljs-comment"># each master and worker node in a Kubernetes cluster.</span><span class="hljs-attr">kind:</span> <span class="hljs-string">DaemonSet</span><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">extensions/v1beta1</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">calico-node</span>  <span class="hljs-attr">namespace:</span> <span class="hljs-string">kube-system</span>  <span class="hljs-attr">labels:</span>    <span class="hljs-attr">k8s-app:</span> <span class="hljs-string">calico-node</span><span class="hljs-attr">spec:</span>  <span class="hljs-attr">selector:</span>    <span class="hljs-attr">matchLabels:</span>      <span class="hljs-attr">k8s-app:</span> <span class="hljs-string">calico-node</span>  <span class="hljs-attr">updateStrategy:</span>    <span class="hljs-attr">type:</span> <span class="hljs-string">RollingUpdate</span>    <span class="hljs-attr">rollingUpdate:</span>      <span class="hljs-attr">maxUnavailable:</span> <span class="hljs-number">1</span>  <span class="hljs-attr">template:</span>    <span class="hljs-attr">metadata:</span>      <span class="hljs-attr">labels:</span>        <span class="hljs-attr">k8s-app:</span> <span class="hljs-string">calico-node</span>      <span class="hljs-attr">annotations:</span>        <span class="hljs-comment"># This, along with the CriticalAddonsOnly toleration below,</span>        <span class="hljs-comment"># marks the pod as a critical add-on, ensuring it gets</span>        <span class="hljs-comment"># priority scheduling and that its resources are reserved</span>        <span class="hljs-comment"># if it ever gets evicted.</span>        <span class="hljs-attr">scheduler.alpha.kubernetes.io/critical-pod:</span> <span class="hljs-string">&#x27;&#x27;</span>    <span class="hljs-attr">spec:</span>      <span class="hljs-attr">nodeSelector:</span>        <span class="hljs-attr">beta.kubernetes.io/os:</span> <span class="hljs-string">linux</span>      <span class="hljs-attr">hostNetwork:</span> <span class="hljs-literal">true</span>      <span class="hljs-attr">tolerations:</span>        <span class="hljs-comment"># Make sure calico-node gets scheduled on all nodes.</span>        <span class="hljs-bullet">-</span> <span class="hljs-attr">effect:</span> <span class="hljs-string">NoSchedule</span>          <span class="hljs-attr">operator:</span> <span class="hljs-string">Exists</span>        <span class="hljs-comment"># Mark the pod as a critical add-on for rescheduling.</span>        <span class="hljs-bullet">-</span> <span class="hljs-attr">key:</span> <span class="hljs-string">CriticalAddonsOnly</span>          <span class="hljs-attr">operator:</span> <span class="hljs-string">Exists</span>        <span class="hljs-bullet">-</span> <span class="hljs-attr">effect:</span> <span class="hljs-string">NoExecute</span>          <span class="hljs-attr">operator:</span> <span class="hljs-string">Exists</span>      <span class="hljs-attr">serviceAccountName:</span> <span class="hljs-string">calico-node</span>      <span class="hljs-comment"># Minimize downtime during a rolling upgrade or deletion; tell Kubernetes to do a &quot;force</span>      <span class="hljs-comment"># deletion&quot;: https://kubernetes.io/docs/concepts/workloads/pods/pod/#termination-of-pods.</span>      <span class="hljs-attr">terminationGracePeriodSeconds:</span> <span class="hljs-number">0</span>      <span class="hljs-attr">initContainers:</span>        <span class="hljs-comment"># This container installs the Calico CNI binaries</span>        <span class="hljs-comment"># and CNI network config file on each node.</span>        <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">install-cni</span>          <span class="hljs-attr">image:</span> <span class="hljs-string">calico/cni:v3.6.0</span>          <span class="hljs-attr">command:</span> [<span class="hljs-string">&quot;/install-cni.sh&quot;</span>]          <span class="hljs-attr">env:</span>            <span class="hljs-comment"># Name of the CNI config file to create.</span>            <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">CNI_CONF_NAME</span>              <span class="hljs-attr">value:</span> <span class="hljs-string">&quot;10-calico.conflist&quot;</span>            <span class="hljs-comment"># The CNI network config to install on each node.</span>            <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">CNI_NETWORK_CONFIG</span>              <span class="hljs-attr">valueFrom:</span>                <span class="hljs-attr">configMapKeyRef:</span>                  <span class="hljs-attr">name:</span> <span class="hljs-string">calico-config</span>                  <span class="hljs-attr">key:</span> <span class="hljs-string">cni_network_config</span>            <span class="hljs-comment"># The location of the Calico etcd cluster.</span>            <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">ETCD_ENDPOINTS</span>              <span class="hljs-attr">valueFrom:</span>                <span class="hljs-attr">configMapKeyRef:</span>                  <span class="hljs-attr">name:</span> <span class="hljs-string">calico-config</span>                  <span class="hljs-attr">key:</span> <span class="hljs-string">etcd_endpoints</span>            <span class="hljs-comment"># CNI MTU Config variable</span>            <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">CNI_MTU</span>              <span class="hljs-attr">valueFrom:</span>                <span class="hljs-attr">configMapKeyRef:</span>                  <span class="hljs-attr">name:</span> <span class="hljs-string">calico-config</span>                  <span class="hljs-attr">key:</span> <span class="hljs-string">veth_mtu</span>            <span class="hljs-comment"># Prevents the container from sleeping forever.</span>            <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">SLEEP</span>              <span class="hljs-attr">value:</span> <span class="hljs-string">&quot;false&quot;</span>          <span class="hljs-attr">volumeMounts:</span>            <span class="hljs-bullet">-</span> <span class="hljs-attr">mountPath:</span> <span class="hljs-string">/host/opt/cni/bin</span>              <span class="hljs-attr">name:</span> <span class="hljs-string">cni-bin-dir</span>            <span class="hljs-bullet">-</span> <span class="hljs-attr">mountPath:</span> <span class="hljs-string">/host/etc/cni/net.d</span>              <span class="hljs-attr">name:</span> <span class="hljs-string">cni-net-dir</span>            <span class="hljs-bullet">-</span> <span class="hljs-attr">mountPath:</span> <span class="hljs-string">/calico-secrets</span>              <span class="hljs-attr">name:</span> <span class="hljs-string">etcd-certs</span>      <span class="hljs-attr">containers:</span>        <span class="hljs-comment"># Runs calico/node container on each Kubernetes node.  This</span>        <span class="hljs-comment"># container programs network policy and routes on each</span>        <span class="hljs-comment"># host.</span>        <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">calico-node</span>          <span class="hljs-attr">image:</span> <span class="hljs-string">calico/node:v3.6.0</span>          <span class="hljs-attr">env:</span>            <span class="hljs-comment"># The location of the Calico etcd cluster.</span>            <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">ETCD_ENDPOINTS</span>              <span class="hljs-attr">valueFrom:</span>                <span class="hljs-attr">configMapKeyRef:</span>                  <span class="hljs-attr">name:</span> <span class="hljs-string">calico-config</span>                  <span class="hljs-attr">key:</span> <span class="hljs-string">etcd_endpoints</span>            <span class="hljs-comment"># Location of the CA certificate for etcd.</span>            <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">ETCD_CA_CERT_FILE</span>              <span class="hljs-attr">valueFrom:</span>                <span class="hljs-attr">configMapKeyRef:</span>                  <span class="hljs-attr">name:</span> <span class="hljs-string">calico-config</span>                  <span class="hljs-attr">key:</span> <span class="hljs-string">etcd_ca</span>            <span class="hljs-comment"># Location of the client key for etcd.</span>            <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">ETCD_KEY_FILE</span>              <span class="hljs-attr">valueFrom:</span>                <span class="hljs-attr">configMapKeyRef:</span>                  <span class="hljs-attr">name:</span> <span class="hljs-string">calico-config</span>                  <span class="hljs-attr">key:</span> <span class="hljs-string">etcd_key</span>            <span class="hljs-comment"># Location of the client certificate for etcd.</span>            <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">ETCD_CERT_FILE</span>              <span class="hljs-attr">valueFrom:</span>                <span class="hljs-attr">configMapKeyRef:</span>                  <span class="hljs-attr">name:</span> <span class="hljs-string">calico-config</span>                  <span class="hljs-attr">key:</span> <span class="hljs-string">etcd_cert</span>            <span class="hljs-comment"># Set noderef for node controller.</span>            <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">CALICO_K8S_NODE_REF</span>              <span class="hljs-attr">valueFrom:</span>                <span class="hljs-attr">fieldRef:</span>                  <span class="hljs-attr">fieldPath:</span> <span class="hljs-string">spec.nodeName</span>            <span class="hljs-comment"># Choose the backend to use.</span>            <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">CALICO_NETWORKING_BACKEND</span>              <span class="hljs-attr">valueFrom:</span>                <span class="hljs-attr">configMapKeyRef:</span>                  <span class="hljs-attr">name:</span> <span class="hljs-string">calico-config</span>                  <span class="hljs-attr">key:</span> <span class="hljs-string">calico_backend</span>            <span class="hljs-comment"># Cluster type to identify the deployment type</span>            <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">CLUSTER_TYPE</span>              <span class="hljs-attr">value:</span> <span class="hljs-string">&quot;k8s,bgp&quot;</span>            <span class="hljs-comment"># Auto-detect the BGP IP address.</span>            <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">IP</span>              <span class="hljs-attr">value:</span> <span class="hljs-string">&quot;autodetect&quot;</span>            <span class="hljs-comment"># Enable IPIP</span>            <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">CALICO_IPV4POOL_IPIP</span>              <span class="hljs-attr">value:</span> <span class="hljs-string">&quot;Always&quot;</span>            <span class="hljs-comment"># Set MTU for tunnel device used if ipip is enabled</span>            <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">FELIX_IPINIPMTU</span>              <span class="hljs-attr">valueFrom:</span>                <span class="hljs-attr">configMapKeyRef:</span>                  <span class="hljs-attr">name:</span> <span class="hljs-string">calico-config</span>                  <span class="hljs-attr">key:</span> <span class="hljs-string">veth_mtu</span>            <span class="hljs-comment"># The default IPv4 pool to create on startup if none exists. Pod IPs will be</span>            <span class="hljs-comment"># chosen from this range. Changing this value after installation will have</span>            <span class="hljs-comment"># no effect. This should fall within `--cluster-cidr`.</span>            <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">CALICO_IPV4POOL_CIDR</span>              <span class="hljs-attr">value:</span> <span class="hljs-string">&quot;10.20.0.0/16&quot;</span>            <span class="hljs-comment"># Disable file logging so `kubectl logs` works.</span>            <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">CALICO_DISABLE_FILE_LOGGING</span>              <span class="hljs-attr">value:</span> <span class="hljs-string">&quot;true&quot;</span>            <span class="hljs-comment"># Set Felix endpoint to host default action to ACCEPT.</span>            <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">FELIX_DEFAULTENDPOINTTOHOSTACTION</span>              <span class="hljs-attr">value:</span> <span class="hljs-string">&quot;ACCEPT&quot;</span>            <span class="hljs-comment"># Disable IPv6 on Kubernetes.</span>            <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">FELIX_IPV6SUPPORT</span>              <span class="hljs-attr">value:</span> <span class="hljs-string">&quot;false&quot;</span>            <span class="hljs-comment"># Set Felix logging to &quot;info&quot;</span>            <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">FELIX_LOGSEVERITYSCREEN</span>              <span class="hljs-attr">value:</span> <span class="hljs-string">&quot;info&quot;</span>            <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">FELIX_HEALTHENABLED</span>              <span class="hljs-attr">value:</span> <span class="hljs-string">&quot;true&quot;</span>            <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">IP_AUTODETECTION_METHOD</span>              <span class="hljs-attr">value:</span> <span class="hljs-string">can-reach=192.168.1.51</span>          <span class="hljs-attr">securityContext:</span>            <span class="hljs-attr">privileged:</span> <span class="hljs-literal">true</span>          <span class="hljs-attr">resources:</span>            <span class="hljs-attr">requests:</span>              <span class="hljs-attr">cpu:</span> <span class="hljs-string">250m</span>          <span class="hljs-attr">livenessProbe:</span>            <span class="hljs-attr">httpGet:</span>              <span class="hljs-attr">path:</span> <span class="hljs-string">/liveness</span>              <span class="hljs-attr">port:</span> <span class="hljs-number">9099</span>              <span class="hljs-attr">host:</span> <span class="hljs-string">localhost</span>            <span class="hljs-attr">periodSeconds:</span> <span class="hljs-number">10</span>            <span class="hljs-attr">initialDelaySeconds:</span> <span class="hljs-number">10</span>            <span class="hljs-attr">failureThreshold:</span> <span class="hljs-number">6</span>          <span class="hljs-attr">readinessProbe:</span>            <span class="hljs-attr">exec:</span>              <span class="hljs-attr">command:</span>              <span class="hljs-bullet">-</span> <span class="hljs-string">/bin/calico-node</span>              <span class="hljs-bullet">-</span> <span class="hljs-string">-bird-ready</span>              <span class="hljs-bullet">-</span> <span class="hljs-string">-felix-ready</span>            <span class="hljs-attr">periodSeconds:</span> <span class="hljs-number">10</span>          <span class="hljs-attr">volumeMounts:</span>            <span class="hljs-bullet">-</span> <span class="hljs-attr">mountPath:</span> <span class="hljs-string">/lib/modules</span>              <span class="hljs-attr">name:</span> <span class="hljs-string">lib-modules</span>              <span class="hljs-attr">readOnly:</span> <span class="hljs-literal">true</span>            <span class="hljs-bullet">-</span> <span class="hljs-attr">mountPath:</span> <span class="hljs-string">/run/xtables.lock</span>              <span class="hljs-attr">name:</span> <span class="hljs-string">xtables-lock</span>              <span class="hljs-attr">readOnly:</span> <span class="hljs-literal">false</span>            <span class="hljs-bullet">-</span> <span class="hljs-attr">mountPath:</span> <span class="hljs-string">/var/run/calico</span>              <span class="hljs-attr">name:</span> <span class="hljs-string">var-run-calico</span>              <span class="hljs-attr">readOnly:</span> <span class="hljs-literal">false</span>            <span class="hljs-bullet">-</span> <span class="hljs-attr">mountPath:</span> <span class="hljs-string">/var/lib/calico</span>              <span class="hljs-attr">name:</span> <span class="hljs-string">var-lib-calico</span>              <span class="hljs-attr">readOnly:</span> <span class="hljs-literal">false</span>            <span class="hljs-bullet">-</span> <span class="hljs-attr">mountPath:</span> <span class="hljs-string">/calico-secrets</span>              <span class="hljs-attr">name:</span> <span class="hljs-string">etcd-certs</span>      <span class="hljs-attr">volumes:</span>        <span class="hljs-comment"># Used by calico/node.</span>        <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">lib-modules</span>          <span class="hljs-attr">hostPath:</span>            <span class="hljs-attr">path:</span> <span class="hljs-string">/lib/modules</span>        <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">var-run-calico</span>          <span class="hljs-attr">hostPath:</span>            <span class="hljs-attr">path:</span> <span class="hljs-string">/var/run/calico</span>        <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">var-lib-calico</span>          <span class="hljs-attr">hostPath:</span>            <span class="hljs-attr">path:</span> <span class="hljs-string">/var/lib/calico</span>        <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">xtables-lock</span>          <span class="hljs-attr">hostPath:</span>            <span class="hljs-attr">path:</span> <span class="hljs-string">/run/xtables.lock</span>            <span class="hljs-attr">type:</span> <span class="hljs-string">FileOrCreate</span>        <span class="hljs-comment"># Used to install CNI.</span>        <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">cni-bin-dir</span>          <span class="hljs-attr">hostPath:</span>            <span class="hljs-attr">path:</span> <span class="hljs-string">/opt/cni/bin</span>        <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">cni-net-dir</span>          <span class="hljs-attr">hostPath:</span>            <span class="hljs-attr">path:</span> <span class="hljs-string">/etc/cni/net.d</span>        <span class="hljs-comment"># Mount in the etcd TLS secrets with mode 400.</span>        <span class="hljs-comment"># See https://kubernetes.io/docs/concepts/configuration/secret/</span>        <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">etcd-certs</span>          <span class="hljs-attr">secret:</span>            <span class="hljs-attr">secretName:</span> <span class="hljs-string">calico-etcd-secrets</span>            <span class="hljs-attr">defaultMode:</span> <span class="hljs-number">0400</span><span class="hljs-meta">---</span><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span><span class="hljs-attr">kind:</span> <span class="hljs-string">ServiceAccount</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">calico-node</span>  <span class="hljs-attr">namespace:</span> <span class="hljs-string">kube-system</span><span class="hljs-meta">---</span><span class="hljs-comment"># Source: calico/templates/calico-kube-controllers.yaml</span><span class="hljs-comment"># This manifest deploys the Calico Kubernetes controllers.</span><span class="hljs-comment"># See https://github.com/projectcalico/kube-controllers</span><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">extensions/v1beta1</span><span class="hljs-attr">kind:</span> <span class="hljs-string">Deployment</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">calico-kube-controllers</span>  <span class="hljs-attr">namespace:</span> <span class="hljs-string">kube-system</span>  <span class="hljs-attr">labels:</span>    <span class="hljs-attr">k8s-app:</span> <span class="hljs-string">calico-kube-controllers</span>  <span class="hljs-attr">annotations:</span>    <span class="hljs-attr">scheduler.alpha.kubernetes.io/critical-pod:</span> <span class="hljs-string">&#x27;&#x27;</span><span class="hljs-attr">spec:</span>  <span class="hljs-comment"># The controllers can only have a single active instance.</span>  <span class="hljs-attr">replicas:</span> <span class="hljs-number">1</span>  <span class="hljs-attr">strategy:</span>    <span class="hljs-attr">type:</span> <span class="hljs-string">Recreate</span>  <span class="hljs-attr">template:</span>    <span class="hljs-attr">metadata:</span>      <span class="hljs-attr">name:</span> <span class="hljs-string">calico-kube-controllers</span>      <span class="hljs-attr">namespace:</span> <span class="hljs-string">kube-system</span>      <span class="hljs-attr">labels:</span>        <span class="hljs-attr">k8s-app:</span> <span class="hljs-string">calico-kube-controllers</span>    <span class="hljs-attr">spec:</span>      <span class="hljs-attr">nodeSelector:</span>        <span class="hljs-attr">beta.kubernetes.io/os:</span> <span class="hljs-string">linux</span>      <span class="hljs-comment"># The controllers must run in the host network namespace so that</span>      <span class="hljs-comment"># it isn&#x27;t governed by policy that would prevent it from working.</span>      <span class="hljs-attr">hostNetwork:</span> <span class="hljs-literal">true</span>      <span class="hljs-attr">tolerations:</span>        <span class="hljs-comment"># Mark the pod as a critical add-on for rescheduling.</span>        <span class="hljs-bullet">-</span> <span class="hljs-attr">key:</span> <span class="hljs-string">CriticalAddonsOnly</span>          <span class="hljs-attr">operator:</span> <span class="hljs-string">Exists</span>        <span class="hljs-bullet">-</span> <span class="hljs-attr">key:</span> <span class="hljs-string">node-role.kubernetes.io/master</span>          <span class="hljs-attr">effect:</span> <span class="hljs-string">NoSchedule</span>      <span class="hljs-attr">serviceAccountName:</span> <span class="hljs-string">calico-kube-controllers</span>      <span class="hljs-attr">containers:</span>        <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">calico-kube-controllers</span>          <span class="hljs-attr">image:</span> <span class="hljs-string">calico/kube-controllers:v3.6.0</span>          <span class="hljs-attr">env:</span>            <span class="hljs-comment"># The location of the Calico etcd cluster.</span>            <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">ETCD_ENDPOINTS</span>              <span class="hljs-attr">valueFrom:</span>                <span class="hljs-attr">configMapKeyRef:</span>                  <span class="hljs-attr">name:</span> <span class="hljs-string">calico-config</span>                  <span class="hljs-attr">key:</span> <span class="hljs-string">etcd_endpoints</span>            <span class="hljs-comment"># Location of the CA certificate for etcd.</span>            <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">ETCD_CA_CERT_FILE</span>              <span class="hljs-attr">valueFrom:</span>                <span class="hljs-attr">configMapKeyRef:</span>                  <span class="hljs-attr">name:</span> <span class="hljs-string">calico-config</span>                  <span class="hljs-attr">key:</span> <span class="hljs-string">etcd_ca</span>            <span class="hljs-comment"># Location of the client key for etcd.</span>            <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">ETCD_KEY_FILE</span>              <span class="hljs-attr">valueFrom:</span>                <span class="hljs-attr">configMapKeyRef:</span>                  <span class="hljs-attr">name:</span> <span class="hljs-string">calico-config</span>                  <span class="hljs-attr">key:</span> <span class="hljs-string">etcd_key</span>            <span class="hljs-comment"># Location of the client certificate for etcd.</span>            <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">ETCD_CERT_FILE</span>              <span class="hljs-attr">valueFrom:</span>                <span class="hljs-attr">configMapKeyRef:</span>                  <span class="hljs-attr">name:</span> <span class="hljs-string">calico-config</span>                  <span class="hljs-attr">key:</span> <span class="hljs-string">etcd_cert</span>            <span class="hljs-comment"># Choose which controllers to run.</span>            <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">ENABLED_CONTROLLERS</span>              <span class="hljs-attr">value:</span> <span class="hljs-string">policy,namespace,serviceaccount,workloadendpoint,node</span>          <span class="hljs-attr">volumeMounts:</span>            <span class="hljs-comment"># Mount in the etcd TLS secrets.</span>            <span class="hljs-bullet">-</span> <span class="hljs-attr">mountPath:</span> <span class="hljs-string">/calico-secrets</span>              <span class="hljs-attr">name:</span> <span class="hljs-string">etcd-certs</span>          <span class="hljs-attr">readinessProbe:</span>            <span class="hljs-attr">exec:</span>              <span class="hljs-attr">command:</span>              <span class="hljs-bullet">-</span> <span class="hljs-string">/usr/bin/check-status</span>              <span class="hljs-bullet">-</span> <span class="hljs-string">-r</span>      <span class="hljs-attr">volumes:</span>        <span class="hljs-comment"># Mount in the etcd TLS secrets with mode 400.</span>        <span class="hljs-comment"># See https://kubernetes.io/docs/concepts/configuration/secret/</span>        <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">etcd-certs</span>          <span class="hljs-attr">secret:</span>            <span class="hljs-attr">secretName:</span> <span class="hljs-string">calico-etcd-secrets</span>            <span class="hljs-attr">defaultMode:</span> <span class="hljs-number">0400</span><span class="hljs-meta">---</span><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span><span class="hljs-attr">kind:</span> <span class="hljs-string">ServiceAccount</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">calico-kube-controllers</span>  <span class="hljs-attr">namespace:</span> <span class="hljs-string">kube-system</span></code></pre></div><p><strong>需要注意的是我们添加了 <code>IP_AUTODETECTION_METHOD</code> 变量，这个变量会设置 calcio 获取 node ip 的方式；默认情况下采用 <a href="https://docs.projectcalico.org/v3.6/reference/node/configuration#ip-autodetection-methods">first-found</a> 方式获取，即获取第一个有效网卡的 IP 作为 node ip；在某些多网卡机器上可能会出现问题；这里将值设置为 <code>can-reach=192.168.1.51</code>，即使用第一个能够访问 master <code>192.168.1.51</code> 的网卡地址作为 node ip</strong></p><p>最后执行创建即可，创建成功后如下所示</p><div class="hljs code-wrapper"><pre><code class="hljs sh">docker1.node ➜  ~ kubectl get pod -o wide -n kube-systemNAME                                      READY   STATUS    RESTARTS   AGE   IP             NODE           NOMINATED NODE   READINESS GATEScalico-kube-controllers-65bc6b9f9-cn27f   1/1     Running   0          85s   192.168.1.53   docker3.node   &lt;none&gt;           &lt;none&gt;calico-node-c5nl8                         1/1     Running   0          85s   192.168.1.53   docker3.node   &lt;none&gt;           &lt;none&gt;calico-node-fqknv                         1/1     Running   0          85s   192.168.1.51   docker1.node   &lt;none&gt;           &lt;none&gt;calico-node-ldfzs                         1/1     Running   0          85s   192.168.1.55   docker5.node   &lt;none&gt;           &lt;none&gt;calico-node-ngjxc                         1/1     Running   0          85s   192.168.1.52   docker2.node   &lt;none&gt;           &lt;none&gt;calico-node-vj8np                         1/1     Running   0          85s   192.168.1.54   docker4.node   &lt;none&gt;           &lt;none&gt;</code></pre></div><p>此时所有 node 应当变为 Ready 状态</p><h3 id="3-5、部署-DNS"><a href="#3-5、部署-DNS" class="headerlink" title="3.5、部署 DNS"></a>3.5、部署 DNS</h3><p>其他组件全部完成后我们应当部署集群 DNS 使 service 等能够正常解析；集群 DNS 这里采用 coredns，具体安装文档参考 <a href="https://github.com/coredns/deployment/tree/master/kubernetes">coredns/deployment</a>；coredns 完整配置如下</p><ul><li>coredns.yaml</li></ul><div class="hljs code-wrapper"><pre><code class="hljs yaml"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span><span class="hljs-attr">kind:</span> <span class="hljs-string">ServiceAccount</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">coredns</span>  <span class="hljs-attr">namespace:</span> <span class="hljs-string">kube-system</span><span class="hljs-meta">---</span><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">rbac.authorization.k8s.io/v1</span><span class="hljs-attr">kind:</span> <span class="hljs-string">ClusterRole</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">labels:</span>    <span class="hljs-attr">kubernetes.io/bootstrapping:</span> <span class="hljs-string">rbac-defaults</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">system:coredns</span><span class="hljs-attr">rules:</span><span class="hljs-bullet">-</span> <span class="hljs-attr">apiGroups:</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">&quot;&quot;</span>  <span class="hljs-attr">resources:</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">endpoints</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">services</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">pods</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">namespaces</span>  <span class="hljs-attr">verbs:</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">list</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">watch</span><span class="hljs-bullet">-</span> <span class="hljs-attr">apiGroups:</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">&quot;&quot;</span>  <span class="hljs-attr">resources:</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">nodes</span>  <span class="hljs-attr">verbs:</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">get</span><span class="hljs-meta">---</span><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">rbac.authorization.k8s.io/v1</span><span class="hljs-attr">kind:</span> <span class="hljs-string">ClusterRoleBinding</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">annotations:</span>    <span class="hljs-attr">rbac.authorization.kubernetes.io/autoupdate:</span> <span class="hljs-string">&quot;true&quot;</span>  <span class="hljs-attr">labels:</span>    <span class="hljs-attr">kubernetes.io/bootstrapping:</span> <span class="hljs-string">rbac-defaults</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">system:coredns</span><span class="hljs-attr">roleRef:</span>  <span class="hljs-attr">apiGroup:</span> <span class="hljs-string">rbac.authorization.k8s.io</span>  <span class="hljs-attr">kind:</span> <span class="hljs-string">ClusterRole</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">system:coredns</span><span class="hljs-attr">subjects:</span><span class="hljs-bullet">-</span> <span class="hljs-attr">kind:</span> <span class="hljs-string">ServiceAccount</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">coredns</span>  <span class="hljs-attr">namespace:</span> <span class="hljs-string">kube-system</span><span class="hljs-meta">---</span><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span><span class="hljs-attr">kind:</span> <span class="hljs-string">ConfigMap</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">coredns</span>  <span class="hljs-attr">namespace:</span> <span class="hljs-string">kube-system</span><span class="hljs-attr">data:</span>  <span class="hljs-attr">Corefile:</span> <span class="hljs-string">|</span>    <span class="hljs-string">.:53</span> &#123;        <span class="hljs-string">errors</span>        <span class="hljs-string">health</span>        <span class="hljs-string">kubernetes</span> <span class="hljs-string">cluster.local</span> <span class="hljs-string">in-addr.arpa</span> <span class="hljs-string">ip6.arpa</span> &#123;          <span class="hljs-string">pods</span> <span class="hljs-string">insecure</span>          <span class="hljs-string">upstream</span>          <span class="hljs-string">fallthrough</span> <span class="hljs-string">in-addr.arpa</span> <span class="hljs-string">ip6.arpa</span>        &#125;        <span class="hljs-string">prometheus</span> <span class="hljs-string">:9153</span>        <span class="hljs-string">forward</span> <span class="hljs-string">.</span> <span class="hljs-string">/etc/resolv.conf</span>        <span class="hljs-string">cache</span> <span class="hljs-number">30</span>        <span class="hljs-string">loop</span>        <span class="hljs-string">reload</span>        <span class="hljs-string">loadbalance</span>    &#125;<span class="hljs-meta">---</span><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">apps/v1</span><span class="hljs-attr">kind:</span> <span class="hljs-string">Deployment</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">coredns</span>  <span class="hljs-attr">namespace:</span> <span class="hljs-string">kube-system</span>  <span class="hljs-attr">labels:</span>    <span class="hljs-attr">k8s-app:</span> <span class="hljs-string">kube-dns</span>    <span class="hljs-attr">kubernetes.io/name:</span> <span class="hljs-string">&quot;CoreDNS&quot;</span><span class="hljs-attr">spec:</span>  <span class="hljs-attr">replicas:</span> <span class="hljs-number">2</span>  <span class="hljs-attr">strategy:</span>    <span class="hljs-attr">type:</span> <span class="hljs-string">RollingUpdate</span>    <span class="hljs-attr">rollingUpdate:</span>      <span class="hljs-attr">maxUnavailable:</span> <span class="hljs-number">1</span>  <span class="hljs-attr">selector:</span>    <span class="hljs-attr">matchLabels:</span>      <span class="hljs-attr">k8s-app:</span> <span class="hljs-string">kube-dns</span>  <span class="hljs-attr">template:</span>    <span class="hljs-attr">metadata:</span>      <span class="hljs-attr">labels:</span>        <span class="hljs-attr">k8s-app:</span> <span class="hljs-string">kube-dns</span>    <span class="hljs-attr">spec:</span>      <span class="hljs-attr">priorityClassName:</span> <span class="hljs-string">system-cluster-critical</span>      <span class="hljs-attr">serviceAccountName:</span> <span class="hljs-string">coredns</span>      <span class="hljs-attr">tolerations:</span>        <span class="hljs-bullet">-</span> <span class="hljs-attr">key:</span> <span class="hljs-string">&quot;CriticalAddonsOnly&quot;</span>          <span class="hljs-attr">operator:</span> <span class="hljs-string">&quot;Exists&quot;</span>      <span class="hljs-attr">nodeSelector:</span>        <span class="hljs-attr">beta.kubernetes.io/os:</span> <span class="hljs-string">linux</span>      <span class="hljs-attr">containers:</span>      <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">coredns</span>        <span class="hljs-attr">image:</span> <span class="hljs-string">coredns/coredns:1.3.1</span>        <span class="hljs-attr">imagePullPolicy:</span> <span class="hljs-string">IfNotPresent</span>        <span class="hljs-attr">resources:</span>          <span class="hljs-attr">limits:</span>            <span class="hljs-attr">memory:</span> <span class="hljs-string">170Mi</span>          <span class="hljs-attr">requests:</span>            <span class="hljs-attr">cpu:</span> <span class="hljs-string">100m</span>            <span class="hljs-attr">memory:</span> <span class="hljs-string">70Mi</span>        <span class="hljs-attr">args:</span> [ <span class="hljs-string">&quot;-conf&quot;</span>, <span class="hljs-string">&quot;/etc/coredns/Corefile&quot;</span> ]        <span class="hljs-attr">volumeMounts:</span>        <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">config-volume</span>          <span class="hljs-attr">mountPath:</span> <span class="hljs-string">/etc/coredns</span>          <span class="hljs-attr">readOnly:</span> <span class="hljs-literal">true</span>        <span class="hljs-attr">ports:</span>        <span class="hljs-bullet">-</span> <span class="hljs-attr">containerPort:</span> <span class="hljs-number">53</span>          <span class="hljs-attr">name:</span> <span class="hljs-string">dns</span>          <span class="hljs-attr">protocol:</span> <span class="hljs-string">UDP</span>        <span class="hljs-bullet">-</span> <span class="hljs-attr">containerPort:</span> <span class="hljs-number">53</span>          <span class="hljs-attr">name:</span> <span class="hljs-string">dns-tcp</span>          <span class="hljs-attr">protocol:</span> <span class="hljs-string">TCP</span>        <span class="hljs-bullet">-</span> <span class="hljs-attr">containerPort:</span> <span class="hljs-number">9153</span>          <span class="hljs-attr">name:</span> <span class="hljs-string">metrics</span>          <span class="hljs-attr">protocol:</span> <span class="hljs-string">TCP</span>        <span class="hljs-attr">securityContext:</span>          <span class="hljs-attr">allowPrivilegeEscalation:</span> <span class="hljs-literal">false</span>          <span class="hljs-attr">capabilities:</span>            <span class="hljs-attr">add:</span>            <span class="hljs-bullet">-</span> <span class="hljs-string">NET_BIND_SERVICE</span>            <span class="hljs-attr">drop:</span>            <span class="hljs-bullet">-</span> <span class="hljs-string">all</span>          <span class="hljs-attr">readOnlyRootFilesystem:</span> <span class="hljs-literal">true</span>        <span class="hljs-attr">livenessProbe:</span>          <span class="hljs-attr">httpGet:</span>            <span class="hljs-attr">path:</span> <span class="hljs-string">/health</span>            <span class="hljs-attr">port:</span> <span class="hljs-number">8080</span>            <span class="hljs-attr">scheme:</span> <span class="hljs-string">HTTP</span>          <span class="hljs-attr">initialDelaySeconds:</span> <span class="hljs-number">60</span>          <span class="hljs-attr">timeoutSeconds:</span> <span class="hljs-number">5</span>          <span class="hljs-attr">successThreshold:</span> <span class="hljs-number">1</span>          <span class="hljs-attr">failureThreshold:</span> <span class="hljs-number">5</span>        <span class="hljs-attr">readinessProbe:</span>          <span class="hljs-attr">httpGet:</span>            <span class="hljs-attr">path:</span> <span class="hljs-string">/health</span>            <span class="hljs-attr">port:</span> <span class="hljs-number">8080</span>            <span class="hljs-attr">scheme:</span> <span class="hljs-string">HTTP</span>      <span class="hljs-attr">dnsPolicy:</span> <span class="hljs-string">Default</span>      <span class="hljs-attr">volumes:</span>        <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">config-volume</span>          <span class="hljs-attr">configMap:</span>            <span class="hljs-attr">name:</span> <span class="hljs-string">coredns</span>            <span class="hljs-attr">items:</span>            <span class="hljs-bullet">-</span> <span class="hljs-attr">key:</span> <span class="hljs-string">Corefile</span>              <span class="hljs-attr">path:</span> <span class="hljs-string">Corefile</span><span class="hljs-meta">---</span><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span><span class="hljs-attr">kind:</span> <span class="hljs-string">Service</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">kube-dns</span>  <span class="hljs-attr">namespace:</span> <span class="hljs-string">kube-system</span>  <span class="hljs-attr">annotations:</span>    <span class="hljs-attr">prometheus.io/port:</span> <span class="hljs-string">&quot;9153&quot;</span>    <span class="hljs-attr">prometheus.io/scrape:</span> <span class="hljs-string">&quot;true&quot;</span>  <span class="hljs-attr">labels:</span>    <span class="hljs-attr">k8s-app:</span> <span class="hljs-string">kube-dns</span>    <span class="hljs-attr">kubernetes.io/cluster-service:</span> <span class="hljs-string">&quot;true&quot;</span>    <span class="hljs-attr">kubernetes.io/name:</span> <span class="hljs-string">&quot;CoreDNS&quot;</span><span class="hljs-attr">spec:</span>  <span class="hljs-attr">selector:</span>    <span class="hljs-attr">k8s-app:</span> <span class="hljs-string">kube-dns</span>  <span class="hljs-attr">clusterIP:</span> <span class="hljs-number">10.254</span><span class="hljs-number">.0</span><span class="hljs-number">.2</span>  <span class="hljs-attr">ports:</span>  <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">dns</span>    <span class="hljs-attr">port:</span> <span class="hljs-number">53</span>    <span class="hljs-attr">protocol:</span> <span class="hljs-string">UDP</span>  <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">dns-tcp</span>    <span class="hljs-attr">port:</span> <span class="hljs-number">53</span>    <span class="hljs-attr">protocol:</span> <span class="hljs-string">TCP</span>  <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">metrics</span>    <span class="hljs-attr">port:</span> <span class="hljs-number">9153</span>    <span class="hljs-attr">protocol:</span> <span class="hljs-string">TCP</span></code></pre></div><h3 id="3-5、部署-DNS-自动扩容"><a href="#3-5、部署-DNS-自动扩容" class="headerlink" title="3.5、部署 DNS 自动扩容"></a>3.5、部署 DNS 自动扩容</h3><p>在大规模集群的情况下，可能需要集群 DNS 自动扩容，具体文档请参考 <a href="https://github.com/kubernetes/kubernetes/tree/master/cluster/addons/dns-horizontal-autoscaler">DNS Horizontal Autoscaler</a>，DNS 扩容算法可参考 <a href="https://github.com/kubernetes-incubator/cluster-proportional-autoscaler/">Github</a>，如有需要请自行修改；以下为具体配置</p><ul><li>dns-horizontal-autoscaler.yaml</li></ul><div class="hljs code-wrapper"><pre><code class="hljs yaml"><span class="hljs-comment"># Copyright 2016 The Kubernetes Authors.</span><span class="hljs-comment">#</span><span class="hljs-comment"># Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span><span class="hljs-comment"># you may not use this file except in compliance with the License.</span><span class="hljs-comment"># You may obtain a copy of the License at</span><span class="hljs-comment">#</span><span class="hljs-comment">#     http://www.apache.org/licenses/LICENSE-2.0</span><span class="hljs-comment">#</span><span class="hljs-comment"># Unless required by applicable law or agreed to in writing, software</span><span class="hljs-comment"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span><span class="hljs-comment"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span><span class="hljs-comment"># See the License for the specific language governing permissions and</span><span class="hljs-comment"># limitations under the License.</span><span class="hljs-attr">kind:</span> <span class="hljs-string">ServiceAccount</span><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">kube-dns-autoscaler</span>  <span class="hljs-attr">namespace:</span> <span class="hljs-string">kube-system</span>  <span class="hljs-attr">labels:</span>    <span class="hljs-attr">addonmanager.kubernetes.io/mode:</span> <span class="hljs-string">Reconcile</span><span class="hljs-meta">---</span><span class="hljs-attr">kind:</span> <span class="hljs-string">ClusterRole</span><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">rbac.authorization.k8s.io/v1</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">system:kube-dns-autoscaler</span>  <span class="hljs-attr">labels:</span>    <span class="hljs-attr">addonmanager.kubernetes.io/mode:</span> <span class="hljs-string">Reconcile</span><span class="hljs-attr">rules:</span>  <span class="hljs-bullet">-</span> <span class="hljs-attr">apiGroups:</span> [<span class="hljs-string">&quot;&quot;</span>]    <span class="hljs-attr">resources:</span> [<span class="hljs-string">&quot;nodes&quot;</span>]    <span class="hljs-attr">verbs:</span> [<span class="hljs-string">&quot;list&quot;</span>]  <span class="hljs-bullet">-</span> <span class="hljs-attr">apiGroups:</span> [<span class="hljs-string">&quot;&quot;</span>]    <span class="hljs-attr">resources:</span> [<span class="hljs-string">&quot;replicationcontrollers/scale&quot;</span>]    <span class="hljs-attr">verbs:</span> [<span class="hljs-string">&quot;get&quot;</span>, <span class="hljs-string">&quot;update&quot;</span>]  <span class="hljs-bullet">-</span> <span class="hljs-attr">apiGroups:</span> [<span class="hljs-string">&quot;extensions&quot;</span>]    <span class="hljs-attr">resources:</span> [<span class="hljs-string">&quot;deployments/scale&quot;</span>, <span class="hljs-string">&quot;replicasets/scale&quot;</span>]    <span class="hljs-attr">verbs:</span> [<span class="hljs-string">&quot;get&quot;</span>, <span class="hljs-string">&quot;update&quot;</span>]<span class="hljs-comment"># Remove the configmaps rule once below issue is fixed:</span><span class="hljs-comment"># kubernetes-incubator/cluster-proportional-autoscaler#16</span>  <span class="hljs-bullet">-</span> <span class="hljs-attr">apiGroups:</span> [<span class="hljs-string">&quot;&quot;</span>]    <span class="hljs-attr">resources:</span> [<span class="hljs-string">&quot;configmaps&quot;</span>]    <span class="hljs-attr">verbs:</span> [<span class="hljs-string">&quot;get&quot;</span>, <span class="hljs-string">&quot;create&quot;</span>]<span class="hljs-meta">---</span><span class="hljs-attr">kind:</span> <span class="hljs-string">ClusterRoleBinding</span><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">rbac.authorization.k8s.io/v1</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">system:kube-dns-autoscaler</span>  <span class="hljs-attr">labels:</span>    <span class="hljs-attr">addonmanager.kubernetes.io/mode:</span> <span class="hljs-string">Reconcile</span><span class="hljs-attr">subjects:</span>  <span class="hljs-bullet">-</span> <span class="hljs-attr">kind:</span> <span class="hljs-string">ServiceAccount</span>    <span class="hljs-attr">name:</span> <span class="hljs-string">kube-dns-autoscaler</span>    <span class="hljs-attr">namespace:</span> <span class="hljs-string">kube-system</span><span class="hljs-attr">roleRef:</span>  <span class="hljs-attr">kind:</span> <span class="hljs-string">ClusterRole</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">system:kube-dns-autoscaler</span>  <span class="hljs-attr">apiGroup:</span> <span class="hljs-string">rbac.authorization.k8s.io</span><span class="hljs-meta">---</span><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">apps/v1</span><span class="hljs-attr">kind:</span> <span class="hljs-string">Deployment</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">kube-dns-autoscaler</span>  <span class="hljs-attr">namespace:</span> <span class="hljs-string">kube-system</span>  <span class="hljs-attr">labels:</span>    <span class="hljs-attr">k8s-app:</span> <span class="hljs-string">kube-dns-autoscaler</span>    <span class="hljs-attr">kubernetes.io/cluster-service:</span> <span class="hljs-string">&quot;true&quot;</span>    <span class="hljs-attr">addonmanager.kubernetes.io/mode:</span> <span class="hljs-string">Reconcile</span><span class="hljs-attr">spec:</span>  <span class="hljs-attr">selector:</span>    <span class="hljs-attr">matchLabels:</span>      <span class="hljs-attr">k8s-app:</span> <span class="hljs-string">kube-dns-autoscaler</span>  <span class="hljs-attr">template:</span>    <span class="hljs-attr">metadata:</span>      <span class="hljs-attr">labels:</span>        <span class="hljs-attr">k8s-app:</span> <span class="hljs-string">kube-dns-autoscaler</span>      <span class="hljs-attr">annotations:</span>        <span class="hljs-attr">scheduler.alpha.kubernetes.io/critical-pod:</span> <span class="hljs-string">&#x27;&#x27;</span>    <span class="hljs-attr">spec:</span>      <span class="hljs-attr">priorityClassName:</span> <span class="hljs-string">system-cluster-critical</span>      <span class="hljs-attr">containers:</span>      <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">autoscaler</span>        <span class="hljs-attr">image:</span> <span class="hljs-string">gcr.azk8s.cn/google_containers/cluster-proportional-autoscaler-amd64:1.1.2-r2</span>        <span class="hljs-attr">resources:</span>            <span class="hljs-attr">requests:</span>                <span class="hljs-attr">cpu:</span> <span class="hljs-string">&quot;20m&quot;</span>                <span class="hljs-attr">memory:</span> <span class="hljs-string">&quot;10Mi&quot;</span>        <span class="hljs-attr">command:</span>          <span class="hljs-bullet">-</span> <span class="hljs-string">/cluster-proportional-autoscaler</span>          <span class="hljs-bullet">-</span> <span class="hljs-string">--namespace=kube-system</span>          <span class="hljs-bullet">-</span> <span class="hljs-string">--configmap=kube-dns-autoscaler</span>          <span class="hljs-comment"># Should keep target in sync with cluster/addons/dns/kube-dns.yaml.base</span>          <span class="hljs-bullet">-</span> <span class="hljs-string">--target=Deployment/coredns</span>          <span class="hljs-comment"># When cluster is using large nodes(with more cores), &quot;coresPerReplica&quot; should dominate.</span>          <span class="hljs-comment"># If using small nodes, &quot;nodesPerReplica&quot; should dominate.</span>          <span class="hljs-bullet">-</span> <span class="hljs-string">--default-params=&#123;&quot;linear&quot;:&#123;&quot;coresPerReplica&quot;:256,&quot;nodesPerReplica&quot;:16,&quot;preventSinglePointFailure&quot;:true&#125;&#125;</span>          <span class="hljs-bullet">-</span> <span class="hljs-string">--logtostderr=true</span>          <span class="hljs-bullet">-</span> <span class="hljs-string">--v=2</span>      <span class="hljs-attr">tolerations:</span>      <span class="hljs-bullet">-</span> <span class="hljs-attr">key:</span> <span class="hljs-string">&quot;CriticalAddonsOnly&quot;</span>        <span class="hljs-attr">operator:</span> <span class="hljs-string">&quot;Exists&quot;</span>      <span class="hljs-attr">serviceAccountName:</span> <span class="hljs-string">kube-dns-autoscaler</span></code></pre></div><h2 id="四、其他"><a href="#四、其他" class="headerlink" title="四、其他"></a>四、其他</h2><h3 id="4-1、集群测试"><a href="#4-1、集群测试" class="headerlink" title="4.1、集群测试"></a>4.1、集群测试</h3><p>为测试集群工作正常，我们创建一个 deployment 和一个 service，用于测试联通性和 DNS 工作是否正常；测试配置如下</p><ul><li>test.yaml</li></ul><div class="hljs code-wrapper"><pre><code class="hljs yaml"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">apps/v1</span><span class="hljs-attr">kind:</span> <span class="hljs-string">Deployment</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">test</span>  <span class="hljs-attr">labels:</span>    <span class="hljs-attr">app:</span> <span class="hljs-string">test</span><span class="hljs-attr">spec:</span>  <span class="hljs-attr">replicas:</span> <span class="hljs-number">5</span>  <span class="hljs-attr">selector:</span>    <span class="hljs-attr">matchLabels:</span>      <span class="hljs-attr">app:</span> <span class="hljs-string">test</span>  <span class="hljs-attr">template:</span>    <span class="hljs-attr">metadata:</span>      <span class="hljs-attr">labels:</span>        <span class="hljs-attr">app:</span> <span class="hljs-string">test</span>    <span class="hljs-attr">spec:</span>      <span class="hljs-attr">containers:</span>      <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">test</span>        <span class="hljs-attr">image:</span> <span class="hljs-string">nginx:1.14.2-alpine</span>        <span class="hljs-attr">ports:</span>        <span class="hljs-bullet">-</span> <span class="hljs-attr">containerPort:</span> <span class="hljs-number">80</span><span class="hljs-meta">---</span><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span><span class="hljs-attr">kind:</span> <span class="hljs-string">Service</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">test-service</span><span class="hljs-attr">spec:</span>  <span class="hljs-attr">selector:</span>    <span class="hljs-attr">app:</span> <span class="hljs-string">test</span>  <span class="hljs-attr">ports:</span>  <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">nginx</span>    <span class="hljs-attr">port:</span> <span class="hljs-number">80</span>    <span class="hljs-attr">nodePort:</span> <span class="hljs-number">30001</span>    <span class="hljs-attr">targetPort:</span> <span class="hljs-number">80</span>    <span class="hljs-attr">protocol:</span> <span class="hljs-string">TCP</span>  <span class="hljs-attr">type:</span> <span class="hljs-string">NodePort</span></code></pre></div><p>测试方式很简单，进入某一个 pod ping 其他 pod ip 确认网络是否正常，直接访问 service 名称测试 DNS 是否工作正常，这里不再演示</p><h3 id="4-2、其他说明"><a href="#4-2、其他说明" class="headerlink" title="4.2、其他说明"></a>4.2、其他说明</h3><p>此次搭建开启了大部分认证，限于篇幅原因没有将每个选项作用做完整解释，推荐搭建完成后仔细阅读以下 <code>--help</code> 中的描述(官方文档页面有时候更新不完整)；目前 apiserver 仍然保留了 8080 端口(因为直接使用 kubectl 方便)，但是在高安全性环境请关闭 8080 端口，因为即使绑定在 127.0.0.1 上，对于任何能够登录 master 机器的用户仍然能够不经验证操作整个集群</p>]]></content>
    
    
    <summary type="html">年后回来有点懒，也有点忙；1.13 出来好久了，周末还是决定折腾一下吧</summary>
    
    
    
    <category term="Kubernetes" scheme="https://mritd.com/categories/kubernetes/"/>
    
    
    <category term="Kubernetes" scheme="https://mritd.com/tags/kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>Kubernetes sample-cli-plugin 源码分析</title>
    <link href="https://mritd.com/2019/01/16/understand-kubernetes-sample-cli-plugin-source-code/"/>
    <id>https://mritd.com/2019/01/16/understand-kubernetes-sample-cli-plugin-source-code/</id>
    <published>2019-01-16T04:16:42.000Z</published>
    <updated>2019-01-16T04:16:42.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>写这篇文章的目的是为了继续上篇 <a href="https://mritd.me/2018/11/30/kubectl-plugin-new-solution-on-kubernetes-1.12/">Kubernetes 1.12 新的插件机制</a> 中最后部分对 <code>Golang 的插件辅助库</code> 说明；以及为后续使用 Golang 编写自己的 Kubernetes 插件做一个基础铺垫；顺边说一下 <strong>sample-cli-plugin 这个项目是官方为 Golang 开发者编写的一个用于快速切换配置文件中 Namespace 的一个插件样例</strong></p></blockquote><h2 id="一、基础准备"><a href="#一、基础准备" class="headerlink" title="一、基础准备"></a>一、基础准备</h2><p>在开始分析源码之前，<strong>我们假设读者已经熟悉 Golang 语言，至少对基本语法、指针、依赖管理工具有一定认知</strong>；下面介绍一下 <a href="https://github.com/kubernetes/sample-cli-plugin">sample-cli-plugin</a> 这个项目一些基础核心的依赖:</p><h3 id="1-1、Cobra-终端库"><a href="#1-1、Cobra-终端库" class="headerlink" title="1.1、Cobra 终端库"></a>1.1、Cobra 终端库</h3><p>这是一个强大的 Golang 的 command line interface 库，其支持用非常简单的代码创建出符合 Unix 风格的 cli 程序；甚至官方提供了用于创建 cli 工程脚手架的 cli 命令工具；Cobra 官方 Github 地址 <a href="https://github.com/spf13/cobra">点击这里</a>，具体用法请自行 Google，以下只做一个简单的命令定义介绍(docker、kubernetes 终端 cli 都基于这个库)</p><div class="hljs code-wrapper"><pre><code class="hljs golang"># 每一个命令(不论是子命令还是主命令)都会是一个 cobra.Command 对象<span class="hljs-keyword">var</span> lsCmd = &amp;cobra.Command&#123;    <span class="hljs-comment">// 一些命令帮助文档有关的描述信息</span>    Use:   <span class="hljs-string">&quot;ls&quot;</span>,    Short: <span class="hljs-string">&quot;A brief description of your command&quot;</span>,    Long: <span class="hljs-string">`A longer description that spans multiple lines and likely contains examples</span><span class="hljs-string">and usage of using your command. For example:</span><span class="hljs-string"></span><span class="hljs-string">Cobra is a CLI library for Go that empowers applications.</span><span class="hljs-string">This application is a tool to generate the needed files</span><span class="hljs-string">to quickly create a Cobra application.`</span>,    <span class="hljs-comment">// 命令运行时真正执行逻辑，如果需要返回 Error 信息，我们一般设置 RunE</span>    Run: <span class="hljs-function"><span class="hljs-keyword">func</span><span class="hljs-params">(cmd *cobra.Command, args []<span class="hljs-keyword">string</span>)</span></span> &#123;        fmt.Println(<span class="hljs-string">&quot;ls called&quot;</span>)    &#125;,&#125;<span class="hljs-comment">// 为这个命令添加 flag，比如 `--help`、`-p`</span><span class="hljs-comment">// PersistentFlags() 方法添加的 flag 在所有子 command 也会生效</span><span class="hljs-comment">// Cobra 的 command 可以无限级联，比如 `kubectl get pod` 就是在 `kubectl` command 下增加了子 `get` command</span>lsCmd.PersistentFlags().String(<span class="hljs-string">&quot;foo&quot;</span>, <span class="hljs-string">&quot;&quot;</span>, <span class="hljs-string">&quot;A help for foo&quot;</span>)<span class="hljs-comment">// Flags() 方法添加的 flag 仅在直接调用此子命令时生效</span>lsCmd.Flags().BoolP(<span class="hljs-string">&quot;toggle&quot;</span>, <span class="hljs-string">&quot;t&quot;</span>, <span class="hljs-literal">false</span>, <span class="hljs-string">&quot;Help message for toggle&quot;</span>)</code></pre></div><h3 id="1-2、vendor-依赖"><a href="#1-2、vendor-依赖" class="headerlink" title="1.2、vendor 依赖"></a>1.2、vendor 依赖</h3><p>vendor 目录用于存放 Golang 的依赖库，sample-cli-plugin 这个项目采用 <a href="https://github.com/tools/godep">godep</a> 工具管理依赖；依赖配置信息被保存在 <code>Godeps/Godeps.json</code> 中，<strong>一般项目不会上传 vendor 目录，因为它的依赖信息已经在 Godeps.json 中存在，只需要在项目下使用 <code>godep restore</code> 命令恢复就可自动重新下载</strong>；这里上传了 vendor 目录的原因应该是为了方便开发者直接使用 <code>go get</code> 命令安装；顺边说一下在 Golang 新版本已经开始转换到 <code>go mod</code> 依赖管理工具，标志就是项目下会有 <code>go.mod</code> 文件</p><h2 id="二、源码分析"><a href="#二、源码分析" class="headerlink" title="二、源码分析"></a>二、源码分析</h2><h3 id="2-1、环境搭建"><a href="#2-1、环境搭建" class="headerlink" title="2.1、环境搭建"></a>2.1、环境搭建</h3><p>这里准备一笔带过了，基本就是 clone 源码到 <code>$GOPATH/src/k8s.io/sample-cli-plugin</code> 目录，然后在 GoLand 中打开；目前我使用的 Go 版本为最新的 1.11.4；以下时导入源码后的截图</p><p><img src="https://cdn.oss.link/markdown/sn8o8.png" alt="GoLand"></p><h3 id="2-2、定位核心运行方法"><a href="#2-2、定位核心运行方法" class="headerlink" title="2.2、定位核心运行方法"></a>2.2、定位核心运行方法</h3><p>熟悉过 Cobra 库以后，再从整个项目包名上分析，首先想到的启动入口应该在 <code>cmd</code> 包下(一般 <code>cmd</code> 包下的文件都会编译成最终可执行文件名，Kubernetes 也是一样)</p><p><img src="https://cdn.oss.link/markdown/rafeq.png" alt="main"></p><p>从以上截图中可以看出，首先通过 <code>cmd.NewCmdNamespace</code> 方法创建了一个 Command 对象 <code>root</code>，然后调用了 <code>root.Execute</code> 就结束了；那么也就说明 <code>root</code> 这个 Command 是唯一的核心命令对象，整个插件实现都在这个 <code>root</code> 里；所以我们需要查看一下这个 <code>cmd.NewCmdNamespace</code> 是如何对它初始化的，找到 Cobra 中的 <code>Run</code> 或者 <code>RunE</code> 设置</p><p><img src="https://cdn.oss.link/markdown/77krg.png" alt="NewCmdNamespace"></p><p>定位到 <code>NewCmdNamespace</code> 方法以后，基本上就是标准的 Cobra 库的使用方式了；**从截图上可以看到，<code>RunE</code> 设置的函数总共运行了 3 个动作: <code>o.Complete</code>、<code>o.Validate</code>、<code>o.Run</code>**；所以接下来我们主要分析这三个方法就行了</p><h3 id="2-3、NamespaceOptions-结构体"><a href="#2-3、NamespaceOptions-结构体" class="headerlink" title="2.3、NamespaceOptions 结构体"></a>2.3、NamespaceOptions 结构体</h3><p>在分析上面说的这三个方法之前，我们还应当了解一下这个 <code>o</code> 是什么玩意</p><p><img src="https://cdn.oss.link/markdown/4b3cc.png" alt="NamespaceOptions"></p><p>从源码中可以看到，<code>o</code> 这个对象由 <code>NewNamespaceOptions</code> 创建，而 <code>NewNamespaceOptions</code> 方法返回的实际上是一个 <code>NamespaceOptions</code> 结构体；接下来我们需要研究一下这个结构体都是由什么组成的，换句话说要基本大致上整明白结构体的基本结构，比如里面的属性都是干啥的</p><h4 id="2-3-1、-genericclioptions-ConfigFlags"><a href="#2-3-1、-genericclioptions-ConfigFlags" class="headerlink" title="2.3.1、*genericclioptions.ConfigFlags"></a>2.3.1、*genericclioptions.ConfigFlags</h4><p>首先看下第一个属性 <code>configFlags</code>，它的实际类型是 <code>*genericclioptions.ConfigFlags</code>，点击查看以后如下</p><p><img src="https://cdn.oss.link/markdown/li6s4.png" alt="genericclioptions.ConfigFlags"></p><p>从这些字段上来看，我们可以暂且模糊的推测出这应该是个基础配置型的字段，负责存储一些全局基本设置，比如 API Server 认证信息等</p><h4 id="2-3-2、-api-Context"><a href="#2-3-2、-api-Context" class="headerlink" title="2.3.2、*api.Context"></a>2.3.2、*api.Context</h4><p>下面这两个 <code>resultingContext</code>、<code>resultingContextName</code> 就很好理解了，从名字上看就可以知道它们应该是用来存储结果集的 Context 信息的；当然这个 <code>*api.Context</code> 就是 Kubernetes 配置文件中 Context 的 Go 结构体</p><h4 id="2-3-3、userSpecified"><a href="#2-3-3、userSpecified" class="headerlink" title="2.3.3、userSpecified*"></a>2.3.3、userSpecified*</h4><p>这几个字段从名字上就可以区分出，他们应该用于存储用户设置的或者说是通过命令行选项输入的一些指定配置信息，比如 Cluster、Context 等</p><h4 id="2-3-4、rawConfig"><a href="#2-3-4、rawConfig" class="headerlink" title="2.3.4、rawConfig"></a>2.3.4、rawConfig</h4><p>rawConfig 这个变量名字有点子奇怪，不过它实际上是个 <code>api.Config</code>；里面保存了与 API Server 通讯的配置信息；<strong>至于为什么要有这玩意，是因为配置信息输入源有两个: cli 命令行选项(eg: <code>--namespace</code>)和用户配置文件(eg: <code>~/.kube/config</code>)；最终这两个地方的配置合并后会存储在这个 rawConfig 里</strong></p><h4 id="2-3-5、listNamespaces"><a href="#2-3-5、listNamespaces" class="headerlink" title="2.3.5、listNamespaces"></a>2.3.5、listNamespaces</h4><p>这个变量实际上相当于一个 flag，用于存储插件是否使用了 <code>--list</code> 选项；在分析结构体这里没法看出来；不过只要稍稍的多看一眼代码就能看在 <code>NewCmdNamespace</code> 方法中有这么一行代码</p><p><img src="https://cdn.oss.link/markdown/f07l3.png" alt="listNamespaces"></p><h3 id="2-4、核心处理逻辑"><a href="#2-4、核心处理逻辑" class="headerlink" title="2.4、核心处理逻辑"></a>2.4、核心处理逻辑</h3><p>介绍完了结构体的基本属性，最后我们只需要弄明白在核心 Command 方法内运行的这三个核心方法就行了</p><p><img src="https://cdn.oss.link/markdown/8lm4b.png" alt="core func"></p><h4 id="2-4-1、-NamespaceOptions-Complete"><a href="#2-4-1、-NamespaceOptions-Complete" class="headerlink" title="2.4.1、*NamespaceOptions.Complete"></a>2.4.1、*NamespaceOptions.Complete</h4><p>这个方法代码稍微有点多，这里不会对每一行代码都做解释，只要大体明白都在干什么就行了；我们的目的是理解它，后续模仿它创造自己的插件；下面是代码截图</p><p><img src="https://cdn.oss.link/markdown/qqf0f.png" alt="NamespaceOptions.Complete"></p><p>从截图上可以看到，首先弄出了 <code>rawConfig</code> 这个玩意，<code>rawConfig</code> 上面也提到了，它就是终端选项和用户配置文件的最终合并，至于为什么可以查看 <code>ToRawKubeConfigLoader().RawConfig()</code> 这两个方法的注释和实现即可；</p><p>接下来就是各种获取插件执行所需要的变量信息，比如获取用户指定的 <code>Namespace</code>、<code>Cluster</code>、<code>Context</code> 等，其中还包含了一些必要的校验；比如不允许使用 <code>kubectl ns NS_NAME1 --namespace NS_NAME2</code> 这种操作(因为这么干很让人难以理解 “你到底是要切换到 <code>NS_NAME1</code> 还是 <code>NS_NAME2</code>“)</p><p>最后从 <code>153</code> 行 <code>o.resultingContext = api.NewContext()</code> 开始就是创建最终的 <code>resultingContext</code> 对象，把获取到的用户指定的 <code>Namespace</code> 等各种信息赋值好，为下一步将其持久化到配置文件中做准备</p><h4 id="2-4-2、-NamespaceOptions-Validate"><a href="#2-4-2、-NamespaceOptions-Validate" class="headerlink" title="2.4.2、*NamespaceOptions.Validate"></a>2.4.2、*NamespaceOptions.Validate</h4><p>这个方法看名字就知道，里面全是对最终结果的校验；比如检查一下 <code>rawConfig</code> 中的 <code>CurrentContext</code> 是否获取到了，看看命令行参数是否正确，确保你不会瞎鸡儿输入 <code>kubectl ns NS_NAME1 NS_NAME2</code> 这种命令</p><p><img src="https://cdn.oss.link/markdown/frqpb.png" alt="NamespaceOptions.Validate"></p><h4 id="2-4-3、-NamespaceOptions-Run"><a href="#2-4-3、-NamespaceOptions-Run" class="headerlink" title="2.4.3、*NamespaceOptions.Run"></a>2.4.3、*NamespaceOptions.Run</h4><p>第一步合并配置信息并获取到用户设置(输入)的配置，第二部做参数校验；可以说前面的两步操作都是为这一步做准备，<code>Run</code> 方法真正的做了配置文件写入、终端返回结果打印操作</p><p><img src="https://cdn.oss.link/markdown/6tkjz.png" alt="NamespaceOptions.Run"></p><p>可以看到，<code>Run</code> 方法第一步就是更加谨慎的检查了一下参数是否正常，然后调用了 <code>o.setNamespace</code>；这个方法截图如下</p><p><img src="https://cdn.oss.link/markdown/1jc3k.png" alt="NamespaceOptions.setNamespace"></p><p>这个 <code>setNamespace</code>是真正的做了配置文件写入动作的，实际写入方法就是 <code>clientcmd.ModifyConfig</code>；这个是 <code>Kubernetes</code> <code>client-go</code> 提供的方法，这些库的作用就是提供给我们非常方便的 API 操作；比如修改配置文件，你不需要关心配置文件在哪，你更不需要关系文件句柄是否被释放</p><p>从 <code>o.setNamespace</code> 方法以后其实就没什么看头了，毕竟插件的核心功能就是快速修改 <code>Namespace</code>；下面的各种 <code>for</code> 循环遍历其实就是在做打印输出；比如当你没有设置 <code>Namespace</code> 而使用了 <code>--list</code> 选项，插件就通过这里帮你打印设置过那些 <code>Namespace</code></p><h2 id="三、插件总结"><a href="#三、插件总结" class="headerlink" title="三、插件总结"></a>三、插件总结</h2><p>分析完了这个官方的插件，然后想一下自己以后写插件可能的需求，最后对比一下，可以为以后写插件做个总结:</p><ul><li>我们最好也弄个 <code>xxxOptions</code> 这种结构体存存一些配置</li><li>结构体内至少我们应当存储 <code>configFlags</code>、<code>rawConfig</code> 这两个基础配置信息</li><li>结构体内其它参数都应当是跟自己实际业务有关的</li><li>最后在在结构体上增加适当的方法完成自己的业务逻辑并保持好适当的校验</li></ul><p>转载请注明出n，本文采用 [CC4.0](<a href="http://c/">http://c</a> 1.12 新的插件机制](<a href="https://mritd.me/2018/11/30/kubectl-plugin-new-solution-on-kubernetes-1.12/">https://mritd.me/2018/11/30/kubectl-plugin-new-solution-on-kubernetes-1.12/</a>) 中最后部分对 <code>Golang 的插件辅助库</code> 说明；以及为后续使用 Golang 编写自己的 Kubernetes 插件做一个基础铺垫；顺边说一下 <strong>sample-cli-plugin 这个项目是官方为 Golang 开发者编写的一个用于快速切换配置文件中 Namespace 的一个插件样例</strong></p>]]></content>
    
    
    <summary type="html">写这篇文章的目的是为了继续上篇 [Kubernetes 1.12 新的插件机制](https://mritd.me/2018/11/30/kubectl-plugin-new-solution-on-kubernetes-1.12/) 中最后部分对 `Golang 的插件辅助库` 说明；以及为后续使用 Golang 编写自己的 Kubernetes 插件做一个基础铺垫；顺边说一下 **sample-cli-plugin 这个项目是官方为 Golang 开发者编写的一个用于快速切换配置文件中 Namespace 的一个插件样例**</summary>
    
    
    
    <category term="Kubernetes" scheme="https://mritd.com/categories/kubernetes/"/>
    
    
    <category term="Kubernetes" scheme="https://mritd.com/tags/kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>Kubernetes 1.12 新的插件机制</title>
    <link href="https://mritd.com/2018/11/30/kubectl-plugin-new-solution-on-kubernetes-1.12/"/>
    <id>https://mritd.com/2018/11/30/kubectl-plugin-new-solution-on-kubernetes-1.12/</id>
    <published>2018-11-29T16:05:34.000Z</published>
    <updated>2018-11-29T16:05:34.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>在很久以前的版本研究过 kubernetes 的插件机制，当时弄了一个快速切换 <code>namespace</code> 的小插件；最近把自己本机的 kubectl 升级到了 1.12，突然发现插件不能用了；撸了一下文档发现插件机制彻底改了…</p></blockquote><h2 id="一、插件编写语言"><a href="#一、插件编写语言" class="headerlink" title="一、插件编写语言"></a>一、插件编写语言</h2><p>kubernetes 1.12 新的插件机制在编写语言上同以前一样，<strong>可以以任意语言编写，只要能弄一个可执行的文件出来就行</strong>，插件可以是一个 <code>bash</code>、<code>python</code> 脚本，也可以是 <code>Go</code> 等编译语言最终编译的二进制；以下是一个 Copy 自官方文档的 <code>bash</code> 编写的插件样例</p><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-meta">#!/bin/bash</span><span class="hljs-comment"># optional argument handling</span><span class="hljs-keyword">if</span> [[ <span class="hljs-string">&quot;<span class="hljs-variable">$1</span>&quot;</span> == <span class="hljs-string">&quot;version&quot;</span> ]]<span class="hljs-keyword">then</span>    <span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;1.0.0&quot;</span>    <span class="hljs-built_in">exit</span> 0<span class="hljs-keyword">fi</span><span class="hljs-comment"># optional argument handling</span><span class="hljs-keyword">if</span> [[ <span class="hljs-string">&quot;<span class="hljs-variable">$1</span>&quot;</span> == <span class="hljs-string">&quot;config&quot;</span> ]]<span class="hljs-keyword">then</span>    <span class="hljs-built_in">echo</span> <span class="hljs-variable">$KUBECONFIG</span>    <span class="hljs-built_in">exit</span> 0<span class="hljs-keyword">fi</span><span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;I am a plugin named kubectl-foo&quot;</span></code></pre></div><h2 id="二、插件加载方式"><a href="#二、插件加载方式" class="headerlink" title="二、插件加载方式"></a>二、插件加载方式</h2><h3 id="2-1、插件位置"><a href="#2-1、插件位置" class="headerlink" title="2.1、插件位置"></a>2.1、插件位置</h3><p>1.12 kubectl 插件最大的变化就是加载方式变了，由原来的放置在指定位置，还要为其编写 yaml 配置变成了现在的类似 git 扩展命令的方式: <strong>只要放置在 PATH 下，并以 <code>kubectl-</code> 开头的可执行文件都被认为是 <code>kubectl</code> 的插件</strong>；所以你可以随便弄个小脚本(比如上面的代码)，然后改好名字赋予可执行权限，扔到 PATH 下即可</p><p><img src="https://cdn.oss.link/markdown/s64v6.png" alt="test-plugin"></p><h3 id="2-2、插件变量"><a href="#2-2、插件变量" class="headerlink" title="2.2、插件变量"></a>2.2、插件变量</h3><p>同以前不通，<strong>以前版本的执行插件时，<code>kubectl</code> 会向插件传递一些特定的与 <code>kubectl</code> 相关的变量，现在则只会传递标准变量；即 <code>kubectl</code> 能读到什么变量，插件就能读到，其他的私有化变量(比如 <code>KUBECTL_PLUGINS_CURRENT_NAMESPACE</code>)不会再提供</strong></p><p><img src="https://cdn.oss.link/markdown/vs1c3.png" alt="plugin env"></p><p><strong>并且新版本的插件体系，所有选项(<code>flag</code>) 将全部交由插件本身处理，kubectl 不会再解析</strong>，比如下面的 <code>--help</code> 交给了自定义插件处理，由于脚本内没有处理这个选项，所以相当于选项无效了</p><p><img src="https://cdn.oss.link/markdown/8ch88.png" alt="plugin flag"></p><p>还有就是 <strong>传递给插件的第一个参数永远是插件自己的绝对位置，比如这个 <code>test</code> 插件在执行时的 <code>$0</code> 是 <code>/usr/local/bin/kubectl-test</code></strong></p><h3 id="2-3、插件命名及查找"><a href="#2-3、插件命名及查找" class="headerlink" title="2.3、插件命名及查找"></a>2.3、插件命名及查找</h3><p>目前在插件命名及查找顺序上官方文档写的非常详尽，不给过对于普通使用者来说，实际上命名规则和查找与常规的 Linux 下的命令查找机制相同，只不过还做了增强；增强后的基本规则如下</p><ul><li><code>PATH</code> 优先匹配原则</li><li>短横线 <code>-</code> 自动分割匹配以及智能转义</li><li>以最精确匹配为首要目标</li><li>查找失败自动转换参数</li></ul><p><code>PATH</code> 优先匹配原则跟传统的命令查找一致，即当多个路径下存在同名的插件时，则采用最先查找到的插件</p><p><img src="https://cdn.oss.link/markdown/ljyp5.png" alt="plugin path"></p><p>当你的插件文件名中包含 <code>-</code> ，并且 <code>kubectl</code> 在无法精确找到插件时会尝试自动拼接命令来尝试匹配；如下所示，在没有找到 <code>kubectl-test</code> 这个命令时会尝试拼接参数查找</p><p><img src="https://cdn.oss.link/markdown/l85bp.png" alt="auto merge"></p><p>由于以上这种查找机制，<strong>当命令中确实包含 <code>-</code> 时，必须进行转义以 <code>_</code> 替换，否则 <code>kubectl</code> 会提示命令未找到错误</strong>；替换后可直接使用 <code>kubectl 插件命令(包含-)</code> 执行，同时也支持以原始插件名称执行(使用 <code>_</code>)</p><p><img src="https://cdn.oss.link/markdown/7vm0l.png" alt="name contains dash"></p><p>在复杂插件体系下，多个插件可能包含同样的前缀，此时将遵序最精确查找原则；即当两个插件 <code>kubectl-test-aaa</code>、<code>kubectl-test-aaa-bbb</code> 同时存在，并且执行 <code>kubectl test aaa bbb</code> 命令时，优先匹配最精确的插件 <code>kubectl-test-aaa-bbb</code>，<strong>而不是将 <code>bbb</code> 作为参数传递给 <code>kubectl-test-aaa</code> 插件</strong></p><p><img src="https://cdn.oss.link/markdown/god8q.png" alt="precise search"></p><h3 id="2-4、总结"><a href="#2-4、总结" class="headerlink" title="2.4、总结"></a>2.4、总结</h3><p>插件查找机制在一般情况下与传统 PATH 查找方式相同，同时 <code>kubectl</code> 实现了智能的 <code>-</code> 自动匹配查找、更精确的命令命中功能；这两种机制的实现主要为了方便编写插件的命令树(插件命令的子命令…)，类似下面这种</p><div class="hljs code-wrapper"><pre><code class="hljs sh">$ ls ./plugin_command_treekubectl-parentkubectl-parent-subcommandkubectl-parent-subcommand-subsubcommand</code></pre></div><p>当出现多个位置有同名插件时，执行 <code>kubectl plugin list</code> 能够检测出哪些插件由于 PATH 查找顺序原因导致永远不会被执行问题</p><div class="hljs code-wrapper"><pre><code class="hljs sh">$ kubectl plugin listThe following kubectl-compatible plugins are available:<span class="hljs-built_in">test</span>/fixtures/pkg/kubectl/plugins/kubectl-foo/usr/<span class="hljs-built_in">local</span>/bin/kubectl-foo  - warning: /usr/<span class="hljs-built_in">local</span>/bin/kubectl-foo is overshadowed by a similarly named plugin: <span class="hljs-built_in">test</span>/fixtures/pkg/kubectl/plugins/kubectl-fooplugins/kubectl-invalid  - warning: plugins/kubectl-invalid identified as a kubectl plugin, but it is not executableerror: 2 plugin warnings were found</code></pre></div><h3 id="三、Golang-的插件辅助库"><a href="#三、Golang-的插件辅助库" class="headerlink" title="三、Golang 的插件辅助库"></a>三、Golang 的插件辅助库</h3><p>由于插件机制的变更，导致其他语言编写的插件在实时获取某些配置信息、动态修改 <code>kubectl</code> 配置方面可能造成一定的阻碍；为此 kubernetes 提供了一个 <a href="https://github.com/kubernetes/cli-runtime">command line runtime package</a>，使用 Go 编写插件，配合这个库可以更加方便的解析和调整 <code>kubectl</code> 的配置信息</p><p>官方为了演示如何使用这个 <a href="https://github.com/kubernetes/cli-runtime">cli-runtime</a> 库编写了一个 <code>namespace</code> 切换的插件(自己白写了…)，仓库地址在 <a href="https://github.com/kubernetes/sample-cli-plugin">Github</a> 上，基本编译使用如下(直接 <code>go get</code> 后编译文件默认为目录名 <code>cmd</code>)</p><div class="hljs code-wrapper"><pre><code class="hljs sh">➜  ~ go get k8s.io/sample-cli-plugin/cmd➜  ~ sudo mv gopath/bin/cmd /usr/<span class="hljs-built_in">local</span>/bin/kubectl-ns➜  ~ kubectl nsdefault➜  ~ kubectl ns --<span class="hljs-built_in">help</span>View or <span class="hljs-built_in">set</span> the current namespaceUsage:  ns [new-namespace] [flags]Examples:        <span class="hljs-comment"># view the current namespace in your KUBECONFIG</span>        kubectl ns        <span class="hljs-comment"># view all of the namespaces in use by contexts in your KUBECONFIG</span>        kubectl ns --list        <span class="hljs-comment"># switch your current-context to one that contains the desired namespace</span>        kubectl ns fooFlags:      --as string                      Username to impersonate <span class="hljs-keyword">for</span> the operation      --as-group stringArray           Group to impersonate <span class="hljs-keyword">for</span> the operation, this flag can be repeated to specify multiple groups.      --cache-dir string               Default HTTP cache directory (default <span class="hljs-string">&quot;/Users/mritd/.kube/http-cache&quot;</span>)      --certificate-authority string   Path to a cert file <span class="hljs-keyword">for</span> the certificate authority      --client-certificate string      Path to a client certificate file <span class="hljs-keyword">for</span> TLS      --client-key string              Path to a client key file <span class="hljs-keyword">for</span> TLS      --cluster string                 The name of the kubeconfig cluster to use      --context string                 The name of the kubeconfig context to use  -h, --<span class="hljs-built_in">help</span>                           <span class="hljs-built_in">help</span> <span class="hljs-keyword">for</span> ns      --insecure-skip-tls-verify       If <span class="hljs-literal">true</span>, the server<span class="hljs-string">&#x27;s certificate will not be checked for validity. This will make your HTTPS connections insecure</span><span class="hljs-string">      --kubeconfig string              Path to the kubeconfig file to use for CLI requests.</span><span class="hljs-string">      --list                           if true, print the list of all namespaces in the current KUBECONFIG</span><span class="hljs-string">  -n, --namespace string               If present, the namespace scope for this CLI request</span><span class="hljs-string">      --request-timeout string         The length of time to wait before giving up on a single server request. Non-zero values should contain a corresponding time unit (e.g. 1s, 2m, 3h). A value of zero means don&#x27;</span>t timeout requests. (default <span class="hljs-string">&quot;0&quot;</span>)  -s, --server string                  The address and port of the Kubernetes API server      --token string                   Bearer token <span class="hljs-keyword">for</span> authentication to the API server      --user string                    The name of the kubeconfig user to use</code></pre></div><p>限于篇幅原因，具体这个 <code>cli-runtime</code> 包怎么用请自行参考官方写的这个 <code>sample-cli-plugin</code> (其实并不怎么 “simple”…)</p><p>本文参考文档:</p><ul><li><a href="https://kubernetes.io/docs/tasks/extend-kubectl/kubectl-plugins/">Extend kubectl with plugins</a></li><li><a href="https://github.com/kubernetes/cli-runtime">cli-runtime</a></li><li><a href="https://github.com/kubernetes/sample-cli-plugin">sample-cli-plugin</a></li></ul>]]></content>
    
    
    <summary type="html">在很久以前的版本研究过 kubernetes 的插件机制，当时弄了一个快速切换 `namespace` 的小插件；最近把自己本机的 kubectl 升级到了 1.12，突然发现插件不能用了；撸了一下文档发现插件机制彻底改了...</summary>
    
    
    
    <category term="Kubernetes" scheme="https://mritd.com/categories/kubernetes/"/>
    
    
    <category term="Kubernetes" scheme="https://mritd.com/tags/kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>Go 编写的一些常用小工具</title>
    <link href="https://mritd.com/2018/11/27/simple-tool-written-in-golang/"/>
    <id>https://mritd.com/2018/11/27/simple-tool-written-in-golang/</id>
    <published>2018-11-27T04:45:46.000Z</published>
    <updated>2018-11-27T04:45:46.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>迫于 Github 上 Star 的项目有点多，今天整理一下一些有意思的 Go 编写的小工具；大多数为终端下的实用工具，装逼的比如天气预报啥的就不写了</p></blockquote><h3 id="syncthing"><a href="#syncthing" class="headerlink" title="syncthing"></a>syncthing</h3><p>强大的文件同步工具，构建私人同步盘 👉 <a href="https://github.com/syncthing%E3%80%81syncthing">Github</a></p><p><img src="https://cdn.oss.link/markdown/er3tj.jpg" alt="syncthing"></p><h3 id="fzf"><a href="#fzf" class="headerlink" title="fzf"></a>fzf</h3><p>一个强大的终端文件浏览器 👉 <a href="https://github.com/junegunn/fzf">Github</a></p><p><img src="https://cdn.oss.link/markdown/ihhqy.jpg" alt="fzf"></p><h3 id="hey"><a href="#hey" class="headerlink" title="hey"></a>hey</h3><p>http 负载测试工具，简单好用 👉 <a href="https://github.com/rakyll/hey">Github</a></p><div class="hljs code-wrapper"><pre><code class="hljs sh">Usage: hey [options...] &lt;url&gt;Options:  -n  Number of requests to run. Default is 200.  -c  Number of requests to run concurrently. Total number of requests cannot      be smaller than the concurrency level. Default is 50.  -q  Rate <span class="hljs-built_in">limit</span>, <span class="hljs-keyword">in</span> queries per second (QPS). Default is no rate <span class="hljs-built_in">limit</span>.  -z  Duration of application to send requests. When duration is reached,      application stops and exits. If duration is specified, n is ignored.      Examples: -z 10s -z 3m.  -o  Output <span class="hljs-built_in">type</span>. If none provided, a summary is printed.      <span class="hljs-string">&quot;csv&quot;</span> is the only supported alternative. Dumps the response      metrics <span class="hljs-keyword">in</span> comma-separated values format.  -m  HTTP method, one of GET, POST, PUT, DELETE, HEAD, OPTIONS.  -H  Custom HTTP header. You can specify as many as needed by repeating the flag.      For example, -H <span class="hljs-string">&quot;Accept: text/html&quot;</span> -H <span class="hljs-string">&quot;Content-Type: application/xml&quot;</span> .  -t  Timeout <span class="hljs-keyword">for</span> each request <span class="hljs-keyword">in</span> seconds. Default is 20, use 0 <span class="hljs-keyword">for</span> infinite.  -A  HTTP Accept header.  -d  HTTP request body.  -D  HTTP request body from file. For example, /home/user/file.txt or ./file.txt.  -T  Content-type, defaults to <span class="hljs-string">&quot;text/html&quot;</span>.  -a  Basic authentication, username:password.  -x  HTTP Proxy address as host:port.  -h2 Enable HTTP/2.  -host    HTTP Host header.  -disable-compression  Disable compression.  -disable-keepalive    Disable keep-alive, prevents re-use of TCP                        connections between different HTTP requests.  -disable-redirects    Disable following of HTTP redirects  -cpus                 Number of used cpu cores.                        (default <span class="hljs-keyword">for</span> current machine is 8 cores)</code></pre></div><h3 id="vegeta"><a href="#vegeta" class="headerlink" title="vegeta"></a>vegeta</h3><p>http 负载测试工具，功能强大 👉 <a href="https://github.com/tsenart/vegeta">Github</a></p><div class="hljs code-wrapper"><pre><code class="hljs sh">Usage: vegeta [global flags] &lt;<span class="hljs-built_in">command</span>&gt; [<span class="hljs-built_in">command</span> flags]global flags:  -cpus int        Number of CPUs to use (default 8)  -profile string        Enable profiling of [cpu, heap]  -version        Print version and <span class="hljs-built_in">exit</span>attack <span class="hljs-built_in">command</span>:  -body string        Requests body file  -cert string        TLS client PEM encoded certificate file  -connections int        Max open idle connections per target host (default 10000)  -duration duration        Duration of the <span class="hljs-built_in">test</span> [0 = forever]  -format string        Targets format [http, json] (default <span class="hljs-string">&quot;http&quot;</span>)  -h2c        Send HTTP/2 requests without TLS encryption  -header value        Request header  -http2        Send HTTP/2 requests when supported by the server (default <span class="hljs-literal">true</span>)  -insecure        Ignore invalid server TLS certificates  -keepalive        Use persistent connections (default <span class="hljs-literal">true</span>)  -key string        TLS client PEM encoded private key file  -laddr value        Local IP address (default 0.0.0.0)  -lazy        Read targets lazily  -max-body value        Maximum number of bytes to capture from response bodies. [-1 = no <span class="hljs-built_in">limit</span>] (default -1)  -name string        Attack name  -output string        Output file (default <span class="hljs-string">&quot;stdout&quot;</span>)  -rate value        Number of requests per time unit (default 50/1s)  -redirects int        Number of redirects to follow. -1 will not follow but marks as success (default 10)  -resolvers value        List of addresses (ip:port) to use <span class="hljs-keyword">for</span> DNS resolution. Disables use of <span class="hljs-built_in">local</span> system DNS. (comma separated list)  -root-certs value        TLS root certificate files (comma separated list)  -targets string        Targets file (default <span class="hljs-string">&quot;stdin&quot;</span>)  -timeout duration        Requests timeout (default 30s)  -workers uint        Initial number of workers (default 10)encode <span class="hljs-built_in">command</span>:  -output string        Output file (default <span class="hljs-string">&quot;stdout&quot;</span>)  -to string        Output encoding [csv, gob, json] (default <span class="hljs-string">&quot;json&quot;</span>)plot <span class="hljs-built_in">command</span>:  -output string        Output file (default <span class="hljs-string">&quot;stdout&quot;</span>)  -threshold int        Threshold of data points above <span class="hljs-built_in">which</span> series are downsampled. (default 4000)  -title string        Title and header of the resulting HTML page (default <span class="hljs-string">&quot;Vegeta Plot&quot;</span>)report <span class="hljs-built_in">command</span>:  -every duration        Report interval  -output string        Output file (default <span class="hljs-string">&quot;stdout&quot;</span>)  -<span class="hljs-built_in">type</span> string        Report <span class="hljs-built_in">type</span> to generate [text, json, hist[buckets]] (default <span class="hljs-string">&quot;text&quot;</span>)examples:  <span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;GET http://localhost/&quot;</span> | vegeta attack -duration=5s | tee results.bin | vegeta report  vegeta report -<span class="hljs-built_in">type</span>=json results.bin &gt; metrics.json  cat results.bin | vegeta plot &gt; plot.html  cat results.bin | vegeta report -<span class="hljs-built_in">type</span>=<span class="hljs-string">&quot;hist[0,100ms,200ms,300ms]&quot;</span></code></pre></div><h3 id="dive"><a href="#dive" class="headerlink" title="dive"></a>dive</h3><p>功能强大的 Docker 镜像分析工具，可以查看每层镜像的具体差异等 👉 <a href="https://github.com/wagoodman/dive">Github</a></p><p><img src="https://cdn.oss.link/markdown/ik3ng.gif" alt="dive"></p><h3 id="ctop"><a href="#ctop" class="headerlink" title="ctop"></a>ctop</h3><p>容器运行时资源分析，如 CPU、内存消耗等 👉 <a href="https://github.com/bcicen/ctop">Github</a></p><p><img src="https://cdn.oss.link/markdown/mr3x3.gif" alt="ctop"></p><h3 id="container-diff"><a href="#container-diff" class="headerlink" title="container-diff"></a>container-diff</h3><p>Google 推出的工具，功能就顾名思义了 👉 <a href="https://github.com/GoogleContainerTools/container-diff">Github</a></p><p><img src="https://cdn.oss.link/markdown/dtapx.png" alt="container-diff"></p><h3 id="transfer-sh"><a href="#transfer-sh" class="headerlink" title="transfer.sh"></a>transfer.sh</h3><p>快捷的终端文件分享工具 👉 <a href="https://github.com/dutchcoders/transfer.sh">Github</a></p><p><img src="https://cdn.oss.link/markdown/76vh0.png" alt="transfer.sh"></p><h3 id="vuls"><a href="#vuls" class="headerlink" title="vuls"></a>vuls</h3><p> Linux/FreeBSD 漏洞扫描工具 👉 <a href="https://github.com/future-architect/vuls">Github</a></p><p> <img src="https://cdn.oss.link/markdown/bpsps.jpg" alt="vuls"></p><h3 id="restic"><a href="#restic" class="headerlink" title="restic"></a>restic</h3><p>高性能安全的文件备份工具 👉 <a href="https://github.com/restic/restic">Github</a></p><p><img src="https://cdn.oss.link/markdown/g51z4.png" alt="restic"></p><h3 id="gitql"><a href="#gitql" class="headerlink" title="gitql"></a>gitql</h3><p>使用 sql 的方式查询 git 提交 👉 <a href="https://github.com/cloudson/gitql">Github</a></p><p><img src="https://cdn.oss.link/markdown/4h095.gif" alt="gitql"></p><h3 id="gitflow-toolkit"><a href="#gitflow-toolkit" class="headerlink" title="gitflow-toolkit"></a>gitflow-toolkit</h3><p>帮助生成满足 Gitflow 格式 commit message 的小工具(自己写的) 👉 <a href="https://github.com/mritd/gitflow-toolkit">Github</a></p><p><img src="https://cdn.oss.link/markdown/1e2v1.gif" alt="gitflow-toolkit"></p><h3 id="git-chglog"><a href="#git-chglog" class="headerlink" title="git-chglog"></a>git-chglog</h3><p>对主流的 Gitflow 格式的 commit message 生成 CHANGELOG 👉 <a href="https://github.com/git-chglog/git-chglog">Github</a></p><p><img src="https://cdn.oss.link/markdown/zphxd.gif" alt="git-chglog"></p><h3 id="grv"><a href="#grv" class="headerlink" title="grv"></a>grv</h3><p>一个 git 终端图形化浏览工具 👉 <a href="https://github.com/rgburke/grv">Github</a></p><p><img src="https://cdn.oss.link/markdown/k1vh2.jpg" alt="grv"></p><h3 id="jid"><a href="#jid" class="headerlink" title="jid"></a>jid</h3><p>命令行 json 格式化处理工具，类似 jq，不过感觉更加强大 👉 <a href="https://github.com/simeji/jid">Github</a></p><p><img src="https://cdn.oss.link/markdown/3k4ue.gif" alt="jid"></p><h3 id="annie"><a href="#annie" class="headerlink" title="annie"></a>annie</h3><p>类似 youget 的一个视频下载工具，可以解析大部分视频网站直接下载 👉 <a href="https://github.com/iawia002/annie">Github</a></p><div class="hljs code-wrapper"><pre><code class="hljs sh">$ annie -i https://www.youtube.com/watch?v=dQw4w9WgXcQ Site:      YouTube youtube.com Title:     Rick Astley - Never Gonna Give You Up (Video) Type:      video Streams:   <span class="hljs-comment"># All available quality</span>     [248]  -------------------     Quality:         1080p video/webm; codecs=<span class="hljs-string">&quot;vp9&quot;</span>     Size:            49.29 MiB (51687554 Bytes)     <span class="hljs-comment"># download with: annie -f 248 ...</span>     [137]  -------------------     Quality:         1080p video/mp4; codecs=<span class="hljs-string">&quot;avc1.640028&quot;</span>     Size:            43.45 MiB (45564306 Bytes)     <span class="hljs-comment"># download with: annie -f 137 ...</span>     [398]  -------------------     Quality:         720p video/mp4; codecs=<span class="hljs-string">&quot;av01.0.05M.08&quot;</span>     Size:            37.12 MiB (38926432 Bytes)     <span class="hljs-comment"># download with: annie -f 398 ...</span>     [136]  -------------------     Quality:         720p video/mp4; codecs=<span class="hljs-string">&quot;avc1.4d401f&quot;</span>     Size:            31.34 MiB (32867324 Bytes)     <span class="hljs-comment"># download with: annie -f 136 ...</span>     [247]  -------------------     Quality:         720p video/webm; codecs=<span class="hljs-string">&quot;vp9&quot;</span>     Size:            31.03 MiB (32536181 Bytes)     <span class="hljs-comment"># download with: annie -f 247 ...</span></code></pre></div><h3 id="up"><a href="#up" class="headerlink" title="up"></a>up</h3><p>Linux 下管道式终端搜索工具 👉 <a href="https://github.com/akavel/up">Github</a></p><p><img src="https://cdn.oss.link/markdown/n8zdj.gif" alt="up"></p><h3 id="lego"><a href="#lego" class="headerlink" title="lego"></a>lego</h3><p>Let’s Encrypt 证书申请工具 👉 <a href="https://github.com/xenolf/lego">Github</a></p><div class="hljs code-wrapper"><pre><code class="hljs sh">NAME:   lego - Let<span class="hljs-string">&#x27;s Encrypt client written in Go</span><span class="hljs-string"></span><span class="hljs-string">USAGE:</span><span class="hljs-string">   lego [global options] command [command options] [arguments...]</span><span class="hljs-string"></span><span class="hljs-string">COMMANDS:</span><span class="hljs-string">     run      Register an account, then create and install a certificate</span><span class="hljs-string">     revoke   Revoke a certificate</span><span class="hljs-string">     renew    Renew a certificate</span><span class="hljs-string">     dnshelp  Shows additional help for the --dns global option</span><span class="hljs-string">     help, h  Shows a list of commands or help for one command</span><span class="hljs-string"></span><span class="hljs-string">GLOBAL OPTIONS:</span><span class="hljs-string">   --domains value, -d value   Add a domain to the process. Can be specified multiple times.</span><span class="hljs-string">   --csr value, -c value       Certificate signing request filename, if an external CSR is to be used</span><span class="hljs-string">   --server value, -s value    CA hostname (and optionally :port). The server certificate must be trusted in order to avoid further modifications to the client. (default: &quot;https://acme-v02.api.letsencrypt.org/directory&quot;)</span><span class="hljs-string">   --email value, -m value     Email used for registration and recovery contact.</span><span class="hljs-string">   --filename value            Filename of the generated certificate</span><span class="hljs-string">   --accept-tos, -a            By setting this flag to true you indicate that you accept the current Let&#x27;</span>s Encrypt terms of service.   --eab                       Use External Account Binding <span class="hljs-keyword">for</span> account registration. Requires --kid and --hmac.   --kid value                 Key identifier from External CA. Used <span class="hljs-keyword">for</span> External Account Binding.   --hmac value                MAC key from External CA. Should be <span class="hljs-keyword">in</span> Base64 URL Encoding without padding format. Used <span class="hljs-keyword">for</span> External Account Binding.   --key-type value, -k value  Key <span class="hljs-built_in">type</span> to use <span class="hljs-keyword">for</span> private keys. Supported: rsa2048, rsa4096, rsa8192, ec256, ec384 (default: <span class="hljs-string">&quot;rsa2048&quot;</span>)   --path value                Directory to use <span class="hljs-keyword">for</span> storing the data (default: <span class="hljs-string">&quot;./.lego&quot;</span>)   --exclude value, -x value   Explicitly disallow solvers by name from being used. Solvers: <span class="hljs-string">&quot;http-01&quot;</span>, <span class="hljs-string">&quot;dns-01&quot;</span>, <span class="hljs-string">&quot;tls-alpn-01&quot;</span>.   --webroot value             Set the webroot folder to use <span class="hljs-keyword">for</span> HTTP based challenges to write directly <span class="hljs-keyword">in</span> a file <span class="hljs-keyword">in</span> .well-known/acme-challenge   --memcached-host value      Set the memcached host(s) to use <span class="hljs-keyword">for</span> HTTP based challenges. Challenges will be written to all specified hosts.   --http value                Set the port and interface to use <span class="hljs-keyword">for</span> HTTP based challenges to listen on. Supported: interface:port or :port   --tls value                 Set the port and interface to use <span class="hljs-keyword">for</span> TLS based challenges to listen on. Supported: interface:port or :port   --dns value                 Solve a DNS challenge using the specified provider. Disables all other challenges. Run <span class="hljs-string">&#x27;lego dnshelp&#x27;</span> <span class="hljs-keyword">for</span> <span class="hljs-built_in">help</span> on usage.   --http-timeout value        Set the HTTP timeout value to a specific value <span class="hljs-keyword">in</span> seconds. The default is 10 seconds. (default: 0)   --dns-timeout value         Set the DNS timeout value to a specific value <span class="hljs-keyword">in</span> seconds. The default is 10 seconds. (default: 0)   --dns-resolvers value       Set the resolvers to use <span class="hljs-keyword">for</span> performing recursive DNS queries. Supported: host:port. The default is to use the system resolvers, or Google<span class="hljs-string">&#x27;s DNS resolvers if the system&#x27;</span>s cannot be determined.   --pem                       Generate a .pem file by concatenating the .key and .crt files together.   --<span class="hljs-built_in">help</span>, -h                  show <span class="hljs-built_in">help</span>   --version, -v               <span class="hljs-built_in">print</span> the version</code></pre></div><h3 id="noti"><a href="#noti" class="headerlink" title="noti"></a>noti</h3><p>贼好用的终端命令异步执行通知工具 👉 <a href="https://github.com/variadico/noti">Github</a></p><p><img src="https://cdn.oss.link/markdown/m2r1e.jpg" alt="noti"></p><h3 id="gosu"><a href="#gosu" class="headerlink" title="gosu"></a>gosu</h3><p>临时切换到指定用户运行特定命令，方便测试权限问题 👉 <a href="https://github.com/tianon/gosu">Github</a></p><div class="hljs code-wrapper"><pre><code class="hljs sh">$ gosuUsage: ./gosu user-spec <span class="hljs-built_in">command</span> [args]   eg: ./gosu tianon bash       ./gosu nobody:root bash -c <span class="hljs-string">&#x27;whoami &amp;&amp; id&#x27;</span>       ./gosu 1000:1 id</code></pre></div><h3 id="sup"><a href="#sup" class="headerlink" title="sup"></a>sup</h3><p>类似 Ansible 的一个批量执行工具，暂且称之为低配版 Ansible 👉 <a href="https://github.com/pressly/sup">Github</a></p><p><img src="https://cdn.oss.link/markdown/x0eaz.gif" alt="sup"></p><h3 id="aptly"><a href="#aptly" class="headerlink" title="aptly"></a>aptly</h3><p>Debian 仓库管理工具 👉 <a href="https://github.com/aptly-dev/aptly">Github</a></p><p><img src="https://cdn.oss.link/markdown/8e0ml.jpg" alt="aptly"></p><h3 id="mmh"><a href="#mmh" class="headerlink" title="mmh"></a>mmh</h3><p>支持无限跳板机登录的 ssh 小工具(自己写的) 👉 <a href="https://github.com/mritd/mmh">Github</a></p><p><img src="https://cdn.oss.link/markdown/37638.gif" alt="mmh"></p>]]></content>
    
    
    <summary type="html">迫于 Github 上 Star 的项目有点多，今天整理一下一些有意思的 Go 编写的小工具；大多数为终端下的实用工具，装逼的比如天气预报啥的就不写了</summary>
    
    
    
    <category term="Golang" scheme="https://mritd.com/categories/golang/"/>
    
    
    <category term="Golang" scheme="https://mritd.com/tags/golang/"/>
    
  </entry>
  
  <entry>
    <title>远程 Debug kubeadm</title>
    <link href="https://mritd.com/2018/11/25/kubeadm-remote-debug/"/>
    <id>https://mritd.com/2018/11/25/kubeadm-remote-debug/</id>
    <published>2018-11-25T03:11:28.000Z</published>
    <updated>2018-11-25T03:11:28.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>最近在看 kubeadm 的源码，不过有些东西光看代码还是没法太清楚，还是需要实际运行才能看到具体代码怎么跑的，还得打断点 debug；无奈的是本机是 mac，debug 得在 Linux 下，so 研究了一下 remote debug</p></blockquote><h2 id="一、环境准备"><a href="#一、环境准备" class="headerlink" title="一、环境准备"></a>一、环境准备</h2><ul><li>GoLand 2018.2.4</li><li>Golang 1.11.2</li><li>delve v1.1.0</li><li>Kubernetest master</li><li>Ubuntu 18.04</li><li>能够高速访问外网(自行理解)</li></ul><p><strong>这里不会详细写如何安装 Go 开发环境以及 GoLand 安装，本文默认读者已经至少已经对 Go 开发环境以及代码有一定了解；顺便提一下 GoLand，这玩意属于 jetbrains 系列 IDE，在大约 2018.1 版本后在线激活服务器已经全部失效，不过网上还有其他本地离线激活工具，具体请自行 Google，如果后续工资能支撑得起，请补票支持正版(感恩节全家桶半价真香😂)</strong></p><h3 id="1-1、获取源码"><a href="#1-1、获取源码" class="headerlink" title="1.1、获取源码"></a>1.1、获取源码</h3><p>需要注意的是 Kubernetes 源码虽然托管在 Github，但是在使用 <code>go get</code> 的时候要使用 <code>k8s.io</code> 域名</p><div class="hljs code-wrapper"><pre><code class="hljs sh">go get -d k8s.io/kubernetes</code></pre></div><p><code>go get</code> 命令是接受标准的 http 代理的，这个源码下载会非常慢，源码大约 1G 左右，所以最好使用加速工具下载</p><div class="hljs code-wrapper"><pre><code class="hljs sh">➜  ~ <span class="hljs-built_in">which</span> proxy/usr/<span class="hljs-built_in">local</span>/bin/proxy➜  ~ cat /usr/<span class="hljs-built_in">local</span>/bin/proxy<span class="hljs-meta">#!/bin/bash</span>http_proxy=http://127.0.0.1:8123 https_proxy=http://127.0.0.1:8123 $*➜  ~ proxy go get -d k8s.io/kubernetes</code></pre></div><h3 id="1-2、安装-delve"><a href="#1-2、安装-delve" class="headerlink" title="1.2、安装 delve"></a>1.2、安装 delve</h3><p>delve 是一个 Golang 的 debug 工具，有点类似 gdb，不过是专门针对 Golang 的，GoLand 的 debug 实际上就是使用的这个开源工具；为了进行远程 debug，运行 kubeadm 的机器必须安装 delve，从而进行远程连接</p><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment"># 同样这里省略在 Linux 安装 go 环境操作</span>go get -u github.com/derekparker/delve/cmd/dlv</code></pre></div><h2 id="二、远程-Debug"><a href="#二、远程-Debug" class="headerlink" title="二、远程 Debug"></a>二、远程 Debug</h2><h3 id="2-1、重新编译-kubeadm"><a href="#2-1、重新编译-kubeadm" class="headerlink" title="2.1、重新编译 kubeadm"></a>2.1、重新编译 kubeadm</h3><p>默认情况下直接编译出的 kubeadm 是无法进行 debug 的，因为 Golang 的编译器会进行编译优化，比如进行内联等；所以要关闭编译优化和内联，方便 debug</p><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-built_in">cd</span> <span class="hljs-variable">$&#123;GOPATH&#125;</span>/src/k8s.io/kubernetes/cmd/kubeadmGOOS=<span class="hljs-string">&quot;linux&quot;</span> GOARCH=<span class="hljs-string">&quot;amd64&quot;</span> go build -gcflags <span class="hljs-string">&quot;all=-N -l&quot;</span></code></pre></div><h3 id="2-2、远程运行-kubeadm"><a href="#2-2、远程运行-kubeadm" class="headerlink" title="2.2、远程运行 kubeadm"></a>2.2、远程运行 kubeadm</h3><p>将编译好的 kubeadm 复制到远程，并且使用 delve 启动它，此时 delve 会监听 api 端口，GoLand 就可以远程连接过来了</p><div class="hljs code-wrapper"><pre><code class="hljs sh">dlv --listen=192.168.1.61:2345 --headless=<span class="hljs-literal">true</span> --api-version=2 <span class="hljs-built_in">exec</span> ./kubeadm init</code></pre></div><p><strong>注意: 要指定需要 debug 的 kubeadm 的子命令，否则可能出现连接上以后 GoLand 无反应的情况</strong></p><h3 id="2-3、运行-GoLand"><a href="#2-3、运行-GoLand" class="headerlink" title="2.3、运行 GoLand"></a>2.3、运行 GoLand</h3><p>在 GoLand 中打开 kubernetes 源码，在需要 debug 的代码中打上断点，这里以 init 子命令为例</p><p>首先新建一个远程 debug configuration</p><p><img src="https://cdn.oss.link/markdown/i6oed.png" alt="create configuration"></p><p>名字可以随便写，主要是地址和端口</p><p><img src="https://cdn.oss.link/markdown/rmczj.png" alt="conifg delve"></p><p>接下来在目标源码位置打断点，以下为 init 子命令的源码位置</p><p><img src="https://cdn.oss.link/markdown/ylf97.png" alt="create breakpoint"></p><p>最后只需要点击 debug 按钮即可</p><p><img src="https://cdn.oss.link/markdown/ns2yw.png" alt="debug"></p><p><strong>在没有运行 GoLand debug 之前，目标机器的实际指令是不会运行的，也就是说在 GoLand 没有连接到远程 delve 启动的 <code>kubeadm init</code> 命令之前，<code>kubeadm init</code> 并不会真正运行；当点击 GoLand 的终止 debug 按钮后，远程的 delve 也会随之退出</strong></p><p><img src="https://cdn.oss.link/markdown/lmdke.png" alt="stop"></p>]]></content>
    
    
    <summary type="html">最近在看 kubeadm 的源码，不过有些东西光看代码还是没法太清楚，还是需要实际运行才能看到具体代码怎么跑的，还得打断点 debug；无奈的是本机是 mac，debug 得在 Linux 下，so 研究了一下 remote debug</summary>
    
    
    
    <category term="Kubernetes" scheme="https://mritd.com/categories/kubernetes/"/>
    
    <category term="Golang" scheme="https://mritd.com/categories/kubernetes/golang/"/>
    
    
    <category term="Kubernetes" scheme="https://mritd.com/tags/kubernetes/"/>
    
    <category term="Golang" scheme="https://mritd.com/tags/golang/"/>
    
  </entry>
  
  <entry>
    <title>Mac: Extract JDK to folder, without running installer</title>
    <link href="https://mritd.com/2018/11/23/extract-jdk-to-folder-on-mac/"/>
    <id>https://mritd.com/2018/11/23/extract-jdk-to-folder-on-mac/</id>
    <published>2018-11-23T04:33:20.000Z</published>
    <updated>2018-11-23T04:33:20.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>重装了 mac 系统，由于一些公司项目必须使用 Oracle JDK(验证码等组件用了一些 Oracle 独有的 API) 所以又得重新安装；但是 Oracle 只提供了 pkg 的安装方式，研究半天找到了一个解包 pkg 的安装方式，这里记录一下</p></blockquote><p>不使用 pkg 的原因是每次更新版本都要各种安装，最烦人的是 IDEA 选择 JDK 时候弹出的文件浏览器没法进入到这种方式安装的 JDK 的系统目录…mmp，后来从国外网站找到了一篇文章，基本套路如下</p><ul><li>下载 Oracle JDK，从 dmg 中拷贝 pkg 到任意位置</li><li>解压 pkg 到任意位置 <code>pkgutil --expand your_jdk.pkg jdkdir</code></li><li>进入到目录中，解压主文件 <code>cd jdkdir/jdk_version.pkg &amp;&amp; cpio -idv &lt; Payload</code></li><li>移动 jdk 到任意位置 <code>mv Contents/Home ~/myjdk</code></li></ul><p>原文地址: <a href="https://augustl.com/blog/2014/extracting_java_to_folder_no_installer_osx/">OS X: Extract JDK to folder, without running installer</a></p>]]></content>
    
    
    <summary type="html">重装了 mac 系统，由于一些公司项目必须使用 Oracle JDK(验证码等组件用了一些 Oracle 独有的 API) 所以又得重新安装；但是 Oracle 只提供了 pkg 的安装方式，研究半天找到了一个解包 pkg 的安装方式，这里记录一下</summary>
    
    
    
    <category term="Java" scheme="https://mritd.com/categories/java/"/>
    
    
    <category term="Java" scheme="https://mritd.com/tags/java/"/>
    
  </entry>
  
  <entry>
    <title>Go ssh 交互式执行命令</title>
    <link href="https://mritd.com/2018/11/09/go-interactive-shell/"/>
    <id>https://mritd.com/2018/11/09/go-interactive-shell/</id>
    <published>2018-11-09T15:13:44.000Z</published>
    <updated>2018-11-09T15:13:44.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>最近在写一个跳板机登录的小工具，其中涉及到了用 Go 来进行交互式执行命令，简单地说就是弄个终端出来；一开始随便 Google 了一下，copy 下来基本上就是能跑了…但是后来发现了一些各种各样的小问题，强迫症的我实在受不了，最后翻了一下 Teleport 的源码，从中学到了不少有用的知识，这里记录一下</p></blockquote><h2 id="一、原始版本"><a href="#一、原始版本" class="headerlink" title="一、原始版本"></a>一、原始版本</h2><blockquote><p>不想看太多可以直接跳转到 <a href="#%E4%B8%89%E5%AE%8C%E6%95%B4%E4%BB%A3%E7%A0%81">第三部分</a> 拿代码</p></blockquote><h3 id="1-1、样例代码"><a href="#1-1、样例代码" class="headerlink" title="1.1、样例代码"></a>1.1、样例代码</h3><p>一开始随便 Google 出来的代码，copy 上就直接跑；代码基本如下:</p><div class="hljs code-wrapper"><pre><code class="hljs golang"><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-title">main</span><span class="hljs-params">()</span></span> &#123;<span class="hljs-comment">// 创建 ssh 配置</span>sshConfig := &amp;ssh.ClientConfig&#123;User: <span class="hljs-string">&quot;root&quot;</span>,Auth: []ssh.AuthMethod&#123;ssh.Password(<span class="hljs-string">&quot;password&quot;</span>),&#125;,HostKeyCallback: ssh.InsecureIgnoreHostKey(),Timeout:         <span class="hljs-number">5</span> * time.Second,&#125;<span class="hljs-comment">// 创建 client</span>client, err := ssh.Dial(<span class="hljs-string">&quot;tcp&quot;</span>, <span class="hljs-string">&quot;192.168.1.20:22&quot;</span>, sshConfig)checkErr(err)<span class="hljs-keyword">defer</span> client.Close()<span class="hljs-comment">// 获取 session</span>session, err := client.NewSession()checkErr(err)<span class="hljs-keyword">defer</span> session.Close()<span class="hljs-comment">// 拿到当前终端文件描述符</span>fd := <span class="hljs-keyword">int</span>(os.Stdin.Fd())termWidth, termHeight, err := terminal.GetSize(fd)<span class="hljs-comment">// request pty</span>err = session.RequestPty(<span class="hljs-string">&quot;xterm-256color&quot;</span>, termHeight, termWidth, ssh.TerminalModes&#123;&#125;)checkErr(err)<span class="hljs-comment">// 对接 std</span>session.Stdout = os.Stdoutsession.Stderr = os.Stderrsession.Stdin = os.Stdinerr = session.Shell()checkErr(err)err = session.Wait()checkErr(err)&#125;<span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-title">checkErr</span><span class="hljs-params">(err error)</span></span> &#123;<span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;fmt.Println(err)os.Exit(<span class="hljs-number">1</span>)&#125;&#125;</code></pre></div><h3 id="1-2、遇到的问题"><a href="#1-2、遇到的问题" class="headerlink" title="1.2、遇到的问题"></a>1.2、遇到的问题</h3><p>以上代码跑起来后，基本上遇到了以下问题:</p><ul><li>执行命令有回显，表现为敲一个 <code>ls</code> 出现两行</li><li>本地终端大小调整，远端完全无反应，导致显示不全</li><li>Tmux 下终端连接后窗口标题显示的是原始命令，而不是目标机器 shell 环境的目录位置</li><li>首次连接一些刚装完系统的机器可能出现执行命令后回显不换行</li></ul><h2 id="二、改进代码"><a href="#二、改进代码" class="headerlink" title="二、改进代码"></a>二、改进代码</h2><h3 id="2-1、回显问题"><a href="#2-1、回显问题" class="headerlink" title="2.1、回显问题"></a>2.1、回显问题</h3><p>关于回显问题，实际上解决方案很简单，设置当前终端进入 <code>raw</code> 模式即可；代码如下:</p><div class="hljs code-wrapper"><pre><code class="hljs golang"><span class="hljs-comment">// 拿到当前终端文件描述符</span>fd := <span class="hljs-keyword">int</span>(os.Stdin.Fd())<span class="hljs-comment">// make raw</span>state, err := terminal.MakeRaw(fd)checkErr(err)<span class="hljs-keyword">defer</span> terminal.Restore(fd, state)</code></pre></div><p>代码很简单，网上一大堆，But…基本没有文章详细说这个 <code>raw</code> 模式到底是个啥玩意；好在万能的 StackOverflow 对于不熟悉 Linux 的人给出了一个很清晰的解释: <a href="https://unix.stackexchange.com/questions/21752/what-s-the-difference-between-a-raw-and-a-cooked-device-driver">What’s the difference between a “raw” and a “cooked” device driver?</a></p><p>大致意思就是说 <strong>在终端处于 <code>Cooked</code> 模式时，当你输入一些字符后，默认是被当前终端 cache 住的，在你敲了回车之前这些文本都在 cache 中，这样允许应用程序做一些处理，比如捕获 <code>Cntl-D</code> 等按键，这时候就会出现敲回车后本地终端帮你打印了一下，导致出现类似回显的效果；当设置终端为 <code>raw</code> 模式后，所有的输入将不被 cache，而是发送到应用程序，在我们的代码中表现为通过 <code>io.Copy</code> 直接发送到了远端 shell 程序</strong></p><h3 id="2-2、终端大小问题"><a href="#2-2、终端大小问题" class="headerlink" title="2.2、终端大小问题"></a>2.2、终端大小问题</h3><p>当本地调整了终端大小后，远程终端毫无反应；后来发现在 <code>*ssh.Session</code> 上有一个 <code>WindowChange</code> 方法，用于向远端发送窗口调整事件；解决方案就是启动一个 <code>goroutine</code> 在后台不断监听窗口改变事件，然后调用 <code>WindowChange</code> 即可；代码如下:</p><div class="hljs code-wrapper"><pre><code class="hljs golang"><span class="hljs-keyword">go</span> <span class="hljs-function"><span class="hljs-keyword">func</span><span class="hljs-params">()</span></span> &#123;<span class="hljs-comment">// 监听窗口变更事件</span>sigwinchCh := <span class="hljs-built_in">make</span>(<span class="hljs-keyword">chan</span> os.Signal, <span class="hljs-number">1</span>)signal.Notify(sigwinchCh, syscall.SIGWINCH)fd := <span class="hljs-keyword">int</span>(os.Stdin.Fd())termWidth, termHeight, err := terminal.GetSize(fd)<span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;fmt.Println(err)&#125;<span class="hljs-keyword">for</span> &#123;<span class="hljs-keyword">select</span> &#123;<span class="hljs-comment">// 阻塞读取</span><span class="hljs-keyword">case</span> sigwinch := &lt;-sigwinchCh:<span class="hljs-keyword">if</span> sigwinch == <span class="hljs-literal">nil</span> &#123;<span class="hljs-keyword">return</span>&#125;currTermWidth, currTermHeight, err := terminal.GetSize(fd)<span class="hljs-comment">// 判断一下窗口尺寸是否有改变</span><span class="hljs-keyword">if</span> currTermHeight == termHeight &amp;&amp; currTermWidth == termWidth &#123;<span class="hljs-keyword">continue</span>&#125;<span class="hljs-comment">// 更新远端大小</span>session.WindowChange(currTermHeight, currTermWidth)<span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;fmt.Printf(<span class="hljs-string">&quot;Unable to send window-change reqest: %s.&quot;</span>, err)<span class="hljs-keyword">continue</span>&#125;termWidth, termHeight = currTermWidth, currTermHeight&#125;&#125;&#125;()</code></pre></div><h3 id="2-3、Tmux-标题以及回显不换行"><a href="#2-3、Tmux-标题以及回显不换行" class="headerlink" title="2.3、Tmux 标题以及回显不换行"></a>2.3、Tmux 标题以及回显不换行</h3><p>这两个问题实际上都是由于我们直接对接了 <code>stderr</code>、<code>stdout</code> 和 <code>stdin</code> 造成的，实际上我们应当启动一个异步的管道式复制行为，并且最好带有 buf 的发送；代码如下:</p><div class="hljs code-wrapper"><pre><code class="hljs golang">stdin, err := session.StdinPipe()checkErr(err)stdout, err := session.StdoutPipe()checkErr(err)stderr, err := session.StderrPipe()checkErr(err)<span class="hljs-keyword">go</span> io.Copy(os.Stderr, stderr)<span class="hljs-keyword">go</span> io.Copy(os.Stdout, stdout)<span class="hljs-keyword">go</span> <span class="hljs-function"><span class="hljs-keyword">func</span><span class="hljs-params">()</span></span> &#123;buf := <span class="hljs-built_in">make</span>([]<span class="hljs-keyword">byte</span>, <span class="hljs-number">128</span>)<span class="hljs-keyword">for</span> &#123;n, err := os.Stdin.Read(buf)<span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;fmt.Println(err)<span class="hljs-keyword">return</span>&#125;<span class="hljs-keyword">if</span> n &gt; <span class="hljs-number">0</span> &#123;_, err = stdin.Write(buf[:n])<span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;checkErr(err)&#125;&#125;&#125;&#125;()</code></pre></div><h2 id="三、完整代码"><a href="#三、完整代码" class="headerlink" title="三、完整代码"></a>三、完整代码</h2><div class="hljs code-wrapper"><pre><code class="hljs golang"><span class="hljs-keyword">type</span> SSHTerminal <span class="hljs-keyword">struct</span> &#123;Session *ssh.SessionexitMsg <span class="hljs-keyword">string</span>stdout  io.Readerstdin   io.Writerstderr  io.Reader&#125;<span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-title">main</span><span class="hljs-params">()</span></span> &#123;sshConfig := &amp;ssh.ClientConfig&#123;User: <span class="hljs-string">&quot;root&quot;</span>,Auth: []ssh.AuthMethod&#123;ssh.Password(<span class="hljs-string">&quot;password&quot;</span>),&#125;,HostKeyCallback: ssh.InsecureIgnoreHostKey(),&#125;client, err := ssh.Dial(<span class="hljs-string">&quot;tcp&quot;</span>, <span class="hljs-string">&quot;192.168.1.20:22&quot;</span>, sshConfig)<span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;fmt.Println(err)&#125;<span class="hljs-keyword">defer</span> client.Close()err = New(client)<span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;fmt.Println(err)&#125;&#125;<span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(t *SSHTerminal)</span> <span class="hljs-title">updateTerminalSize</span><span class="hljs-params">()</span></span> &#123;<span class="hljs-keyword">go</span> <span class="hljs-function"><span class="hljs-keyword">func</span><span class="hljs-params">()</span></span> &#123;<span class="hljs-comment">// SIGWINCH is sent to the process when the window size of the terminal has</span><span class="hljs-comment">// changed.</span>sigwinchCh := <span class="hljs-built_in">make</span>(<span class="hljs-keyword">chan</span> os.Signal, <span class="hljs-number">1</span>)signal.Notify(sigwinchCh, syscall.SIGWINCH)fd := <span class="hljs-keyword">int</span>(os.Stdin.Fd())termWidth, termHeight, err := terminal.GetSize(fd)<span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;fmt.Println(err)&#125;<span class="hljs-keyword">for</span> &#123;<span class="hljs-keyword">select</span> &#123;<span class="hljs-comment">// The client updated the size of the local PTY. This change needs to occur</span><span class="hljs-comment">// on the server side PTY as well.</span><span class="hljs-keyword">case</span> sigwinch := &lt;-sigwinchCh:<span class="hljs-keyword">if</span> sigwinch == <span class="hljs-literal">nil</span> &#123;<span class="hljs-keyword">return</span>&#125;currTermWidth, currTermHeight, err := terminal.GetSize(fd)<span class="hljs-comment">// Terminal size has not changed, don&#x27;t do anything.</span><span class="hljs-keyword">if</span> currTermHeight == termHeight &amp;&amp; currTermWidth == termWidth &#123;<span class="hljs-keyword">continue</span>&#125;t.Session.WindowChange(currTermHeight, currTermWidth)<span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;fmt.Printf(<span class="hljs-string">&quot;Unable to send window-change reqest: %s.&quot;</span>, err)<span class="hljs-keyword">continue</span>&#125;termWidth, termHeight = currTermWidth, currTermHeight&#125;&#125;&#125;()&#125;<span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(t *SSHTerminal)</span> <span class="hljs-title">interactiveSession</span><span class="hljs-params">()</span> <span class="hljs-title">error</span></span> &#123;<span class="hljs-keyword">defer</span> <span class="hljs-function"><span class="hljs-keyword">func</span><span class="hljs-params">()</span></span> &#123;<span class="hljs-keyword">if</span> t.exitMsg == <span class="hljs-string">&quot;&quot;</span> &#123;fmt.Fprintln(os.Stdout, <span class="hljs-string">&quot;the connection was closed on the remote side on &quot;</span>, time.Now().Format(time.RFC822))&#125; <span class="hljs-keyword">else</span> &#123;fmt.Fprintln(os.Stdout, t.exitMsg)&#125;&#125;()fd := <span class="hljs-keyword">int</span>(os.Stdin.Fd())state, err := terminal.MakeRaw(fd)<span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;<span class="hljs-keyword">return</span> err&#125;<span class="hljs-keyword">defer</span> terminal.Restore(fd, state)termWidth, termHeight, err := terminal.GetSize(fd)<span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;<span class="hljs-keyword">return</span> err&#125;termType := os.Getenv(<span class="hljs-string">&quot;TERM&quot;</span>)<span class="hljs-keyword">if</span> termType == <span class="hljs-string">&quot;&quot;</span> &#123;termType = <span class="hljs-string">&quot;xterm-256color&quot;</span>&#125;err = t.Session.RequestPty(termType, termHeight, termWidth, ssh.TerminalModes&#123;&#125;)<span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;<span class="hljs-keyword">return</span> err&#125;t.updateTerminalSize()t.stdin, err = t.Session.StdinPipe()<span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;<span class="hljs-keyword">return</span> err&#125;t.stdout, err = t.Session.StdoutPipe()<span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;<span class="hljs-keyword">return</span> err&#125;t.stderr, err = t.Session.StderrPipe()<span class="hljs-keyword">go</span> io.Copy(os.Stderr, t.stderr)<span class="hljs-keyword">go</span> io.Copy(os.Stdout, t.stdout)<span class="hljs-keyword">go</span> <span class="hljs-function"><span class="hljs-keyword">func</span><span class="hljs-params">()</span></span> &#123;buf := <span class="hljs-built_in">make</span>([]<span class="hljs-keyword">byte</span>, <span class="hljs-number">128</span>)<span class="hljs-keyword">for</span> &#123;n, err := os.Stdin.Read(buf)<span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;fmt.Println(err)<span class="hljs-keyword">return</span>&#125;<span class="hljs-keyword">if</span> n &gt; <span class="hljs-number">0</span> &#123;_, err = t.stdin.Write(buf[:n])<span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;fmt.Println(err)t.exitMsg = err.Error()<span class="hljs-keyword">return</span>&#125;&#125;&#125;&#125;()err = t.Session.Shell()<span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;<span class="hljs-keyword">return</span> err&#125;err = t.Session.Wait()<span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;<span class="hljs-keyword">return</span> err&#125;<span class="hljs-keyword">return</span> <span class="hljs-literal">nil</span>&#125;<span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-title">New</span><span class="hljs-params">(client *ssh.Client)</span> <span class="hljs-title">error</span></span> &#123;session, err := client.NewSession()<span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;<span class="hljs-keyword">return</span> err&#125;<span class="hljs-keyword">defer</span> session.Close()s := SSHTerminal&#123;Session: session,&#125;<span class="hljs-keyword">return</span> s.interactiveSession()&#125;</code></pre></div>]]></content>
    
    
    <summary type="html">最近在写一个跳板机登录的小工具，其中涉及到了用 Go 来进行交互式执行命令，简单地说就是弄个终端出来；一开始随便 Google 了一下，copy 下来基本上就是能跑了...但是后来发现了一些各种各样的小问题，强迫症的我实在受不了，最后翻了一下 Teleport 的源码，从中学到了不少有用的知识，这里记录一下</summary>
    
    
    
    <category term="Golang" scheme="https://mritd.com/categories/golang/"/>
    
    
    <category term="Golang" scheme="https://mritd.com/tags/golang/"/>
    
  </entry>
  
  <entry>
    <title>Go 代码的扩展套路</title>
    <link href="https://mritd.com/2018/10/23/golang-code-plugin/"/>
    <id>https://mritd.com/2018/10/23/golang-code-plugin/</id>
    <published>2018-10-23T13:32:13.000Z</published>
    <updated>2018-10-23T13:32:13.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>折腾 Go 已经有一段时间了，最近在用 Go 写点 web 的东西；在搭建脚手架的过程中总是有点不适应，尤其对可扩展性上总是感觉没有 Java 那么顺手；索性看了下 coredns 的源码，最后追踪到 caddy 源码；突然发现他们对代码内的 plugin 机制有一些骚套路，这里索性记录一下</p></blockquote><h3 id="一、问题由来"><a href="#一、问题由来" class="headerlink" title="一、问题由来"></a>一、问题由来</h3><p>纵观现在所有的 Go web 框架，在文档上可以看到使用方式很简明；非常符合我对 Go 的一贯感受: “所写即所得”；就拿 Gin 这个来说，在 README.md 上可以很轻松的看到 <code>engine</code> 或者说 <code>router</code> 这玩意的使用，比如下面这样:</p><div class="hljs code-wrapper"><pre><code class="hljs golang"><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-title">main</span><span class="hljs-params">()</span></span> &#123;<span class="hljs-comment">// Disable Console Color</span><span class="hljs-comment">// gin.DisableConsoleColor()</span><span class="hljs-comment">// Creates a gin router with default middleware:</span><span class="hljs-comment">// logger and recovery (crash-free) middleware</span>router := gin.Default()router.GET(<span class="hljs-string">&quot;/someGet&quot;</span>, getting)router.POST(<span class="hljs-string">&quot;/somePost&quot;</span>, posting)router.PUT(<span class="hljs-string">&quot;/somePut&quot;</span>, putting)router.DELETE(<span class="hljs-string">&quot;/someDelete&quot;</span>, deleting)router.PATCH(<span class="hljs-string">&quot;/somePatch&quot;</span>, patching)router.HEAD(<span class="hljs-string">&quot;/someHead&quot;</span>, head)router.OPTIONS(<span class="hljs-string">&quot;/someOptions&quot;</span>, options)<span class="hljs-comment">// By default it serves on :8080 unless a</span><span class="hljs-comment">// PORT environment variable was defined.</span>router.Run()<span class="hljs-comment">// router.Run(&quot;:3000&quot;) for a hard coded port</span>&#125;</code></pre></div><p>乍一看简单到爆，但实际使用中，在脚手架搭建上，我们需要规划好 <strong>包结构、配置文件、命令行参数、数据库连接、cache</strong> 等等；直到目前为止，至少我没有找到一种非常规范的后端 MVC 的标准架子结构；这点目前确实不如 Java 的生态；作为最初的脚手架搭建者，站在这个角度，我想我们更应当考虑如何做好适当的抽象、隔离；以防止后面开发者对系统基础功能可能造成的破坏。</p><p>综上所述，再配合 Gin 或者说 Go 的代码风格，这就形成了一种强烈的冲突；在 Java 中，由于有注解(<code>Annotation</code>)的存在，事实上你是可以有这种操作的: <strong>新建一个 Class，创建 func，在上面加上合适的注解，最终框架会通过注解扫描的方式以适当的形式进行初始化</strong>；而 Go 中并没有 <code>Annotation</code> 这玩意，我们很难实现在 <strong>代码运行时扫描自身做出一种策略性调整</strong>；从而下面这个需求很难实现: <strong>作为脚手架搭建者，我希望我的基础代码安全的放在一个特定位置，后续开发者开发应当以一种类似可热插拔的形式注入进来</strong>，比如 Gin 的 router 路由设置，我不希望每次有修改都会有人动我的 router 核心配置文件。</p><h3 id="二、Caddy-的套路"><a href="#二、Caddy-的套路" class="headerlink" title="二、Caddy 的套路"></a>二、Caddy 的套路</h3><p>在翻了 coredns 的源码后，我发现他是依赖于 Caddy 这框架运行的，coredns 的代码内的插件机制也是直接调用的 Caddy；所以接着我就翻到了 Caddy 源码，其中的代码如下(完整代码<a href="https://github.com/mholt/caddy/blob/master/plugins.go">点击这里</a>):</p><div class="hljs code-wrapper"><pre><code class="hljs golang"><span class="hljs-comment">// RegisterPlugin plugs in plugin. All plugins should register</span><span class="hljs-comment">// themselves, even if they do not perform an action associated</span><span class="hljs-comment">// with a directive. It is important for the process to know</span><span class="hljs-comment">// which plugins are available.</span><span class="hljs-comment">//</span><span class="hljs-comment">// The plugin MUST have a name: lower case and one word.</span><span class="hljs-comment">// If this plugin has an action, it must be the name of</span><span class="hljs-comment">// the directive that invokes it. A name is always required</span><span class="hljs-comment">// and must be unique for the server type.</span><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-title">RegisterPlugin</span><span class="hljs-params">(name <span class="hljs-keyword">string</span>, plugin Plugin)</span></span> &#123;<span class="hljs-keyword">if</span> name == <span class="hljs-string">&quot;&quot;</span> &#123;<span class="hljs-built_in">panic</span>(<span class="hljs-string">&quot;plugin must have a name&quot;</span>)&#125;<span class="hljs-keyword">if</span> _, ok := plugins[plugin.ServerType]; !ok &#123;plugins[plugin.ServerType] = <span class="hljs-built_in">make</span>(<span class="hljs-keyword">map</span>[<span class="hljs-keyword">string</span>]Plugin)&#125;<span class="hljs-keyword">if</span> _, dup := plugins[plugin.ServerType][name]; dup &#123;<span class="hljs-built_in">panic</span>(<span class="hljs-string">&quot;plugin named &quot;</span> + name + <span class="hljs-string">&quot; already registered for server type &quot;</span> + plugin.ServerType)&#125;plugins[plugin.ServerType][name] = plugin&#125;</code></pre></div><p>套路很清奇，为了实现我上面说的那个需求: “后面开发不需要动我核心代码，我还能允许他们动态添加”，Caddy 套路就是<strong>定义一个 map，map 里用于存放一种特定形式的 func，并且暴露出一个方法用于向 map 内添加指定 func，然后在合适的时机遍历这个 map，并执行其中的 func。</strong>这种套路利用了 Go 函数式编程的特性，将行为先存储在容器中，然后后续再去调用这些行为。</p><h3 id="三、总结"><a href="#三、总结" class="headerlink" title="三、总结"></a>三、总结</h3><p>长篇大论这么久，实际上我也是在一边折腾 Go 的过程中一边总结和对比跟 Java 的差异；在 Java 中扫描自己注解的套路 Go 中没法实现，但是 Go 利用其函数式编程的优势也可以利用一些延迟加载方式实现对应的功能；总结来说，不同语言有其自己的特性，当有对比的时候，可能更加深刻。</p>]]></content>
    
    
    <summary type="html">折腾 Go 已经有一段时间了，最近在用 Go 写点 web 的东西；在搭建脚手架的过程中总是有点不适应，尤其对可扩展性上总是感觉没有 Java 那么顺手；索性看了下 coredns 的源码，最后追踪到 caddy 源码；突然发现他们对代码内的 plugin 机制有一些骚套路，这里索性记录一下</summary>
    
    
    
    <category term="Golang" scheme="https://mritd.com/categories/golang/"/>
    
    
    <category term="Golang" scheme="https://mritd.com/tags/golang/"/>
    
  </entry>
  
  <entry>
    <title>Google container registry 同步</title>
    <link href="https://mritd.com/2018/09/17/google-container-registry-sync/"/>
    <id>https://mritd.com/2018/09/17/google-container-registry-sync/</id>
    <published>2018-09-17T13:19:40.000Z</published>
    <updated>2018-09-17T13:19:40.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="一、起因"><a href="#一、起因" class="headerlink" title="一、起因"></a>一、起因</h2><p>玩 Kubenretes 的基本都很清楚，Kubernetes 很多组件的镜像全部托管在 <code>gcr.io</code> 这个域名下(现在换成了 <code>k8s.gcr.io</code>)；由于众所周知的原因，这个网站在国内是不可达的；当时由于 Docker Hub 提供了 <code>Auto Build</code> 功能，机智的想到一个解决办法；就是利用 Docker Hub 的 <code>Auto Build</code>，创建只有一行的 Dockerfile，里面就一句 <code>FROM gcr.io/xxxx</code>，然后让 Docker Hub 帮你构建完成后拉取即可</p><p>这种套路的基本方案就是利用一个第三方公共仓库，这个仓库可以访问不可达的 <code>gcr.io</code>，然后生成镜像，我们再从这个仓库 pull 即可；为此我创建了一个 Github 仓库(<a href="https://github.com/mritd/docker-library">docker-library</a>)；时隔这么久以后，我猜想大家都已经有了这种自己的仓库…不过最近发现这个仓库仍然在有人 fork…</p><p>为了一劳永逸的解决这个问题，只能撸点代码解决这个问题了</p><h2 id="二、仓库使用"><a href="#二、仓库使用" class="headerlink" title="二、仓库使用"></a>二、仓库使用</h2><p>为了解决上述问题，我写了一个 <a href="https://github.com/mritd/gcrsync">gcrsync</a> 工具，并且借助 <a href="https://travis-ci.org/mritd/gcrsync">Travis CI</a> 让其每天自动运行，将所有用得到的 <code>gcr.io</code> 下的镜像同步到了 Docker Hub</p><p><strong>目前对于一个 <code>gcr.io</code> 下的镜像，可以直接替换为 <code>gcrxio</code> 用户名，然后从 Docker Hub 直接拉取</strong>，以下为一个示例:</p><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment"># 原始命令</span>docker pull k8s.gcr.io/kubernetes-dashboard-amd64:v1.10.0<span class="hljs-comment"># 使用同步仓库</span>docker pull gcrxio/kubernetes-dashboard-amd64:v1.10.0</code></pre></div><h2 id="三、同步细节说明"><a href="#三、同步细节说明" class="headerlink" title="三、同步细节说明"></a>三、同步细节说明</h2><p>为了保证同步镜像的安全性，同步工具已经开源在 <a href="https://github.com/mritd/gcrsync">gcrsync</a> 仓库，同步细节如下:</p><ul><li>工具每天由 <a href="https://travis-ci.org/mritd/gcrsync">Travis CI</a> 自动进行一次 build，然后进行推送</li><li>工具每次推送前首先 clone 元数据仓库 <a href="https://github.com/mritd/gcr">gcr</a></li><li>工具每次推送首先获取 <code>gcr.io</code> 指定 <code>namespace</code> 下的所有镜像(<code>namesapce</code> 由 <a href="https://github.com/mritd/gcrsync/blob/master/.travis.yml">.travis.yml</a> <code>script</code> 段定义)</li><li>获取 <code>gcr.io</code> 镜像后，再读取元数据仓库(<a href="https://github.com/mritd/gcr">gcr</a>) 中与 <code>namesapce</code> 同名文件(实际是个 json)</li><li>接着对比双方差异，得出需要同步的镜像</li><li>最后通过 API 调用本地的 docker 进行 <code>pull</code>、<code>tag</code>、<code>push</code> 操作，完成镜像推送</li><li>所有镜像推送成功后，更新元数据仓库内 <code>namespace</code> 对应的 json 文件，最后在生成 <a href="https://github.com/mritd/gcr/blob/master/CHANGELOG.md">CHANGELOG</a>，执行 <code>git push</code> 到远程元数据仓库</li></ul><p>综上所述，如果想得知<strong>具体 <code>gcrxio</code> 用户下都有那些镜像，可直接访问 <a href="https://github.com/mritd/gcr">gcr</a> 元数据仓库，查看对应 <code>namesapce</code> 同名的 json 文件即可；每天增量同步的信息会追加到 <a href="https://github.com/mritd/gcr">gcr</a> 仓库的 <code>CHANGELOG.md</code> 文件中</strong></p><h2 id="四、gcrsync"><a href="#四、gcrsync" class="headerlink" title="四、gcrsync"></a>四、gcrsync</h2><p>为方便审查镜像安全性，以下为 <a href="https://github.com/mritd/gcrsync">gcrsync</a> 工具的代码简介，代码仓库文件如下:</p><div class="hljs code-wrapper"><pre><code class="hljs sh">➜  gcrsync git:(master) tree -I vendor.├── CHANGELOG.md├── Gopkg.lock├── Gopkg.toml├── LICENSE├── README.md├── cmd│   ├── compare.go│   ├── monitor.go│   ├── root.go│   ├── sync.go│   └── test.go├── dist│   ├── gcrsync_darwin_amd64│   ├── gcrsync_linux_386│   └── gcrsync_linux_amd64├── main.go└── pkg    ├── gcrsync    │   ├── docker.go    │   ├── gcr.go    │   ├── git.go    │   ├── registry.go    │   └── sync.go    └── utils        └── common.go</code></pre></div><p>cmd 目录下为标准的 <code>cobra</code> 框架生成的子命令文件，其中每个命令包含了对应的 flag 设置，如 <code>namesapce</code>、<code>proxy</code> 等；<code>pkg/gcrsync</code> 目录下的文件为核心代码:</p><ul><li><code>docker.go</code> 包含了对本地 docker daemon API 调用，包括 <code>pull</code>、<code>tag</code>、<code>push</code> 操作</li><li><code>gcr.go</code> 包含了对 <code>gcr.io</code> 指定 <code>namespace</code> 下镜像列表获取操作</li><li><code>registry.go</code> 包含了对 Docker Hub 下指定用户(默认 <code>gcrxio</code>)的镜像列表获取操作(其主要用于首次执行 <code>compare</code> 命令生成 json 文件)</li><li><code>sync.go</code> 为主要的程序入口，其中包含了对其他文件内方法的调用，设置并发池等</li></ul><h2 id="五、其他说明"><a href="#五、其他说明" class="headerlink" title="五、其他说明"></a>五、其他说明</h2><p>该仓库不保证镜像实时同步，默认每天同步一次(由 <a href="https://travis-ci.org/mritd/gcrsync">Travis CI</a> 执行)，如有特殊需求，如增加 <code>namesapce</code> 等请开启 issue；最后，请不要再 fork <a href="https://github.com/mritd/docker-library">docker-library</a> 这个仓库了</p>]]></content>
    
    
    <summary type="html">Google container registry 同步</summary>
    
    
    
    <category term="Kubernetes" scheme="https://mritd.com/categories/kubernetes/"/>
    
    
    <category term="Docker" scheme="https://mritd.com/tags/docker/"/>
    
    <category term="Kubernetes" scheme="https://mritd.com/tags/kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>使用 Bootstrap Token 完成 TLS Bootstrapping</title>
    <link href="https://mritd.com/2018/08/28/kubernetes-tls-bootstrapping-with-bootstrap-token/"/>
    <id>https://mritd.com/2018/08/28/kubernetes-tls-bootstrapping-with-bootstrap-token/</id>
    <published>2018-08-28T08:54:43.000Z</published>
    <updated>2018-08-28T08:54:43.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>最近在测试 Kubernetes 1.11.2 新版本的相关东西，发现新版本的 Bootstrap Token 功能已经进入 Beta 阶段，索性便尝试了一下；虽说目前是为 kubeadm 设计的，不过手动挡用起来也不错，这里记录一下使用方式</p></blockquote><h2 id="一、环境准备"><a href="#一、环境准备" class="headerlink" title="一、环境准备"></a>一、环境准备</h2><p>首先需要有一个运行状态正常的 Master 节点，目前我测试的是版本是 1.11.2，低版本我没测试；其次本文默认 Node 节点 Docker、kubelet 二进制文件、systemd service 配置等都已经处理好，更具体的环境如下:</p><p><strong>Master 节点 IP 为 <code>192.168.1.61</code>，Node 节点 IP 为 <code>192.168.1.64</code></strong></p><div class="hljs code-wrapper"><pre><code class="hljs sh">docker1.node ➜  ~ kubectl versionClient Version: version.Info&#123;Major:<span class="hljs-string">&quot;1&quot;</span>, Minor:<span class="hljs-string">&quot;11&quot;</span>, GitVersion:<span class="hljs-string">&quot;v1.11.2&quot;</span>, GitCommit:<span class="hljs-string">&quot;bb9ffb1654d4a729bb4cec18ff088eacc153c239&quot;</span>, GitTreeState:<span class="hljs-string">&quot;clean&quot;</span>, BuildDate:<span class="hljs-string">&quot;2018-08-07T23:08:19Z&quot;</span>, GoVersion:<span class="hljs-string">&quot;go1.10.3&quot;</span>, Compiler:<span class="hljs-string">&quot;gc&quot;</span>, Platform:<span class="hljs-string">&quot;linux/amd64&quot;</span>&#125;Server Version: version.Info&#123;Major:<span class="hljs-string">&quot;1&quot;</span>, Minor:<span class="hljs-string">&quot;11&quot;</span>, GitVersion:<span class="hljs-string">&quot;v1.11.2&quot;</span>, GitCommit:<span class="hljs-string">&quot;bb9ffb1654d4a729bb4cec18ff088eacc153c239&quot;</span>, GitTreeState:<span class="hljs-string">&quot;clean&quot;</span>, BuildDate:<span class="hljs-string">&quot;2018-08-07T23:08:19Z&quot;</span>, GoVersion:<span class="hljs-string">&quot;go1.10.3&quot;</span>, Compiler:<span class="hljs-string">&quot;gc&quot;</span>, Platform:<span class="hljs-string">&quot;linux/amd64&quot;</span>&#125;docker1.node ➜  ~ docker infoContainers: 0 Running: 0 Paused: 0 Stopped: 0Images: 0Server Version: 18.06.1-ceStorage Driver: overlay2 Backing Filesystem: xfs Supports d_type: <span class="hljs-literal">true</span> Native Overlay Diff: <span class="hljs-literal">true</span>Logging Driver: json-fileCgroup Driver: cgroupfsPlugins: Volume: <span class="hljs-built_in">local</span> Network: bridge host macvlan null overlay Log: awslogs fluentd gcplogs gelf journald json-file logentries splunk syslogSwarm: inactiveRuntimes: runcDefault Runtime: runcInit Binary: docker-initcontainerd version: 468a545b9edcd5932818eb9de8e72413e616e86erunc version: 69663f0bd4b60df09991c08812a60108003fa340init version: fec3683Security Options: apparmor seccomp  Profile: defaultKernel Version: 4.15.0-33-genericOperating System: Ubuntu 18.04.1 LTSOSType: linuxArchitecture: x86_64CPUs: 2Total Memory: 3.847GiBName: docker1.nodeID: AJOD:RBJZ:YP3G:HCGV:KT4R:D4AF:SBDN:5B76:JM4M:OCJA:YJMJ:OCYQDocker Root Dir: /data/dockerDebug Mode (client): <span class="hljs-literal">false</span>Debug Mode (server): <span class="hljs-literal">false</span>Registry: https://index.docker.io/v1/Labels:Experimental: <span class="hljs-literal">false</span>Insecure Registries: 127.0.0.0/8Live Restore Enabled: <span class="hljs-literal">false</span></code></pre></div><h2 id="二、TLS-Bootstrapping-回顾"><a href="#二、TLS-Bootstrapping-回顾" class="headerlink" title="二、TLS Bootstrapping 回顾"></a>二、TLS Bootstrapping 回顾</h2><p>在正式进行 TLS Bootstrapping 操作之前，**如果对 TLS Bootstrapping 完全没接触过的请先阅读 <a href="https://mritd.me/2018/01/07/kubernetes-tls-bootstrapping-note">Kubernetes TLS bootstrapping 那点事</a>**；我想这里有必要简单说明下使用 Token 时整个启动引导过程:</p><ul><li>在集群内创建特定的 <code>Bootstrap Token Secret</code>，该 Secret 将替代以前的 <code>token.csv</code> 内置用户声明文件</li><li>在集群内创建首次 TLS Bootstrap 申请证书的 ClusterRole、后续 renew Kubelet client/server 的 ClusterRole，以及其相关对应的 ClusterRoleBinding；并绑定到对应的组或用户</li><li>调整 Controller Manager 配置，以使其能自动签署相关证书和自动清理过期的 TLS Bootstrapping Token</li><li>生成特定的包含 TLS Bootstrapping Token 的 <code>bootstrap.kubeconfig</code> 以供 kubelet 启动时使用</li><li>调整 Kubelet 配置，使其首次启动加载 <code>bootstrap.kubeconfig</code> 并使用其中的 TLS Bootstrapping Token 完成首次证书申请</li><li>证书被 Controller Manager 签署，成功下发，Kubelet 自动重载完成引导流程</li><li>后续 Kubelet 自动 renew 相关证书</li><li>可选的: 集群搭建成功后立即清除 <code>Bootstrap Token Secret</code>，或等待 Controller Manager 待其过期后删除，以防止被恶意利用</li></ul><h2 id="三、使用-Bootstrap-Token"><a href="#三、使用-Bootstrap-Token" class="headerlink" title="三、使用 Bootstrap Token"></a>三、使用 Bootstrap Token</h2><p>第二部分算作大纲了，这部分将会按照第二部分的总体流程来走，同时会对一些细节进行详细说明</p><h3 id="3-1、创建-Bootstrap-Token"><a href="#3-1、创建-Bootstrap-Token" class="headerlink" title="3.1、创建 Bootstrap Token"></a>3.1、创建 Bootstrap Token</h3><p>既然整个功能都时刻强调这个 Token，那么第一步肯定是生成一个 token，生成方式如下:</p><div class="hljs code-wrapper"><pre><code class="hljs sh">➜  ~ <span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;<span class="hljs-subst">$(head -c 6 /dev/urandom | md5sum | head -c 6)</span>&quot;</span>.<span class="hljs-string">&quot;<span class="hljs-subst">$(head -c 16 /dev/urandom | md5sum | head -c 16)</span>&quot;</span>47f392.d22d04e89a65eb22</code></pre></div><p>这个 <code>47f392.d22d04e89a65eb22</code> 就是生成的 Bootstrap Token，保存好 token，因为后续要用；关于这个 token 解释如下:</p><p>Token 必须满足 <code>[a-z0-9]&#123;6&#125;\.[a-z0-9]&#123;16&#125;</code> 格式；以 <code>.</code> 分割，前面的部分被称作  <code>Token ID</code>，<code>Token ID</code> 并不是 “机密信息”，它可以暴露出去；相对的后面的部分称为 <code>Token Secret</code>，它应该是保密的</p><p>本部分官方文档地址 <a href="https://kubernetes.io/docs/reference/access-authn-authz/bootstrap-tokens/#token-format">Token Format</a></p><h3 id="3-2、创建-Bootstrap-Token-Secret"><a href="#3-2、创建-Bootstrap-Token-Secret" class="headerlink" title="3.2、创建 Bootstrap Token Secret"></a>3.2、创建 Bootstrap Token Secret</h3><p>对于 Kubernetes 来说 <code>Bootstrap Token Secret</code> 也仅仅是一个特殊的 <code>Secret</code> 而已；对于这个特殊的 <code>Secret</code> 样例 yaml 配置如下:</p><div class="hljs code-wrapper"><pre><code class="hljs yaml"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span><span class="hljs-attr">kind:</span> <span class="hljs-string">Secret</span><span class="hljs-attr">metadata:</span>  <span class="hljs-comment"># Name MUST be of form &quot;bootstrap-token-&lt;token id&gt;&quot;</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">bootstrap-token-07401b</span>  <span class="hljs-attr">namespace:</span> <span class="hljs-string">kube-system</span><span class="hljs-comment"># Type MUST be &#x27;bootstrap.kubernetes.io/token&#x27;</span><span class="hljs-attr">type:</span> <span class="hljs-string">bootstrap.kubernetes.io/token</span><span class="hljs-attr">stringData:</span>  <span class="hljs-comment"># Human readable description. Optional.</span>  <span class="hljs-attr">description:</span> <span class="hljs-string">&quot;The default bootstrap token generated by &#x27;kubeadm init&#x27;.&quot;</span>  <span class="hljs-comment"># Token ID and secret. Required.</span>  <span class="hljs-attr">token-id:</span> <span class="hljs-string">47f392</span>  <span class="hljs-attr">token-secret:</span> <span class="hljs-string">d22d04e89a65eb22</span>  <span class="hljs-comment"># Expiration. Optional.</span>  <span class="hljs-attr">expiration:</span> <span class="hljs-number">2018-09-10T00:00:11Z</span>  <span class="hljs-comment"># Allowed usages.</span>  <span class="hljs-attr">usage-bootstrap-authentication:</span> <span class="hljs-string">&quot;true&quot;</span>  <span class="hljs-attr">usage-bootstrap-signing:</span> <span class="hljs-string">&quot;true&quot;</span>  <span class="hljs-comment"># Extra groups to authenticate the token as. Must start with &quot;system:bootstrappers:&quot;</span>  <span class="hljs-attr">auth-extra-groups:</span> <span class="hljs-string">system:bootstrappers:worker,system:bootstrappers:ingress</span></code></pre></div><p>需要注意几点:</p><ul><li>作为 <code>Bootstrap Token Secret</code> 的 type 必须为 <code>bootstrap.kubernetes.io/token</code>，name 必须为 <code>bootstrap-token-&lt;token id&gt;</code> (Token ID 就是上一步创建的 Token 前一部分)</li><li><code>usage-bootstrap-authentication</code>、<code>usage-bootstrap-signing</code> 必须存才且设置为 <code>true</code> (我个人感觉 <code>usage-bootstrap-signing</code> 可以没有，具体见文章最后部分)</li><li><code>expiration</code> 字段是可选的，如果设置则 <code>Secret</code> 到期后将由 Controller Manager 中的 <code>tokencleaner</code> 自动清理</li><li><code>auth-extra-groups</code> 也是可选的，令牌的扩展认证组，组必须以 <code>system:bootstrappers:</code> 开头</li></ul><p>最后使用 <code>kubectl create -f bootstrap.secret.yaml</code> 创建即可</p><p>本部分官方文档地址 <a href="https://kubernetes.io/docs/reference/access-authn-authz/bootstrap-tokens/#bootstrap-token-secret-format">Bootstrap Token Secret Format</a></p><h3 id="3-3、创建-ClusterRole-和-ClusterRoleBinding"><a href="#3-3、创建-ClusterRole-和-ClusterRoleBinding" class="headerlink" title="3.3、创建 ClusterRole 和 ClusterRoleBinding"></a>3.3、创建 ClusterRole 和 ClusterRoleBinding</h3><p>具体都有哪些 <code>ClusterRole</code> 和 <code>ClusterRoleBinding</code>，以及其作用请参考上一篇的 <a href="https://mritd.me/2018/01/07/kubernetes-tls-bootstrapping-note">Kubernetes TLS bootstrapping 那点事</a>，不想在这里重复了</p><p>在 1.8 以后三个 <code>ClusterRole</code> 中有两个已经有了，我们只需要创建剩下的一个即可:</p><div class="hljs code-wrapper"><pre><code class="hljs yaml"><span class="hljs-comment"># A ClusterRole which instructs the CSR approver to approve a node requesting a</span><span class="hljs-comment"># serving cert matching its client cert.</span><span class="hljs-attr">kind:</span> <span class="hljs-string">ClusterRole</span><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">rbac.authorization.k8s.io/v1</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">system:certificates.k8s.io:certificatesigningrequests:selfnodeserver</span><span class="hljs-attr">rules:</span><span class="hljs-bullet">-</span> <span class="hljs-attr">apiGroups:</span> [<span class="hljs-string">&quot;certificates.k8s.io&quot;</span>]  <span class="hljs-attr">resources:</span> [<span class="hljs-string">&quot;certificatesigningrequests/selfnodeserver&quot;</span>]  <span class="hljs-attr">verbs:</span> [<span class="hljs-string">&quot;create&quot;</span>]</code></pre></div><p>然后是三个 <code>ClusterRole</code> 对应的 <code>ClusterRoleBinding</code>；需要注意的是 <strong>在使用 <code>Bootstrap Token</code> 进行引导时，Kubelet 组件使用 Token 发起的请求其用户名为 <code>system:bootstrap:&lt;token id&gt;</code>，用户组为 <code>system:bootstrappers</code>；so 我们在创建 <code>ClusterRoleBinding</code> 时要绑定到这个用户或者组上</strong>；当然我选择懒一点，全部绑定到组上</p><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment"># 允许 system:bootstrappers 组用户创建 CSR 请求</span>kubectl create clusterrolebinding kubelet-bootstrap --clusterrole=system:node-bootstrapper --group=system:bootstrappers<span class="hljs-comment"># 自动批准 system:bootstrappers 组用户 TLS bootstrapping 首次申请证书的 CSR 请求</span>kubectl create clusterrolebinding node-client-auto-approve-csr --clusterrole=system:certificates.k8s.io:certificatesigningrequests:nodeclient --group=system:bootstrappers<span class="hljs-comment"># 自动批准 system:nodes 组用户更新 kubelet 自身与 apiserver 通讯证书的 CSR 请求</span>kubectl create clusterrolebinding node-client-auto-renew-crt --clusterrole=system:certificates.k8s.io:certificatesigningrequests:selfnodeclient --group=system:nodes<span class="hljs-comment"># 自动批准 system:nodes 组用户更新 kubelet 10250 api 端口证书的 CSR 请求</span>kubectl create clusterrolebinding node-server-auto-renew-crt --clusterrole=system:certificates.k8s.io:certificatesigningrequests:selfnodeserver --group=system:nodes</code></pre></div><p>关于本部分首次请求用户名变为 <code>system:bootstrap:&lt;token id&gt;</code> 官方文档原文如下:</p><blockquote><p>Tokens authenticate as the username system:bootstrap:<token id> and are members of the group system:bootstrappers. Additional groups may be specified in the token’s Secret.</p></blockquote><h3 id="3-4、调整-Controller-Manager"><a href="#3-4、调整-Controller-Manager" class="headerlink" title="3.4、调整 Controller Manager"></a>3.4、调整 Controller Manager</h3><p>根据官方文档描述，Controller Manager 需要启用 <code>tokencleaner</code> 和 <code>bootstrapsigner</code> (目测这个 <code>bootstrapsigner</code> 实际上并不需要，顺便加着吧)，完整配置如下(为什么贴完整配置? 文章凑数啊…):</p><div class="hljs code-wrapper"><pre><code class="hljs sh">KUBE_CONTROLLER_MANAGER_ARGS=<span class="hljs-string">&quot;  --address=127.0.0.1 \</span><span class="hljs-string">                                --bind-address=192.168.1.61 \</span><span class="hljs-string">                                --port=10252 \</span><span class="hljs-string">                                --secure-port=10258 \</span><span class="hljs-string">                                --cluster-name=kubernetes \</span><span class="hljs-string">                                --cluster-signing-cert-file=/etc/kubernetes/ssl/k8s-root-ca.pem \</span><span class="hljs-string">                                --cluster-signing-key-file=/etc/kubernetes/ssl/k8s-root-ca-key.pem \</span><span class="hljs-string">                                --controllers=*,bootstrapsigner,tokencleaner \</span><span class="hljs-string">                                --deployment-controller-sync-period=10s \</span><span class="hljs-string">                                --experimental-cluster-signing-duration=86700h0m0s \</span><span class="hljs-string">                                --enable-garbage-collector=true \</span><span class="hljs-string">                                --leader-elect=true \</span><span class="hljs-string">                                --master=http://127.0.0.1:8080 \</span><span class="hljs-string">                                --node-monitor-grace-period=40s \</span><span class="hljs-string">                                --node-monitor-period=5s \</span><span class="hljs-string">                                --pod-eviction-timeout=5m0s \</span><span class="hljs-string">                                --terminated-pod-gc-threshold=50 \</span><span class="hljs-string">                                --root-ca-file=/etc/kubernetes/ssl/k8s-root-ca.pem \</span><span class="hljs-string">                                --service-account-private-key-file=/etc/kubernetes/ssl/k8s-root-ca-key.pem \</span><span class="hljs-string">                                --feature-gates=RotateKubeletServerCertificate=true&quot;</span></code></pre></div><h3 id="3-5、生成-bootstrap-kubeconfig"><a href="#3-5、生成-bootstrap-kubeconfig" class="headerlink" title="3.5、生成 bootstrap.kubeconfig"></a>3.5、生成 bootstrap.kubeconfig</h3><p>前面所有步骤实际上都是在处理 Api Server、Controller Manager 这一块，为的就是 “老子启动后 TLS Bootstarpping 发证书申请你两个要立马允许，不能拒绝老子”；接下来就是比较重要的 <code>bootstrap.kubeconfig</code> 配置生成，这个 <code>bootstrap.kubeconfig</code> 是最终被 Kubelet 使用的，里面包含了相关的 Token，以帮助 Kubelet 在第一次通讯时能成功沟通 Api Server；生成方式如下:</p><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment"># 设置集群参数</span>kubectl config set-cluster kubernetes \  --certificate-authority=/etc/kubernetes/ssl/k8s-root-ca.pem \  --embed-certs=<span class="hljs-literal">true</span> \  --server=https://127.0.0.1:6443 \  --kubeconfig=bootstrap.kubeconfig<span class="hljs-comment"># 设置客户端认证参数</span>kubectl config set-credentials system:bootstrap:47f392 \  --token=47f392.d22d04e89a65eb22 \  --kubeconfig=bootstrap.kubeconfig<span class="hljs-comment"># 设置上下文参数</span>kubectl config set-context default \  --cluster=kubernetes \  --user=system:bootstrap:47f392 \  --kubeconfig=bootstrap.kubeconfig<span class="hljs-comment"># 设置默认上下文</span>kubectl config use-context default --kubeconfig=bootstrap.kubeconfig</code></pre></div><h3 id="3-6、调整-Kubelet"><a href="#3-6、调整-Kubelet" class="headerlink" title="3.6、调整 Kubelet"></a>3.6、调整 Kubelet</h3><p>Kubelet 启动参数需要做一些相应调整，以使其能正确的使用 <code>Bootstartp Token</code>，完整配置如下(与使用 token.csv 配置没什么变化，因为主要变更在 bootstrap.kubeconfig 中):</p><div class="hljs code-wrapper"><pre><code class="hljs sh">KUBELET_ARGS=<span class="hljs-string">&quot;  --address=192.168.1.64 \</span><span class="hljs-string">                --allow-privileged=true \</span><span class="hljs-string">                --alsologtostderr \</span><span class="hljs-string">                --anonymous-auth=true \</span><span class="hljs-string">                --bootstrap-kubeconfig=/etc/kubernetes/bootstrap.kubeconfig \</span><span class="hljs-string">                --cert-dir=/etc/kubernetes/ssl \</span><span class="hljs-string">                --cgroup-driver=cgroupfs \</span><span class="hljs-string">                --cluster-dns=10.254.0.2 \</span><span class="hljs-string">                --cluster-domain=cluster.local. \</span><span class="hljs-string">                --fail-swap-on=false \</span><span class="hljs-string">                --healthz-port=10248 \</span><span class="hljs-string">                --healthz-bind-address=192.168.1.64 \</span><span class="hljs-string">                --feature-gates=RotateKubeletClientCertificate=true,RotateKubeletServerCertificate=true \</span><span class="hljs-string">                --node-labels=node-role.kubernetes.io/k8s-master=true \</span><span class="hljs-string">                --image-gc-high-threshold=70 \</span><span class="hljs-string">                --image-gc-low-threshold=50 \</span><span class="hljs-string">                --kube-reserved=cpu=500m,memory=512Mi,ephemeral-storage=1Gi \</span><span class="hljs-string">                --kubeconfig=/etc/kubernetes/kubelet.kubeconfig \</span><span class="hljs-string">                --system-reserved=cpu=1000m,memory=1024Mi,ephemeral-storage=1Gi \</span><span class="hljs-string">                --serialize-image-pulls=false \</span><span class="hljs-string">                --sync-frequency=30s \</span><span class="hljs-string">                --pod-infra-container-image=k8s.gcr.io/pause:3.1 \</span><span class="hljs-string">                --resolv-conf=/etc/resolv.conf \</span><span class="hljs-string">                --rotate-certificates&quot;</span></code></pre></div><p><strong>一切准备就绪后，执行 <code>systemctl daemon-reload &amp;&amp; systemctl start kubelet</code> 启动即可</strong></p><h2 id="四、其他说明"><a href="#四、其他说明" class="headerlink" title="四、其他说明"></a>四、其他说明</h2><p>可能有人已经注意到，在官方文档中最后部分有关于 <a href="https://kubernetes.io/docs/reference/access-authn-authz/bootstrap-tokens/#configmap-signing">ConfigMap Signing</a> 的相关描述，同时要求了启用 <code>bootstrapsigner</code> 这个 controller，而且在上文创建 <code>Bootstrap Token Secret</code> 中我也说 <code>usage-bootstrap-signing</code> 这个可以不设置；其中官方文档上的描述我们能看到的大致只说了这么两段稍微有点用的话:</p><blockquote><p>In addition to authentication, the tokens can be used to sign a ConfigMap. This is used early in a cluster bootstrap process before the client trusts the API server. The signed ConfigMap can be authenticated by the shared token.</p></blockquote><blockquote><p>The ConfigMap that is signed is cluster-info in the kube-public namespace. The typical flow is that a client reads this ConfigMap while unauthenticated and ignoring TLS errors. It then validates the payload of the ConfigMap by looking at a signature embedded in the ConfigMap.</p></blockquote><p>从这两段话中我们只能得出两个结论:</p><ul><li>Bootstrap Token 能对 ConfigMap 签名</li><li>可以签名一个 <code>kube-public</code> NameSpace 下的名字叫 <code>cluster-info</code> 的 ConfigMap，并且这个 ConfigMap 可以在没进行引导之前强行读取</li></ul><p>说实话这两段话搞得我百思不得<del>骑姐</del>其解，最终我在 kubeadm 的相关文档中找到了真正的说明及作用:</p><ul><li>在使用 <code>kubeadm init</code> 时创建 <code>cluster-info</code> 这个 ConfigMap，ConfigMap 中包含了集群基本信息</li><li>在使用 <code>kubeadm join</code> 时目标节点强行读取 ConfigMap 以得知集群基本信息，然后进行 <code>join</code></li></ul><p><strong>综上所述，我个人认为手动部署下，在仅仅使用 Bootstrap Token 进行 TLS Bootstrapping 时，<code>bootstrapsigner</code> 这个 controller 和 <code>Bootstrap Token Secret</code> 中的 <code>usage-bootstrap-signing</code> 选项是没有必要的，当然我还没测试(胡吹谁不会)…</strong></p><p>最后附上 <code>kubeadm</code> 的文档说明: <a href="https://kubernetes.io/docs/reference/setup-tools/kubeadm/implementation-details/#create-the-public-cluster-info-configmap">Create the public cluster-info ConfigMap</a>、<a href="https://kubernetes.io/docs/reference/setup-tools/kubeadm/implementation-details/#discovery-cluster-info">Discovery cluster-info</a></p>]]></content>
    
    
    <summary type="html">最近在测试 Kubernetes 1.11.2 新版本的相关东西，发现新版本的 Bootstrap Token 功能已经进入 Beta 阶段，索性便尝试了一下；虽说目前是为 kubeadm 设计的，不过手动挡用起来也不错，这里记录一下使用方式</summary>
    
    
    
    <category term="Kubernetes" scheme="https://mritd.com/categories/kubernetes/"/>
    
    
    <category term="Kubernetes" scheme="https://mritd.com/tags/kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>Kubernetes 证书配置</title>
    <link href="https://mritd.com/2018/08/26/kubernetes-certificate-configuration/"/>
    <id>https://mritd.com/2018/08/26/kubernetes-certificate-configuration/</id>
    <published>2018-08-26T14:54:16.000Z</published>
    <updated>2018-08-26T14:54:16.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>一直以来自己的 Kubernetes 集群大部分证书配置全部都在使用一个 CA，而事实上很多教程也没有具体的解释过这些证书代表的作用以及含义；今天索性仔细的翻了翻，顺便看到了一篇老外的文章，感觉写的不错，这里顺带着自己的理解总结一下。</p></blockquote><h2 id="一、Kubernetes-证书分类"><a href="#一、Kubernetes-证书分类" class="headerlink" title="一、Kubernetes 证书分类"></a>一、Kubernetes 证书分类</h2><p>这里的证书分类只是我自己定义的一种 “并不 ok” 的概念；从整体的作用上 Kubernetes 证书大致上应当分为两类:</p><ul><li>API Server 用于校验请求合法性证书</li><li>对其他敏感信息进行签名的证书(如 Service Account)</li></ul><p>对于 API Server 用于检验请求合法性的证书配置一般会在 API Server 中配置好，而对其他敏感信息签名加密的证书一般会可能放在 Controller Manager 中配置，也可能还在 API Server，具体不同版本需要撸文档</p><p>另外需要明确的是: <strong>Kubernetes 中 CA 证书并不一定只有一个，很多证书配置实际上是不相干的，只是大家为了方便普遍选择了使用一个 CA 进行签发；同时有一些证书如果不设置也会自动默认一个，就目前我所知的大约有 5 个可以完全不同的证书签发体系(或者说由不同的 CA 签发)</strong></p><h2 id="二、API-Server-中的证书配置"><a href="#二、API-Server-中的证书配置" class="headerlink" title="二、API Server 中的证书配置"></a>二、API Server 中的证书配置</h2><h3 id="2-1、API-Server-证书"><a href="#2-1、API-Server-证书" class="headerlink" title="2.1、API Server 证书"></a>2.1、API Server 证书</h3><p>API Server 证书配置中最应当明确的两个选项应该是以下两个:</p><div class="hljs code-wrapper"><pre><code class="hljs sh">--tls-cert-file string    File containing the default x509 Certificate <span class="hljs-keyword">for</span> HTTPS. (CA cert, <span class="hljs-keyword">if</span> any, concatenated after server cert). If HTTPS serving is enabled, and --tls-cert-file and --tls-private-key-file are not provided, a self-signed certificate and key are generated <span class="hljs-keyword">for</span> the public address and saved to the directory specified by --cert-dir.--tls-private-key-file string    File containing the default x509 private key matching --tls-cert-file.</code></pre></div><p>从描述上就可以看出，这两个选项配置的就是 API Server HTTPS 端点应当使用的证书</p><h3 id="2-2、Client-CA-证书"><a href="#2-2、Client-CA-证书" class="headerlink" title="2.2、Client CA 证书"></a>2.2、Client CA 证书</h3><p>接下来就是我们常见的 CA 配置:</p><div class="hljs code-wrapper"><pre><code class="hljs sh">--client-ca-file string    If <span class="hljs-built_in">set</span>, any request presenting a client certificate signed by one of the authorities <span class="hljs-keyword">in</span> the client-ca-file is authenticated with an identity corresponding to the CommonName of the client certificate.</code></pre></div><p>该配置明确了 Clent 连接 API Server 时，API Server 应当确保其证书源自哪个 CA 签发；如果其证书不是由该 CA 签发，则拒绝请求；事实上，这个 CA 不必与 HTTPS 端点所使用的证书 CA 相同；同时这里的 Client 是一个泛指的，可以是 kubectl，也可能是你自己开发的应用</p><h3 id="2-3、请求头证书"><a href="#2-3、请求头证书" class="headerlink" title="2.3、请求头证书"></a>2.3、请求头证书</h3><p>由于 API Server 是支持多种认证方式的，其中一种就是使用 HTTP 头中的指定字段来进行认证，相关配置如下:</p><div class="hljs code-wrapper"><pre><code class="hljs sh">--requestheader-allowed-names stringSlice    List of client certificate common names to allow to provide usernames <span class="hljs-keyword">in</span> headers specified by --requestheader-username-headers. If empty, any client certificate validated by the authorities <span class="hljs-keyword">in</span> --requestheader-client-ca-file is allowed.--requestheader-client-ca-file string    Root certificate bundle to use to verify client certificates on incoming requests before trusting usernames <span class="hljs-keyword">in</span> headers specified by --requestheader-username-headers. WARNING: generally <span class="hljs-keyword">do</span> not depend on authorization being already <span class="hljs-keyword">done</span> <span class="hljs-keyword">for</span> incoming requests.</code></pre></div><p>当指定这个 CA 证书后，则 API Server 使用 HTTP 头进行认证时会检测其 HTTP 头中发送的证书是否由这个 CA 签发；同样它也可独立于其他 CA(可以是个独立的 CA)；具体可以参考 <a href="https://kubernetes.io/docs/reference/access-authn-authz/authentication/#authenticating-proxy">Authenticating Proxy</a></p><h3 id="2-4、Kubelet-证书"><a href="#2-4、Kubelet-证书" class="headerlink" title="2.4、Kubelet 证书"></a>2.4、Kubelet 证书</h3><p>对于 Kubelet 组件，API Server 单独提供了证书配置选项，同时 Kubelet 组件也提供了反向设置的相关选项:</p><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment"># API Server</span>--kubelet-certificate-authority string    Path to a cert file <span class="hljs-keyword">for</span> the certificate authority.--kubelet-client-certificate string    Path to a client cert file <span class="hljs-keyword">for</span> TLS.--kubelet-client-key string    Path to a client key file <span class="hljs-keyword">for</span> TLS.<span class="hljs-comment"># Kubelet</span>--client-ca-file string    If <span class="hljs-built_in">set</span>, any request presenting a client certificate signed by one of the authorities <span class="hljs-keyword">in</span> the client-ca-file is authenticated with an identity corresponding to the CommonName of the client certificate.--tls-cert-file string    File containing x509 Certificate used <span class="hljs-keyword">for</span> serving HTTPS (with intermediate certs, <span class="hljs-keyword">if</span> any, concatenated after server cert). If --tls-cert-file and --tls-private-key-file are not provided, a self-signed certificate and key are generated <span class="hljs-keyword">for</span> the public address and saved to the directory passed to --cert-dir.--tls-private-key-file string    File containing x509 private key matching --tls-cert-file.</code></pre></div><p>相信这个配置不用多说就能猜到，这个就是用于指定 API Server 与 Kubelet 通讯所使用的证书以及其签署的 CA；同样这个 CA 可以完全独立与上述其他CA</p><h2 id="三、Service-Account-证书"><a href="#三、Service-Account-证书" class="headerlink" title="三、Service Account 证书"></a>三、Service Account 证书</h2><p>在 API Server 配置中，对于 Service Account 同样有两个证书配置:</p><div class="hljs code-wrapper"><pre><code class="hljs sh">--service-account-key-file stringArray    File containing PEM-encoded x509 RSA or ECDSA private or public keys, used to verify ServiceAccount tokens. The specified file can contain multiple keys, and the flag can be specified multiple <span class="hljs-built_in">times</span> with different files. If unspecified, --tls-private-key-file is used. Must be specified when --service-account-signing-key is provided--service-account-signing-key-file string    Path to the file that contains the current private key of the service account token issuer. The issuer will sign issued ID tokens with this private key. (Requires the <span class="hljs-string">&#x27;TokenRequest&#x27;</span> feature gate.)</code></pre></div><p>这两个配置描述了对 Service Account 进行签名验证时所使用的证书；不过需要注意的是这里并没有明确要求证书 CA，所以这两个证书的 CA 理论上也是可以完全独立的；至于未要求 CA 问题，可能是由于 jwt 库并不支持 CA 验证</p><h2 id="四、总结"><a href="#四、总结" class="headerlink" title="四、总结"></a>四、总结</h2><p>Kubernetes 中大部分证书都是用于 API Server 各种鉴权使用的；在不同鉴权方案或者对象上实际证书体系可以完全不同；具体是使用多个 CA 好还是都用一个，取决于集群规模、安全性要求等等因素，至少目前来说没有明确的那个好与不好</p><p>最后，嗯…吹牛逼就吹到这，有点晚了，得睡觉了…</p>]]></content>
    
    
    <summary type="html">一直以来自己的 Kubernetes 集群大部分证书配置全部都在使用一个 CA，而事实上很多教程也没有具体的解释过这些证书代表的作用以及含义；今天索性仔细的翻了翻，顺便看到了一篇老外的文章，感觉写的不错，这里顺带着自己的理解总结一下。</summary>
    
    
    
    <category term="Kubernetes" scheme="https://mritd.com/categories/kubernetes/"/>
    
    
    <category term="Kubernetes" scheme="https://mritd.com/tags/kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>编写 kubectl 插件</title>
    <link href="https://mritd.com/2018/08/09/create-a-plugin-for-kubectl/"/>
    <id>https://mritd.com/2018/08/09/create-a-plugin-for-kubectl/</id>
    <published>2018-08-09T14:19:35.000Z</published>
    <updated>2018-08-09T14:19:35.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>最近忙的晕头转向，博客停更了 1 个月，感觉对不起党、对不起人民、对不起 <del>CCAV</del>…不过在忙的时候操作 Kubernetes 集群要频繁的使用 <code>kubectl</code> 命令，而在多个 NameSpace 下来回切换每次都得加个 <code>-n</code> 简直让我想打人；索性翻了下 <code>kubectl</code> 的插件机制，顺便写了一个快速切换 NameSpace 的小插件，以下记录一下插件编写过程</p></blockquote><h2 id="一、插件介绍"><a href="#一、插件介绍" class="headerlink" title="一、插件介绍"></a>一、插件介绍</h2><p><code>kubectl</code> 命令从 <code>v1.8.0</code> 版本开始引入了 alpha feature 的插件机制；在此机制下我们可以对 <code>kubectl</code> 命令进行扩展，从而编写一些自己的插件集成进 <code>kubectl</code> 命令中；**<code>kubectl</code> 插件机制是与语言无关的，也就是说你可以用任何语言编写插件，可以是 <code>bash</code>、<code>python</code> 脚本，也可以是 <code>go</code>、<code>java</code> 等编译型语言；所以选择你熟悉的语言即可**，以下是一个用 <code>go</code> 编写的用于快速切换 NameSpace 的小插件，运行截图如下:</p><p><strong>所谓: 开局一张图，功能全靠编 😂</strong><br><img src="https://cdn.oss.link/markdown/6t89g.gif" alt="swns.gif"></p><p>当前插件代码放在 <a href="https://github.com/mritd/swns">mritd/swns</a> 这个项目下面</p><h2 id="二、插件加载"><a href="#二、插件加载" class="headerlink" title="二、插件加载"></a>二、插件加载</h2><p><code>kubectl</code> 插件机制目前并不提供包管理器一样的功能，比如你想执行 <code>kuebctl plugin install xxx</code> 这种操作目前还没有实现(个人感觉差个规范)；所以一旦我们编写或者下载一个插件后，我们只有正确放在特定目录才会生效；</p><p><strong>目前插件根据文档描述只有两部分内容: <code>plugin.yaml</code> 和其依赖的二进制/脚本等可执行文件</strong>；根据文档说明，<code>kubectl</code> 会尝试在如下位置查找并加载插件，所以我们只需要将 <code>plugin.yaml</code> 和相关二进制放在在对应位置即可:</p><ul><li><code>$&#123;KUBECTL_PLUGINS_PATH&#125;</code>: 如果这个环境变量定义了，那么 <code>kubectl</code> <strong>只会</strong>从这里查找；<strong>注意: 这个变量可以是多个目录，类似 PATH 变量一样，做好分割即可</strong></li><li><code>$&#123;XDG_DATA_DIRS&#125;/kubectl/plugins</code>: 关于这个变量具体请看 <a href="https://specifications.freedesktop.org/basedir-spec/basedir-spec-latest.html">XDG System Directory Structure</a>，我了解也不多；<strong>如果这个变量没定义则默认为 <code>/usr/local/share:/usr/share</code></strong></li><li><code>~/.kube/plugins</code>: 这个没啥可说的，我推荐还是将插件放在这个位置比较友好一点</li></ul><p>所以最终插件目录结构类似这样:</p><div class="hljs code-wrapper"><pre><code class="hljs sh">➜  ~ tree .kube.kube├── config└── plugins    └── swns        ├── plugin.yaml        └── swns</code></pre></div><h2 id="三、Plugin-yaml"><a href="#三、Plugin-yaml" class="headerlink" title="三、Plugin.yaml"></a>三、Plugin.yaml</h2><p><code>plugin.yaml</code> 这个文件实际上才是插件的核心，在这个文件里声明了插件如何使用、调用的二进制/脚本等重要配置；<strong>一个插件可以没有任何脚本/二进制可执行文件，但至少应当有一个 <code>plugin.yaml</code> 描述文件</strong>；目前 <code>plugin.yaml</code> 的结构如下:</p><div class="hljs code-wrapper"><pre><code class="hljs yaml"><span class="hljs-attr">name:</span> <span class="hljs-string">&quot;targaryen&quot;</span>                 <span class="hljs-comment"># 必填项: 用于 kuebctl 调用的插件名称</span><span class="hljs-attr">shortDesc:</span> <span class="hljs-string">&quot;Dragonized plugin&quot;</span>    <span class="hljs-comment"># 必填项: 用于 help 该插件时的简短描述</span><span class="hljs-attr">longDesc:</span> <span class="hljs-string">&quot;&quot;</span>                      <span class="hljs-comment"># 非必填: 插件的长描述</span><span class="hljs-attr">example:</span> <span class="hljs-string">&quot;&quot;</span>                       <span class="hljs-comment"># 非必填: 插件的使用样例</span><span class="hljs-attr">command:</span> <span class="hljs-string">&quot;./dracarys&quot;</span>             <span class="hljs-comment"># 必填项: 插件实际执行的文件位置，可以相对路径 or 绝对路径，或者在 PATH 里也行</span><span class="hljs-attr">flags:</span>                            <span class="hljs-comment"># 非必填: 插件支持的 flag</span>  <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">&quot;heat&quot;</span>                  <span class="hljs-comment"># 必填项: 如果你写了支持的 flag，那么此项必填</span>    <span class="hljs-attr">shorthand:</span> <span class="hljs-string">&quot;h&quot;</span>                <span class="hljs-comment"># 非必填: 该选项的缩短形式</span>    <span class="hljs-attr">desc:</span> <span class="hljs-string">&quot;Fire heat&quot;</span>             <span class="hljs-comment"># 必填项: 同样每个 flag 都必须书写描述</span>    <span class="hljs-attr">defValue:</span> <span class="hljs-string">&quot;extreme&quot;</span>           <span class="hljs-comment"># 非必填: 默认值</span><span class="hljs-attr">tree:</span>                             <span class="hljs-comment"># 允许定义一些子命令</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">...</span>                           <span class="hljs-comment"># 子命令支持同样的设置属性(我想知道子命令的子命令的子命令支不支持...我还没去试过)</span></code></pre></div><h2 id="四、插件环境变量"><a href="#四、插件环境变量" class="headerlink" title="四、插件环境变量"></a>四、插件环境变量</h2><p>在编写插件时，<strong>有时插件运行时需要获取到一些参数，比如 <code>kubectl</code> 执行时的全局 flag 等，</strong>为了方便插件开发者，<code>kuebctl</code> 的插件机制提供一些预置的环境变量方便我们读取；即如果你用 <code>bash</code> 写插件，那么这些变量你只需要 <code>$&#123;xxxx&#125;</code> 即可拿到，然后做一些你想做的事情；这些变量目前支持如下:</p><ul><li><code>KUBECTL_PLUGINS_CALLER</code>: <code>kubectl</code> 二进制文件所在位置；<strong>作为插件编写者，我们无需关系 api server 是否能联通，因为配置是否正确应当由使用者决定；在需要时我们只需要直接调用 <code>kubectl</code> 即可；</strong>比如在 <code>bash</code> 脚本中执行 <code>get pod</code> 等</li><li><code>KUBECTL_PLUGINS_CURRENT_NAMESPACE</code>: 当前 <code>kuebctl</code> 命令所对应的 NameSpace，<strong>插件机制确保了该值一定正确；即这是经过解析了 <code>--namespace</code> 选项或者 <code>kubeconfig</code> 配置后的最终结果；作为插件编写者，我们无需关心处理过程</strong>；想详细了解的的可以去看源码，以及 <code>Cobra</code> 库(Kubernetes 用这个库解析命令行参数和配置)</li><li><code>KUBECTL_PLUGINS_DESCRIPTOR_*</code>: 插件自己本身位于 <code>plugin.yaml</code> 中的描述信息，比如 <code>KUBECTL_PLUGINS_DESCRIPTOR_NAME</code> 输出 <code>plugin.yaml</code> 下的 <code>name</code> 属性；一般可以用作插件输出自己的帮助文档等</li><li><code>KUBECTL_PLUGINS_GLOBAL_FLAG_*</code>: 获取 <code>kubectl</code> 所有全局 flag 值的变量，比如 <code>KUBECTL_PLUGINS_GLOBAL_FLAG_NAMESPACE</code> 能拿到 <code>--namespace</code> 选项的值</li><li><code>KUBECTL_PLUGINS_LOCAL_FLAG_*</code>: 同上面类似，只不过这个是获取插件自己本身 flag 的值，个人认为在脚本语言中，比如 <code>bash</code> 等处理选项不怎么好用时，可以考虑直接从变量拿</li></ul><p>以上变量我并未都测试，具体以测试为准，<strong>删库跑路等情况本人概不负责</strong></p><h2 id="五、写一个切换-NameSpace-的插件"><a href="#五、写一个切换-NameSpace-的插件" class="headerlink" title="五、写一个切换 NameSpace 的插件"></a>五、写一个切换 NameSpace 的插件</h2><p>前面墨迹一大堆只是为了描述清楚 <strong>要写一个插件应该怎么干</strong> 的问题，下面开始 <strong>这么干</strong></p><h3 id="5-1、编写配置"><a href="#5-1、编写配置" class="headerlink" title="5.1、编写配置"></a>5.1、编写配置</h3><p>上面已经介绍好了 <code>plugin.yaml</code> 怎么写，那么根据我自己的需求，我写的这个切换 NameSpace 插件的名字暂且叫做 <code>swns</code>；我希望 <code>swns</code> 执行后接受一个 NameSpace 的字符串，然后调用 <code>kuebctl config</code> 去设置当前默认的 NameSpace，这样在后续命令中我就不用再一直加个 <code>-n xxx</code> 参数了；同时我希望使用更方便点，当执行 <code>swns</code> 命令时，如果不提供 NameSpace 的字符串，那我就弹出下拉列表供用户选择；综上需求自己想明白后，就写一个 <code>plugin.yaml</code>，如下:</p><div class="hljs code-wrapper"><pre><code class="hljs yaml"><span class="hljs-attr">name:</span> <span class="hljs-string">&quot;swns&quot;</span><span class="hljs-attr">shortDesc:</span> <span class="hljs-string">&quot;Switch NameSpace&quot;</span><span class="hljs-attr">longDesc:</span> <span class="hljs-string">&quot;Switch Kubernetes current context namespace.&quot;</span><span class="hljs-attr">example:</span> <span class="hljs-string">&quot;kubectl plugin swns [NAMESPACE]&quot;</span><span class="hljs-attr">command:</span> <span class="hljs-string">&quot;./swns&quot;</span></code></pre></div><h3 id="5-2、编写插件"><a href="#5-2、编写插件" class="headerlink" title="5.2、编写插件"></a>5.2、编写插件</h3><p>上面 <code>plugin.yaml</code> 已经定义好了，那么接下来就简单了，撸代码实现了就好；代码如下:</p><div class="hljs code-wrapper"><pre><code class="hljs go"><span class="hljs-comment">// 注意: 下面的模板语法大括号中间没有空格，此处空格是为了防止博客渲染出错</span><span class="hljs-keyword">package</span> main<span class="hljs-keyword">import</span> (<span class="hljs-string">&quot;fmt&quot;</span><span class="hljs-string">&quot;os&quot;</span><span class="hljs-string">&quot;os/exec&quot;</span><span class="hljs-string">&quot;strings&quot;</span><span class="hljs-string">&quot;github.com/mritd/promptx&quot;</span>)<span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-title">main</span><span class="hljs-params">()</span></span> &#123;<span class="hljs-comment">// 先拿到当前的 context</span>cmd := exec.Command(<span class="hljs-string">&quot;kubectl&quot;</span>, <span class="hljs-string">&quot;config&quot;</span>, <span class="hljs-string">&quot;current-context&quot;</span>)cmd.Stdin = os.Stdincmd.Stderr = os.Stderrb, err := cmd.Output()checkAndExit(err)currentContext := strings.TrimSpace(<span class="hljs-keyword">string</span>(b))<span class="hljs-comment">// 如果提供了 NameSpace 字符串，我直接改就行了</span><span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(os.Args) &gt; <span class="hljs-number">1</span> &#123;cmd = exec.Command(<span class="hljs-string">&quot;kubectl&quot;</span>, <span class="hljs-string">&quot;config&quot;</span>, <span class="hljs-string">&quot;set-context&quot;</span>, currentContext, <span class="hljs-string">&quot;--namespace=&quot;</span>+os.Args[<span class="hljs-number">1</span>])cmd.Stdout = os.StdoutcheckAndExit(cmd.Run())fmt.Printf(<span class="hljs-string">&quot;Kubernetes namespace switch to %s.\n&quot;</span>, os.Args[<span class="hljs-number">1</span>])&#125; <span class="hljs-keyword">else</span> &#123;<span class="hljs-comment">// 没提供我就得先把所有的 NameSpace 弄出来</span>cmd = exec.Command(<span class="hljs-string">&quot;kubectl&quot;</span>, <span class="hljs-string">&quot;get&quot;</span>, <span class="hljs-string">&quot;ns&quot;</span>, <span class="hljs-string">&quot;-o&quot;</span>, <span class="hljs-string">&quot;template&quot;</span>, <span class="hljs-string">&quot;--template&quot;</span>, <span class="hljs-string">&quot;&#123; &#123; range .items &#125; &#125;&#123; &#123; .metadata.name &#125; &#125; &#123; &#123; end &#125; &#125;&quot;</span>)b, err = cmd.Output()checkAndExit(err)allNameSpace := strings.Fields(<span class="hljs-keyword">string</span>(b))<span class="hljs-comment">// 弄到所有的 NameSpace 后，我在弄一个下拉列表(这是我自己造的一个下拉列表库)</span>cfg := &amp;promptx.SelectConfig&#123;ActiveTpl:    <span class="hljs-string">&quot;»  &#123; &#123; . | cyan &#125; &#125;&quot;</span>,InactiveTpl:  <span class="hljs-string">&quot;  &#123; &#123; . | white &#125; &#125;&quot;</span>,SelectPrompt: <span class="hljs-string">&quot;NameSpace&quot;</span>,SelectedTpl:  <span class="hljs-string">&quot;&#123; &#123; \&quot;» \&quot; | green &#125; &#125;&#123; &#123;\&quot;NameSpace:\&quot; | cyan &#125; &#125; &#123; &#123; . &#125; &#125;&quot;</span>,DisPlaySize:  <span class="hljs-number">9</span>,DetailsTpl:   <span class="hljs-string">` `</span>,&#125;s := &amp;promptx.Select&#123;Items:  allNameSpace,Config: cfg,&#125;<span class="hljs-comment">// 用户选中一个 NameSpace 后我就拿到了想要设置的 NameSpace 字符串</span>selectNameSpace := allNameSpace[s.Run()]<span class="hljs-comment">// 跟上面套路一样，写进去就行了</span>cmd = exec.Command(<span class="hljs-string">&quot;kubectl&quot;</span>, <span class="hljs-string">&quot;config&quot;</span>, <span class="hljs-string">&quot;set-context&quot;</span>, currentContext, <span class="hljs-string">&quot;--namespace=&quot;</span>+selectNameSpace)cmd.Stdout = os.StdoutcheckAndExit(cmd.Run())fmt.Printf(<span class="hljs-string">&quot;Kubernetes namespace switch to %s.\n&quot;</span>, selectNameSpace)&#125;&#125;<span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-title">checkErr</span><span class="hljs-params">(err error)</span> <span class="hljs-title">bool</span></span> &#123;<span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;fmt.Println(err)<span class="hljs-keyword">return</span> <span class="hljs-literal">false</span>&#125;<span class="hljs-keyword">return</span> <span class="hljs-literal">true</span>&#125;<span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-title">checkAndExit</span><span class="hljs-params">(err error)</span></span> &#123;<span class="hljs-keyword">if</span> !checkErr(err) &#123;os.Exit(<span class="hljs-number">1</span>)&#125;&#125;</code></pre></div><p>最后编译后放到上面所说的插件加载目录即可</p><p>到此，**”全局一张图，功能全靠编”** 图上面也有了，编的的也差不多 😂</p>]]></content>
    
    
    <summary type="html">最近忙的晕头转向，博客停更了 1 个月，感觉对不起党、对不起人民、对不起 ~~CCAV~~...不过在忙的时候操作 Kubernetes 集群要频繁的使用 `kubectl` 命令，而在多个 NameSpace 下来回切换每次都得加个 `-n` 简直让我想打人；索性翻了下 `kubectl` 的插件机制，顺便写了一个快速切换 NameSpace 的小插件，以下记录一下插件编写过程</summary>
    
    
    
    <category term="Kubernetes" scheme="https://mritd.com/categories/kubernetes/"/>
    
    
    <category term="Kubernetes" scheme="https://mritd.com/tags/kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>Traefik 另类的服务暴露方式</title>
    <link href="https://mritd.com/2018/05/24/kubernetes-traefik-service-exposure/"/>
    <id>https://mritd.com/2018/05/24/kubernetes-traefik-service-exposure/</id>
    <published>2018-05-24T15:53:09.000Z</published>
    <updated>2018-05-24T15:53:09.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>最近准备重新折腾一下 Kubernetes 的服务暴露方式，以前的方式是彻底剥离 Kubenretes 本身的服务发现，然后改动应用实现 应用+Consul+Fabio 的服务暴露方式；总感觉这种方式不算优雅，所以折腾了一下 Traefik，试了下效果还不错，以下记录了使用 Traefik 的新的服务暴露方式(本文仅针对 HTTP 协议)；</p></blockquote><h2 id="一、Traefik-服务暴露方案"><a href="#一、Traefik-服务暴露方案" class="headerlink" title="一、Traefik 服务暴露方案"></a>一、Traefik 服务暴露方案</h2><h3 id="1-1、以前的-Consul-Fabio-方案"><a href="#1-1、以前的-Consul-Fabio-方案" class="headerlink" title="1.1、以前的 Consul+Fabio 方案"></a>1.1、以前的 Consul+Fabio 方案</h3><p>以前的服务暴露方案是修改应用代码，使其能对接 Consul，然后 Consul 负责健康监测，检测通过后 Fabio 负责读取，最终上层 Nginx 将流量打到 Fabio 上，Fabio 再将流量路由到健康的 Pod 上；总体架构如下</p><p><img src="https://cdn.oss.link/markdown/hkwp3.png" alt="consul+fabio"></p><p>这种架构目前有几点不太好的地方，首先是必须应用能成功集成 Consul，需要动应用代码不通用；其次组件过多增加维护成本，尤其是调用链日志不好追踪；这里面需要吐槽下 Consul 和 Fabio，Consul 的集群设计模式要想做到完全的 HA 那么需要在每个 pod 中启动一个 agent，因为只要这个 agent 挂了那么集群认为其上所有注册服务都挂了，这点很恶心人；而 Fabio 的日志目前好像还是不支持合理的输出，好像只能 stdout；目前来看不论是组件复杂度还是维护成本都不怎么友好</p><h3 id="1-2、新的-Traefik-方案"><a href="#1-2、新的-Traefik-方案" class="headerlink" title="1.2、新的 Traefik 方案"></a>1.2、新的 Traefik 方案</h3><p>使用 Traefik 首先想到就是直接怼 Ingress，这个确实方便也简单；但是在集群 kube-proxy 不走 ipvs 的情况下 iptables 性能确实堪忧；虽说 Traefik 会直连 Pod，但是你 Ingress 暴露 80、443 端口在本机没有对应 Ingress Controller 的情况下还是会走一下 iptables；<strong>不论是换 kube-router、kube-proxy 走 ipvs 都不是我想要的，我们需要一种完全远离 Kubernetes Service 的新路子</strong>；在看 Traefik 文档的时候，其中说明了 Traefik 只利用 Kubernetes 的 API 来读取相关后端数据，那么我们就可以以此来使用如下的套路</p><p><img src="https://cdn.oss.link/markdown/tfo2f.png" alt="traefik"></p><p>这个套路的方案很简单，<strong>将 Traefik 部署在物理机上，让其直连 Kubernets api 以读取 Ingress 配置和 Pod IP 等信息，然后在这几台物理机上部署好 Kubernetes 的网络组件使其能直连 Pod IP</strong>；这种方案能够让流量经过 Traefik 直接路由到后端 Pod，健康检测还是由集群来做；<strong>由于 Traefik 连接 Kubernetes api 需要获取一些数据；所以在集群内还是像往常一样创建 Ingress，只不过此时我们并没有 Ingress Controller；这样避免了经过 iptables 转发，不占用全部集群机器的 80、443 端口，同时还能做到高可控</strong></p><h2 id="二、Traefik-部署"><a href="#二、Traefik-部署" class="headerlink" title="二、Traefik 部署"></a>二、Traefik 部署</h2><p>部署之前首先需要有一个正常访问的集群，然后在另外几台机器上部署 Kubernetes 的网络组件；最终要保证另外几台机器能够直接连通 Pod 的 IP，我这里偷懒直接在 Kubernetes 的其中几个 Node 上部署 Traefik</p><h3 id="2-1、Docker-Compose"><a href="#2-1、Docker-Compose" class="headerlink" title="2.1、Docker Compose"></a>2.1、Docker Compose</h3><p>Traefik 的 Docker Compose 如下</p><div class="hljs code-wrapper"><pre><code class="hljs yml"><span class="hljs-attr">version:</span> <span class="hljs-string">&#x27;3.5&#x27;</span><span class="hljs-attr">services:</span>  <span class="hljs-attr">traefik:</span>    <span class="hljs-attr">image:</span> <span class="hljs-string">traefik:v1.6.1-alpine</span>    <span class="hljs-attr">container_name:</span> <span class="hljs-string">traefik</span>    <span class="hljs-attr">command:</span> <span class="hljs-string">--configFile=/etc/traefik.toml</span>    <span class="hljs-attr">ports:</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">&quot;2080:2080&quot;</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">&quot;2180:2180&quot;</span>    <span class="hljs-attr">volumes:</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">./traefik.toml:/etc/traefik.toml</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">./k8s-root-ca.pem:/etc/kubernetes/ssl/k8s-root-ca.pem</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">./log:/var/log/traefik</span></code></pre></div><p>由于 Kubernetes 集群开启了 RBAC 认证同时采用 TLS 通讯，所以需要挂载 Kubernetes CA 证书，还需要为 Traefik 创建对应的 RBAC 账户以使其能够访问 Kubernetes API</p><h3 id="2-2、创建-RBAC-账户"><a href="#2-2、创建-RBAC-账户" class="headerlink" title="2.2、创建 RBAC 账户"></a>2.2、创建 RBAC 账户</h3><p>Traefik 连接 Kubernetes API 时需要使用 Service Account 的 Token，Service Account 以及 ClusterRole 等配置具体见 <a href="https://docs.traefik.io/user-guide/kubernetes/">官方文档</a>，下面是我从当前版本的文档中 Copy 出来的</p><div class="hljs code-wrapper"><pre><code class="hljs yml"><span class="hljs-meta">---</span><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span><span class="hljs-attr">kind:</span> <span class="hljs-string">ServiceAccount</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">traefik-ingress-controller</span>  <span class="hljs-attr">namespace:</span> <span class="hljs-string">kube-system</span><span class="hljs-meta">---</span><span class="hljs-attr">kind:</span> <span class="hljs-string">ClusterRole</span><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">rbac.authorization.k8s.io/v1beta1</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">traefik-ingress-controller</span><span class="hljs-attr">rules:</span>  <span class="hljs-bullet">-</span> <span class="hljs-attr">apiGroups:</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">&quot;&quot;</span>    <span class="hljs-attr">resources:</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">services</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">endpoints</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">secrets</span>    <span class="hljs-attr">verbs:</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">get</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">list</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">watch</span>  <span class="hljs-bullet">-</span> <span class="hljs-attr">apiGroups:</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">extensions</span>    <span class="hljs-attr">resources:</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">ingresses</span>    <span class="hljs-attr">verbs:</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">get</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">list</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">watch</span><span class="hljs-meta">---</span><span class="hljs-attr">kind:</span> <span class="hljs-string">ClusterRoleBinding</span><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">rbac.authorization.k8s.io/v1beta1</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">traefik-ingress-controller</span><span class="hljs-attr">roleRef:</span>  <span class="hljs-attr">apiGroup:</span> <span class="hljs-string">rbac.authorization.k8s.io</span>  <span class="hljs-attr">kind:</span> <span class="hljs-string">ClusterRole</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">traefik-ingress-controller</span><span class="hljs-attr">subjects:</span><span class="hljs-bullet">-</span> <span class="hljs-attr">kind:</span> <span class="hljs-string">ServiceAccount</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">traefik-ingress-controller</span>  <span class="hljs-attr">namespace:</span> <span class="hljs-string">kube-system</span></code></pre></div><p>创建好以后需要提取 Service Account 的 Token 方便下面使用，提取命令如下</p><div class="hljs code-wrapper"><pre><code class="hljs sh">kubectl describe secret -n kube-system $(kubectl get secrets -n kube-system | grep traefik-ingress-controller | cut -f1 -d <span class="hljs-string">&#x27; &#x27;</span>) | grep -E <span class="hljs-string">&#x27;^token&#x27;</span></code></pre></div><h3 id="2-3、创建-Traefik-配置"><a href="#2-3、创建-Traefik-配置" class="headerlink" title="2.3、创建 Traefik 配置"></a>2.3、创建 Traefik 配置</h3><p>Traefik 的具体配置细节请参考 <a href="https://docs.traefik.io/configuration/commons/">官方文档</a>，以下仅给出一个样例配置</p><div class="hljs code-wrapper"><pre><code class="hljs toml"><span class="hljs-comment"># DEPRECATED - for general usage instruction see [lifeCycle.graceTimeOut].</span><span class="hljs-comment">#</span><span class="hljs-comment"># If both the deprecated option and the new one are given, the deprecated one</span><span class="hljs-comment"># takes precedence.</span><span class="hljs-comment"># A value of zero is equivalent to omitting the parameter, causing</span><span class="hljs-comment"># [lifeCycle.graceTimeOut] to be effective. Pass zero to the new option in</span><span class="hljs-comment"># order to disable the grace period.</span><span class="hljs-comment">#</span><span class="hljs-comment"># Optional</span><span class="hljs-comment"># Default: &quot;0s&quot;</span><span class="hljs-comment">#</span><span class="hljs-comment"># graceTimeOut = &quot;10s&quot;</span><span class="hljs-comment"># Enable debug mode.</span><span class="hljs-comment"># This will install HTTP handlers to expose Go expvars under /debug/vars and</span><span class="hljs-comment"># pprof profiling data under /debug/pprof.</span><span class="hljs-comment"># The log level will be set to DEBUG unless `logLevel` is specified.</span><span class="hljs-comment">#</span><span class="hljs-comment"># Optional</span><span class="hljs-comment"># Default: false</span><span class="hljs-comment">#</span><span class="hljs-comment"># debug = true</span><span class="hljs-comment"># Periodically check if a new version has been released.</span><span class="hljs-comment">#</span><span class="hljs-comment"># Optional</span><span class="hljs-comment"># Default: true</span><span class="hljs-comment">#</span><span class="hljs-attr">checkNewVersion</span> = <span class="hljs-literal">false</span><span class="hljs-comment"># Backends throttle duration.</span><span class="hljs-comment">#</span><span class="hljs-comment"># Optional</span><span class="hljs-comment"># Default: &quot;2s&quot;</span><span class="hljs-comment">#</span><span class="hljs-comment"># providersThrottleDuration = &quot;2s&quot;</span><span class="hljs-comment"># Controls the maximum idle (keep-alive) connections to keep per-host.</span><span class="hljs-comment">#</span><span class="hljs-comment"># Optional</span><span class="hljs-comment"># Default: 200</span><span class="hljs-comment">#</span><span class="hljs-comment"># maxIdleConnsPerHost = 200</span><span class="hljs-comment"># If set to true invalid SSL certificates are accepted for backends.</span><span class="hljs-comment"># This disables detection of man-in-the-middle attacks so should only be used on secure backend networks.</span><span class="hljs-comment">#</span><span class="hljs-comment"># Optional</span><span class="hljs-comment"># Default: false</span><span class="hljs-comment">#</span><span class="hljs-comment"># insecureSkipVerify = true</span><span class="hljs-comment"># Register Certificates in the rootCA.</span><span class="hljs-comment">#</span><span class="hljs-comment"># Optional</span><span class="hljs-comment"># Default: []</span><span class="hljs-comment">#</span><span class="hljs-comment"># rootCAs = [ &quot;/mycert.cert&quot; ]</span><span class="hljs-comment"># Entrypoints to be used by frontends that do not specify any entrypoint.</span><span class="hljs-comment"># Each frontend can specify its own entrypoints.</span><span class="hljs-comment">#</span><span class="hljs-comment"># Optional</span><span class="hljs-comment"># Default: [&quot;http&quot;]</span><span class="hljs-comment">#</span><span class="hljs-comment"># defaultEntryPoints = [&quot;http&quot;, &quot;https&quot;]</span><span class="hljs-comment"># Allow the use of 0 as server weight.</span><span class="hljs-comment"># - false: a weight 0 means internally a weight of 1.</span><span class="hljs-comment"># - true: a weight 0 means internally a weight of 0 (a server with a weight of 0 is removed from the available servers).</span><span class="hljs-comment">#</span><span class="hljs-comment"># Optional</span><span class="hljs-comment"># Default: false</span><span class="hljs-comment">#</span><span class="hljs-comment"># AllowMinWeightZero = true</span><span class="hljs-attr">logLevel</span> = <span class="hljs-string">&quot;INFO&quot;</span><span class="hljs-section">[traefikLog]</span>  <span class="hljs-attr">filePath</span> = <span class="hljs-string">&quot;/var/log/traefik/traefik.log&quot;</span>  <span class="hljs-attr">format</span>   = <span class="hljs-string">&quot;json&quot;</span><span class="hljs-section">[accessLog]</span>  <span class="hljs-attr">filePath</span> = <span class="hljs-string">&quot;/var/log/traefik/access.log&quot;</span>  <span class="hljs-attr">format</span> = <span class="hljs-string">&quot;json&quot;</span>  <span class="hljs-section">[accessLog.filters]</span>    <span class="hljs-attr">statusCodes</span> = [<span class="hljs-string">&quot;200-511&quot;</span>]    <span class="hljs-attr">retryAttempts</span> = <span class="hljs-literal">true</span><span class="hljs-comment">#  [accessLog.fields]</span><span class="hljs-comment">#    defaultMode = &quot;keep&quot;</span><span class="hljs-comment">#    [accessLog.fields.names]</span><span class="hljs-comment">#      &quot;ClientUsername&quot; = &quot;drop&quot;</span><span class="hljs-comment">#      # ...</span><span class="hljs-comment">#</span><span class="hljs-comment">#    [accessLog.fields.headers]</span><span class="hljs-comment">#      defaultMode = &quot;keep&quot;</span><span class="hljs-comment">#      [accessLog.fields.headers.names]</span><span class="hljs-comment">#        &quot;User-Agent&quot; = &quot;redact&quot;</span><span class="hljs-comment">#        &quot;Authorization&quot; = &quot;drop&quot;</span><span class="hljs-comment">#        &quot;Content-Type&quot; = &quot;keep&quot;</span><span class="hljs-comment">#        # ...</span><span class="hljs-section">[entryPoints]</span>  <span class="hljs-section">[entryPoints.http]</span>    <span class="hljs-attr">address</span> = <span class="hljs-string">&quot;:2080&quot;</span>    <span class="hljs-attr">compress</span> = <span class="hljs-literal">true</span>  <span class="hljs-section">[entryPoints.traefik]</span>    <span class="hljs-attr">address</span> = <span class="hljs-string">&quot;:2180&quot;</span>    <span class="hljs-attr">compress</span> = <span class="hljs-literal">true</span><span class="hljs-comment">#    [entryPoints.http.whitelist]</span><span class="hljs-comment">#      sourceRange = [&quot;192.168.1.0/24&quot;]</span><span class="hljs-comment">#      useXForwardedFor = true</span><span class="hljs-comment">#    [entryPoints.http.tls]</span><span class="hljs-comment">#      minVersion = &quot;VersionTLS12&quot;</span><span class="hljs-comment">#      cipherSuites = [</span><span class="hljs-comment">#        &quot;TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256&quot;,</span><span class="hljs-comment">#        &quot;TLS_RSA_WITH_AES_256_GCM_SHA384&quot;</span><span class="hljs-comment">#       ]</span><span class="hljs-comment">#      [[entryPoints.http.tls.certificates]]</span><span class="hljs-comment">#        certFile = &quot;path/to/my.cert&quot;</span><span class="hljs-comment">#        keyFile = &quot;path/to/my.key&quot;</span><span class="hljs-comment">#      [[entryPoints.http.tls.certificates]]</span><span class="hljs-comment">#        certFile = &quot;path/to/other.cert&quot;</span><span class="hljs-comment">#        keyFile = &quot;path/to/other.key&quot;</span><span class="hljs-comment">#      # ...</span><span class="hljs-comment">#      [entryPoints.http.tls.clientCA]</span><span class="hljs-comment">#        files = [&quot;path/to/ca1.crt&quot;, &quot;path/to/ca2.crt&quot;]</span><span class="hljs-comment">#        optional = false</span><span class="hljs-comment">#</span><span class="hljs-comment">#    [entryPoints.http.redirect]</span><span class="hljs-comment">#      entryPoint = &quot;https&quot;</span><span class="hljs-comment">#      regex = &quot;^http://localhost/(.*)&quot;</span><span class="hljs-comment">#      replacement = &quot;http://mydomain/$1&quot;</span><span class="hljs-comment">#      permanent = true</span><span class="hljs-comment">#</span><span class="hljs-comment">#    [entryPoints.http.auth]</span><span class="hljs-comment">#      headerField = &quot;X-WebAuth-User&quot;</span><span class="hljs-comment">#      [entryPoints.http.auth.basic]</span><span class="hljs-comment">#        users = [</span><span class="hljs-comment">#          &quot;test:$apr1$H6uskkkW$IgXLP6ewTrSuBkTrqE8wj/&quot;,</span><span class="hljs-comment">#          &quot;test2:$apr1$d9hr9HBB$4HxwgUir3HP4EsggP/QNo0&quot;,</span><span class="hljs-comment">#        ]</span><span class="hljs-comment">#        usersFile = &quot;/path/to/.htpasswd&quot;</span><span class="hljs-comment">#      [entryPoints.http.auth.digest]</span><span class="hljs-comment">#        users = [</span><span class="hljs-comment">#          &quot;test:traefik:a2688e031edb4be6a3797f3882655c05&quot;,</span><span class="hljs-comment">#          &quot;test2:traefik:518845800f9e2bfb1f1f740ec24f074e&quot;,</span><span class="hljs-comment">#        ]</span><span class="hljs-comment">#        usersFile = &quot;/path/to/.htdigest&quot;</span><span class="hljs-comment">#      [entryPoints.http.auth.forward]</span><span class="hljs-comment">#        address = &quot;https://authserver.com/auth&quot;</span><span class="hljs-comment">#        trustForwardHeader = true</span><span class="hljs-comment">#        [entryPoints.http.auth.forward.tls]</span><span class="hljs-comment">#          ca =  [ &quot;path/to/local.crt&quot;]</span><span class="hljs-comment">#          caOptional = true</span><span class="hljs-comment">#          cert = &quot;path/to/foo.cert&quot;</span><span class="hljs-comment">#          key = &quot;path/to/foo.key&quot;</span><span class="hljs-comment">#          insecureSkipVerify = true</span><span class="hljs-comment">#</span><span class="hljs-comment">#    [entryPoints.http.proxyProtocol]</span><span class="hljs-comment">#      insecure = true</span><span class="hljs-comment">#      trustedIPs = [&quot;10.10.10.1&quot;, &quot;10.10.10.2&quot;]</span><span class="hljs-comment">#</span><span class="hljs-comment">#    [entryPoints.http.forwardedHeaders]</span><span class="hljs-comment">#      trustedIPs = [&quot;10.10.10.1&quot;, &quot;10.10.10.2&quot;]</span><span class="hljs-comment">#</span><span class="hljs-comment">#  [entryPoints.https]</span><span class="hljs-comment">#    # ...</span><span class="hljs-comment">################################################################</span><span class="hljs-comment"># Kubernetes Ingress configuration backend</span><span class="hljs-comment">################################################################</span><span class="hljs-comment"># Enable Kubernetes Ingress configuration backend.</span><span class="hljs-section">[kubernetes]</span><span class="hljs-comment"># Kubernetes server endpoint.</span><span class="hljs-comment">#</span><span class="hljs-comment"># Optional for in-cluster configuration, required otherwise.</span><span class="hljs-comment"># Default: empty</span><span class="hljs-comment">#</span><span class="hljs-attr">endpoint</span> = <span class="hljs-string">&quot;https://172.16.0.36:6443&quot;</span><span class="hljs-comment"># Bearer token used for the Kubernetes client configuration.</span><span class="hljs-comment">#</span><span class="hljs-comment"># Optional</span><span class="hljs-comment"># Default: empty</span><span class="hljs-comment">#</span><span class="hljs-attr">token</span> = <span class="hljs-string">&quot;eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJ0cmFlZmlrLWluZ3Jlc3MtY29udHJvbGxlci10b2tlbi1zbm5iNSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50Lm5hbWUiOiJ0cmFlZmlrLWlyZ3Jlc3MtY29udHJvbGxlciIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50LnVpZCI6ImE4NmI3YWEzLTVmNjQtMTFlOC1hZjYxLWM4MWY2NmUwMzRhNyIsInN1YiI6InN5c3RlbTpxZXJ2aWNlYWNjb3VudDPrdWJlLXN5c3RlbTp0cmFlZmlrLWluZ3Jlc3MtY29udHJvbGxlciJ9.vOFEITuANWGnkER8gukWkTs54BmHXqNpzM55bOb5qXPmI3pZsbei3gtE6tZoqME9P5Lb85cav-8mGZJcoQqqxNBkZJ1YRqy_1O9Apkxa4jA68ipe_NB3L5-exH5cEIrU8iql_r7ycDaKwzsMnAWGPolp1dRkF31u5u8g68oLwF3GR8Z5g4_tLJlTvA53doX7k6Wd6vUygTS3EaQ_qvfXwbcIeaSdWWo2Mym6O0CvIap4jH2w21MbredGURqkRlXEPezKAgRVkr75CdvuvwORnT8YxFLVwuAJs70V-13Ib9v6HAK64GmzcqkAuJtZT8NZKl8Y4TfRGl2_RMq2tk86gD4ShDMedcrto44ZUYHQccsSlpaW5PsN2KBBNPN0-6ca3jIpOmnJojAFUYGM42Wymnx9_4XwHUeeA18-RrercmOaRMdlNq8BzBomAxQB99TqUzRIqpe6m5OotXvouCUnE7qjMwRWmQ5LHjqUGEw_A1pHcalFXQZK0sOCaJOJZIJbc_8rVX-4uxkCBxoIXmzjq8x5a_xPsN4L0aWifkP6co--agw3kOT0O6my8T_CbcZGO9e3OqYPdT4FSl92XlXW8EXHdDpCUJ10aoqJGG2vZSud7IoDxkcScpkj3n6TvyvSRVtk3CtYiIYBpgi7-X2JKkun1a7yFpLogyazz9VlUE4&quot;</span><span class="hljs-comment"># Path to the certificate authority file.</span><span class="hljs-comment"># Used for the Kubernetes client configuration.</span><span class="hljs-comment">#</span><span class="hljs-comment"># Optional</span><span class="hljs-comment"># Default: empty</span><span class="hljs-comment">#</span><span class="hljs-attr">certAuthFilePath</span> = <span class="hljs-string">&quot;/etc/kubernetes/ssl/k8s-root-ca.pem&quot;</span><span class="hljs-comment"># Array of namespaces to watch.</span><span class="hljs-comment">#</span><span class="hljs-comment"># Optional</span><span class="hljs-comment"># Default: all namespaces (empty array).</span><span class="hljs-comment">#</span><span class="hljs-attr">namespaces</span> = [<span class="hljs-string">&quot;default&quot;</span>]<span class="hljs-comment"># Ingress label selector to filter Ingress objects that should be processed.</span><span class="hljs-comment">#</span><span class="hljs-comment"># Optional</span><span class="hljs-comment"># Default: empty (process all Ingresses)</span><span class="hljs-comment">#</span><span class="hljs-comment"># labelselector = &quot;A and not B&quot;</span><span class="hljs-comment"># Value of `kubernetes.io/ingress.class` annotation that identifies Ingress objects to be processed.</span><span class="hljs-comment"># If the parameter is non-empty, only Ingresses containing an annotation with the same value are processed.</span><span class="hljs-comment"># Otherwise, Ingresses missing the annotation, having an empty value, or the value `traefik` are processed.</span><span class="hljs-comment">#</span><span class="hljs-comment"># Note : `ingressClass` option must begin with the &quot;traefik&quot; prefix.</span><span class="hljs-comment">#</span><span class="hljs-comment"># Optional</span><span class="hljs-comment"># Default: empty</span><span class="hljs-comment">#</span><span class="hljs-comment"># ingressClass = &quot;traefik-internal&quot;</span><span class="hljs-comment"># Disable PassHost Headers.</span><span class="hljs-comment">#</span><span class="hljs-comment"># Optional</span><span class="hljs-comment"># Default: false</span><span class="hljs-comment">#</span><span class="hljs-comment"># disablePassHostHeaders = true</span><span class="hljs-comment"># Enable PassTLSCert Headers.</span><span class="hljs-comment">#</span><span class="hljs-comment"># Optional</span><span class="hljs-comment"># Default: false</span><span class="hljs-comment">#</span><span class="hljs-comment"># enablePassTLSCert = true</span><span class="hljs-comment"># Override default configuration template.</span><span class="hljs-comment">#</span><span class="hljs-comment"># Optional</span><span class="hljs-comment"># Default: &lt;built-in template&gt;</span><span class="hljs-comment">#</span><span class="hljs-comment"># filename = &quot;kubernetes.tmpl&quot;</span><span class="hljs-comment"># API definition</span><span class="hljs-section">[api]</span>  <span class="hljs-comment"># Name of the related entry point</span>  <span class="hljs-comment">#</span>  <span class="hljs-comment"># Optional</span>  <span class="hljs-comment"># Default: &quot;traefik&quot;</span>  <span class="hljs-comment">#</span>  <span class="hljs-attr">entryPoint</span> = <span class="hljs-string">&quot;traefik&quot;</span>  <span class="hljs-comment"># Enabled Dashboard</span>  <span class="hljs-comment">#</span>  <span class="hljs-comment"># Optional</span>  <span class="hljs-comment"># Default: true</span>  <span class="hljs-comment">#</span>  <span class="hljs-attr">dashboard</span> = <span class="hljs-literal">true</span>  <span class="hljs-comment"># Enable debug mode.</span>  <span class="hljs-comment"># This will install HTTP handlers to expose Go expvars under /debug/vars and</span>  <span class="hljs-comment"># pprof profiling data under /debug/pprof.</span>  <span class="hljs-comment"># Additionally, the log level will be set to DEBUG.</span>  <span class="hljs-comment">#</span>  <span class="hljs-comment"># Optional</span>  <span class="hljs-comment"># Default: false</span>  <span class="hljs-comment">#</span>  <span class="hljs-attr">debug</span> = <span class="hljs-literal">false</span><span class="hljs-comment"># Ping definition</span><span class="hljs-comment">#[ping]</span><span class="hljs-comment">#  # Name of the related entry point</span><span class="hljs-comment">#  #</span><span class="hljs-comment">#  # Optional</span><span class="hljs-comment">#  # Default: &quot;traefik&quot;</span><span class="hljs-comment">#  #</span><span class="hljs-comment">#  entryPoint = &quot;traefik&quot;</span></code></pre></div><h3 id="2-4、启动-Traefik"><a href="#2-4、启动-Traefik" class="headerlink" title="2.4、启动 Traefik"></a>2.4、启动 Traefik</h3><p>所有文件准备好以后直接执行 <code>docker-compose up -d</code> 启动即可，所有文件目录结构如下</p><div class="hljs code-wrapper"><pre><code class="hljs sh">traefik├── docker-compose.yaml├── k8s-root-ca.pem├── <span class="hljs-built_in">log</span>│   ├── access.log│   └── traefik.log├── rbac.yaml└── traefik.toml</code></pre></div><p>启动成功后可以访问 <code>http://IP:2180</code> 查看 Traefik 的控制面板</p><p><img src="https://cdn.oss.link/markdown/phrps.png" alt="dashboard"></p><h2 id="三、增加-Ingress-配置并测试"><a href="#三、增加-Ingress-配置并测试" class="headerlink" title="三、增加 Ingress 配置并测试"></a>三、增加 Ingress 配置并测试</h2><h3 id="3-1、增加-Ingress-配置"><a href="#3-1、增加-Ingress-配置" class="headerlink" title="3.1、增加 Ingress 配置"></a>3.1、增加 Ingress 配置</h3><p>虽然这种部署方式脱离了 Kubernetes 的 Service 与 Ingress 负载，但是 Traefik 还是需要通过 Kubernetes 的 Ingress 配置来确定后端负载规则，所以 Ingress 对象我们仍需照常创建；以下为一个 Demo 项目的 deployment、service、ingress 配置示例</p><ul><li>demo.deploy.yaml</li></ul><div class="hljs code-wrapper"><pre><code class="hljs yaml"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">apps/v1</span><span class="hljs-attr">kind:</span> <span class="hljs-string">Deployment</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">demo</span><span class="hljs-attr">spec:</span>  <span class="hljs-attr">replicas:</span> <span class="hljs-number">5</span>  <span class="hljs-attr">selector:</span>    <span class="hljs-attr">matchLabels:</span>      <span class="hljs-attr">app:</span> <span class="hljs-string">demo</span>  <span class="hljs-attr">template:</span>    <span class="hljs-attr">metadata:</span>      <span class="hljs-attr">labels:</span>        <span class="hljs-attr">app:</span> <span class="hljs-string">demo</span>    <span class="hljs-attr">spec:</span>      <span class="hljs-attr">containers:</span>      <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">demo</span>        <span class="hljs-attr">image:</span> <span class="hljs-string">mritd/demo</span>        <span class="hljs-attr">imagePullPolicy:</span> <span class="hljs-string">IfNotPresent</span>        <span class="hljs-attr">ports:</span>        <span class="hljs-bullet">-</span> <span class="hljs-attr">containerPort:</span> <span class="hljs-number">80</span></code></pre></div><ul><li>demo.svc.yaml</li></ul><div class="hljs code-wrapper"><pre><code class="hljs yaml"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span><span class="hljs-attr">kind:</span> <span class="hljs-string">Service</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">demo</span>  <span class="hljs-attr">labels:</span>    <span class="hljs-attr">svc:</span> <span class="hljs-string">demo</span><span class="hljs-attr">spec:</span>  <span class="hljs-attr">ports:</span>  <span class="hljs-bullet">-</span> <span class="hljs-attr">port:</span> <span class="hljs-number">8080</span>    <span class="hljs-attr">name:</span> <span class="hljs-string">http</span>    <span class="hljs-attr">targetPort:</span> <span class="hljs-number">80</span>  <span class="hljs-attr">selector:</span>    <span class="hljs-attr">app:</span> <span class="hljs-string">demo</span></code></pre></div><ul><li>demo.ing.yaml</li></ul><div class="hljs code-wrapper"><pre><code class="hljs yaml"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">extensions/v1beta1</span><span class="hljs-attr">kind:</span> <span class="hljs-string">Ingress</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">demo</span>  <span class="hljs-attr">annotations:</span>    <span class="hljs-attr">traefik.ingress.kubernetes.io/preserve-host:</span> <span class="hljs-string">&quot;true&quot;</span><span class="hljs-attr">spec:</span>  <span class="hljs-attr">rules:</span>  <span class="hljs-bullet">-</span> <span class="hljs-attr">host:</span> <span class="hljs-string">demo.mritd.me</span>    <span class="hljs-attr">http:</span>      <span class="hljs-attr">paths:</span>      <span class="hljs-bullet">-</span> <span class="hljs-attr">backend:</span>          <span class="hljs-attr">serviceName:</span> <span class="hljs-string">demo</span>          <span class="hljs-attr">servicePort:</span> <span class="hljs-number">8080</span></code></pre></div><h3 id="3-2、测试访问"><a href="#3-2、测试访问" class="headerlink" title="3.2、测试访问"></a>3.2、测试访问</h3><p>部署好后应当能从 Traefik 的 Dashboard 中看到新增的 demo ingress，如下所示</p><p><img src="https://cdn.oss.link/markdown/zociy.png" alt="dashboard-demo"></p><p>最后我们使用 curl 测试即可</p><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment"># 在不使用 Host 头的情况下 Traefik 会返 404(Traefik 根据 Host 路由后端，具体配置参考官方文档)</span>test36.node ➜  ~ curl http://172.16.0.36:2080404 page not found<span class="hljs-comment"># 指定 Host 头来路由到 demo 的相关 Pod</span>test36.node ➜  ~ curl -H <span class="hljs-string">&quot;Host: demo.mritd.me&quot;</span> http://172.16.0.36:2080&lt;!DOCTYPE html&gt;&lt;html lang=<span class="hljs-string">&quot;zh&quot;</span>&gt;&lt;head&gt;    &lt;meta http-equiv=<span class="hljs-string">&quot;Content-Type&quot;</span> content=<span class="hljs-string">&quot;text/html; charset=UTF-8&quot;</span>&gt;    &lt;title&gt;Running!&lt;/title&gt;    &lt;style <span class="hljs-built_in">type</span>=<span class="hljs-string">&quot;text/css&quot;</span>&gt;        body &#123;            width: 100%;            min-height: 100%;            background: linear-gradient(to bottom, <span class="hljs-comment">#fff 0, #b8edff 50%, #83dfff 100%);</span>            background-attachment: fixed;        &#125;    &lt;/style&gt;&lt;/head&gt;&lt;body class=<span class="hljs-string">&quot; hasGoogleVoiceExt&quot;</span>&gt;&lt;div align=<span class="hljs-string">&quot;center&quot;</span>&gt;    &lt;h1&gt;Your container is running!&lt;/h1&gt;    &lt;img src=<span class="hljs-string">&quot;./docker.png&quot;</span> alt=<span class="hljs-string">&quot;docker&quot;</span>&gt;&lt;/div&gt;&lt;/body&gt;&lt;/html&gt;<span class="hljs-comment">#</span></code></pre></div><h2 id="四、其他说明"><a href="#四、其他说明" class="headerlink" title="四、其他说明"></a>四、其他说明</h2><p>写这篇文章的目的是给予一种新的服务暴露思路，这篇文章的某些配置并不适合生产使用；<strong>生产环境尽量使用独立的机器部署 Traefik，同时最好宿主机二进制方式部署；应用的 Deployment 也应当加入健康检测以防止错误的流量路由</strong>；至于 Traefik 的具体细节配置，比如访问日志、Entrypoints 配置、如何连接 Kubernets HA api 等不在本文范畴内，请自行查阅文档；</p><p>最后说一下，关于 Traefik 的 HA 只需要部署多个实例即可，还有 Traefik 本身不做日志滚动等，需要自行处理一下日志。</p>]]></content>
    
    
    <summary type="html">最近准备重新折腾一下 Kubernetes 的服务暴露方式，以前的方式是彻底剥离 Kubenretes 本身的服务发现，然后改动应用实现 应用+Consul+Fabio 的服务暴露方式；总感觉这种方式不算优雅，所以折腾了一下 Traefik，试了下效果还不错，以下记录了使用 Traefik 的新的服务暴露方式(本文仅针对 HTTP 协议)</summary>
    
    
    
    <category term="Kubernetes" scheme="https://mritd.com/categories/kubernetes/"/>
    
    
    <category term="Kubernetes" scheme="https://mritd.com/tags/kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>为你的 GitLab 增加提交信息检测</title>
    <link href="https://mritd.com/2018/05/11/add-commit-message-style-check-to-your-gitlab/"/>
    <id>https://mritd.com/2018/05/11/add-commit-message-style-check-to-your-gitlab/</id>
    <published>2018-05-11T09:44:40.000Z</published>
    <updated>2018-05-11T09:44:40.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>最近准备对项目生成 Change Log，然而发现提交格式不统一根本没法处理；so 后来大家约定式遵循 GitFlow，并使用 Angular 社区规范的提交格式，同时扩展了一些前缀如 hotfix 等；但是时间长了发现还是有些提交为了 “方便” 不遵循 Angular 社区规范的提交格式，这时候我唯一能做的就是想办法在服务端增加一个提交检测；以下记录了 GitLab 增加自定义 Commit 提交格式检测的方案</p></blockquote><h2 id="一、相关文章资料"><a href="#一、相关文章资料" class="headerlink" title="一、相关文章资料"></a>一、相关文章资料</h2><p>最开始用 Google 搜索到的方案是使用 GitLab 的 Push Rules 功能，具体文档见 <a href="https://docs.gitlab.com/ee/push_rules/push_rules.html">这里</a>，看完了我才发现这是企业版独有的，作为比较有逼格(qiong)的我们是不可能接受这种 “没技术含量” 的方式的；后来找了好多资料，发现还得借助 Git Hook 功能，文档见 <a href="https://docs.gitlab.com/ee/administration/custom_hooks.html">Custom Git Hooks</a>；简单地说 Git Hook 就是在 git 操作的不同阶段执行的预定义脚本，<strong>GitLab 目前仅支持 <code>pre-receive</code> 这个钩子，当然他可以链式调用</strong>；所以一切操作就得从这里入手</p><h2 id="二、pre-receive-实现"><a href="#二、pre-receive-实现" class="headerlink" title="二、pre-receive 实现"></a>二、pre-receive 实现</h2><p>查阅了相关资料得出，在进行 push 时，GitLab 会调用这个钩子文件，这个钩子文件必须放在 <code>/var/opt/gitlab/git-data/repositories/&lt;group&gt;/&lt;project&gt;.git/custom_hooks</code> 目录中，当然具体路径也可能是 <code>/home/git/repositories/&lt;group&gt;/&lt;project&gt;.git/custom_hooks</code>；<code>custom_hooks</code> 目录需要自己创建，具体可以参阅文档的 <a href="https://docs.gitlab.com/ee/administration/custom_hooks.html#setup">Setup</a>；</p><p><strong>在进行 push 操作时，GitLab 会调用这个钩子文件，并且从 stdin 输入三个参数，分别为 之前的版本 commit ID、push 的版本 commit ID 和 push 的分支；根据 commit ID 我们就可以很轻松的获取到提交信息，从而实现进一步检测动作；根据 GitLab 的文档说明，当这个 hook 执行后以非 0 状态退出则认为执行失败，从而拒绝 push；同时会将 stderr 信息返回给 client 端；</strong>说了这么多，下面就可以直接上代码了，为了方便我就直接用 go 造了一个 <a href="https://github.com/mritd/pre-receive">pre-receive</a>，官方文档说明了不限制语言</p><div class="hljs code-wrapper"><pre><code class="hljs golang"><span class="hljs-keyword">package</span> main<span class="hljs-keyword">import</span> (    <span class="hljs-string">&quot;fmt&quot;</span>    <span class="hljs-string">&quot;io/ioutil&quot;</span>    <span class="hljs-string">&quot;os&quot;</span>    <span class="hljs-string">&quot;os/exec&quot;</span>    <span class="hljs-string">&quot;regexp&quot;</span>    <span class="hljs-string">&quot;strings&quot;</span>)<span class="hljs-keyword">type</span> CommitType <span class="hljs-keyword">string</span><span class="hljs-keyword">const</span> (    FEAT     CommitType = <span class="hljs-string">&quot;feat&quot;</span>    FIX      CommitType = <span class="hljs-string">&quot;fix&quot;</span>    DOCS     CommitType = <span class="hljs-string">&quot;docs&quot;</span>    STYLE    CommitType = <span class="hljs-string">&quot;style&quot;</span>    REFACTOR CommitType = <span class="hljs-string">&quot;refactor&quot;</span>    TEST     CommitType = <span class="hljs-string">&quot;test&quot;</span>    CHORE    CommitType = <span class="hljs-string">&quot;chore&quot;</span>    PERF     CommitType = <span class="hljs-string">&quot;perf&quot;</span>    HOTFIX   CommitType = <span class="hljs-string">&quot;hotfix&quot;</span>)<span class="hljs-keyword">const</span> CommitMessagePattern = <span class="hljs-string">`^(?:fixup!\s*)?(\w*)(\(([\w\$\.\*/-].*)\))?\: (.*)|^Merge\ branch(.*)`</span><span class="hljs-keyword">const</span> checkFailedMeassge = <span class="hljs-string">`##############################################################################</span><span class="hljs-string">##                                                                          ##</span><span class="hljs-string">## Commit message style check failed!                                       ##</span><span class="hljs-string">##                                                                          ##</span><span class="hljs-string">## Commit message style must satisfy this regular:                          ##</span><span class="hljs-string">##   ^(?:fixup!\s*)?(\w*)(\(([\w\$\.\*/-].*)\))?\: (. *)|^Merge\ branch(.*) ##</span><span class="hljs-string">##                                                                          ##</span><span class="hljs-string">## Example:                                                                 ##</span><span class="hljs-string">##   feat(test): test commit style check.                                   ##</span><span class="hljs-string">##                                                                          ##</span><span class="hljs-string">##############################################################################`</span><span class="hljs-comment">// 是否开启严格模式，严格模式下将校验所有的提交信息格式(多 commit 下)</span><span class="hljs-keyword">const</span> strictMode = <span class="hljs-literal">false</span><span class="hljs-keyword">var</span> commitMsgReg = regexp.MustCompile(CommitMessagePattern)<span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-title">main</span><span class="hljs-params">()</span></span> &#123;    input, _ := ioutil.ReadAll(os.Stdin)    param := strings.Fields(<span class="hljs-keyword">string</span>(input))    <span class="hljs-comment">// allow branch/tag delete</span>    <span class="hljs-keyword">if</span> param[<span class="hljs-number">1</span>] == <span class="hljs-string">&quot;0000000000000000000000000000000000000000&quot;</span> &#123;        os.Exit(<span class="hljs-number">0</span>)    &#125;    commitMsg := getCommitMsg(param[<span class="hljs-number">0</span>], param[<span class="hljs-number">1</span>])    <span class="hljs-keyword">for</span> _, tmpStr := <span class="hljs-keyword">range</span> commitMsg &#123;        commitTypes := commitMsgReg.FindAllStringSubmatch(tmpStr, <span class="hljs-number">-1</span>)        <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(commitTypes) != <span class="hljs-number">1</span> &#123;            checkFailed()        &#125; <span class="hljs-keyword">else</span> &#123;            <span class="hljs-keyword">switch</span> commitTypes[<span class="hljs-number">0</span>][<span class="hljs-number">1</span>] &#123;            <span class="hljs-keyword">case</span> <span class="hljs-keyword">string</span>(FEAT):            <span class="hljs-keyword">case</span> <span class="hljs-keyword">string</span>(FIX):            <span class="hljs-keyword">case</span> <span class="hljs-keyword">string</span>(DOCS):            <span class="hljs-keyword">case</span> <span class="hljs-keyword">string</span>(STYLE):            <span class="hljs-keyword">case</span> <span class="hljs-keyword">string</span>(REFACTOR):            <span class="hljs-keyword">case</span> <span class="hljs-keyword">string</span>(TEST):            <span class="hljs-keyword">case</span> <span class="hljs-keyword">string</span>(CHORE):            <span class="hljs-keyword">case</span> <span class="hljs-keyword">string</span>(PERF):            <span class="hljs-keyword">case</span> <span class="hljs-keyword">string</span>(HOTFIX):            <span class="hljs-keyword">default</span>:                <span class="hljs-keyword">if</span> !strings.HasPrefix(tmpStr, <span class="hljs-string">&quot;Merge branch&quot;</span>) &#123;                    checkFailed()                &#125;            &#125;        &#125;        <span class="hljs-keyword">if</span> !strictMode &#123;            os.Exit(<span class="hljs-number">0</span>)        &#125;    &#125;&#125;<span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-title">getCommitMsg</span><span class="hljs-params">(odlCommitID, commitID <span class="hljs-keyword">string</span>)</span> []<span class="hljs-title">string</span></span> &#123;    getCommitMsgCmd := exec.Command(<span class="hljs-string">&quot;git&quot;</span>, <span class="hljs-string">&quot;log&quot;</span>, odlCommitID+<span class="hljs-string">&quot;..&quot;</span>+commitID, <span class="hljs-string">&quot;--pretty=format:%s&quot;</span>)    getCommitMsgCmd.Stdin = os.Stdin    getCommitMsgCmd.Stderr = os.Stderr    b, err := getCommitMsgCmd.Output()    <span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;        fmt.Print(err)        os.Exit(<span class="hljs-number">1</span>)    &#125;    commitMsg := strings.Split(<span class="hljs-keyword">string</span>(b), <span class="hljs-string">&quot;\n&quot;</span>)    <span class="hljs-keyword">return</span> commitMsg&#125;<span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-title">checkFailed</span><span class="hljs-params">()</span></span> &#123;    fmt.Fprintln(os.Stderr, checkFailedMeassge)    os.Exit(<span class="hljs-number">1</span>)&#125;</code></pre></div><h2 id="三、安装-pre-receive"><a href="#三、安装-pre-receive" class="headerlink" title="三、安装 pre-receive"></a>三、安装 pre-receive</h2><p>把以上代码编译后生成的 <code>pre-receive</code> 文件复制到对应项目的钩子目录即可；<strong>要注意的是文件名必须为 <code>pre-receive</code>，同时 <code>custom_hooks</code> 目录需要自建；<code>custom_hooks</code> 目录以及 <code>pre-receive</code> 文件用户组必须为 <code>git:git</code>；在删除分支时 commit ID 为 <code>0000000000000000000000000000000000000000</code>，此时不需要检测提交信息，否则可能导致无法删除分支/tag</strong>；最后效果如下所示</p><p><img src="https://cdn.oss.link/markdown/hs9c2.png" alt="commit msg check"></p>]]></content>
    
    
    <summary type="html">最近准备对项目生成 Change Log，然而发现提交格式不统一根本没法处理；so 后来大家约定式遵循 GitFlow，并使用 Angular 社区规范的提交格式，同时扩展了一些前缀如 hotfix 等；但是时间长了发现还是有些提交为了 &quot;方便&quot; 不遵循 Angular 社区规范的提交格式，这时候我唯一能做的就是想办法在服务端增加一个提交检测；以下记录了 GitLab 增加自定义 Commit 提交格式检测的方案</summary>
    
    
    
    <category term="CI/CD" scheme="https://mritd.com/categories/ci-cd/"/>
    
    
    <category term="CI/CD" scheme="https://mritd.com/tags/ci-cd/"/>
    
  </entry>
  
  <entry>
    <title>Kubernetes 1.10.1 集群搭建</title>
    <link href="https://mritd.com/2018/04/19/set-up-kubernetes-1.10.1-cluster-by-hyperkube/"/>
    <id>https://mritd.com/2018/04/19/set-up-kubernetes-1.10.1-cluster-by-hyperkube/</id>
    <published>2018-04-19T08:19:08.000Z</published>
    <updated>2018-04-19T08:19:08.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>年后比较忙，所以 1.9 也没去折腾(其实就是懒)，最近刚有点时间凑巧 1.10 发布；所以就折腾一下 1.10，感觉搭建配置没有太大变化，折腾了 2 天基本算是搞定了，这里记录一下搭建过程；本文用到的被 block 镜像已经上传至 <a href="https://pan.baidu.com/s/14W86QQ4qi8qn8JqaDMcC3g">百度云</a> 密码: dy5p</p></blockquote><h3 id="一、环境准备"><a href="#一、环境准备" class="headerlink" title="一、环境准备"></a>一、环境准备</h3><p>目前搭建仍然采用 5 台虚拟机测试，基本环境如下</p><table><thead><tr><th>IP</th><th>Type</th><th>Docker</th><th>OS</th></tr></thead><tbody><tr><td>192.168.1.61</td><td>master、node、etcd</td><td>18.03.0-ce</td><td>ubuntu 16.04</td></tr><tr><td>192.168.1.62</td><td>master、node、etcd</td><td>18.03.0-ce</td><td>ubuntu 16.04</td></tr><tr><td>192.168.1.63</td><td>master、node、etcd</td><td>18.03.0-ce</td><td>ubuntu 16.04</td></tr><tr><td>192.168.1.64</td><td>node</td><td>18.03.0-ce</td><td>ubuntu 16.04</td></tr><tr><td>192.168.1.65</td><td>node</td><td>18.03.0-ce</td><td>ubuntu 16.04</td></tr></tbody></table><p><strong>搭建前请看完整篇文章后再操作，一些变更说明我放到后面了；还有为了尽可能的懒，也不用什么 rpm、deb 了，直接 <code>hyperkube</code> + <code>service</code> 配置，布吉岛 <code>hyperkube</code> 的请看 <a href="https://github.com/kubernetes/kubernetes/blob/master/cluster/images/hyperkube/README.md">GitHub</a>；本篇文章基于一些小脚本搭建(懒)，所以不会写太详细的步骤，具体请参考 <a href="https://github.com/mritd/ktool">仓库脚本</a>，如果想看更详细的每一步的作用可以参考以前的 1.7、1.8 的搭建文档</strong></p><h3 id="二、搭建-Etcd-集群"><a href="#二、搭建-Etcd-集群" class="headerlink" title="二、搭建 Etcd 集群"></a>二、搭建 Etcd 集群</h3><h4 id="2-1、安装-cfssl"><a href="#2-1、安装-cfssl" class="headerlink" title="2.1、安装 cfssl"></a>2.1、安装 cfssl</h4><p>说实话这个章节我不想写，但是考虑可能有人真的需要，所以还是写了一下；<strong>这个安装脚本使用的是我私人的 cdn，文件可能随时删除，想使用最新版本请自行从 <a href="https://github.com/cloudflare/cfssl">Github</a> clone 并编译</strong></p><div class="hljs code-wrapper"><pre><code class="hljs sh">wget https://mritdftp.b0.upaiyun.com/cfssl/cfssl.tar.gztar -zxvf cfssl.tar.gzmv cfssl cfssljson /usr/<span class="hljs-built_in">local</span>/binchmod +x /usr/<span class="hljs-built_in">local</span>/bin/cfssl /usr/<span class="hljs-built_in">local</span>/bin/cfssljsonrm -f cfssl.tar.gz</code></pre></div><h4 id="2-2、生成-Etcd-证书"><a href="#2-2、生成-Etcd-证书" class="headerlink" title="2.2、生成 Etcd 证书"></a>2.2、生成 Etcd 证书</h4><h5 id="etcd-csr-json"><a href="#etcd-csr-json" class="headerlink" title="etcd-csr.json"></a>etcd-csr.json</h5><div class="hljs code-wrapper"><pre><code class="hljs json">&#123;  <span class="hljs-attr">&quot;key&quot;</span>: &#123;    <span class="hljs-attr">&quot;algo&quot;</span>: <span class="hljs-string">&quot;rsa&quot;</span>,    <span class="hljs-attr">&quot;size&quot;</span>: <span class="hljs-number">2048</span>  &#125;,  <span class="hljs-attr">&quot;names&quot;</span>: [    &#123;      <span class="hljs-attr">&quot;O&quot;</span>: <span class="hljs-string">&quot;etcd&quot;</span>,      <span class="hljs-attr">&quot;OU&quot;</span>: <span class="hljs-string">&quot;etcd Security&quot;</span>,      <span class="hljs-attr">&quot;L&quot;</span>: <span class="hljs-string">&quot;Beijing&quot;</span>,      <span class="hljs-attr">&quot;ST&quot;</span>: <span class="hljs-string">&quot;Beijing&quot;</span>,      <span class="hljs-attr">&quot;C&quot;</span>: <span class="hljs-string">&quot;CN&quot;</span>    &#125;  ],  <span class="hljs-attr">&quot;CN&quot;</span>: <span class="hljs-string">&quot;etcd&quot;</span>,  <span class="hljs-attr">&quot;hosts&quot;</span>: [    <span class="hljs-string">&quot;127.0.0.1&quot;</span>,    <span class="hljs-string">&quot;localhost&quot;</span>,    <span class="hljs-string">&quot;192.168.1.61&quot;</span>,    <span class="hljs-string">&quot;192.168.1.62&quot;</span>,    <span class="hljs-string">&quot;192.168.1.63&quot;</span>  ]&#125;</code></pre></div><h5 id="etcd-gencert-json"><a href="#etcd-gencert-json" class="headerlink" title="etcd-gencert.json"></a>etcd-gencert.json</h5><div class="hljs code-wrapper"><pre><code class="hljs json">&#123;  <span class="hljs-attr">&quot;signing&quot;</span>: &#123;    <span class="hljs-attr">&quot;default&quot;</span>: &#123;        <span class="hljs-attr">&quot;usages&quot;</span>: [          <span class="hljs-string">&quot;signing&quot;</span>,          <span class="hljs-string">&quot;key encipherment&quot;</span>,          <span class="hljs-string">&quot;server auth&quot;</span>,          <span class="hljs-string">&quot;client auth&quot;</span>        ],        <span class="hljs-attr">&quot;expiry&quot;</span>: <span class="hljs-string">&quot;87600h&quot;</span>    &#125;  &#125;&#125;</code></pre></div><h5 id="etcd-root-ca-csr-json"><a href="#etcd-root-ca-csr-json" class="headerlink" title="etcd-root-ca-csr.json"></a>etcd-root-ca-csr.json</h5><div class="hljs code-wrapper"><pre><code class="hljs json">&#123;  <span class="hljs-attr">&quot;key&quot;</span>: &#123;    <span class="hljs-attr">&quot;algo&quot;</span>: <span class="hljs-string">&quot;rsa&quot;</span>,    <span class="hljs-attr">&quot;size&quot;</span>: <span class="hljs-number">4096</span>  &#125;,  <span class="hljs-attr">&quot;names&quot;</span>: [    &#123;      <span class="hljs-attr">&quot;O&quot;</span>: <span class="hljs-string">&quot;etcd&quot;</span>,      <span class="hljs-attr">&quot;OU&quot;</span>: <span class="hljs-string">&quot;etcd Security&quot;</span>,      <span class="hljs-attr">&quot;L&quot;</span>: <span class="hljs-string">&quot;Beijing&quot;</span>,      <span class="hljs-attr">&quot;ST&quot;</span>: <span class="hljs-string">&quot;Beijing&quot;</span>,      <span class="hljs-attr">&quot;C&quot;</span>: <span class="hljs-string">&quot;CN&quot;</span>    &#125;  ],  <span class="hljs-attr">&quot;CN&quot;</span>: <span class="hljs-string">&quot;etcd-root-ca&quot;</span>&#125;</code></pre></div><h5 id="生成证书"><a href="#生成证书" class="headerlink" title="生成证书"></a>生成证书</h5><div class="hljs code-wrapper"><pre><code class="hljs sh">cfssl gencert --initca=<span class="hljs-literal">true</span> etcd-root-ca-csr.json | cfssljson --bare etcd-root-cacfssl gencert --ca etcd-root-ca.pem --ca-key etcd-root-ca-key.pem --config etcd-gencert.json etcd-csr.json | cfssljson --bare etcd</code></pre></div><p>生成后如下</p><p><img src="https://cdn.oss.link/markdown/81203.png" alt="gen etcd certs"></p><h4 id="2-3、安装-Etcd"><a href="#2-3、安装-Etcd" class="headerlink" title="2.3、安装 Etcd"></a>2.3、安装 Etcd</h4><p>Etcd 这里采用最新的 3.2.18 版本，安装方式直接复制二进制文件、systemd service 配置即可，不过需要注意相关用户权限问题，以下脚本配置等参考了 etcd rpm 安装包</p><h5 id="etcd-service"><a href="#etcd-service" class="headerlink" title="etcd.service"></a>etcd.service</h5><div class="hljs code-wrapper"><pre><code class="hljs sh">[Unit]Description=Etcd ServerAfter=network.targetAfter=network-online.targetWants=network-online.target[Service]Type=notifyWorkingDirectory=/var/lib/etcd/EnvironmentFile=-/etc/etcd/etcd.confUser=etcd<span class="hljs-comment"># set GOMAXPROCS to number of processors</span>ExecStart=/bin/bash -c <span class="hljs-string">&quot;GOMAXPROCS=<span class="hljs-subst">$(nproc)</span> /usr/local/bin/etcd --name=\&quot;<span class="hljs-variable">$&#123;ETCD_NAME&#125;</span>\&quot; --data-dir=\&quot;<span class="hljs-variable">$&#123;ETCD_DATA_DIR&#125;</span>\&quot; --listen-client-urls=\&quot;<span class="hljs-variable">$&#123;ETCD_LISTEN_CLIENT_URLS&#125;</span>\&quot;&quot;</span>Restart=on-failureLimitNOFILE=65536[Install]WantedBy=multi-user.target</code></pre></div><h5 id="etcd-conf"><a href="#etcd-conf" class="headerlink" title="etcd.conf"></a>etcd.conf</h5><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment"># [member]</span>ETCD_NAME=etcd1ETCD_DATA_DIR=<span class="hljs-string">&quot;/var/lib/etcd/etcd1.etcd&quot;</span>ETCD_WAL_DIR=<span class="hljs-string">&quot;/var/lib/etcd/wal&quot;</span>ETCD_SNAPSHOT_COUNT=<span class="hljs-string">&quot;100&quot;</span>ETCD_HEARTBEAT_INTERVAL=<span class="hljs-string">&quot;100&quot;</span>ETCD_ELECTION_TIMEOUT=<span class="hljs-string">&quot;1000&quot;</span>ETCD_LISTEN_PEER_URLS=<span class="hljs-string">&quot;https://192.168.1.61:2380&quot;</span>ETCD_LISTEN_CLIENT_URLS=<span class="hljs-string">&quot;https://192.168.1.61:2379,http://127.0.0.1:2379&quot;</span>ETCD_MAX_SNAPSHOTS=<span class="hljs-string">&quot;5&quot;</span>ETCD_MAX_WALS=<span class="hljs-string">&quot;5&quot;</span><span class="hljs-comment">#ETCD_CORS=&quot;&quot;</span><span class="hljs-comment"># [cluster]</span>ETCD_INITIAL_ADVERTISE_PEER_URLS=<span class="hljs-string">&quot;https://192.168.1.61:2380&quot;</span><span class="hljs-comment"># if you use different ETCD_NAME (e.g. test), set ETCD_INITIAL_CLUSTER value for this name, i.e. &quot;test=http://...&quot;</span>ETCD_INITIAL_CLUSTER=<span class="hljs-string">&quot;etcd1=https://192.168.1.61:2380,etcd2=https://192.168.1.62:2380,etcd3=https://192.168.1.63:2380&quot;</span>ETCD_INITIAL_CLUSTER_STATE=<span class="hljs-string">&quot;new&quot;</span>ETCD_INITIAL_CLUSTER_TOKEN=<span class="hljs-string">&quot;etcd-cluster&quot;</span>ETCD_ADVERTISE_CLIENT_URLS=<span class="hljs-string">&quot;https://192.168.1.61:2379&quot;</span><span class="hljs-comment">#ETCD_DISCOVERY=&quot;&quot;</span><span class="hljs-comment">#ETCD_DISCOVERY_SRV=&quot;&quot;</span><span class="hljs-comment">#ETCD_DISCOVERY_FALLBACK=&quot;proxy&quot;</span><span class="hljs-comment">#ETCD_DISCOVERY_PROXY=&quot;&quot;</span><span class="hljs-comment">#ETCD_STRICT_RECONFIG_CHECK=&quot;false&quot;</span><span class="hljs-comment">#ETCD_AUTO_COMPACTION_RETENTION=&quot;0&quot;</span><span class="hljs-comment"># [proxy]</span><span class="hljs-comment">#ETCD_PROXY=&quot;off&quot;</span><span class="hljs-comment">#ETCD_PROXY_FAILURE_WAIT=&quot;5000&quot;</span><span class="hljs-comment">#ETCD_PROXY_REFRESH_INTERVAL=&quot;30000&quot;</span><span class="hljs-comment">#ETCD_PROXY_DIAL_TIMEOUT=&quot;1000&quot;</span><span class="hljs-comment">#ETCD_PROXY_WRITE_TIMEOUT=&quot;5000&quot;</span><span class="hljs-comment">#ETCD_PROXY_READ_TIMEOUT=&quot;0&quot;</span><span class="hljs-comment"># [security]</span>ETCD_CERT_FILE=<span class="hljs-string">&quot;/etc/etcd/ssl/etcd.pem&quot;</span>ETCD_KEY_FILE=<span class="hljs-string">&quot;/etc/etcd/ssl/etcd-key.pem&quot;</span>ETCD_CLIENT_CERT_AUTH=<span class="hljs-string">&quot;true&quot;</span>ETCD_TRUSTED_CA_FILE=<span class="hljs-string">&quot;/etc/etcd/ssl/etcd-root-ca.pem&quot;</span>ETCD_AUTO_TLS=<span class="hljs-string">&quot;true&quot;</span>ETCD_PEER_CERT_FILE=<span class="hljs-string">&quot;/etc/etcd/ssl/etcd.pem&quot;</span>ETCD_PEER_KEY_FILE=<span class="hljs-string">&quot;/etc/etcd/ssl/etcd-key.pem&quot;</span>ETCD_PEER_CLIENT_CERT_AUTH=<span class="hljs-string">&quot;true&quot;</span>ETCD_PEER_TRUSTED_CA_FILE=<span class="hljs-string">&quot;/etc/etcd/ssl/etcd-root-ca.pem&quot;</span>ETCD_PEER_AUTO_TLS=<span class="hljs-string">&quot;true&quot;</span><span class="hljs-comment"># [logging]</span><span class="hljs-comment">#ETCD_DEBUG=&quot;false&quot;</span><span class="hljs-comment"># examples for -log-package-levels etcdserver=WARNING,security=DEBUG</span><span class="hljs-comment">#ETCD_LOG_PACKAGE_LEVELS=&quot;&quot;</span></code></pre></div><h5 id="install-sh"><a href="#install-sh" class="headerlink" title="install.sh"></a>install.sh</h5><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-meta">#!/bin/bash</span><span class="hljs-built_in">set</span> -eETCD_VERSION=<span class="hljs-string">&quot;3.2.18&quot;</span><span class="hljs-keyword">function</span> <span class="hljs-function"><span class="hljs-title">download</span></span>()&#123;    <span class="hljs-keyword">if</span> [ ! -f <span class="hljs-string">&quot;etcd-v<span class="hljs-variable">$&#123;ETCD_VERSION&#125;</span>-linux-amd64.tar.gz&quot;</span> ]; <span class="hljs-keyword">then</span>        wget https://github.com/coreos/etcd/releases/download/v<span class="hljs-variable">$&#123;ETCD_VERSION&#125;</span>/etcd-v<span class="hljs-variable">$&#123;ETCD_VERSION&#125;</span>-linux-amd64.tar.gz        tar -zxvf etcd-v<span class="hljs-variable">$&#123;ETCD_VERSION&#125;</span>-linux-amd64.tar.gz    <span class="hljs-keyword">fi</span>&#125;<span class="hljs-keyword">function</span> <span class="hljs-function"><span class="hljs-title">preinstall</span></span>()&#123;    getent group etcd &gt;/dev/null || groupadd -r etcd    getent passwd etcd &gt;/dev/null || useradd -r -g etcd -d /var/lib/etcd -s /sbin/nologin -c <span class="hljs-string">&quot;etcd user&quot;</span> etcd&#125;<span class="hljs-keyword">function</span> <span class="hljs-function"><span class="hljs-title">install</span></span>()&#123;    <span class="hljs-built_in">echo</span> -e <span class="hljs-string">&quot;\033[32mINFO: Copy etcd...\033[0m&quot;</span>    tar -zxvf etcd-v<span class="hljs-variable">$&#123;ETCD_VERSION&#125;</span>-linux-amd64.tar.gz    cp etcd-v<span class="hljs-variable">$&#123;ETCD_VERSION&#125;</span>-linux-amd64/etcd* /usr/<span class="hljs-built_in">local</span>/bin    rm -rf etcd-v<span class="hljs-variable">$&#123;ETCD_VERSION&#125;</span>-linux-amd64    <span class="hljs-built_in">echo</span> -e <span class="hljs-string">&quot;\033[32mINFO: Copy etcd config...\033[0m&quot;</span>    cp -r conf /etc/etcd    chown -R etcd:etcd /etc/etcd    chmod -R 755 /etc/etcd/ssl    <span class="hljs-built_in">echo</span> -e <span class="hljs-string">&quot;\033[32mINFO: Copy etcd systemd config...\033[0m&quot;</span>    cp systemd/*.service /lib/systemd/system    systemctl daemon-reload&#125;<span class="hljs-keyword">function</span> <span class="hljs-function"><span class="hljs-title">postinstall</span></span>()&#123;    <span class="hljs-keyword">if</span> [ ! -d <span class="hljs-string">&quot;/var/lib/etcd&quot;</span> ]; <span class="hljs-keyword">then</span>        mkdir /var/lib/etcd        chown -R etcd:etcd /var/lib/etcd    <span class="hljs-keyword">fi</span>&#125;downloadpreinstallinstallpostinstall</code></pre></div><p><strong>脚本解释如下:</strong></p><ul><li>download: 从 Github 下载二进制文件并解压</li><li>preinstall: 为 Etcd 安装做准备，创建 etcd 用户，并指定家目录登录 shell 等</li><li>install: 将 etcd 二进制文件复制到安装目录(<code>/usr/local/bin</code>)，复制 conf 目录到 <code>/etc/etcd</code></li><li>postinstall: 安装后收尾工作，比如检测 <code>/var/lib/etcd</code> 是否存在，纠正权限等</li></ul><p>整体目录结构如下</p><div class="hljs code-wrapper"><pre><code class="hljs sh">etcd├── conf│   ├── etcd.conf│   └── ssl│       ├── etcd.csr│       ├── etcd-csr.json│       ├── etcd-gencert.json│       ├── etcd-key.pem│       ├── etcd.pem│       ├── etcd-root-ca.csr│       ├── etcd-root-ca-csr.json│       ├── etcd-root-ca-key.pem│       └── etcd-root-ca.pem├── etcd.service└── install.sh</code></pre></div><p><strong>请自行创建 conf 目录等，并放置好相关文件，保存上面脚本为 <code>install.sh</code>，直接执行即可；在每台机器上更改好对应的配置，如 etcd 名称等，etcd 估计都是轻车熟路了，这里不做过多阐述；安装后启动即可</strong></p><div class="hljs code-wrapper"><pre><code class="hljs sh">systemctl start etcdsystemctl <span class="hljs-built_in">enable</span> etcd</code></pre></div><p><strong>注意: 集群 etcd 要 3 个一起启动，集群模式下单个启动会卡半天最后失败，不要傻等；启动成功后测试如下</strong></p><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-built_in">export</span> ETCDCTL_API=3etcdctl --cacert=/etc/etcd/ssl/etcd-root-ca.pem --cert=/etc/etcd/ssl/etcd.pem --key=/etc/etcd/ssl/etcd-key.pem --endpoints=https://192.168.1.61:2379,https://192.168.1.62:2379,https://192.168.1.63:2379 endpoint health</code></pre></div><p><img src="https://cdn.oss.link/markdown/ji94m.png" alt="check etcd"></p><h3 id="三、安装-Kubernets-集群组件"><a href="#三、安装-Kubernets-集群组件" class="headerlink" title="三、安装 Kubernets 集群组件"></a>三、安装 Kubernets 集群组件</h3><blockquote><p><strong>注意：与以前文档不同的是，这次不依赖 rpm 等特定安装包，而是基于 hyperkube 二进制手动安装，每个节点都会同时安装 Master 与 Node 配置文件，具体作为 Master 还是 Node 取决于服务开启情况</strong></p></blockquote><h4 id="3-1、生成-Kubernetes-证书"><a href="#3-1、生成-Kubernetes-证书" class="headerlink" title="3.1、生成 Kubernetes 证书"></a>3.1、生成 Kubernetes 证书</h4><p>由于 kubelet 和 kube-proxy 用到的 kubeconfig 配置文件需要借助 kubectl 来生成，所以需要先安装一下 kubectl</p><div class="hljs code-wrapper"><pre><code class="hljs sh">wget https://storage.googleapis.com/kubernetes-release/release/v1.10.1/bin/linux/amd64/hyperkube -O hyperkube_1.10.1chmod +x hyperkube_1.10.1cp hyperkube_1.10.1 /usr/<span class="hljs-built_in">local</span>/bin/hyperkubeln -s /usr/<span class="hljs-built_in">local</span>/bin/hyperkube /usr/<span class="hljs-built_in">local</span>/bin/kubectl</code></pre></div><h5 id="admin-csr-json"><a href="#admin-csr-json" class="headerlink" title="admin-csr.json"></a>admin-csr.json</h5><div class="hljs code-wrapper"><pre><code class="hljs json">&#123;  <span class="hljs-attr">&quot;CN&quot;</span>: <span class="hljs-string">&quot;admin&quot;</span>,  <span class="hljs-attr">&quot;hosts&quot;</span>: [],  <span class="hljs-attr">&quot;key&quot;</span>: &#123;    <span class="hljs-attr">&quot;algo&quot;</span>: <span class="hljs-string">&quot;rsa&quot;</span>,    <span class="hljs-attr">&quot;size&quot;</span>: <span class="hljs-number">2048</span>  &#125;,  <span class="hljs-attr">&quot;names&quot;</span>: [    &#123;      <span class="hljs-attr">&quot;C&quot;</span>: <span class="hljs-string">&quot;CN&quot;</span>,      <span class="hljs-attr">&quot;ST&quot;</span>: <span class="hljs-string">&quot;BeiJing&quot;</span>,      <span class="hljs-attr">&quot;L&quot;</span>: <span class="hljs-string">&quot;BeiJing&quot;</span>,      <span class="hljs-attr">&quot;O&quot;</span>: <span class="hljs-string">&quot;system:masters&quot;</span>,      <span class="hljs-attr">&quot;OU&quot;</span>: <span class="hljs-string">&quot;System&quot;</span>    &#125;  ]&#125;</code></pre></div><h5 id="k8s-gencert-json"><a href="#k8s-gencert-json" class="headerlink" title="k8s-gencert.json"></a>k8s-gencert.json</h5><div class="hljs code-wrapper"><pre><code class="hljs json">&#123;  <span class="hljs-attr">&quot;signing&quot;</span>: &#123;    <span class="hljs-attr">&quot;default&quot;</span>: &#123;      <span class="hljs-attr">&quot;expiry&quot;</span>: <span class="hljs-string">&quot;87600h&quot;</span>    &#125;,    <span class="hljs-attr">&quot;profiles&quot;</span>: &#123;      <span class="hljs-attr">&quot;kubernetes&quot;</span>: &#123;        <span class="hljs-attr">&quot;usages&quot;</span>: [            <span class="hljs-string">&quot;signing&quot;</span>,            <span class="hljs-string">&quot;key encipherment&quot;</span>,            <span class="hljs-string">&quot;server auth&quot;</span>,            <span class="hljs-string">&quot;client auth&quot;</span>        ],        <span class="hljs-attr">&quot;expiry&quot;</span>: <span class="hljs-string">&quot;87600h&quot;</span>      &#125;    &#125;  &#125;&#125;</code></pre></div><h5 id="k8s-root-ca-csr-json"><a href="#k8s-root-ca-csr-json" class="headerlink" title="k8s-root-ca-csr.json"></a>k8s-root-ca-csr.json</h5><div class="hljs code-wrapper"><pre><code class="hljs json">&#123;  <span class="hljs-attr">&quot;CN&quot;</span>: <span class="hljs-string">&quot;kubernetes&quot;</span>,  <span class="hljs-attr">&quot;key&quot;</span>: &#123;    <span class="hljs-attr">&quot;algo&quot;</span>: <span class="hljs-string">&quot;rsa&quot;</span>,    <span class="hljs-attr">&quot;size&quot;</span>: <span class="hljs-number">4096</span>  &#125;,  <span class="hljs-attr">&quot;names&quot;</span>: [    &#123;      <span class="hljs-attr">&quot;C&quot;</span>: <span class="hljs-string">&quot;CN&quot;</span>,      <span class="hljs-attr">&quot;ST&quot;</span>: <span class="hljs-string">&quot;BeiJing&quot;</span>,      <span class="hljs-attr">&quot;L&quot;</span>: <span class="hljs-string">&quot;BeiJing&quot;</span>,      <span class="hljs-attr">&quot;O&quot;</span>: <span class="hljs-string">&quot;k8s&quot;</span>,      <span class="hljs-attr">&quot;OU&quot;</span>: <span class="hljs-string">&quot;System&quot;</span>    &#125;  ]&#125;</code></pre></div><h5 id="kube-apiserver-csr-json"><a href="#kube-apiserver-csr-json" class="headerlink" title="kube-apiserver-csr.json"></a>kube-apiserver-csr.json</h5><p><strong>注意: 在以前的文档中这个配置叫 <code>kubernetes-csr.json</code>，为了明确划分职责，这个证书目前被重命名以表示其专属于 <code>apiserver</code> 使用；加了一个 <code>*.kubernetes.master</code> 域名以便内部私有 DNS 解析使用(可删除)；至于很多人问过 <code>kubernetes</code> 这几个能不能删掉，答案是不可以的；因为当集群创建好后，default namespace 下会创建一个叫 <code>kubenretes</code> 的 svc，有一些组件会直接连接这个 svc 来跟 api 通讯的，证书如果不包含可能会出现无法连接的情况；其他几个 <code>kubernetes</code> 开头的域名作用相同</strong></p><div class="hljs code-wrapper"><pre><code class="hljs json">&#123;    <span class="hljs-attr">&quot;CN&quot;</span>: <span class="hljs-string">&quot;kubernetes&quot;</span>,    <span class="hljs-attr">&quot;hosts&quot;</span>: [        <span class="hljs-string">&quot;127.0.0.1&quot;</span>,        <span class="hljs-string">&quot;10.254.0.1&quot;</span>,        <span class="hljs-string">&quot;192.168.1.61&quot;</span>,        <span class="hljs-string">&quot;192.168.1.62&quot;</span>,        <span class="hljs-string">&quot;192.168.1.63&quot;</span>,        <span class="hljs-string">&quot;192.168.1.64&quot;</span>,        <span class="hljs-string">&quot;192.168.1.65&quot;</span>,        <span class="hljs-string">&quot;*.kubernetes.master&quot;</span>,        <span class="hljs-string">&quot;localhost&quot;</span>,        <span class="hljs-string">&quot;kubernetes&quot;</span>,        <span class="hljs-string">&quot;kubernetes.default&quot;</span>,        <span class="hljs-string">&quot;kubernetes.default.svc&quot;</span>,        <span class="hljs-string">&quot;kubernetes.default.svc.cluster&quot;</span>,        <span class="hljs-string">&quot;kubernetes.default.svc.cluster.local&quot;</span>    ],    <span class="hljs-attr">&quot;key&quot;</span>: &#123;        <span class="hljs-attr">&quot;algo&quot;</span>: <span class="hljs-string">&quot;rsa&quot;</span>,        <span class="hljs-attr">&quot;size&quot;</span>: <span class="hljs-number">2048</span>    &#125;,    <span class="hljs-attr">&quot;names&quot;</span>: [        &#123;            <span class="hljs-attr">&quot;C&quot;</span>: <span class="hljs-string">&quot;CN&quot;</span>,            <span class="hljs-attr">&quot;ST&quot;</span>: <span class="hljs-string">&quot;BeiJing&quot;</span>,            <span class="hljs-attr">&quot;L&quot;</span>: <span class="hljs-string">&quot;BeiJing&quot;</span>,            <span class="hljs-attr">&quot;O&quot;</span>: <span class="hljs-string">&quot;k8s&quot;</span>,            <span class="hljs-attr">&quot;OU&quot;</span>: <span class="hljs-string">&quot;System&quot;</span>        &#125;    ]&#125;</code></pre></div><h5 id="kube-proxy-csr-json"><a href="#kube-proxy-csr-json" class="headerlink" title="kube-proxy-csr.json"></a>kube-proxy-csr.json</h5><div class="hljs code-wrapper"><pre><code class="hljs json">&#123;  <span class="hljs-attr">&quot;CN&quot;</span>: <span class="hljs-string">&quot;system:kube-proxy&quot;</span>,  <span class="hljs-attr">&quot;hosts&quot;</span>: [],  <span class="hljs-attr">&quot;key&quot;</span>: &#123;    <span class="hljs-attr">&quot;algo&quot;</span>: <span class="hljs-string">&quot;rsa&quot;</span>,    <span class="hljs-attr">&quot;size&quot;</span>: <span class="hljs-number">2048</span>  &#125;,  <span class="hljs-attr">&quot;names&quot;</span>: [    &#123;      <span class="hljs-attr">&quot;C&quot;</span>: <span class="hljs-string">&quot;CN&quot;</span>,      <span class="hljs-attr">&quot;ST&quot;</span>: <span class="hljs-string">&quot;BeiJing&quot;</span>,      <span class="hljs-attr">&quot;L&quot;</span>: <span class="hljs-string">&quot;BeiJing&quot;</span>,      <span class="hljs-attr">&quot;O&quot;</span>: <span class="hljs-string">&quot;k8s&quot;</span>,      <span class="hljs-attr">&quot;OU&quot;</span>: <span class="hljs-string">&quot;System&quot;</span>    &#125;  ]&#125;</code></pre></div><h5 id="生成证书及配置"><a href="#生成证书及配置" class="headerlink" title="生成证书及配置"></a>生成证书及配置</h5><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment"># 生成 CA</span>cfssl gencert --initca=<span class="hljs-literal">true</span> k8s-root-ca-csr.json | cfssljson --bare k8s-root-ca<span class="hljs-comment"># 依次生成其他组件证书</span><span class="hljs-keyword">for</span> targetName <span class="hljs-keyword">in</span> kube-apiserver admin kube-proxy; <span class="hljs-keyword">do</span>    cfssl gencert --ca k8s-root-ca.pem --ca-key k8s-root-ca-key.pem --config k8s-gencert.json --profile kubernetes <span class="hljs-variable">$targetName</span>-csr.json | cfssljson --bare <span class="hljs-variable">$targetName</span><span class="hljs-keyword">done</span><span class="hljs-comment"># 地址默认为 127.0.0.1:6443</span><span class="hljs-comment"># 如果在 master 上启用 kubelet 请在生成后的 kubeconfig 中</span><span class="hljs-comment"># 修改该地址为 当前MASTER_IP:6443</span>KUBE_APISERVER=<span class="hljs-string">&quot;https://127.0.0.1:6443&quot;</span>BOOTSTRAP_TOKEN=$(head -c 16 /dev/urandom | od -An -t x | tr -d <span class="hljs-string">&#x27; &#x27;</span>)<span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;Tokne: <span class="hljs-variable">$&#123;BOOTSTRAP_TOKEN&#125;</span>&quot;</span><span class="hljs-comment"># 不要质疑 system:bootstrappers 用户组是否写错了，有疑问请参考官方文档</span><span class="hljs-comment"># https://kubernetes.io/docs/admin/kubelet-tls-bootstrapping/</span>cat &gt; token.csv &lt;&lt;<span class="hljs-string">EOF</span><span class="hljs-string">$&#123;BOOTSTRAP_TOKEN&#125;,kubelet-bootstrap,10001,&quot;system:bootstrappers&quot;</span><span class="hljs-string">EOF</span><span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;Create kubelet bootstrapping kubeconfig...&quot;</span><span class="hljs-comment"># 设置集群参数</span>kubectl config set-cluster kubernetes \  --certificate-authority=k8s-root-ca.pem \  --embed-certs=<span class="hljs-literal">true</span> \  --server=<span class="hljs-variable">$&#123;KUBE_APISERVER&#125;</span> \  --kubeconfig=bootstrap.kubeconfig<span class="hljs-comment"># 设置客户端认证参数</span>kubectl config set-credentials kubelet-bootstrap \  --token=<span class="hljs-variable">$&#123;BOOTSTRAP_TOKEN&#125;</span> \  --kubeconfig=bootstrap.kubeconfig<span class="hljs-comment"># 设置上下文参数</span>kubectl config set-context default \  --cluster=kubernetes \  --user=kubelet-bootstrap \  --kubeconfig=bootstrap.kubeconfig<span class="hljs-comment"># 设置默认上下文</span>kubectl config use-context default --kubeconfig=bootstrap.kubeconfig<span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;Create kube-proxy kubeconfig...&quot;</span><span class="hljs-comment"># 设置集群参数</span>kubectl config set-cluster kubernetes \  --certificate-authority=k8s-root-ca.pem \  --embed-certs=<span class="hljs-literal">true</span> \  --server=<span class="hljs-variable">$&#123;KUBE_APISERVER&#125;</span> \  --kubeconfig=kube-proxy.kubeconfig<span class="hljs-comment"># 设置客户端认证参数</span>kubectl config set-credentials kube-proxy \  --client-certificate=kube-proxy.pem \  --client-key=kube-proxy-key.pem \  --embed-certs=<span class="hljs-literal">true</span> \  --kubeconfig=kube-proxy.kubeconfig<span class="hljs-comment"># 设置上下文参数</span>kubectl config set-context default \  --cluster=kubernetes \  --user=kube-proxy \  --kubeconfig=kube-proxy.kubeconfig<span class="hljs-comment"># 设置默认上下文</span>kubectl config use-context default --kubeconfig=kube-proxy.kubeconfig<span class="hljs-comment"># 创建高级审计配置</span>cat &gt;&gt; audit-policy.yaml &lt;&lt;<span class="hljs-string">EOF</span><span class="hljs-string"># Log all requests at the Metadata level.</span><span class="hljs-string">apiVersion: audit.k8s.io/v1beta1</span><span class="hljs-string">kind: Policy</span><span class="hljs-string">rules:</span><span class="hljs-string">- level: Metadata</span><span class="hljs-string">EOF</span></code></pre></div><p>生成后文件如下</p><p><img src="https://cdn.oss.link/markdown/xk8uj.png" alt="k8s certs"></p><h4 id="3-2、准备-systemd-配置"><a href="#3-2、准备-systemd-配置" class="headerlink" title="3.2、准备 systemd 配置"></a>3.2、准备 systemd 配置</h4><p>所有组件的 <code>systemd</code> 配置如下</p><h5 id="kube-apiserver-service"><a href="#kube-apiserver-service" class="headerlink" title="kube-apiserver.service"></a>kube-apiserver.service</h5><div class="hljs code-wrapper"><pre><code class="hljs sh">[Unit]Description=Kubernetes API ServerDocumentation=https://github.com/GoogleCloudPlatform/kubernetesAfter=network.targetAfter=etcd.service[Service]EnvironmentFile=-/etc/kubernetes/configEnvironmentFile=-/etc/kubernetes/apiserverUser=kubeExecStart=/usr/<span class="hljs-built_in">local</span>/bin/hyperkube apiserver \            <span class="hljs-variable">$KUBE_LOGTOSTDERR</span> \            <span class="hljs-variable">$KUBE_LOG_LEVEL</span> \            <span class="hljs-variable">$KUBE_ETCD_SERVERS</span> \            <span class="hljs-variable">$KUBE_API_ADDRESS</span> \            <span class="hljs-variable">$KUBE_API_PORT</span> \            <span class="hljs-variable">$KUBELET_PORT</span> \            <span class="hljs-variable">$KUBE_ALLOW_PRIV</span> \            <span class="hljs-variable">$KUBE_SERVICE_ADDRESSES</span> \            <span class="hljs-variable">$KUBE_ADMISSION_CONTROL</span> \            <span class="hljs-variable">$KUBE_API_ARGS</span>Restart=on-failureType=notifyLimitNOFILE=65536[Install]WantedBy=multi-user.target</code></pre></div><h5 id="kube-controller-manager-service"><a href="#kube-controller-manager-service" class="headerlink" title="kube-controller-manager.service"></a>kube-controller-manager.service</h5><div class="hljs code-wrapper"><pre><code class="hljs sh">[Unit]Description=Kubernetes Controller ManagerDocumentation=https://github.com/GoogleCloudPlatform/kubernetes[Service]EnvironmentFile=-/etc/kubernetes/configEnvironmentFile=-/etc/kubernetes/controller-managerUser=kubeExecStart=/usr/<span class="hljs-built_in">local</span>/bin/hyperkube controller-manager \            <span class="hljs-variable">$KUBE_LOGTOSTDERR</span> \            <span class="hljs-variable">$KUBE_LOG_LEVEL</span> \            <span class="hljs-variable">$KUBE_MASTER</span> \            <span class="hljs-variable">$KUBE_CONTROLLER_MANAGER_ARGS</span>Restart=on-failureLimitNOFILE=65536[Install]WantedBy=multi-user.target</code></pre></div><h5 id="kubelet-service"><a href="#kubelet-service" class="headerlink" title="kubelet.service"></a>kubelet.service</h5><div class="hljs code-wrapper"><pre><code class="hljs sh">[Unit]Description=Kubernetes Kubelet ServerDocumentation=https://github.com/GoogleCloudPlatform/kubernetesAfter=docker.serviceRequires=docker.service[Service]WorkingDirectory=/var/lib/kubeletEnvironmentFile=-/etc/kubernetes/configEnvironmentFile=-/etc/kubernetes/kubeletExecStart=/usr/<span class="hljs-built_in">local</span>/bin/hyperkube kubelet \            <span class="hljs-variable">$KUBE_LOGTOSTDERR</span> \            <span class="hljs-variable">$KUBE_LOG_LEVEL</span> \            <span class="hljs-variable">$KUBELET_API_SERVER</span> \            <span class="hljs-variable">$KUBELET_ADDRESS</span> \            <span class="hljs-variable">$KUBELET_PORT</span> \            <span class="hljs-variable">$KUBELET_HOSTNAME</span> \            <span class="hljs-variable">$KUBE_ALLOW_PRIV</span> \            <span class="hljs-variable">$KUBELET_ARGS</span>Restart=on-failureKillMode=process[Install]WantedBy=multi-user.target</code></pre></div><h5 id="kube-proxy-service"><a href="#kube-proxy-service" class="headerlink" title="kube-proxy.service"></a>kube-proxy.service</h5><div class="hljs code-wrapper"><pre><code class="hljs sh">[Unit]Description=Kubernetes Kube-Proxy ServerDocumentation=https://github.com/GoogleCloudPlatform/kubernetesAfter=network.target[Service]EnvironmentFile=-/etc/kubernetes/configEnvironmentFile=-/etc/kubernetes/proxyExecStart=/usr/<span class="hljs-built_in">local</span>/bin/hyperkube proxy \            <span class="hljs-variable">$KUBE_LOGTOSTDERR</span> \            <span class="hljs-variable">$KUBE_LOG_LEVEL</span> \            <span class="hljs-variable">$KUBE_MASTER</span> \            <span class="hljs-variable">$KUBE_PROXY_ARGS</span>Restart=on-failureLimitNOFILE=65536[Install]WantedBy=multi-user.target</code></pre></div><h5 id="kube-scheduler-service"><a href="#kube-scheduler-service" class="headerlink" title="kube-scheduler.service"></a>kube-scheduler.service</h5><div class="hljs code-wrapper"><pre><code class="hljs sh">[Unit]Description=Kubernetes Scheduler PluginDocumentation=https://github.com/GoogleCloudPlatform/kubernetes[Service]EnvironmentFile=-/etc/kubernetes/configEnvironmentFile=-/etc/kubernetes/schedulerUser=kubeExecStart=/usr/<span class="hljs-built_in">local</span>/bin/hyperkube scheduler \            <span class="hljs-variable">$KUBE_LOGTOSTDERR</span> \            <span class="hljs-variable">$KUBE_LOG_LEVEL</span> \            <span class="hljs-variable">$KUBE_MASTER</span> \            <span class="hljs-variable">$KUBE_SCHEDULER_ARGS</span>Restart=on-failureLimitNOFILE=65536[Install]WantedBy=multi-user.target</code></pre></div><h4 id="3-3、Master-节点配置"><a href="#3-3、Master-节点配置" class="headerlink" title="3.3、Master 节点配置"></a>3.3、Master 节点配置</h4><p>Master 节点主要会运行 3 各组件: <code>kube-apiserver</code>、<code>kube-controller-manager</code>、<code>kube-scheduler</code>，其中用到的配置文件如下</p><h5 id="config"><a href="#config" class="headerlink" title="config"></a>config</h5><p><strong>config 是一个通用配置文件，值得注意的是由于安装时对于 Node、Master 节点都会包含该文件，在 Node 节点上请注释掉 <code>KUBE_MASTER</code> 变量，因为 Node 节点需要做 HA，要连接本地的 6443 加密端口；而这个变量将会覆盖 <code>kubeconfig</code> 中指定的 <code>127.0.0.1:6443</code> 地址</strong></p><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment">###</span><span class="hljs-comment"># kubernetes system config</span><span class="hljs-comment">#</span><span class="hljs-comment"># The following values are used to configure various aspects of all</span><span class="hljs-comment"># kubernetes services, including</span><span class="hljs-comment">#</span><span class="hljs-comment">#   kube-apiserver.service</span><span class="hljs-comment">#   kube-controller-manager.service</span><span class="hljs-comment">#   kube-scheduler.service</span><span class="hljs-comment">#   kubelet.service</span><span class="hljs-comment">#   kube-proxy.service</span><span class="hljs-comment"># logging to stderr means we get it in the systemd journal</span>KUBE_LOGTOSTDERR=<span class="hljs-string">&quot;--logtostderr=true&quot;</span><span class="hljs-comment"># journal message level, 0 is debug</span>KUBE_LOG_LEVEL=<span class="hljs-string">&quot;--v=2&quot;</span><span class="hljs-comment"># Should this cluster be allowed to run privileged docker containers</span>KUBE_ALLOW_PRIV=<span class="hljs-string">&quot;--allow-privileged=true&quot;</span><span class="hljs-comment"># How the controller-manager, scheduler, and proxy find the apiserver</span>KUBE_MASTER=<span class="hljs-string">&quot;--master=http://127.0.0.1:8080&quot;</span></code></pre></div><h5 id="apiserver"><a href="#apiserver" class="headerlink" title="apiserver"></a>apiserver</h5><p>apiserver 配置相对于 1.8 略有变动，其中准入控制器(<code>admission control</code>)选项名称变为了 <code>--enable-admission-plugins</code>，控制器列表也有相应变化，这里采用官方推荐配置，具体请参考 <a href="https://kubernetes.io/docs/admin/admission-controllers/#is-there-a-recommended-set-of-admission-controllers-to-use">官方文档</a></p><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment">###</span><span class="hljs-comment"># kubernetes system config</span><span class="hljs-comment">#</span><span class="hljs-comment"># The following values are used to configure the kube-apiserver</span><span class="hljs-comment">#</span><span class="hljs-comment"># The address on the local server to listen to.</span>KUBE_API_ADDRESS=<span class="hljs-string">&quot;--advertise-address=192.168.1.61 --bind-address=192.168.1.61&quot;</span><span class="hljs-comment"># The port on the local server to listen on.</span>KUBE_API_PORT=<span class="hljs-string">&quot;--secure-port=6443&quot;</span><span class="hljs-comment"># Port minions listen on</span><span class="hljs-comment"># KUBELET_PORT=&quot;--kubelet-port=10250&quot;</span><span class="hljs-comment"># Comma separated list of nodes in the etcd cluster</span>KUBE_ETCD_SERVERS=<span class="hljs-string">&quot;--etcd-servers=https://192.168.1.61:2379,https://192.168.1.62:2379,https://192.168.1.63:2379&quot;</span><span class="hljs-comment"># Address range to use for services</span>KUBE_SERVICE_ADDRESSES=<span class="hljs-string">&quot;--service-cluster-ip-range=10.254.0.0/16&quot;</span><span class="hljs-comment"># default admission control policies</span>KUBE_ADMISSION_CONTROL=<span class="hljs-string">&quot;--enable-admission-plugins=NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota,NodeRestriction&quot;</span><span class="hljs-comment"># Add your own!</span>KUBE_API_ARGS=<span class="hljs-string">&quot; --anonymous-auth=false \</span><span class="hljs-string">                --apiserver-count=3 \</span><span class="hljs-string">                --audit-log-maxage=30 \</span><span class="hljs-string">                --audit-log-maxbackup=3 \</span><span class="hljs-string">                --audit-log-maxsize=100 \</span><span class="hljs-string">                --audit-log-path=/var/log/kube-audit/audit.log \</span><span class="hljs-string">                --audit-policy-file=/etc/kubernetes/audit-policy.yaml \</span><span class="hljs-string">                --authorization-mode=Node,RBAC \</span><span class="hljs-string">                --client-ca-file=/etc/kubernetes/ssl/k8s-root-ca.pem \</span><span class="hljs-string">                --enable-bootstrap-token-auth \</span><span class="hljs-string">                --enable-garbage-collector \</span><span class="hljs-string">                --enable-logs-handler \</span><span class="hljs-string">                --enable-swagger-ui \</span><span class="hljs-string">                --etcd-cafile=/etc/etcd/ssl/etcd-root-ca.pem \</span><span class="hljs-string">                --etcd-certfile=/etc/etcd/ssl/etcd.pem \</span><span class="hljs-string">                --etcd-keyfile=/etc/etcd/ssl/etcd-key.pem \</span><span class="hljs-string">                --etcd-compaction-interval=5m0s \</span><span class="hljs-string">                --etcd-count-metric-poll-period=1m0s \</span><span class="hljs-string">                --event-ttl=48h0m0s \</span><span class="hljs-string">                --kubelet-https=true \</span><span class="hljs-string">                --kubelet-timeout=3s \</span><span class="hljs-string">                --log-flush-frequency=5s \</span><span class="hljs-string">                --token-auth-file=/etc/kubernetes/token.csv \</span><span class="hljs-string">                --tls-cert-file=/etc/kubernetes/ssl/kube-apiserver.pem \</span><span class="hljs-string">                --tls-private-key-file=/etc/kubernetes/ssl/kube-apiserver-key.pem \</span><span class="hljs-string">                --service-node-port-range=30000-50000 \</span><span class="hljs-string">                --service-account-key-file=/etc/kubernetes/ssl/k8s-root-ca.pem \</span><span class="hljs-string">                --storage-backend=etcd3 \</span><span class="hljs-string">                --enable-swagger-ui=true&quot;</span></code></pre></div><h5 id="controller-manager"><a href="#controller-manager" class="headerlink" title="controller-manager"></a>controller-manager</h5><p>controller manager 配置默认开启了证书轮换能力用于自动签署 kueblet 证书，并且证书时间也设置了 10 年，可自行调整；增加了 <code>--controllers</code> 选项以指定开启全部控制器</p><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment">###</span><span class="hljs-comment"># The following values are used to configure the kubernetes controller-manager</span><span class="hljs-comment"># defaults from config and apiserver should be adequate</span><span class="hljs-comment"># Add your own!</span>KUBE_CONTROLLER_MANAGER_ARGS=<span class="hljs-string">&quot;  --bind-address=0.0.0.0 \</span><span class="hljs-string">                                --cluster-name=kubernetes \</span><span class="hljs-string">                                --cluster-signing-cert-file=/etc/kubernetes/ssl/k8s-root-ca.pem \</span><span class="hljs-string">                                --cluster-signing-key-file=/etc/kubernetes/ssl/k8s-root-ca-key.pem \</span><span class="hljs-string">                                --controllers=*,bootstrapsigner,tokencleaner \</span><span class="hljs-string">                                --deployment-controller-sync-period=10s \</span><span class="hljs-string">                                --experimental-cluster-signing-duration=86700h0m0s \</span><span class="hljs-string">                                --leader-elect=true \</span><span class="hljs-string">                                --node-monitor-grace-period=40s \</span><span class="hljs-string">                                --node-monitor-period=5s \</span><span class="hljs-string">                                --pod-eviction-timeout=5m0s \</span><span class="hljs-string">                                --terminated-pod-gc-threshold=50 \</span><span class="hljs-string">                                --root-ca-file=/etc/kubernetes/ssl/k8s-root-ca.pem \</span><span class="hljs-string">                                --service-account-private-key-file=/etc/kubernetes/ssl/k8s-root-ca-key.pem \</span><span class="hljs-string">                                --feature-gates=RotateKubeletServerCertificate=true&quot;</span></code></pre></div><h5 id="scheduler"><a href="#scheduler" class="headerlink" title="scheduler"></a>scheduler</h5><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment">###</span><span class="hljs-comment"># kubernetes scheduler config</span><span class="hljs-comment"># default config should be adequate</span><span class="hljs-comment"># Add your own!</span>KUBE_SCHEDULER_ARGS=<span class="hljs-string">&quot;   --address=0.0.0.0 \</span><span class="hljs-string">                        --leader-elect=true \</span><span class="hljs-string">                        --algorithm-provider=DefaultProvider&quot;</span></code></pre></div><h4 id="3-4、Node-节点配置"><a href="#3-4、Node-节点配置" class="headerlink" title="3.4、Node 节点配置"></a>3.4、Node 节点配置</h4><p>Node 节点上主要有 <code>kubelet</code>、<code>kube-proxy</code> 组件，用到的配置如下</p><h5 id="kubelet"><a href="#kubelet" class="headerlink" title="kubelet"></a>kubelet</h5><p>kubeket 默认也开启了证书轮换能力以保证自动续签相关证书，同时增加了 <code>--node-labels</code> 选项为 node 打一个标签，关于这个标签最后部分会有讨论，<strong>如果在 master 上启动 kubelet，请将 <code>node-role.kubernetes.io/k8s-node=true</code> 修改为 <code>node-role.kubernetes.io/k8s-master=true</code></strong></p><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment">###</span><span class="hljs-comment"># kubernetes kubelet (minion) config</span><span class="hljs-comment"># The address for the info server to serve on (set to 0.0.0.0 or &quot;&quot; for all interfaces)</span>KUBELET_ADDRESS=<span class="hljs-string">&quot;--node-ip=192.168.1.61&quot;</span><span class="hljs-comment"># The port for the info server to serve on</span><span class="hljs-comment"># KUBELET_PORT=&quot;--port=10250&quot;</span><span class="hljs-comment"># You may leave this blank to use the actual hostname</span>KUBELET_HOSTNAME=<span class="hljs-string">&quot;--hostname-override=k1.node&quot;</span><span class="hljs-comment"># location of the api-server</span><span class="hljs-comment"># KUBELET_API_SERVER=&quot;&quot;</span><span class="hljs-comment"># Add your own!</span>KUBELET_ARGS=<span class="hljs-string">&quot;  --bootstrap-kubeconfig=/etc/kubernetes/bootstrap.kubeconfig \</span><span class="hljs-string">                --cert-dir=/etc/kubernetes/ssl \</span><span class="hljs-string">                --cgroup-driver=cgroupfs \</span><span class="hljs-string">                --cluster-dns=10.254.0.2 \</span><span class="hljs-string">                --cluster-domain=cluster.local. \</span><span class="hljs-string">                --fail-swap-on=false \</span><span class="hljs-string">                --feature-gates=RotateKubeletClientCertificate=true,RotateKubeletServerCertificate=true \</span><span class="hljs-string">                --node-labels=node-role.kubernetes.io/k8s-node=true \</span><span class="hljs-string">                --image-gc-high-threshold=70 \</span><span class="hljs-string">                --image-gc-low-threshold=50 \</span><span class="hljs-string">                --kube-reserved=cpu=500m,memory=512Mi,ephemeral-storage=1Gi \</span><span class="hljs-string">                --kubeconfig=/etc/kubernetes/kubelet.kubeconfig \</span><span class="hljs-string">                --system-reserved=cpu=1000m,memory=1024Mi,ephemeral-storage=1Gi \</span><span class="hljs-string">                --serialize-image-pulls=false \</span><span class="hljs-string">                --sync-frequency=30s \</span><span class="hljs-string">                --pod-infra-container-image=k8s.gcr.io/pause-amd64:3.0 \</span><span class="hljs-string">                --resolv-conf=/etc/resolv.conf \</span><span class="hljs-string">                --rotate-certificates&quot;</span></code></pre></div><h5 id="proxy"><a href="#proxy" class="headerlink" title="proxy"></a>proxy</h5><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment">###</span><span class="hljs-comment"># kubernetes proxy config</span><span class="hljs-comment"># default config should be adequate</span><span class="hljs-comment"># Add your own!</span>KUBE_PROXY_ARGS=<span class="hljs-string">&quot;--bind-address=0.0.0.0 \</span><span class="hljs-string">                 --hostname-override=k1.node \</span><span class="hljs-string">                 --kubeconfig=/etc/kubernetes/kube-proxy.kubeconfig \</span><span class="hljs-string">                 --cluster-cidr=10.254.0.0/16&quot;</span></code></pre></div><h4 id="3-5、安装集群组件"><a href="#3-5、安装集群组件" class="headerlink" title="3.5、安装集群组件"></a>3.5、安装集群组件</h4><p>上面已经准备好了相关配置文件，接下来将这些配置文件组织成如下目录结构以便后续脚本安装</p><div class="hljs code-wrapper"><pre><code class="hljs sh">k8s├── conf│   ├── apiserver│   ├── audit-policy.yaml│   ├── bootstrap.kubeconfig│   ├── config│   ├── controller-manager│   ├── kubelet│   ├── kube-proxy.kubeconfig│   ├── proxy│   ├── scheduler│   ├── ssl│   │   ├── admin.csr│   │   ├── admin-csr.json│   │   ├── admin-key.pem│   │   ├── admin.pem│   │   ├── k8s-gencert.json│   │   ├── k8s-root-ca.csr│   │   ├── k8s-root-ca-csr.json│   │   ├── k8s-root-ca-key.pem│   │   ├── k8s-root-ca.pem│   │   ├── kube-apiserver.csr│   │   ├── kube-apiserver-csr.json│   │   ├── kube-apiserver-key.pem│   │   ├── kube-apiserver.pem│   │   ├── kube-proxy.csr│   │   ├── kube-proxy-csr.json│   │   ├── kube-proxy-key.pem│   │   └── kube-proxy.pem│   └── token.csv├── hyperkube_1.10.1├── install.sh└── systemd    ├── kube-apiserver.service    ├── kube-controller-manager.service    ├── kubelet.service    ├── kube-proxy.service    └── kube-scheduler.service</code></pre></div><p>其中 <code>install.sh</code> 内容如下</p><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-meta">#!/bin/bash</span><span class="hljs-built_in">set</span> -eKUBE_VERSION=<span class="hljs-string">&quot;1.10.1&quot;</span><span class="hljs-keyword">function</span> <span class="hljs-function"><span class="hljs-title">download_k8s</span></span>()&#123;    <span class="hljs-keyword">if</span> [ ! -f <span class="hljs-string">&quot;hyperkube_<span class="hljs-variable">$&#123;KUBE_VERSION&#125;</span>&quot;</span> ]; <span class="hljs-keyword">then</span>        wget https://storage.googleapis.com/kubernetes-release/release/v<span class="hljs-variable">$&#123;KUBE_VERSION&#125;</span>/bin/linux/amd64/hyperkube -O hyperkube_<span class="hljs-variable">$&#123;KUBE_VERSION&#125;</span>        chmod +x hyperkube_<span class="hljs-variable">$&#123;KUBE_VERSION&#125;</span>    <span class="hljs-keyword">fi</span>&#125;<span class="hljs-keyword">function</span> <span class="hljs-function"><span class="hljs-title">preinstall</span></span>()&#123;    getent group kube &gt;/dev/null || groupadd -r kube    getent passwd kube &gt;/dev/null || useradd -r -g kube -d / -s /sbin/nologin -c <span class="hljs-string">&quot;Kubernetes user&quot;</span> kube&#125;<span class="hljs-keyword">function</span> <span class="hljs-function"><span class="hljs-title">install_k8s</span></span>()&#123;    <span class="hljs-built_in">echo</span> -e <span class="hljs-string">&quot;\033[32mINFO: Copy hyperkube...\033[0m&quot;</span>    cp hyperkube_<span class="hljs-variable">$&#123;KUBE_VERSION&#125;</span> /usr/<span class="hljs-built_in">local</span>/bin/hyperkube    <span class="hljs-built_in">echo</span> -e <span class="hljs-string">&quot;\033[32mINFO: Create symbolic link...\033[0m&quot;</span>    ln -sf /usr/<span class="hljs-built_in">local</span>/bin/hyperkube /usr/<span class="hljs-built_in">local</span>/bin/kubectl    <span class="hljs-built_in">echo</span> -e <span class="hljs-string">&quot;\033[32mINFO: Copy kubernetes config...\033[0m&quot;</span>    cp -r conf /etc/kubernetes    <span class="hljs-keyword">if</span> [ -d <span class="hljs-string">&quot;/etc/kubernetes/ssl&quot;</span> ]; <span class="hljs-keyword">then</span>        chown -R kube:kube /etc/kubernetes/ssl    <span class="hljs-keyword">fi</span>    <span class="hljs-built_in">echo</span> -e <span class="hljs-string">&quot;\033[32mINFO: Copy kubernetes systemd config...\033[0m&quot;</span>    cp systemd/*.service /lib/systemd/system    systemctl daemon-reload&#125;<span class="hljs-keyword">function</span> <span class="hljs-function"><span class="hljs-title">postinstall</span></span>()&#123;    <span class="hljs-keyword">if</span> [ ! -d <span class="hljs-string">&quot;/var/log/kube-audit&quot;</span> ]; <span class="hljs-keyword">then</span>        mkdir /var/<span class="hljs-built_in">log</span>/kube-audit    <span class="hljs-keyword">fi</span>    <span class="hljs-keyword">if</span> [ ! -d <span class="hljs-string">&quot;/var/lib/kubelet&quot;</span> ]; <span class="hljs-keyword">then</span>        mkdir /var/lib/kubelet    <span class="hljs-keyword">fi</span>    <span class="hljs-keyword">if</span> [ ! -d <span class="hljs-string">&quot;/usr/libexec&quot;</span> ]; <span class="hljs-keyword">then</span>        mkdir /usr/libexec    <span class="hljs-keyword">fi</span>    chown -R kube:kube /var/<span class="hljs-built_in">log</span>/kube-audit /var/lib/kubelet /usr/libexec&#125;download_k8spreinstallinstall_k8spostinstall</code></pre></div><p><strong>脚本解释如下:</strong></p><ul><li>download_k8s: 下载 hyperkube 二进制文件</li><li>preinstall: 安装前处理，同 etcd 一样创建 kube 普通用户指定家目录、shell 等</li><li>install_k8s: 复制 hyperkube 到安装目录，为 kubectl 创建软连接(为啥创建软连接就能执行请自行阅读 <a href="https://github.com/kubernetes/kubernetes/blob/cce67ed8e7d461657d350a1cdd55791d1637fc43/cmd/hyperkube/main.go#L69">源码</a>)，复制相关配置到对应目录，并处理权限</li><li>postinstall: 收尾工作，创建日志目录等，并处理权限</li></ul><p>最后执行此脚本安装即可，<strong>此外，应确保每个节点安装了 <code>ipset</code>、<code>conntrack</code> 两个包，因为 kube-proxy 组件会使用其处理 iptables 规则等</strong></p><h3 id="四、启动-Kubernetes-Master-节点"><a href="#四、启动-Kubernetes-Master-节点" class="headerlink" title="四、启动 Kubernetes Master 节点"></a>四、启动 Kubernetes Master 节点</h3><p>对于 <code>master</code> 节点启动无需做过多处理，多个 <code>master</code> 只要保证 <code>apiserver</code> 等配置中的 ip 地址监听没问题后直接启动即可</p><div class="hljs code-wrapper"><pre><code class="hljs sh">systemctl daemon-reloadsystemctl start kube-apiserversystemctl start kube-controller-managersystemctl start kube-schedulersystemctl <span class="hljs-built_in">enable</span> kube-apiserversystemctl <span class="hljs-built_in">enable</span> kube-controller-managersystemctl <span class="hljs-built_in">enable</span> kube-scheduler</code></pre></div><p>成功后截图如下</p><p><img src="https://cdn.oss.link/markdown/lqur1.png" alt="Master success"></p><h3 id="五、启动-Kubernetes-Node-节点"><a href="#五、启动-Kubernetes-Node-节点" class="headerlink" title="五、启动 Kubernetes Node 节点"></a>五、启动 Kubernetes Node 节点</h3><p>由于 HA 等功能需要，对于 Node 需要做一些处理才能启动，主要有以下两个地方需要处理</p><h4 id="5-1、nginx-proxy"><a href="#5-1、nginx-proxy" class="headerlink" title="5.1、nginx-proxy"></a>5.1、nginx-proxy</h4><p>在启动 <code>kubelet</code>、<code>kube-proxy</code> 服务之前，需要在本地启动 <code>nginx</code> 来 tcp 负载均衡 <code>apiserver</code> 6443 端口，<code>nginx-proxy</code> 使用 <code>docker</code> + <code>systemd</code> 启动，配置如下</p><p><strong>注意: 对于在 master 节点启动 kubelet 来说，不需要 nginx 做负载均衡；可以跳过此步骤，并修改 <code>kubelet.kubeconfig</code>、<code>kube-proxy.kubeconfig</code> 中的 apiserver 地址为当前 master ip 6443 端口即可</strong></p><ul><li>nginx-proxy.service</li></ul><div class="hljs code-wrapper"><pre><code class="hljs sh">[Unit]Description=kubernetes apiserver docker wrapperWants=docker.socketAfter=docker.service[Service]User=rootPermissionsStartOnly=<span class="hljs-literal">true</span>ExecStart=/usr/bin/docker run -p 127.0.0.1:6443:6443 \                              -v /etc/nginx:/etc/nginx \                              --name nginx-proxy \                              --net=host \                              --restart=on-failure:5 \                              --memory=512M \                              nginx:1.13.12-alpineExecStartPre=-/usr/bin/docker rm -f nginx-proxyExecStop=/usr/bin/docker stop nginx-proxyRestart=alwaysRestartSec=15sTimeoutStartSec=30s[Install]WantedBy=multi-user.target</code></pre></div><ul><li>nginx.conf</li></ul><div class="hljs code-wrapper"><pre><code class="hljs sh">error_log stderr notice;worker_processes auto;events &#123;        multi_accept on;        use epoll;        worker_connections 1024;&#125;stream &#123;    upstream kube_apiserver &#123;        least_conn;        server 192.168.1.61:6443;        server 192.168.1.62:6443;        server 192.168.1.63:6443;    &#125;    server &#123;        listen        0.0.0.0:6443;        proxy_pass    kube_apiserver;        proxy_timeout 10m;        proxy_connect_timeout 1s;    &#125;&#125;</code></pre></div><p><strong>启动 apiserver 的本地负载均衡</strong></p><div class="hljs code-wrapper"><pre><code class="hljs sh">mkdir /etc/nginxcp nginx.conf /etc/nginxcp nginx-proxy.service /lib/systemd/systemsystemctl daemon-reloadsystemctl start nginx-proxysystemctl <span class="hljs-built_in">enable</span> nginx-proxy</code></pre></div><h4 id="5-2、TLS-bootstrapping"><a href="#5-2、TLS-bootstrapping" class="headerlink" title="5.2、TLS bootstrapping"></a>5.2、TLS bootstrapping</h4><p>创建好 <code>nginx-proxy</code> 后不要忘记为 <code>TLS Bootstrap</code> 创建相应的 <code>RBAC</code> 规则，这些规则能实现证自动签署 <code>TLS Bootstrap</code> 发出的 <code>CSR</code> 请求，从而实现证书轮换(创建一次即可)；详情请参考 <a href="https://mritd.me/2018/01/07/kubernetes-tls-bootstrapping-note/">Kubernetes TLS bootstrapping 那点事</a></p><ul><li>tls-bootstrapping-clusterrole.yaml(与 1.8 一样)</li></ul><div class="hljs code-wrapper"><pre><code class="hljs yaml"><span class="hljs-comment"># A ClusterRole which instructs the CSR approver to approve a node requesting a</span><span class="hljs-comment"># serving cert matching its client cert.</span><span class="hljs-attr">kind:</span> <span class="hljs-string">ClusterRole</span><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">rbac.authorization.k8s.io/v1</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">system:certificates.k8s.io:certificatesigningrequests:selfnodeserver</span><span class="hljs-attr">rules:</span><span class="hljs-bullet">-</span> <span class="hljs-attr">apiGroups:</span> [<span class="hljs-string">&quot;certificates.k8s.io&quot;</span>]  <span class="hljs-attr">resources:</span> [<span class="hljs-string">&quot;certificatesigningrequests/selfnodeserver&quot;</span>]  <span class="hljs-attr">verbs:</span> [<span class="hljs-string">&quot;create&quot;</span>]</code></pre></div><p><strong>在 master 执行创建</strong></p><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment"># 给与 kubelet-bootstrap 用户进行 node-bootstrapper 的权限</span>kubectl create clusterrolebinding kubelet-bootstrap \    --clusterrole=system:node-bootstrapper \    --user=kubelet-bootstrapkubectl create -f tls-bootstrapping-clusterrole.yaml<span class="hljs-comment"># 自动批准 system:bootstrappers 组用户 TLS bootstrapping 首次申请证书的 CSR 请求</span>kubectl create clusterrolebinding node-client-auto-approve-csr \        --clusterrole=system:certificates.k8s.io:certificatesigningrequests:nodeclient \        --group=system:bootstrappers<span class="hljs-comment"># 自动批准 system:nodes 组用户更新 kubelet 自身与 apiserver 通讯证书的 CSR 请求</span>kubectl create clusterrolebinding node-client-auto-renew-crt \        --clusterrole=system:certificates.k8s.io:certificatesigningrequests:selfnodeclient \        --group=system:nodes<span class="hljs-comment"># 自动批准 system:nodes 组用户更新 kubelet 10250 api 端口证书的 CSR 请求</span>kubectl create clusterrolebinding node-server-auto-renew-crt \        --clusterrole=system:certificates.k8s.io:certificatesigningrequests:selfnodeserver \        --group=system:nodes</code></pre></div><h4 id="5-3、执行启动"><a href="#5-3、执行启动" class="headerlink" title="5.3、执行启动"></a>5.3、执行启动</h4><p>多节点部署时先启动好 <code>nginx-proxy</code>，然后修改好相应配置的 ip 地址等配置，最终直接启动即可(master 上启动 kubelet 不要忘了修改 kubeconfig 中的 apiserver 地址，还有对应的 kubelet 的 node label)</p><div class="hljs code-wrapper"><pre><code class="hljs sh">systemctl daemon-reloadsystemctl start kubeletsystemctl start kube-proxysystemctl <span class="hljs-built_in">enable</span> kubeletsystemctl <span class="hljs-built_in">enable</span> kube-proxy</code></pre></div><p>最后启动成功后如下</p><p><img src="https://cdn.oss.link/markdown/r4s34.png" alt="cluster started"></p><h3 id="五、安装-Calico"><a href="#五、安装-Calico" class="headerlink" title="五、安装 Calico"></a>五、安装 Calico</h3><p>Calico 安装仍然延续以前的方案，使用 Daemonset 安装 cni 组件，使用 systemd 控制 calico-node 以确保 calico-node 能正确的拿到主机名等</p><h4 id="5-1、修改-Calico-配置"><a href="#5-1、修改-Calico-配置" class="headerlink" title="5.1、修改 Calico 配置"></a>5.1、修改 Calico 配置</h4><div class="hljs code-wrapper"><pre><code class="hljs sh">wget https://docs.projectcalico.org/v3.1/getting-started/kubernetes/installation/hosted/calico.yaml -O calico.example.yamlETCD_CERT=`cat /etc/etcd/ssl/etcd.pem | base64 | tr -d <span class="hljs-string">&#x27;\n&#x27;</span>`ETCD_KEY=`cat /etc/etcd/ssl/etcd-key.pem | base64 | tr -d <span class="hljs-string">&#x27;\n&#x27;</span>`ETCD_CA=`cat /etc/etcd/ssl/etcd-root-ca.pem | base64 | tr -d <span class="hljs-string">&#x27;\n&#x27;</span>`ETCD_ENDPOINTS=<span class="hljs-string">&quot;https://192.168.1.61:2379,https://192.168.1.62:2379,https://192.168.1.63:2379&quot;</span>cp calico.example.yaml calico.yamlsed -i <span class="hljs-string">&quot;s@.*etcd_endpoints:.*@\ \ etcd_endpoints:\ \&quot;<span class="hljs-variable">$&#123;ETCD_ENDPOINTS&#125;</span>\&quot;@gi&quot;</span> calico.yamlsed -i <span class="hljs-string">&quot;s@.*etcd-cert:.*@\ \ etcd-cert:\ <span class="hljs-variable">$&#123;ETCD_CERT&#125;</span>@gi&quot;</span> calico.yamlsed -i <span class="hljs-string">&quot;s@.*etcd-key:.*@\ \ etcd-key:\ <span class="hljs-variable">$&#123;ETCD_KEY&#125;</span>@gi&quot;</span> calico.yamlsed -i <span class="hljs-string">&quot;s@.*etcd-ca:.*@\ \ etcd-ca:\ <span class="hljs-variable">$&#123;ETCD_CA&#125;</span>@gi&quot;</span> calico.yamlsed -i <span class="hljs-string">&#x27;s@.*etcd_ca:.*@\ \ etcd_ca:\ &quot;/calico-secrets/etcd-ca&quot;@gi&#x27;</span> calico.yamlsed -i <span class="hljs-string">&#x27;s@.*etcd_cert:.*@\ \ etcd_cert:\ &quot;/calico-secrets/etcd-cert&quot;@gi&#x27;</span> calico.yamlsed -i <span class="hljs-string">&#x27;s@.*etcd_key:.*@\ \ etcd_key:\ &quot;/calico-secrets/etcd-key&quot;@gi&#x27;</span> calico.yaml<span class="hljs-comment"># 注释掉 calico-node 部分(由 Systemd 接管)</span>sed -i <span class="hljs-string">&#x27;123,219s@.*@#&amp;@gi&#x27;</span> calico.yaml</code></pre></div><h4 id="5-2、创建-Systemd-文件"><a href="#5-2、创建-Systemd-文件" class="headerlink" title="5.2、创建 Systemd 文件"></a>5.2、创建 Systemd 文件</h4><p><strong>注意: 创建 systemd service 配置文件要在每个节点上都执行</strong></p><div class="hljs code-wrapper"><pre><code class="hljs sh">K8S_MASTER_IP=<span class="hljs-string">&quot;192.168.1.61&quot;</span>HOSTNAME=`cat /etc/hostname`ETCD_ENDPOINTS=<span class="hljs-string">&quot;https://192.168.1.61:2379,https://192.168.1.62:2379,https://192.168.1.63:2379&quot;</span>cat &gt; /lib/systemd/system/calico-node.service &lt;&lt;<span class="hljs-string">EOF</span><span class="hljs-string">[Unit]</span><span class="hljs-string">Description=calico node</span><span class="hljs-string">After=docker.service</span><span class="hljs-string">Requires=docker.service</span><span class="hljs-string"></span><span class="hljs-string">[Service]</span><span class="hljs-string">User=root</span><span class="hljs-string">Environment=ETCD_ENDPOINTS=$&#123;ETCD_ENDPOINTS&#125;</span><span class="hljs-string">PermissionsStartOnly=true</span><span class="hljs-string">ExecStart=/usr/bin/docker run   --net=host --privileged --name=calico-node \\</span><span class="hljs-string">                                -e ETCD_ENDPOINTS=\$&#123;ETCD_ENDPOINTS&#125; \\</span><span class="hljs-string">                                -e ETCD_CA_CERT_FILE=/etc/etcd/ssl/etcd-root-ca.pem \\</span><span class="hljs-string">                                -e ETCD_CERT_FILE=/etc/etcd/ssl/etcd.pem \\</span><span class="hljs-string">                                -e ETCD_KEY_FILE=/etc/etcd/ssl/etcd-key.pem \\</span><span class="hljs-string">                                -e NODENAME=$&#123;HOSTNAME&#125; \\</span><span class="hljs-string">                                -e IP= \\</span><span class="hljs-string">                                -e IP_AUTODETECTION_METHOD=can-reach=$&#123;K8S_MASTER_IP&#125; \\</span><span class="hljs-string">                                -e AS=64512 \\</span><span class="hljs-string">                                -e CLUSTER_TYPE=k8s,bgp \\</span><span class="hljs-string">                                -e CALICO_IPV4POOL_CIDR=10.20.0.0/16 \\</span><span class="hljs-string">                                -e CALICO_IPV4POOL_IPIP=always \\</span><span class="hljs-string">                                -e CALICO_LIBNETWORK_ENABLED=true \\</span><span class="hljs-string">                                -e CALICO_NETWORKING_BACKEND=bird \\</span><span class="hljs-string">                                -e CALICO_DISABLE_FILE_LOGGING=true \\</span><span class="hljs-string">                                -e FELIX_IPV6SUPPORT=false \\</span><span class="hljs-string">                                -e FELIX_DEFAULTENDPOINTTOHOSTACTION=ACCEPT \\</span><span class="hljs-string">                                -e FELIX_LOGSEVERITYSCREEN=info \\</span><span class="hljs-string">                                -e FELIX_IPINIPMTU=1440 \\</span><span class="hljs-string">                                -e FELIX_HEALTHENABLED=true \\</span><span class="hljs-string">                                -e CALICO_K8S_NODE_REF=$&#123;HOSTNAME&#125; \\</span><span class="hljs-string">                                -v /etc/calico/etcd-root-ca.pem:/etc/etcd/ssl/etcd-root-ca.pem \\</span><span class="hljs-string">                                -v /etc/calico/etcd.pem:/etc/etcd/ssl/etcd.pem \\</span><span class="hljs-string">                                -v /etc/calico/etcd-key.pem:/etc/etcd/ssl/etcd-key.pem \\</span><span class="hljs-string">                                -v /lib/modules:/lib/modules \\</span><span class="hljs-string">                                -v /var/lib/calico:/var/lib/calico \\</span><span class="hljs-string">                                -v /var/run/calico:/var/run/calico \\</span><span class="hljs-string">                                quay.io/calico/node:v3.1.0</span><span class="hljs-string">ExecStop=/usr/bin/docker rm -f calico-node</span><span class="hljs-string">Restart=always</span><span class="hljs-string">RestartSec=10</span><span class="hljs-string"></span><span class="hljs-string">[Install]</span><span class="hljs-string">WantedBy=multi-user.target</span><span class="hljs-string">EOF</span></code></pre></div><p><strong>对于以上脚本中的 <code>K8S_MASTER_IP</code> 变量，只需要填写一个 master ip 即可，这个变量用于 calico 自动选择 IP 使用；在宿主机有多张网卡的情况下，calcio node 会自动获取一个 IP，获取原则就是尝试是否能够联通这个 master ip</strong></p><p>由于 calico 需要使用 etcd 存储数据，所以需要复制 etcd 证书到相关目录，**<code>/etc/calico</code> 需要在每个节点都有**</p><div class="hljs code-wrapper"><pre><code class="hljs sh">cp -r /etc/etcd/ssl /etc/calico</code></pre></div><h4 id="5-3、修改-kubelet-配置"><a href="#5-3、修改-kubelet-配置" class="headerlink" title="5.3、修改 kubelet 配置"></a>5.3、修改 kubelet 配置</h4><p>使用 Calico 后需要修改 kubelet 配置增加 CNI 设置(<code>--network-plugin=cni</code>)，修改后配置如下</p><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment">###</span><span class="hljs-comment"># kubernetes kubelet (minion) config</span><span class="hljs-comment"># The address for the info server to serve on (set to 0.0.0.0 or &quot;&quot; for all interfaces)</span>KUBELET_ADDRESS=<span class="hljs-string">&quot;--node-ip=192.168.1.61&quot;</span><span class="hljs-comment"># The port for the info server to serve on</span><span class="hljs-comment"># KUBELET_PORT=&quot;--port=10250&quot;</span><span class="hljs-comment"># You may leave this blank to use the actual hostname</span>KUBELET_HOSTNAME=<span class="hljs-string">&quot;--hostname-override=k1.node&quot;</span><span class="hljs-comment"># location of the api-server</span><span class="hljs-comment"># KUBELET_API_SERVER=&quot;&quot;</span><span class="hljs-comment"># Add your own!</span>KUBELET_ARGS=<span class="hljs-string">&quot;  --bootstrap-kubeconfig=/etc/kubernetes/bootstrap.kubeconfig \</span><span class="hljs-string">                --cert-dir=/etc/kubernetes/ssl \</span><span class="hljs-string">                --cgroup-driver=cgroupfs \</span><span class="hljs-string">                --network-plugin=cni \</span><span class="hljs-string">                --cluster-dns=10.254.0.2 \</span><span class="hljs-string">                --cluster-domain=cluster.local. \</span><span class="hljs-string">                --fail-swap-on=false \</span><span class="hljs-string">                --feature-gates=RotateKubeletClientCertificate=true,RotateKubeletServerCertificate=true \</span><span class="hljs-string">                --node-labels=node-role.kubernetes.io/k8s-master=true \</span><span class="hljs-string">                --image-gc-high-threshold=70 \</span><span class="hljs-string">                --image-gc-low-threshold=50 \</span><span class="hljs-string">                --kube-reserved=cpu=500m,memory=512Mi,ephemeral-storage=1Gi \</span><span class="hljs-string">                --kubeconfig=/etc/kubernetes/kubelet.kubeconfig \</span><span class="hljs-string">                --system-reserved=cpu=1000m,memory=1024Mi,ephemeral-storage=1Gi \</span><span class="hljs-string">                --serialize-image-pulls=false \</span><span class="hljs-string">                --sync-frequency=30s \</span><span class="hljs-string">                --pod-infra-container-image=k8s.gcr.io/pause-amd64:3.0 \</span><span class="hljs-string">                --resolv-conf=/etc/resolv.conf \</span><span class="hljs-string">                --rotate-certificates&quot;</span></code></pre></div><h4 id="5-4、创建-Calico-Daemonset"><a href="#5-4、创建-Calico-Daemonset" class="headerlink" title="5.4、创建 Calico Daemonset"></a>5.4、创建 Calico Daemonset</h4><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment"># 先创建 RBAC</span>kubectl apply -f \https://docs.projectcalico.org/v3.1/getting-started/kubernetes/installation/rbac.yaml<span class="hljs-comment"># 再创建 Calico Daemonset</span>kubectl create -f calico.yaml</code></pre></div><h4 id="5-5、启动-Calico-Node"><a href="#5-5、启动-Calico-Node" class="headerlink" title="5.5、启动 Calico Node"></a>5.5、启动 Calico Node</h4><div class="hljs code-wrapper"><pre><code class="hljs sh">systemctl daemon-reloadsystemctl restart calico-nodesystemctl <span class="hljs-built_in">enable</span> calico-node<span class="hljs-comment"># 等待 20s 拉取镜像</span>sleep 20systemctl restart kubelet</code></pre></div><h4 id="5-6、测试网络"><a href="#5-6、测试网络" class="headerlink" title="5.6、测试网络"></a>5.6、测试网络</h4><p>网络测试与其他几篇文章一样，创建几个 pod 测试即可</p><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment"># 创建 deployment</span>cat &lt;&lt; <span class="hljs-string">EOF &gt;&gt; demo.deploy.yml</span><span class="hljs-string">apiVersion: apps/v1</span><span class="hljs-string">kind: Deployment</span><span class="hljs-string">metadata:</span><span class="hljs-string">  name: demo-deployment</span><span class="hljs-string">spec:</span><span class="hljs-string">  replicas: 5</span><span class="hljs-string">  selector:</span><span class="hljs-string">    matchLabels:</span><span class="hljs-string">      app: demo</span><span class="hljs-string">  template:</span><span class="hljs-string">    metadata:</span><span class="hljs-string">      labels:</span><span class="hljs-string">        app: demo</span><span class="hljs-string">    spec:</span><span class="hljs-string">      containers:</span><span class="hljs-string">      - name: demo</span><span class="hljs-string">        image: mritd/demo</span><span class="hljs-string">        imagePullPolicy: IfNotPresent</span><span class="hljs-string">        ports:</span><span class="hljs-string">        - containerPort: 80</span><span class="hljs-string">EOF</span>kubectl create -f demo.deploy.yml</code></pre></div><p>测试结果如图所示</p><p><img src="https://cdn.oss.link/markdown/u9j3v.png" alt="test calico"></p><h3 id="六、部署集群-DNS"><a href="#六、部署集群-DNS" class="headerlink" title="六、部署集群 DNS"></a>六、部署集群 DNS</h3><h4 id="6-1、部署-CoreDNS"><a href="#6-1、部署-CoreDNS" class="headerlink" title="6.1、部署 CoreDNS"></a>6.1、部署 CoreDNS</h4><p>CoreDNS 给出了标准的 deployment 配置，如下</p><ul><li>coredns.yaml.sed</li></ul><div class="hljs code-wrapper"><pre><code class="hljs sh">apiVersion: v1kind: ServiceAccountmetadata:  name: coredns  namespace: kube-system---apiVersion: rbac.authorization.k8s.io/v1beta1kind: ClusterRolemetadata:  labels:    kubernetes.io/bootstrapping: rbac-defaults  name: system:corednsrules:- apiGroups:  - <span class="hljs-string">&quot;&quot;</span>  resources:  - endpoints  - services  - pods  - namespaces  verbs:  - list  - watch---apiVersion: rbac.authorization.k8s.io/v1beta1kind: ClusterRoleBindingmetadata:  annotations:    rbac.authorization.kubernetes.io/autoupdate: <span class="hljs-string">&quot;true&quot;</span>  labels:    kubernetes.io/bootstrapping: rbac-defaults  name: system:corednsroleRef:  apiGroup: rbac.authorization.k8s.io  kind: ClusterRole  name: system:corednssubjects:- kind: ServiceAccount  name: coredns  namespace: kube-system---apiVersion: v1kind: ConfigMapmetadata:  name: coredns  namespace: kube-systemdata:  Corefile: |    .:53 &#123;        errors        health        kubernetes CLUSTER_DOMAIN REVERSE_CIDRS &#123;          pods insecure          upstream          fallthrough in-addr.arpa ip6.arpa        &#125;        prometheus :9153        proxy . /etc/resolv.conf        cache 30    &#125;---apiVersion: extensions/v1beta1kind: Deploymentmetadata:  name: coredns  namespace: kube-system  labels:    k8s-app: kube-dns    kubernetes.io/name: <span class="hljs-string">&quot;CoreDNS&quot;</span>spec:  replicas: 2  strategy:    <span class="hljs-built_in">type</span>: RollingUpdate    rollingUpdate:      maxUnavailable: 1  selector:    matchLabels:      k8s-app: kube-dns  template:    metadata:      labels:        k8s-app: kube-dns    spec:      serviceAccountName: coredns      tolerations:        - key: <span class="hljs-string">&quot;CriticalAddonsOnly&quot;</span>          operator: <span class="hljs-string">&quot;Exists&quot;</span>      containers:      - name: coredns        image: coredns/coredns:1.1.1        imagePullPolicy: IfNotPresent        args: [ <span class="hljs-string">&quot;-conf&quot;</span>, <span class="hljs-string">&quot;/etc/coredns/Corefile&quot;</span> ]        volumeMounts:        - name: config-volume          mountPath: /etc/coredns        ports:        - containerPort: 53          name: dns          protocol: UDP        - containerPort: 53          name: dns-tcp          protocol: TCP        - containerPort: 9153          name: metrics          protocol: TCP        livenessProbe:          httpGet:            path: /health            port: 8080            scheme: HTTP          initialDelaySeconds: 60          timeoutSeconds: 5          successThreshold: 1          failureThreshold: 5      dnsPolicy: Default      volumes:        - name: config-volume          configMap:            name: coredns            items:            - key: Corefile              path: Corefile---apiVersion: v1kind: Servicemetadata:  name: kube-dns  namespace: kube-system  annotations:    prometheus.io/scrape: <span class="hljs-string">&quot;true&quot;</span>  labels:    k8s-app: kube-dns    kubernetes.io/cluster-service: <span class="hljs-string">&quot;true&quot;</span>    kubernetes.io/name: <span class="hljs-string">&quot;CoreDNS&quot;</span>spec:  selector:    k8s-app: kube-dns  clusterIP: CLUSTER_DNS_IP  ports:  - name: dns    port: 53    protocol: UDP  - name: dns-tcp    port: 53    protocol: TCP</code></pre></div><p>然后直接使用脚本替换即可(脚本变量我已经修改了)</p><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-meta">#!/bin/bash</span><span class="hljs-comment"># Deploys CoreDNS to a cluster currently running Kube-DNS.</span>SERVICE_CIDR=<span class="hljs-variable">$&#123;1:-10.254.0.0/16&#125;</span>POD_CIDR=<span class="hljs-variable">$&#123;2:-10.20.0.0/16&#125;</span>CLUSTER_DNS_IP=<span class="hljs-variable">$&#123;3:-10.254.0.2&#125;</span>CLUSTER_DOMAIN=<span class="hljs-variable">$&#123;4:-cluster.local&#125;</span>YAML_TEMPLATE=<span class="hljs-variable">$&#123;5:-`pwd`/coredns.yaml.sed&#125;</span>sed -e s/CLUSTER_DNS_IP/<span class="hljs-variable">$CLUSTER_DNS_IP</span>/g -e s/CLUSTER_DOMAIN/<span class="hljs-variable">$CLUSTER_DOMAIN</span>/g -e s?SERVICE_CIDR?<span class="hljs-variable">$SERVICE_CIDR</span>?g -e s?POD_CIDR?<span class="hljs-variable">$POD_CIDR</span>?g <span class="hljs-variable">$YAML_TEMPLATE</span> &gt; coredns.yaml</code></pre></div><p>最后使用 <code>kubectl</code> 创建一下</p><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment"># 执行上面的替换脚本</span>./deploy.sh<span class="hljs-comment"># 创建 CoreDNS</span>kubectl create -f coredns.yaml</code></pre></div><p>测试截图如下</p><p><img src="https://cdn.oss.link/markdown/v1jdc.png" alt="test dns"></p><h4 id="6-2、部署-DNS-自动扩容"><a href="#6-2、部署-DNS-自动扩容" class="headerlink" title="6.2、部署 DNS 自动扩容"></a>6.2、部署 DNS 自动扩容</h4><p>自动扩容跟以往一样，yaml 创建一下就行</p><ul><li>dns-horizontal-autoscaler.yaml</li></ul><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment"># Copyright 2016 The Kubernetes Authors.</span><span class="hljs-comment">#</span><span class="hljs-comment"># Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span><span class="hljs-comment"># you may not use this file except in compliance with the License.</span><span class="hljs-comment"># You may obtain a copy of the License at</span><span class="hljs-comment">#</span><span class="hljs-comment">#     http://www.apache.org/licenses/LICENSE-2.0</span><span class="hljs-comment">#</span><span class="hljs-comment"># Unless required by applicable law or agreed to in writing, software</span><span class="hljs-comment"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span><span class="hljs-comment"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span><span class="hljs-comment"># See the License for the specific language governing permissions and</span><span class="hljs-comment"># limitations under the License.</span>kind: ServiceAccountapiVersion: v1metadata:  name: kube-dns-autoscaler  namespace: kube-system  labels:    addonmanager.kubernetes.io/mode: Reconcile---kind: ClusterRoleapiVersion: rbac.authorization.k8s.io/v1metadata:  name: system:kube-dns-autoscaler  labels:    addonmanager.kubernetes.io/mode: Reconcilerules:  - apiGroups: [<span class="hljs-string">&quot;&quot;</span>]    resources: [<span class="hljs-string">&quot;nodes&quot;</span>]    verbs: [<span class="hljs-string">&quot;list&quot;</span>]  - apiGroups: [<span class="hljs-string">&quot;&quot;</span>]    resources: [<span class="hljs-string">&quot;replicationcontrollers/scale&quot;</span>]    verbs: [<span class="hljs-string">&quot;get&quot;</span>, <span class="hljs-string">&quot;update&quot;</span>]  - apiGroups: [<span class="hljs-string">&quot;extensions&quot;</span>]    resources: [<span class="hljs-string">&quot;deployments/scale&quot;</span>, <span class="hljs-string">&quot;replicasets/scale&quot;</span>]    verbs: [<span class="hljs-string">&quot;get&quot;</span>, <span class="hljs-string">&quot;update&quot;</span>]<span class="hljs-comment"># Remove the configmaps rule once below issue is fixed:</span><span class="hljs-comment"># kubernetes-incubator/cluster-proportional-autoscaler#16</span>  - apiGroups: [<span class="hljs-string">&quot;&quot;</span>]    resources: [<span class="hljs-string">&quot;configmaps&quot;</span>]    verbs: [<span class="hljs-string">&quot;get&quot;</span>, <span class="hljs-string">&quot;create&quot;</span>]---kind: ClusterRoleBindingapiVersion: rbac.authorization.k8s.io/v1metadata:  name: system:kube-dns-autoscaler  labels:    addonmanager.kubernetes.io/mode: Reconcilesubjects:  - kind: ServiceAccount    name: kube-dns-autoscaler    namespace: kube-systemroleRef:  kind: ClusterRole  name: system:kube-dns-autoscaler  apiGroup: rbac.authorization.k8s.io---apiVersion: apps/v1kind: Deploymentmetadata:  name: kube-dns-autoscaler  namespace: kube-system  labels:    k8s-app: kube-dns-autoscaler    kubernetes.io/cluster-service: <span class="hljs-string">&quot;true&quot;</span>    addonmanager.kubernetes.io/mode: Reconcilespec:  selector:    matchLabels:      k8s-app: kube-dns-autoscaler  template:    metadata:      labels:        k8s-app: kube-dns-autoscaler      annotations:        scheduler.alpha.kubernetes.io/critical-pod: <span class="hljs-string">&#x27;&#x27;</span>    spec:      priorityClassName: system-cluster-critical      containers:      - name: autoscaler        image: k8s.gcr.io/cluster-proportional-autoscaler-amd64:1.1.2-r2        resources:            requests:                cpu: <span class="hljs-string">&quot;20m&quot;</span>                memory: <span class="hljs-string">&quot;10Mi&quot;</span>        <span class="hljs-built_in">command</span>:          - /cluster-proportional-autoscaler          - --namespace=kube-system          - --configmap=kube-dns-autoscaler          <span class="hljs-comment"># Should keep target in sync with cluster/addons/dns/kube-dns.yaml.base</span>          - --target=Deployment/kube-dns          <span class="hljs-comment"># When cluster is using large nodes(with more cores), &quot;coresPerReplica&quot; should dominate.</span>          <span class="hljs-comment"># If using small nodes, &quot;nodesPerReplica&quot; should dominate.</span>          - --default-params=&#123;<span class="hljs-string">&quot;linear&quot;</span>:&#123;<span class="hljs-string">&quot;coresPerReplica&quot;</span>:256,<span class="hljs-string">&quot;nodesPerReplica&quot;</span>:16,<span class="hljs-string">&quot;preventSinglePointFailure&quot;</span>:<span class="hljs-literal">true</span>&#125;&#125;          - --logtostderr=<span class="hljs-literal">true</span>          - --v=2      tolerations:      - key: <span class="hljs-string">&quot;CriticalAddonsOnly&quot;</span>        operator: <span class="hljs-string">&quot;Exists&quot;</span>      serviceAccountName: kube-dns-autoscaler</code></pre></div><h3 id="七、部署-heapster"><a href="#七、部署-heapster" class="headerlink" title="七、部署 heapster"></a>七、部署 heapster</h3><p>heapster 部署相对简单的多，yaml 创建一下就可以了</p><div class="hljs code-wrapper"><pre><code class="hljs sh">kubectl create -f https://raw.githubusercontent.com/kubernetes/heapster/master/deploy/kube-config/influxdb/grafana.yamlkubectl create -f https://raw.githubusercontent.com/kubernetes/heapster/master/deploy/kube-config/influxdb/heapster.yamlkubectl create -f https://raw.githubusercontent.com/kubernetes/heapster/master/deploy/kube-config/influxdb/influxdb.yamlkubectl create -f https://raw.githubusercontent.com/kubernetes/heapster/master/deploy/kube-config/rbac/heapster-rbac.yaml</code></pre></div><h3 id="八、部署-Dashboard"><a href="#八、部署-Dashboard" class="headerlink" title="八、部署 Dashboard"></a>八、部署 Dashboard</h3><h4 id="8-1、部署-Dashboard"><a href="#8-1、部署-Dashboard" class="headerlink" title="8.1、部署 Dashboard"></a>8.1、部署 Dashboard</h4><p>Dashboard 部署同 heapster 一样，不过为了方便访问，我设置了 NodePort，还注意到一点是 yaml 拉取策略已经没有比较傻的 <code>Always</code> 了</p><div class="hljs code-wrapper"><pre><code class="hljs sh">wget https://raw.githubusercontent.com/kubernetes/dashboard/master/src/deploy/recommended/kubernetes-dashboard.yaml -O kubernetes-dashboard.yaml</code></pre></div><p>将最后部分的端口暴露修改如下</p><div class="hljs code-wrapper"><pre><code class="hljs yaml"><span class="hljs-comment"># ------------------- Dashboard Service ------------------- #</span><span class="hljs-attr">kind:</span> <span class="hljs-string">Service</span><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">labels:</span>    <span class="hljs-attr">k8s-app:</span> <span class="hljs-string">kubernetes-dashboard</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">kubernetes-dashboard</span>  <span class="hljs-attr">namespace:</span> <span class="hljs-string">kube-system</span><span class="hljs-attr">spec:</span>  <span class="hljs-attr">type:</span> <span class="hljs-string">NodePort</span>  <span class="hljs-attr">ports:</span>    <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">dashboard-tls</span>      <span class="hljs-attr">port:</span> <span class="hljs-number">443</span>      <span class="hljs-attr">targetPort:</span> <span class="hljs-number">8443</span>      <span class="hljs-attr">nodePort:</span> <span class="hljs-number">30000</span>      <span class="hljs-attr">protocol:</span> <span class="hljs-string">TCP</span>  <span class="hljs-attr">selector:</span>    <span class="hljs-attr">k8s-app:</span> <span class="hljs-string">kubernetes-dashboard</span></code></pre></div><p>然后执行 <code>kubectl create -f kubernetes-dashboard.yaml</code> 即可</p><h4 id="8-2、创建-admin-账户"><a href="#8-2、创建-admin-账户" class="headerlink" title="8.2、创建 admin 账户"></a>8.2、创建 admin 账户</h4><p>默认情况下部署成功后可以直接访问 <code>https://NODE_IP:30000</code> 访问，但是想要登录进去查看的话需要使用 kubeconfig 或者 access token 的方式；实际上这个就是 RBAC 授权控制，以下提供一个创建 admin access token 的脚本，更细节的权限控制比如只读用户可以参考 <a href="https://mritd.me/2018/03/20/use-rbac-to-control-kubectl-permissions/">使用 RBAC 控制 kubectl 权限</a>，RBAC 权限控制原理是一样的</p><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-meta">#!/bin/bash</span><span class="hljs-keyword">if</span> kubectl get sa dashboard-admin -n kube-system &amp;&gt; /dev/null;<span class="hljs-keyword">then</span>    <span class="hljs-built_in">echo</span> -e <span class="hljs-string">&quot;\033[33mWARNING: ServiceAccount dashboard-admin exist!\033[0m&quot;</span><span class="hljs-keyword">else</span>    kubectl create sa dashboard-admin -n kube-system    kubectl create clusterrolebinding dashboard-admin --clusterrole=cluster-admin --serviceaccount=kube-system:dashboard-admin<span class="hljs-keyword">fi</span>kubectl describe secret -n kube-system $(kubectl get secrets -n kube-system | grep dashboard-admin | cut -f1 -d <span class="hljs-string">&#x27; &#x27;</span>) | grep -E <span class="hljs-string">&#x27;^token&#x27;</span></code></pre></div><p>将以上脚本保存为 <code>create_dashboard_sa.sh</code> 执行即可，成功后访问截图如下(<strong>如果访问不了的话请检查下 iptable FORWARD 默认规则是否为 DROP，如果是将其改为 ACCEPT 即可</strong>)</p><p><img src="https://cdn.oss.link/markdown/oxmms.png" alt="create_dashboard_sa"></p><p><img src="https://cdn.oss.link/markdown/pyplb.png" alt="dashboard"></p><h3 id="九、其他说明"><a href="#九、其他说明" class="headerlink" title="九、其他说明"></a>九、其他说明</h3><h4 id="9-1、选项-label-等说明"><a href="#9-1、选项-label-等说明" class="headerlink" title="9.1、选项 label 等说明"></a>9.1、选项 label 等说明</h4><p>部署过程中注意到一些选项已经做了名称更改，比如 <code>--network-plugin-dir</code> 变更为 <code>--cni-bin-dir</code> 等，具体的那些选项做了变更请自行对比配置，以及查看官方文档；</p><p>对于 Node label <code>--node-labels=node-role.kubernetes.io/k8s-node=true</code> 这个选项，它的作用只是在 <code>kubectl get node</code> 时 ROLES 栏显示是什么节点；不过需要注意 <strong>master 上的 kubelet 不要将 <code>node-role.kubernetes.io/k8s-master=true</code> 更改成 <code>node-role.kubernetes.io/master=xxxx</code>；后面这个 <code>node-role.kubernetes.io/master</code> 是 kubeadm 用的，这个 label 会告诉 k8s 调度器当前节点为 master，从而执行一些特定动作，比如 <code>node-role.kubernetes.io/master:NoSchedule</code> 此节点将不会被分配 pod；具体参见 <a href="https://github.com/kubernetes-incubator/kubespray/issues/2108">kubespray issue</a> 以及 <a href="https://github.com/kubernetes/kubeadm/blob/master/docs/design/design_v1.9.md#mark-master">官方设计文档</a></strong></p><p>很多人可能会发现大约 1 小时候 <code>kubectl get csr</code> 看不到任何 csr 了，这是因为最新版本增加了 csr 清理功能，<strong>默认对于 <code>approved</code> 和 <code>denied</code> 状态的 csr 一小时后会被清理，对于 <code>pending</code> 状态的 csr 24 小时后会被清理，想问时间从哪来的请看 <a href="https://github.com/kubernetes/kubernetes/blob/fa85bf7094a8a503fede964b7038eed51360ffc7/pkg/controller/certificates/cleaner/cleaner.go#L47">代码</a>；PR issue 我忘记了，增加这个功能的起因大致就是因为当开启了证书轮换后，csr 会不断增加，所以需要增加一个清理功能</strong></p><h4 id="9-2、异常及警告说明"><a href="#9-2、异常及警告说明" class="headerlink" title="9.2、异常及警告说明"></a>9.2、异常及警告说明</h4><p>在部署过程中我记录了一些异常警告等，以下做一下统一说明</p><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment"># https://github.com/kubernetes/kubernetes/issues/42158</span><span class="hljs-comment"># 这个问题还没解决，PR 没有合并被关闭了，可以关注一下上面这个 issue，被关闭的 PR 在下面</span><span class="hljs-comment"># https://github.com/kubernetes/kubernetes/pull/49567</span>Failed to update statusUpdateNeeded field <span class="hljs-keyword">in</span> actual state of world: Failed to <span class="hljs-built_in">set</span> statusUpdateNeeded to needed <span class="hljs-literal">true</span>, because nodeName=...<span class="hljs-comment"># https://github.com/kubernetes/kubernetes/issues/59993</span><span class="hljs-comment"># 这个似乎已经解决了，没时间测试，PR 地址在下面，我大致 debug 一下 好像是 cAdvisor 的问题</span><span class="hljs-comment"># https://github.com/opencontainers/runc/pull/1722</span>Failed to get system container stats <span class="hljs-keyword">for</span> <span class="hljs-string">&quot;/kubepods&quot;</span>: failed to get cgroup stats <span class="hljs-keyword">for</span> <span class="hljs-string">&quot;/kubepods&quot;</span>: failed to get container info <span class="hljs-keyword">for</span> <span class="hljs-string">&quot;/kubepods&quot;</span>: unknown containe <span class="hljs-string">&quot;/kubepods&quot;</span><span class="hljs-comment"># https://github.com/kubernetes/kubernetes/issues/58217</span><span class="hljs-comment"># 注意: 这个问题现在仍未解决，可关注上面的 issue，这个问题可能影响 node image gc</span><span class="hljs-comment"># 强烈依赖于 kubelet 做 宿主机 image gc 的需要注意一下</span>Image garbage collection failed once. Stats initialization may not have completed yet: failed to get imageFs info: unable to find data <span class="hljs-keyword">for</span> container /<span class="hljs-comment"># 没找到太多资料，不过感觉跟上面问题类似</span>failed to construct signal: <span class="hljs-string">&quot;allocatableMemory.available&quot;</span> error: system container <span class="hljs-string">&quot;pods&quot;</span> not found <span class="hljs-keyword">in</span> metrics</code></pre></div>]]></content>
    
    
    <summary type="html">年后比较忙，所以 1.9 也没去折腾(其实就是懒)，最近刚有点时间凑巧 1.10 发布；所以就折腾一下 1.10，感觉搭建配置没有太大变化，折腾了 2 天基本算是搞定了，这里记录一下搭建过程；本文用到的被 block 镜像已经上传至 [百度云](https://pan.baidu.com/s/14W86QQ4qi8qn8JqaDMcC3g) 密码: dy5p</summary>
    
    
    
    <category term="Kubernetes" scheme="https://mritd.com/categories/kubernetes/"/>
    
    
    <category term="Kubernetes" scheme="https://mritd.com/tags/kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>Drone CI 搭建</title>
    <link href="https://mritd.com/2018/03/30/set-up-drone-ci/"/>
    <id>https://mritd.com/2018/03/30/set-up-drone-ci/</id>
    <published>2018-03-30T13:38:29.000Z</published>
    <updated>2018-03-30T13:38:29.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>最近感觉 GitLab CI 稍有繁琐，所以尝试了一下 Drone CI，这里记录一下搭建过程；虽然 Drone CI 看似简单，但是坑还是有不少的</p></blockquote><h2 id="一、环境准备"><a href="#一、环境准备" class="headerlink" title="一、环境准备"></a>一、环境准备</h2><p>基本环境如下:</p><ul><li>Docker: 17.09.0-ce</li><li>GitLab: 10.4.3-ce.0</li><li>Drone: 0.8.5</li></ul><p>其中 GitLab 采用 TLS 链接，为了方便使用 git 协议 clone 代码，所以 docker compose 部署时采用了 macvlan 网络获取独立 IP</p><h2 id="二、GitLab-配置"><a href="#二、GitLab-配置" class="headerlink" title="二、GitLab 配置"></a>二、GitLab 配置</h2><h3 id="2-1、GitLab-搭建"><a href="#2-1、GitLab-搭建" class="headerlink" title="2.1、GitLab 搭建"></a>2.1、GitLab 搭建</h3><p>为了测试 CI build 需要一个 GitLab 服务器以及测试项目，GitLab 这里直接采用 docker compose 启动，同时为了方便 git clone，网络使用了 macvlan 方式，macvlan 网络接口、IP 等参数请自行修改</p><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment"># config refs ==&gt; https://gitlab.com/gitlab-org/omnibus-gitlab/blob/master/files/gitlab-config-template/gitlab.rb.template</span>version: <span class="hljs-string">&#x27;3&#x27;</span>services:  gitlab:    image: <span class="hljs-string">&#x27;gitlab/gitlab-ce:10.4.3-ce.0&#x27;</span>    container_name: gitlab    restart: always    hostname: <span class="hljs-string">&#x27;gitlab.mritd.me&#x27;</span>    environment:      GITLAB_OMNIBUS_CONFIG: |        external_url <span class="hljs-string">&#x27;https://gitlab.mritd.me&#x27;</span>        nginx[<span class="hljs-string">&#x27;redirect_http_to_https&#x27;</span>] = <span class="hljs-literal">true</span>        nginx[<span class="hljs-string">&#x27;ssl_certificate&#x27;</span>] = <span class="hljs-string">&quot;/etc/gitlab/ssl/mritd.me.cer&quot;</span>        nginx[<span class="hljs-string">&#x27;ssl_certificate_key&#x27;</span>] = <span class="hljs-string">&quot;/etc/gitlab/ssl/mritd.me.key&quot;</span>        nginx[<span class="hljs-string">&#x27;real_ip_header&#x27;</span>] = <span class="hljs-string">&#x27;X-Real-IP&#x27;</span>        nginx[<span class="hljs-string">&#x27;real_ip_recursive&#x27;</span>] = <span class="hljs-string">&#x27;on&#x27;</span>        <span class="hljs-comment">#gitlab_rails[&#x27;ldap_enabled&#x27;] = true</span>        <span class="hljs-comment">#gitlab_rails[&#x27;ldap_servers&#x27;] = YAML.load &lt;&lt;-EOS # remember to close this block with &#x27;EOS&#x27; below</span>        <span class="hljs-comment">#main: # &#x27;main&#x27; is the GitLab &#x27;provider ID&#x27; of this LDAP server</span>        <span class="hljs-comment">#  ## label</span>        <span class="hljs-comment">#  #</span>        <span class="hljs-comment">#  # A human-friendly name for your LDAP server. It is OK to change the label later,</span>        <span class="hljs-comment">#  # for instance if you find out it is too large to fit on the web page.</span>        <span class="hljs-comment">#  #</span>        <span class="hljs-comment">#  # Example: &#x27;Paris&#x27; or &#x27;Acme, Ltd.&#x27;</span>        <span class="hljs-comment">#  label: &#x27;LDAP&#x27;</span>        <span class="hljs-comment">#  host: &#x27;mail.mritd.me&#x27;</span>        <span class="hljs-comment">#  port: 389 # or 636</span>        <span class="hljs-comment">#  uid: &#x27;uid&#x27;</span>        <span class="hljs-comment">#  method: &#x27;plain&#x27; # &quot;tls&quot; or &quot;ssl&quot; or &quot;plain&quot;</span>        <span class="hljs-comment">#  bind_dn: &#x27;uid=zimbra,cn=admins,cn=zimbra&#x27;</span>        <span class="hljs-comment">#  password: &#x27;PASSWORD&#x27;</span>        <span class="hljs-comment">#  # This setting specifies if LDAP server is Active Directory LDAP server.</span>        <span class="hljs-comment">#  # For non AD servers it skips the AD specific queries.</span>        <span class="hljs-comment">#  # If your LDAP server is not AD, set this to false.</span>        <span class="hljs-comment">#  active_directory: true</span>        <span class="hljs-comment">#  # If allow_username_or_email_login is enabled, GitLab will ignore everything</span>        <span class="hljs-comment">#  # after the first &#x27;@&#x27; in the LDAP username submitted by the user on login.</span>        <span class="hljs-comment">#  #</span>        <span class="hljs-comment">#  # Example:</span>        <span class="hljs-comment">#  # - the user enters &#x27;jane.doe@example.com&#x27; and &#x27;p@ssw0rd&#x27; as LDAP credentials;</span>        <span class="hljs-comment">#  # - GitLab queries the LDAP server with &#x27;jane.doe&#x27; and &#x27;p@ssw0rd&#x27;.</span>        <span class="hljs-comment">#  #</span>        <span class="hljs-comment">#  # If you are using &quot;uid: &#x27;userPrincipalName&#x27;&quot; on ActiveDirectory you need to</span>        <span class="hljs-comment">#  # disable this setting, because the userPrincipalName contains an &#x27;@&#x27;.</span>        <span class="hljs-comment">#  allow_username_or_email_login: true</span>        <span class="hljs-comment">#  # Base where we can search for users</span>        <span class="hljs-comment">#  #</span>        <span class="hljs-comment">#  #   Ex. ou=People,dc=gitlab,dc=example</span>        <span class="hljs-comment">#  #</span>        <span class="hljs-comment">#  base: &#x27;&#x27;</span>        <span class="hljs-comment">#  # Filter LDAP users</span>        <span class="hljs-comment">#  #</span>        <span class="hljs-comment">#  #   Format: RFC 4515 http://tools.ietf.org/search/rfc4515</span>        <span class="hljs-comment">#  #   Ex. (employeeType=developer)</span>        <span class="hljs-comment">#  #</span>        <span class="hljs-comment">#  #   Note: GitLab does not support omniauth-ldap&#x27;s custom filter syntax.</span>        <span class="hljs-comment">#  #</span>        <span class="hljs-comment">#  user_filter: &#x27;&#x27;</span>        <span class="hljs-comment">#EOS</span>        gitlab_rails[<span class="hljs-string">&#x27;log_directory&#x27;</span>] = <span class="hljs-string">&quot;/var/log/gitlab/gitlab-rails&quot;</span>        unicorn[<span class="hljs-string">&#x27;log_directory&#x27;</span>] = <span class="hljs-string">&quot;/var/log/gitlab/unicorn&quot;</span>        registry[<span class="hljs-string">&#x27;log_directory&#x27;</span>] = <span class="hljs-string">&quot;/var/log/gitlab/registry&quot;</span>        <span class="hljs-comment"># Below are some of the default settings</span>        logging[<span class="hljs-string">&#x27;logrotate_frequency&#x27;</span>] = <span class="hljs-string">&quot;daily&quot;</span> <span class="hljs-comment"># rotate logs daily</span>        logging[<span class="hljs-string">&#x27;logrotate_size&#x27;</span>] = nil <span class="hljs-comment"># do not rotate by size by default</span>        logging[<span class="hljs-string">&#x27;logrotate_rotate&#x27;</span>] = 30 <span class="hljs-comment"># keep 30 rotated logs</span>        logging[<span class="hljs-string">&#x27;logrotate_compress&#x27;</span>] = <span class="hljs-string">&quot;compress&quot;</span> <span class="hljs-comment"># see &#x27;man logrotate&#x27;</span>        logging[<span class="hljs-string">&#x27;logrotate_method&#x27;</span>] = <span class="hljs-string">&quot;copytruncate&quot;</span> <span class="hljs-comment"># see &#x27;man logrotate&#x27;</span>        logging[<span class="hljs-string">&#x27;logrotate_postrotate&#x27;</span>] = nil <span class="hljs-comment"># no postrotate command by default</span>        logging[<span class="hljs-string">&#x27;logrotate_dateformat&#x27;</span>] = nil <span class="hljs-comment"># use date extensions for rotated files rather than numbers e.g. a value of &quot;-%Y-%m-%d&quot; would give rotated files like p</span>        <span class="hljs-comment"># You can add overrides per service</span>        nginx[<span class="hljs-string">&#x27;logrotate_frequency&#x27;</span>] = nil        nginx[<span class="hljs-string">&#x27;logrotate_size&#x27;</span>] = <span class="hljs-string">&quot;200M&quot;</span>        <span class="hljs-comment"># You can also disable the built-in logrotate service if you want</span>        logrotate[<span class="hljs-string">&#x27;enable&#x27;</span>] = <span class="hljs-literal">false</span>        gitlab_rails[<span class="hljs-string">&#x27;smtp_enable&#x27;</span>] = <span class="hljs-literal">true</span>        gitlab_rails[<span class="hljs-string">&#x27;smtp_address&#x27;</span>] = <span class="hljs-string">&quot;mail.mritd.me&quot;</span>        gitlab_rails[<span class="hljs-string">&#x27;smtp_port&#x27;</span>] = 25        gitlab_rails[<span class="hljs-string">&#x27;smtp_user_name&#x27;</span>] = <span class="hljs-string">&quot;no-reply@mritd.me&quot;</span>        gitlab_rails[<span class="hljs-string">&#x27;smtp_password&#x27;</span>] = <span class="hljs-string">&quot;PASSWORD&quot;</span>        gitlab_rails[<span class="hljs-string">&#x27;smtp_domain&#x27;</span>] = <span class="hljs-string">&quot;mritd.me&quot;</span>        gitlab_rails[<span class="hljs-string">&#x27;smtp_authentication&#x27;</span>] = <span class="hljs-string">&quot;login&quot;</span>        gitlab_rails[<span class="hljs-string">&#x27;smtp_enable_starttls_auto&#x27;</span>] = <span class="hljs-literal">true</span>        gitlab_rails[<span class="hljs-string">&#x27;smtp_openssl_verify_mode&#x27;</span>] = <span class="hljs-string">&#x27;peer&#x27;</span>        <span class="hljs-comment"># If your SMTP server does not like the default &#x27;From: gitlab@localhost&#x27; you</span>        <span class="hljs-comment"># can change the &#x27;From&#x27; with this setting.</span>        gitlab_rails[<span class="hljs-string">&#x27;gitlab_email_from&#x27;</span>] = <span class="hljs-string">&#x27;gitlab@mritd.me&#x27;</span>        gitlab_rails[<span class="hljs-string">&#x27;gitlab_email_reply_to&#x27;</span>] = <span class="hljs-string">&#x27;no-reply@mritd.me&#x27;</span>        gitlab_rails[<span class="hljs-string">&#x27;initial_root_password&#x27;</span>] = <span class="hljs-string">&#x27;PASSWORD&#x27;</span>        gitlab_rails[<span class="hljs-string">&#x27;initial_shared_runners_registration_token&#x27;</span>] = <span class="hljs-string">&quot;iuLaUhGZYyFgTxAyZ6HbdFUZ&quot;</span>    networks:      macvlan:        ipv4_address: 172.16.0.70    ports:      - <span class="hljs-string">&#x27;80:80&#x27;</span>      - <span class="hljs-string">&#x27;443:443&#x27;</span>      - <span class="hljs-string">&#x27;22:22&#x27;</span>    volumes:      - config:/etc/gitlab      - logs:/var/<span class="hljs-built_in">log</span>/gitlab      - data:/var/opt/gitlabnetworks:  macvlan:    driver: macvlan    driver_opts:      parent: ens18    ipam:      config:      - subnet: 172.16.0.0/19volumes:  config:  logs:  data:</code></pre></div><h3 id="2-2、创建-Drone-App"><a href="#2-2、创建-Drone-App" class="headerlink" title="2.2、创建 Drone App"></a>2.2、创建 Drone App</h3><p>Drone CI 工作时需要接入 GitLab 以完成项目同步等功能，所以在搭建好 GitLab 后需要为其创建 Application，创建方式如下所示</p><p><img src="https://cdn.oss.link/markdown/lzm4j.png" alt="create drone app"></p><p>创建 Application 时请自行更换回调地址域名，创建好后如下所示(后续 Drone CI 需要使用这两个 key)</p><p><img src="https://cdn.oss.link/markdown/sl4yl.png" alt="drone app create success"></p><h2 id="三、Drone-服务端配置"><a href="#三、Drone-服务端配置" class="headerlink" title="三、Drone 服务端配置"></a>三、Drone 服务端配置</h2><h3 id="3-1、Drone-CI-搭建"><a href="#3-1、Drone-CI-搭建" class="headerlink" title="3.1、Drone CI 搭建"></a>3.1、Drone CI 搭建</h3><p>Drone CI 服务器与 GitLab 等传统 CI 相似，都是 CS 模式，为了方便测试这里将 Agent 与 Server 端都放在一个 docker compose 中启动；docker compose 配置如下</p><div class="hljs code-wrapper"><pre><code class="hljs sh">version: <span class="hljs-string">&#x27;3&#x27;</span>services:  drone-server:    image: drone/drone:0.8-alpine    container_name: drone-server    ports:      - 8000:8000      - 9000:9000    volumes:      - data:/var/lib/drone/    restart: always    environment:      - DRONE_OPEN=<span class="hljs-literal">true</span>      - DRONE_ADMIN=drone,mritd      - DRONE_HOST=https://drone.mritd.me      - DRONE_GITLAB=<span class="hljs-literal">true</span>      - DRONE_GITLAB_PRIVATE_MODE=<span class="hljs-literal">true</span>      - DRONE_GITLAB_URL=https://gitlab.mritd.me      - DRONE_GITLAB_CLIENT=76155ab75bafd73d4ebfe0a02d9d6284a032f7d8667d558e3f929a64805d1fa1      - DRONE_GITLAB_SECRET=6957b06f53b80d4dd17051ceb36f9139ae83b9077e345a404f476e317b0c8f3d      - DRONE_SECRET=XsJnj4DmzuXBKkcgHeUAJQxq  drone-agent:    image: drone/agent:0.8    container_name: drone-agent    <span class="hljs-built_in">command</span>: agent    restart: always    volumes:      - /var/run/docker.sock:/var/run/docker.sock    environment:      - DRONE_SERVER=172.16.0.36:9000      - DRONE_SECRET=XsJnj4DmzuXBKkcgHeUAJQxqvolumes:  data:</code></pre></div><p>docker compose 中 <code>DRONE_GITLAB_CLIENT</code> 为 GitLab 创建 Application 时的 <code>Application Id</code>，<code>DRONE_GITLAB_SECRET</code> 为 <code>Secret</code>；其他环境变量解释如下:</p><ul><li>DRONE_OPEN: 是否允许开放注册</li><li>DRONE_ADMIN: 注册后的管理员用户</li><li>DRONE_HOST: Server 地址</li><li>DRONE_GITLAB: 声明 Drone CI 对接为 GitLab</li><li>DRONE_GITLAB_PRIVATE_MODE: GitLab 私有化部署</li><li>DRONE_GITLAB_URL: GitLab 地址</li><li>DRONE_SECRET: Server 端认证秘钥，Agent 连接时需要</li></ul><p>实际上 Agent 可以与 Server 分离部署，不过需要注意 Server 端 9000 端口走的是 grpc 协议基于 HTTP2，nginx 等反向代理时需要做好对应处理</p><p>搭建成功这里外面套了一层 nginx 用来反向代理 Drone Server 的 8000 端口，Nginx 配置如下:</p><div class="hljs code-wrapper"><pre><code class="hljs sh">upstream drone&#123;    server 172.16.0.36:8000;&#125;server &#123;    listen 80;    listen [::]:80;    server_name drone.mritd.me;    <span class="hljs-comment"># Redirect all HTTP requests to HTTPS with a 301 Moved Permanently response.</span>    <span class="hljs-built_in">return</span> 301 https://$host<span class="hljs-variable">$request_uri</span>;&#125;server &#123;    listen 443 ssl http2;    listen [::]:443 ssl http2;    server_name drone.mritd.me;    <span class="hljs-comment"># certs sent to the client in SERVER HELLO are concatenated in ssl_certificate</span>    ssl_certificate /etc/nginx/ssl/mritd.me.cer;    ssl_certificate_key /etc/nginx/ssl/mritd.me.key;    ssl_session_timeout 1d;    ssl_session_cache shared:SSL:50m;    ssl_session_tickets off;        <span class="hljs-comment"># Diffie-Hellman parameter for DHE ciphersuites, recommended 2048 bits</span>    ssl_dhparam /etc/nginx/ssl/dhparam.pem;    <span class="hljs-comment"># intermediate configuration. tweak to your needs.</span>    ssl_protocols TLSv1 TLSv1.1 TLSv1.2;    ssl_ciphers <span class="hljs-string">&#x27;ECDHE-ECDSA-CHACHA20-POLY1305:ECDHE-RSA-CHACHA20-POLY1305:ECDHE-ECDSA-AES128-GCM-SHA256:ECDHE-RSA-AES128-GCM-SHA256:ECDHE-ECDSA-AES256-GCM-SHA384:EC</span><span class="hljs-string">DHE-RSA-AES256-GCM-SHA384:DHE-RSA-AES128-GCM-SHA256:DHE-RSA-AES256-GCM-SHA384:ECDHE-ECDSA-AES128-SHA256:ECDHE-RSA-AES128-SHA256:ECDHE-ECDSA-AES128-SHA:ECDHE-RSA-AES2</span><span class="hljs-string">56-SHA384:ECDHE-RSA-AES128-SHA:ECDHE-ECDSA-AES256-SHA384:ECDHE-ECDSA-AES256-SHA:ECDHE-RSA-AES256-SHA:DHE-RSA-AES128-SHA256:DHE-RSA-AES128-SHA:DHE-RSA-AES256-SHA256:D</span><span class="hljs-string">HE-RSA-AES256-SHA:ECDHE-ECDSA-DES-CBC3-SHA:ECDHE-RSA-DES-CBC3-SHA:EDH-RSA-DES-CBC3-SHA:AES128-GCM-SHA256:AES256-GCM-SHA384:AES128-SHA256:AES256-SHA256:AES128-SHA:AES</span><span class="hljs-string">256-SHA:DES-CBC3-SHA:!DSS&#x27;</span>;    ssl_prefer_server_ciphers on;    <span class="hljs-comment"># HSTS (ngx_http_headers_module is required) (15768000 seconds = 6 months)</span>    add_header Strict-Transport-Security max-age=15768000;    <span class="hljs-comment"># OCSP Stapling ---</span>    <span class="hljs-comment"># fetch OCSP records from URL in ssl_certificate and cache them</span>    ssl_stapling on;    ssl_stapling_verify on;    <span class="hljs-comment">## verify chain of trust of OCSP response using Root CA and Intermediate certs</span>    ssl_trusted_certificate /etc/nginx/ssl/mritd-ca.cer;    <span class="hljs-comment">#resolver &lt;IP DNS resolver&gt;;</span>    location / &#123;        log_not_found on;        proxy_set_header X-Forwarded-For <span class="hljs-variable">$remote_addr</span>;        proxy_set_header X-Forwarded-Proto <span class="hljs-variable">$scheme</span>;        proxy_set_header Host <span class="hljs-variable">$http_host</span>;        proxy_pass http://drone;        proxy_redirect off;        proxy_http_version 1.1;        proxy_buffering off;        chunked_transfer_encoding off;    &#125;&#125;</code></pre></div><p>然后访问 <code>https://YOUR_DRONE_SERVER</code> 将会自动跳转到 GitLab Auth2 授权界面，授权登录即可；随后将会返回 Drone CI 界面，界面上会列出相应的项目列表，点击后面的开关按钮来开启对应项目的 Drone CI 服务</p><p><img src="https://cdn.oss.link/markdown/6u4fk.png" alt="drone ci project list"></p><h3 id="3-2、创建示例项目"><a href="#3-2、创建示例项目" class="headerlink" title="3.2、创建示例项目"></a>3.2、创建示例项目</h3><p>这里的示例项目为 Java 项目，采用 Gradle 构建，项目整体结构如下所示，源码可以从 <a href="">GitHub</a> 下载</p><p><img src="https://cdn.oss.link/markdown/ybrjc.png" alt="drone test project"></p><p>将此项目推送到 GitLab 就会触发 Drone CI 自动构建(第一次肯定构建失败，具体看下面配置)</p><h3 id="3-3、Drone-CLI"><a href="#3-3、Drone-CLI" class="headerlink" title="3.3、Drone CLI"></a>3.3、Drone CLI</h3><p>这里不得不说一下官方文档真的很烂，有些东西只能自己摸索，而且各种错误提示也是烂的不能再烂，经常遇到 <code>Client Error 404:</code> 这种错误，后面任何提示信息也没有；官方文档中介绍了有些操作只能通过 cli 执行，CLI 下载需要到 GitHub 下载页下载，地址 <a href="https://github.com/drone/drone-cli/releases">点这里</a></p><p>cli 工具下载后需要进行配置，目前只支持读取环境变量，使用前需要 <code>export</code> 以下两个变量</p><ul><li>DRONE_SERVER: Drone CI 地址</li><li>DRONE_TOKEN: cli 控制 Server 端使用的用户 Token</li></ul><p>其中 Token 可以在用户设置页面找到，如下</p><p><img src="https://cdn.oss.link/markdown/5fkvi.png" alt="drone user token"></p><p>配置好以后就可以使用 cli 操作 CI Server 了</p><h3 id="3-4、Drone-CI-配置文件"><a href="#3-4、Drone-CI-配置文件" class="headerlink" title="3.4、Drone CI 配置文件"></a>3.4、Drone CI 配置文件</h3><p>Drone CI 对一个项目进行 CI 构建取决于两个因素，第一必须保证该项目在 Drone 控制面板中开启了构建(构建按钮开启)，第二保证项目根目录下存在 <code>.drone.yml</code>；满足这两点后每次提交 Drone 就会根据 <code>.drone.yml</code> 中配置进行按步骤构建；本示例中 <code>.drone.yml</code> 配置如下</p><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-built_in">clone</span>:  git:    image: plugins/gitpipeline:  backend:    image: reg.mritd.me/base/build:2.1.5    commands:      - gradle --no-daemon clean assemble    when:      branch:        event: [ push, pull_request ]        include: [ master ]        exclude: [ develop ]<span class="hljs-comment">#  rebuild-cache:</span><span class="hljs-comment">#    image: drillster/drone-volume-cache</span><span class="hljs-comment">#    rebuild: true</span><span class="hljs-comment">#    mount:</span><span class="hljs-comment">#      - ./build</span><span class="hljs-comment">#    volumes:</span><span class="hljs-comment">#      - /data/drone/$DRONE_COMMIT_SHA:/cache</span>  docker:    image: mritd/docker-kubectl:v1.8.8    commands:      - bash build_image.sh    volumes:      - /var/run/docker.sock:/var/run/docker.sock<span class="hljs-comment"># Pipeline Conditions</span>branches:  include: [ master, feature/* ]  exclude: [ develop, <span class="hljs-built_in">test</span>/* ]</code></pre></div><p>Drone CI 配置文件为 docker compose 的超集，<strong>Drone CI 构建思想是使用不同的阶段定义完成对 CI 流程的整体划分，然后每个阶段内定义不同的任务(task)，这些任务所有操作无论是 build、package 等全部由单独的 Docker 镜像完成，同时以 <code>plugins</code> 开头的 image 被解释为内部插件；其他的插件实际上可以看做为标准的 Docker image</strong></p><p>第一段 <code>clone</code> 配置声明了源码版本控制系统拉取方式，具体参见 <a href="http://docs.drone.io/cloning">cloning</a>部分，定义后 Drone CI 将自动拉取源码</p><p>此后的 <code>pipeline</code> 配置段为定义整个 CI 流程段，该段中可以自定义具体 task，比如后端构建可以取名字为 <code>backend</code>，前端构建可以叫做 <code>frontend</code>；中间可以穿插辅助的如打包 docker 镜像等 task；同 GitLab CI 一样，Agent 在使用 Docker 进行构建时必然涉及到拉取私有镜像，Drone CI 想要拉取私有镜像目前仅能通过 cli 命令行进行设置，而且仅针对项目级设置(全局需要企业版…这也行)</p><div class="hljs code-wrapper"><pre><code class="hljs sh">drone registry add --repository drone/DroneCI-TestProject --hostname reg.mritd.me --username gitlab --password 123456</code></pre></div><p>在构建时需要注意一点，Drone CI 不同的 task 之间共享源码文件，<strong>也就是说如果你在第一个 task 中对源码或者编译后的发布物做了什么更改，在下一个 task 中同样可见，Drone CI 并没有 GitLab CI 在每个 task 中都进行还原的机制</strong></p><p>除此之外，某些特殊性的挂载行为默认也是不被允许的，需要在 Drone CI 中对项目做 <code>Trusted</code> 设置</p><p><img src="https://cdn.oss.link/markdown/gd60v.png" alt="Drone Project Trusted Setting"></p><h2 id="四、与-GitLab-CI-对比"><a href="#四、与-GitLab-CI-对比" class="headerlink" title="四、与 GitLab CI 对比"></a>四、与 GitLab CI 对比</h2><p>写到这里基本接近尾声了，可能常看我博客的人现在想喷我，这篇文章确实有点水…因为我真不推荐用这玩意，未来发展倒是不确定；下面对比一下与 GitLab CI 的区别</p><p>先说一下 Drone CI 的优点，Drone CI 更加轻量级，而且也支持 HA 等设置，配置文件使用 docker compose 的方式对于玩容器多的人确实很爽，启动速度等感觉也比 GitLab CI 要快；而且我个人用 GitLab CI Docker build 的方式时也是尽量将不同功能交给不同的镜像，通过切换镜像实现不同的功能；这个思想在 Drone CI 中表现的非常明显</p><p>至于 Drone CI 的缺点，目前我最大的吐槽就是文档烂，报错烂；很多时候搞得莫名其妙，比如上来安装讲的那个管理员账户配置，我现在也没明白怎么能关闭注册启动然后添加用户(可能是我笨)；还有就是报错问题，感觉就像写代码不打 log 一样，比如 CI Server 在没有 agent 链接时，如果触发了 build 任务，Drone CI 不会报错，只会在任务上显示一个小闹钟，也没有超时…我傻傻的等了 1 小时；其他的比如全局变量、全局加密参数等都需要企业版才能支持，同时一些细节东西也缺失，比如查看当前 Server 连接的 Agent，对 Agent 打标签实现不同 task 分配等等</p><p>总结: Drone CI 目前还是个小玩具阶段，与传统 CI 基本没有抗衡之力，文档功能呢也是缺失比较严重，出问题很难排查</p>]]></content>
    
    
    <summary type="html">最近感觉 GitLab CI 稍有繁琐，所以尝试了一下 Drone CI，这里记录一下搭建过程；虽然 Drone CI 看似简单，但是坑还是有不少的</summary>
    
    
    
    <category term="Docker" scheme="https://mritd.com/categories/docker/"/>
    
    <category term="CI/CD" scheme="https://mritd.com/categories/docker/ci-cd/"/>
    
    
    <category term="CI/CD" scheme="https://mritd.com/tags/ci-cd/"/>
    
    <category term="Linux" scheme="https://mritd.com/tags/linux/"/>
    
    <category term="Docker" scheme="https://mritd.com/tags/docker/"/>
    
    <category term="Drone" scheme="https://mritd.com/tags/drone/"/>
    
  </entry>
  
  <entry>
    <title>Unix 平台下各种加速配置</title>
    <link href="https://mritd.com/2018/03/28/unix-proxy-setting/"/>
    <id>https://mritd.com/2018/03/28/unix-proxy-setting/</id>
    <published>2018-03-28T14:09:57.000Z</published>
    <updated>2018-03-28T14:09:57.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>本文主要阐述在 *Uinx 平台下，各种常用开发工具的加速配置，<strong>加速前提是你需要有一个能够加速的 socks5 端口，常用工具请自行搭建</strong>；本文档包括 docker、terminal、git、chrome 常用加速配置，其他工具可能后续补充</p></blockquote><h3 id="一、加速类型"><a href="#一、加速类型" class="headerlink" title="一、加速类型"></a>一、加速类型</h3><p>目前大部分工具在原始版本都是只提供 socks5 加速，常用平台一些工具已经支持手动设置加速端口，如 telegram、mega 同步客户端等等；但是某些工具并不支持 socks5，通用的加速目前各个平台只支持 http、https 设置(包括 terminal 下)；<strong>综上所述，在设置之前你至少需要保证有一个 socks5 端口能够进行加速，然后根据以下教程将 socks5 转换成 http，最后配置各个软件或系统的加速方式为 http，这也是我们常用的某些带有图形化客户端实际的背后实现</strong></p><h3 id="二、socks5-to-http"><a href="#二、socks5-to-http" class="headerlink" title="二、socks5 to http"></a>二、socks5 to http</h3><p>sock5 转 http 这里采用 privoxy 进行转换，根据各个平台不同，安装方式可能不同，主要就是包管理器的区别，以下只列举 Ubuntu、Mac 下的命令，其他平台自行 Google</p><ul><li>Mac: <code>brew install privoxy</code></li><li>Ubuntu: <code>apt-get -y install privoxy</code></li></ul><p>安装成功后，需要修改配置以指定 socks5 端口以及不代理的白名单，配置文件位置如下:</p><ul><li>Mac: <code>/usr/local/etc/privoxy/config</code></li><li>Ubuntu: <code>/etc/privoxy/config</code></li></ul><p>在修改之前请备份默认配置文件，这是个好习惯，备份后修改内容如下:</p><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment"># 转发地址</span>forward-socks5   /               127.0.0.1:1080 .<span class="hljs-comment"># 监听地址</span>listen-address  localhost:8118<span class="hljs-comment"># local network do not use proxy</span>forward         192.168.*.*/     .forward            10.*.*.*/     .forward           127.*.*.*/     .</code></pre></div><p><strong>其中 <code>127.0.0.1:1080</code> 为你的 socks5 ip 及 端口，<code>localhost:8118</code> 为你转换后的 http 监听地址和端口</strong>；配置完成后启动 privoxy 即可，启动命令如下:</p><ul><li>Mac: <code>brew services start privoxy</code></li><li>Ubuntu: <code>systemctl start privoxy</code></li></ul><h3 id="三、Docker-加速拉取-gcr-io-镜像"><a href="#三、Docker-加速拉取-gcr-io-镜像" class="headerlink" title="三、Docker 加速拉取 gcr.io 镜像"></a>三、Docker 加速拉取 gcr.io 镜像</h3><p>对于 docker 来说，terminal 下执行 <code>docker pull</code> 等命令实质上都是通过调用 docker daemon 操作的；而 docker daemon 是由 systemd 启动的(就目前来讲，别跟我掰什么 service start…)；对于 docker daemon 来说，一旦它启动以后就不会再接受加速设置，所以我们需要在 systemd 的 service 配置中配置它的加速。</p><p>目前 docker daemon 接受标准的终端加速设置(读取 <code>http_proxy</code>、<code>https_proxy</code>)，同时也支持 socks5 加速；为了保证配置清晰方便修改，这里采用创建单独配置文件的方式来配置 daemon 的 socks5 加速，配置脚本如下(Ubuntu、CentOS):</p><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-meta">#!/bin/bash</span><span class="hljs-built_in">set</span> -eOS_TYPE=<span class="hljs-variable">$1</span>PROXY_ADDRESS=<span class="hljs-variable">$2</span><span class="hljs-keyword">if</span> [ <span class="hljs-string">&quot;<span class="hljs-variable">$&#123;PROXY_ADDRESS&#125;</span>&quot;</span> == <span class="hljs-string">&quot;&quot;</span> ]; <span class="hljs-keyword">then</span>    <span class="hljs-built_in">echo</span> -e <span class="hljs-string">&quot;\033[31mError: PROXY_ADDRESS is blank!\033[0m&quot;</span>    <span class="hljs-built_in">echo</span> -e <span class="hljs-string">&quot;\033[32mUse: sudo <span class="hljs-variable">$0</span> centos|ubuntu 1.2.3.4:1080\033[0m&quot;</span>    <span class="hljs-built_in">exit</span> 1<span class="hljs-keyword">fi</span><span class="hljs-keyword">if</span> [ <span class="hljs-string">&quot;<span class="hljs-variable">$&#123;OS_TYPE&#125;</span>&quot;</span> == <span class="hljs-string">&quot;&quot;</span> ];<span class="hljs-keyword">then</span>    <span class="hljs-built_in">echo</span> -e <span class="hljs-string">&quot;\033[31mError: OS_TYPE is blank!\033[0m&quot;</span>    <span class="hljs-built_in">echo</span> -e <span class="hljs-string">&quot;\033[32mUse: sudo <span class="hljs-variable">$0</span> centos|ubuntu\033[0m&quot;</span>    <span class="hljs-built_in">exit</span> 1<span class="hljs-keyword">elif</span> [ <span class="hljs-string">&quot;<span class="hljs-variable">$&#123;OS_TYPE&#125;</span>&quot;</span> == <span class="hljs-string">&quot;centos&quot;</span> ];<span class="hljs-keyword">then</span>    mkdir /etc/systemd/system/docker.service.d || <span class="hljs-literal">true</span>    tee /etc/systemd/system/docker.service.d/socks5-proxy.conf &lt;&lt;-<span class="hljs-string">EOF</span><span class="hljs-string">[Service]</span><span class="hljs-string">Environment=&quot;ALL_PROXY=socks5://$&#123;PROXY_ADDRESS&#125;&quot;</span><span class="hljs-string">EOF</span><span class="hljs-keyword">elif</span> [ <span class="hljs-string">&quot;<span class="hljs-variable">$&#123;OS_TYPE&#125;</span>&quot;</span> == <span class="hljs-string">&quot;ubuntu&quot;</span> ];<span class="hljs-keyword">then</span>    mkdir /lib/systemd/system/docker.service.d || <span class="hljs-literal">true</span>    tee /lib/systemd/system/docker.service.d/socks5-proxy.conf &lt;&lt;-<span class="hljs-string">EOF</span><span class="hljs-string">[Service]</span><span class="hljs-string">Environment=&quot;ALL_PROXY=socks5://$&#123;PROXY_ADDRESS&#125;&quot;</span><span class="hljs-string">EOF</span><span class="hljs-keyword">fi</span>systemctl daemon-reloadsystemctl restart dockersystemctl show docker --property Environment</code></pre></div><p>将该脚本内容保存为 <code>docker_proxy.sh</code>，终端执行 <code>bash docker_proxy.sh ubuntu 1.2.3.4:1080</code> 即可(自行替换 socks5 地址)；脚本实际上很简单，就是创建一个与 <code>docker.service</code> 文件同级的 <code>docker.service.d</code> 目录，然后在里面写入一个 <code>socks5-proxy.conf</code>，配置内容只有两行:</p><div class="hljs code-wrapper"><pre><code class="hljs sh">[Service]Environment=<span class="hljs-string">&quot;ALL_PROXY=socks5://1.2.3.4:1080</span></code></pre></div><p>这样 systemd 会自动读取，只需要 reload 一下，然后 restart docker daemon 即可，此后  docker 就可以通过加速端口直接 pull <code>gcr.io</code> 的镜像；<strong>注意: 配置加速后，docker 将无法 pull 私服镜像(一般私服都是内网 DNS 解析)，但是不会影响容器启动以及启动后的容器中的网络</strong></p><h3 id="四、Chrome-加速访问"><a href="#四、Chrome-加速访问" class="headerlink" title="四、Chrome 加速访问"></a>四、Chrome 加速访问</h3><p>对于 Chrome 浏览器来说，目前有比较好的插件实现用来配置根据策略的加速访问；这里使用的插件为 <code>SwitchyOmega</code></p><h4 id="4-1、SwitchyOmega-下载"><a href="#4-1、SwitchyOmega-下载" class="headerlink" title="4.1、SwitchyOmega 下载"></a>4.1、SwitchyOmega 下载</h4><p>默认情况下 <code>SwitchyOmega</code> 可以通过 Chrome 进行在线安装，但是众所周知的原因这是不可能的，不过国内有一些网站提供代理下载 Chrome 扩展的服务，如 <code>https://chrome-extension-downloader.com</code>、<code>http://yurl.sinaapp.com/crx.php</code>，这些网站只需要提供插件 ID 即可帮你下载下来；**<code>SwitchyOmega</code> 插件的 ID 为 <code>padekgcemlokbadohgkifijomclgjgif</code>，注意下载时不要使用 chrome 下载，因为他自身的防护机制会阻止你下载扩展程序**；下载后打开 chrome 的扩展设置页，将 crx 文件拖入安装即可，如下所示:</p><p><img src="https://cdn.oss.link/markdown/zruoq.png" alt="install chrome plugin"></p><h4 id="4-2、SwitchyOmega-配置"><a href="#4-2、SwitchyOmega-配置" class="headerlink" title="4.2、SwitchyOmega 配置"></a>4.2、SwitchyOmega 配置</h4><p>SwitchyOmega 安装成功后在 Chrome 右上角有显示，右键点击该图标，进入选项设置后如下所示:</p><p><img src="https://cdn.oss.link/markdown/ouh48.png" alt="SwitchyOmega detail"></p><p>默认情况下左侧只有两个加速模式，一个叫做 <code>proxy</code> 另一个叫做 <code>autoproxy</code>；根据加速模式不同 SwitchyOmega 在浏览网页时选择的加速通道也不同，不同的加速方式可以通过点击 <strong>新建情景模式</strong> 按钮创建，下面介绍一下常用的两种情景模式:</p><p><strong>代理服务器:</strong> 这种情景模式创建后需要填写一个代理地址，该地址可以是 http(s)/socks5(4) 类型；创建成功后，浏览器右上角切换到该情景模式，<strong>浏览器访问所有网页的流量全部通过该代理地址发出</strong>，不论你是访问百度还是 Google</p><p> <img src="https://cdn.oss.link/markdown/idbi4.png" alt="create test proxy1"></p><p> <img src="https://cdn.oss.link/markdown/52m7b.png" alt="create test proxy2"></p><p><strong>自动切换模式:</strong> 这种情景模式并不需要填写实际的代理地址，而是需要填写一些规则；创建完成后插件中选择此种情景模式时，浏览器访问所有网页流量会根据填写的规则自动路由，然后选择合适的代理情景模式；可以实现智能切换代理</p><p> <img src="https://cdn.oss.link/markdown/7u6mv.png" alt="create test auto proxy1"></p><p> <img src="https://cdn.oss.link/markdown/m5x36.png" alt="create test auto proxy2"></p><p>综上所述，首先应该创建(或者修改默认的 proxy 情景模式)一个代理服务器的情景模式，然后填写好你的加速 IP 和对应的协议端口；接下来在浏览器中切换到该情景模式尝试访问 kubenretes.io 等网站测试加速效果；成功后再次新建一个自动切换情景模式，**保证 <code>规则列表规则</code> 一栏后面的下拉列表对应到你刚刚创建的代理服务器情景模式，<code>默认情景模式</code> 后面的下拉列表对应到直接连接情景模式，然后点击下面的 <code>添加规则列表</code> 按钮，选择 <code>AutoProxy</code> 单选框，<code>规则列表网址</code> 填写 <code>https://raw.githubusercontent.com/gfwlist/gfwlist/master/gfwlist.txt</code>(这是一个开源项目收集的需要加速的网址列表)**；最后在浏览器中切换到自动切换情景模式，然后访问 kubernetes.io、baidu.com 等网站测试是否能自动切换情景模式</p><h3 id="五、Terminal-加速"><a href="#五、Terminal-加速" class="headerlink" title="五、Terminal 加速"></a>五、Terminal 加速</h3><h4 id="5-1、脚本方式"><a href="#5-1、脚本方式" class="headerlink" title="5.1、脚本方式"></a>5.1、脚本方式</h4><p>对于终端下的应用程序，百分之九十的程序都会识别 <code>http_proxy</code> 和 <code>https_proxy</code> 两个变量；所以终端加速最简单的方式就是在执行命令前声明这两个变量即可，为了方便起见也可以写个小脚本，示例如下:</p><div class="hljs code-wrapper"><pre><code class="hljs sh">sudo tee /usr/<span class="hljs-built_in">local</span>/bin/proxy &lt;&lt;-<span class="hljs-string">EOF</span><span class="hljs-string">#!/bin/bash</span><span class="hljs-string">http_proxy=http://1.2.3.4:8118 https_proxy=http://1.2.3.4:8118 \$*</span><span class="hljs-string">EOF</span>sudo chmod +x /usr/<span class="hljs-built_in">local</span>/bin/proxy</code></pre></div><p>将上面的地址自行更换成你的 http 加速地址后，终端运行 <code>proxy curl ip.cn</code> 即可测试加速效果</p><h4 id="5-2、proxychains-ng"><a href="#5-2、proxychains-ng" class="headerlink" title="5.2、proxychains-ng"></a>5.2、proxychains-ng</h4><p>proxychains-ng 是一个终端下的工具，它可以 hook libc 下的网络相关方法实现加速效果；目前支持后端为 http(s)/socks5(4a)，前段协议仅支持对 TCP 加速；</p><p>Mac 下安装方式:</p><div class="hljs code-wrapper"><pre><code class="hljs sh">brew install proxychains-ng</code></pre></div><p>Ubuntu 等平台下需要手动编译安装:</p><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment"># 安装编译依赖</span>apt-get -y install gcc make git<span class="hljs-comment"># 下载源码</span>git <span class="hljs-built_in">clone</span> https://github.com/rofl0r/proxychains-ng.git<span class="hljs-comment"># 编译安装</span><span class="hljs-built_in">cd</span> /proxychains-ng./configure --prefix=/usr --sysconfdir=/etcsudo make installsudo make install-config</code></pre></div><p>安装完成后编辑配置使用即可，Mac 下配置位于 <code>/usr/local/etc/proxychains.conf</code>，Ubuntu 下配置位于 <code>/etc/proxychains.conf</code>；配置修改如下:</p><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment"># 主要修改 [ProxyList] 下的加速地址</span>[ProxyList]socks5 1.2.3.4 1080</code></pre></div><p>然后命令行使用 <code>proxychains4 curl ip.cn</code> 测试即可</p><h3 id="六、Git-加速"><a href="#六、Git-加速" class="headerlink" title="六、Git 加速"></a>六、Git 加速</h3><p>目前 Git 的协议大致上只有三种 <code>https</code>、<code>ssh</code> 和 <code>git</code>，对于使用 <code>https</code> 方式进行 clone 和 push 操作时，可以使用第五部分 Terminal 加速方案即可实现对 Git 的加速；对于 <code>ssh</code>、<code>git</code> 协议，实际上都在调用 ssh 协议相关进行通讯(具体细节请 Google，这里的描述可能不精准)，此时同样可以使用 <code>proxychains-ng</code> 进行加速，**不过需要注意 <code>proxychains-ng</code> 要自行编译安装，同时 <code>./configure</code> 增加 <code>--fat-binary</code> 选项，具体参考 <a href="https://github.com/rofl0r/proxychains-ng/issues/109">GitHub Issue</a>**；<code>ssh</code>、<code>git</code> 由于都在调用 ssh 协议进行通讯，所以实际上还可以通过设置 ssh 的 <code>ProxyCommand</code> 来实现，具体操作如下:</p><div class="hljs code-wrapper"><pre><code class="hljs sh">sudo tee /usr/<span class="hljs-built_in">local</span>/bin/proxy-wrapper &lt;&lt;-<span class="hljs-string">EOF</span><span class="hljs-string">#!/bin/bash</span><span class="hljs-string">nc -x1.2.3.4:1080 -X5 \$*</span><span class="hljs-string">#connect-proxy -S 1.2.3.4:1080 \$*</span><span class="hljs-string">EOF</span>sudo chmod +x /usr/<span class="hljs-built_in">local</span>/bin/proxy-wrappersudo tee ~/.ssh/config &lt;&lt;-<span class="hljs-string">EOF</span><span class="hljs-string">Host github.com</span><span class="hljs-string">    ProxyCommand /usr/local/bin/proxy-wrapper &#x27;%h %p&#x27;</span><span class="hljs-string">EOF</span></code></pre></div><p>需要注意: <strong>nc 命令是 netcat-openbsd 版本，Mac 下默认提供，Ubuntu 下需要使用 <code>apt-get install -y netcat-openbsd</code> 安装；CentOS 没有 netcat-openbsd，需要安装 EPEL 源，然后安装 connect-proxy 包，使用 connect-proxy 命令替代</strong></p>]]></content>
    
    
    <summary type="html">本文主要阐述在 *Uinx 平台下，各种常用开发工具的加速配置，**加速前提是你需要有一个能够加速的 socks5 端口，常用工具请自行搭建**；本文档包括 docker、terminal、git、chrome 常用加速配置，其他工具可能后续补充</summary>
    
    
    
    <category term="Linux" scheme="https://mritd.com/categories/linux/"/>
    
    
    <category term="Linux" scheme="https://mritd.com/tags/linux/"/>
    
  </entry>
  
  <entry>
    <title>使用 RBAC 控制 kubectl 权限</title>
    <link href="https://mritd.com/2018/03/20/use-rbac-to-control-kubectl-permissions/"/>
    <id>https://mritd.com/2018/03/20/use-rbac-to-control-kubectl-permissions/</id>
    <published>2018-03-20T15:58:37.000Z</published>
    <updated>2018-03-20T15:58:37.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>好久没写文章了，过年以后就有点懒… 最近也在学习 golang，再加上不断造轮子所以没太多时间；凑巧最近想控制一下 kubectl 权限，这里便记录一下。</p></blockquote><h3 id="一、RBAC-相关"><a href="#一、RBAC-相关" class="headerlink" title="一、RBAC 相关"></a>一、RBAC 相关</h3><p>相信现在大部分人用的集群已经都是 1.6 版本以上，而且在安装各种组件的时候也已经或多或少的处理过 RBAC 的东西，所以这里不做太细节性的讲述，RBAC 文档我以前胡乱翻译过一篇，请看 <a href="https://mritd.me/2017/07/17/kubernetes-rbac-chinese-translation/">这里</a>，以下内容仅说主要的</p><h4 id="1-1、RBAC-用户角色相关"><a href="#1-1、RBAC-用户角色相关" class="headerlink" title="1.1、RBAC 用户角色相关"></a>1.1、RBAC 用户角色相关</h4><p>我在第一次接触 Kubernetes RBAC 的时候，对于基于角色控制权限这种做法是有了解的，基本结构主要就是三个:</p><ul><li>权限: 即对系统中指定资源的增删改查权限</li><li>角色: 将一定的权限组合在一起产生权限组，如管理员角色</li><li>用户: 具体的使用者，具有唯一身份标识(ID)，其后与角色绑定便拥有角色的对应权限</li></ul><p>但是翻了一会文档，最晕的就是 <strong>这个用户标识(ID)存在哪</strong>，因为传统的授权模型都是下面这样</p><p><img src="https://cdn.oss.link/markdown/sn1qp.png" alt="ctrole"></p><p>不论怎样，在进行授权时总要有个地方存放用户信息(DB/文件)，但是在 Kubernetes 里却没找到；后来翻阅文档，找到<a href="https://kubernetes.io/docs/admin/authentication/">这么一段</a></p><div class="hljs code-wrapper"><pre><code class="hljs livecodeserver">Normal users are assumed <span class="hljs-built_in">to</span> be managed <span class="hljs-keyword">by</span> <span class="hljs-keyword">an</span> outside, independent service. An admin distributing <span class="hljs-keyword">private</span> <span class="hljs-built_in">keys</span>, <span class="hljs-keyword">a</span> user store like Keystone <span class="hljs-keyword">or</span> Google Accounts, even <span class="hljs-keyword">a</span> <span class="hljs-built_in">file</span> <span class="hljs-keyword">with</span> <span class="hljs-keyword">a</span> list <span class="hljs-keyword">of</span> usernames <span class="hljs-keyword">and</span> passwords.</code></pre></div><p><strong>也就是说，Kubernetes 是不负责维护存储用户数据的；对于 Kubernetes 来说，它识别或者说认识一个用户主要就几种方式</strong></p><ul><li>X509 Client Certs: 使用由 k8s 根 CA 签发的证书，提取 O 字段</li><li>Static Token File: 预先在 API Server 放置 Token 文件(bootstrap 阶段使用过)</li><li>Bootstrap Tokens: 一种在集群内创建的 Bootstrap 专用 Token(新的 Bootstarp 推荐)</li><li>Static Password File: 跟静态 Token 类似</li><li>Service Account Tokens: 使用 Service Account 的 Token</li></ul><p>其他不再一一列举，具体请看文档 <a href="https://kubernetes.io/docs/admin/authentication/">Authenticating</a>；了解了这些，后面我们使用 RBAC 控制 kubectl 权限的时候就要使用如上几种方法创建对应用户</p><h4 id="1-2、RBAC-权限相关"><a href="#1-2、RBAC-权限相关" class="headerlink" title="1.2、RBAC 权限相关"></a>1.2、RBAC 权限相关</h4><p>RBAC 权限定义部分主要有三个层级</p><ul><li>apiGroups: 指定那个 API 组下的权限</li><li>resources: 该组下具体资源，如 pod 等</li><li>verbs: 指对该资源具体执行哪些动作</li></ul><p>定义一组权限(角色)时要根据其所需的真正需求做最细粒度的划分</p><h3 id="二、创建一个只读的用户"><a href="#二、创建一个只读的用户" class="headerlink" title="二、创建一个只读的用户"></a>二、创建一个只读的用户</h3><h4 id="2-1、创建用户"><a href="#2-1、创建用户" class="headerlink" title="2.1、创建用户"></a>2.1、创建用户</h4><p>首先根据上文可以得知，Kubernetes 不存储用户具体细节信息，也就是说只要通过它的那几种方式能进来的用户，Kubernetes 就认为它是合法的；那么为了让 kubectl 只读，所以我们需要先给它创建一个用来承载只读权限的用户；这里用户创建我们选择使用证书方式</p><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment"># 首先先创建一个用于签发证书的 json(证书创建使用 cfssl)</span>&#123;  <span class="hljs-string">&quot;CN&quot;</span>: <span class="hljs-string">&quot;readonly&quot;</span>,  <span class="hljs-string">&quot;hosts&quot;</span>: [],  <span class="hljs-string">&quot;key&quot;</span>: &#123;    <span class="hljs-string">&quot;algo&quot;</span>: <span class="hljs-string">&quot;rsa&quot;</span>,    <span class="hljs-string">&quot;size&quot;</span>: 2048  &#125;,  <span class="hljs-string">&quot;names&quot;</span>: [    &#123;      <span class="hljs-string">&quot;C&quot;</span>: <span class="hljs-string">&quot;CN&quot;</span>,      <span class="hljs-string">&quot;ST&quot;</span>: <span class="hljs-string">&quot;BeiJing&quot;</span>,      <span class="hljs-string">&quot;L&quot;</span>: <span class="hljs-string">&quot;BeiJing&quot;</span>,      <span class="hljs-string">&quot;O&quot;</span>: <span class="hljs-string">&quot;develop:readonly&quot;</span>,      <span class="hljs-string">&quot;OU&quot;</span>: <span class="hljs-string">&quot;develop&quot;</span>    &#125;  ]&#125;</code></pre></div><p>然后基于以 Kubernetes CA 证书创建只读用户的证书</p><div class="hljs code-wrapper"><pre><code class="hljs sh">cfssl gencert --ca /etc/kubernetes/ssl/k8s-root-ca.pem \              --ca-key /etc/kubernetes/ssl/k8s-root-ca-key.pem \              --config k8s-gencert.json \              --profile kubernetes readonly.json | \              cfssljson --bare <span class="hljs-built_in">readonly</span></code></pre></div><p>以上命令会生成 <code>readonly-key.pem</code>、<code>readonly.pem</code> 两个证书文件以及一个 csr 请求文件</p><h4 id="2-2、创建-kubeconfig"><a href="#2-2、创建-kubeconfig" class="headerlink" title="2.2、创建 kubeconfig"></a>2.2、创建 kubeconfig</h4><p>有了用于证明身份的证书以后，接下来创建一个 kubeconfig 文件方便 kubectl 使用</p><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-meta">#!/bin/bash</span>KUBE_API_SERVER=<span class="hljs-string">&quot;https://172.16.0.18:6443&quot;</span>CERT_DIR=<span class="hljs-variable">$&#123;2:-&quot;/etc/kubernetes/ssl&quot;&#125;</span>kubectl config set-cluster default-cluster --server=<span class="hljs-variable">$&#123;KUBE_API_SERVER&#125;</span> \    --certificate-authority=<span class="hljs-variable">$&#123;CERT_DIR&#125;</span>/k8s-root-ca.pem \    --embed-certs=<span class="hljs-literal">true</span> \    --kubeconfig=readonly.kubeconfigkubectl config set-credentials develop-readonly \    --certificate-authority=<span class="hljs-variable">$&#123;CERT_DIR&#125;</span>/k8s-root-ca.pem \    --embed-certs=<span class="hljs-literal">true</span> \    --client-key=readonly-key.pem \    --client-certificate=readonly.pem \    --kubeconfig=readonly.kubeconfigkubectl config set-context default-system --cluster=default-cluster \    --user=develop-readonly \    --kubeconfig=readonly.kubeconfigkubectl config use-context default-system --kubeconfig=readonly.kubeconfig</code></pre></div><p>这条命令会将证书也写入到 readonly.kubeconfig 配置文件中，将该文件放在 <code>~/.kube/config</code> 位置，kubectl 会自动读取</p><h4 id="2-3、创建-ClusterRole"><a href="#2-3、创建-ClusterRole" class="headerlink" title="2.3、创建 ClusterRole"></a>2.3、创建 ClusterRole</h4><p>本示例创建的只读用户权限范围为 Cluster 集群范围，所以先创建一个只读权限的 ClusterRole；创建 ClusterRole 不知道都有哪些权限的话，最简单的办法是将集群的 admin ClusterRole 保存出来，然后做修改</p><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment"># 导出 admin ClusterRole</span>kubectl get clusterrole admin -o yaml &gt; readonly.yaml</code></pre></div><p>这个 admin ClusterRole 是默认存在的，导出后我们根据自己需求修改就行；最基本的原则就是像 update、delete 这种权限必须删掉(我们要创建只读用户)，修改后如下</p><div class="hljs code-wrapper"><pre><code class="hljs yaml"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">rbac.authorization.k8s.io/v1beta1</span><span class="hljs-attr">kind:</span> <span class="hljs-string">ClusterRole</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">cluster-readonly</span><span class="hljs-attr">rules:</span><span class="hljs-bullet">-</span> <span class="hljs-attr">apiGroups:</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">&quot;&quot;</span>  <span class="hljs-attr">resources:</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">pods</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">pods/attach</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">pods/exec</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">pods/portforward</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">pods/proxy</span>  <span class="hljs-attr">verbs:</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">get</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">list</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">watch</span><span class="hljs-bullet">-</span> <span class="hljs-attr">apiGroups:</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">&quot;&quot;</span>  <span class="hljs-attr">resources:</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">configmaps</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">endpoints</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">persistentvolumeclaims</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">replicationcontrollers</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">replicationcontrollers/scale</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">secrets</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">serviceaccounts</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">services</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">services/proxy</span>  <span class="hljs-attr">verbs:</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">get</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">list</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">watch</span><span class="hljs-bullet">-</span> <span class="hljs-attr">apiGroups:</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">&quot;&quot;</span>  <span class="hljs-attr">resources:</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">bindings</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">events</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">limitranges</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">namespaces/status</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">pods/log</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">pods/status</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">replicationcontrollers/status</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">resourcequotas</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">resourcequotas/status</span>  <span class="hljs-attr">verbs:</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">get</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">list</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">watch</span><span class="hljs-bullet">-</span> <span class="hljs-attr">apiGroups:</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">&quot;&quot;</span>  <span class="hljs-attr">resources:</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">namespaces</span>  <span class="hljs-attr">verbs:</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">get</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">list</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">watch</span><span class="hljs-bullet">-</span> <span class="hljs-attr">apiGroups:</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">apps</span>  <span class="hljs-attr">resources:</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">deployments</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">deployments/rollback</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">deployments/scale</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">statefulsets</span>  <span class="hljs-attr">verbs:</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">get</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">list</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">watch</span><span class="hljs-bullet">-</span> <span class="hljs-attr">apiGroups:</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">autoscaling</span>  <span class="hljs-attr">resources:</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">horizontalpodautoscalers</span>  <span class="hljs-attr">verbs:</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">get</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">list</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">watch</span><span class="hljs-bullet">-</span> <span class="hljs-attr">apiGroups:</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">batch</span>  <span class="hljs-attr">resources:</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">cronjobs</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">jobs</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">scheduledjobs</span>  <span class="hljs-attr">verbs:</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">get</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">list</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">watch</span><span class="hljs-bullet">-</span> <span class="hljs-attr">apiGroups:</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">extensions</span>  <span class="hljs-attr">resources:</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">daemonsets</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">deployments</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">ingresses</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">replicasets</span>  <span class="hljs-attr">verbs:</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">get</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">list</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">watch</span></code></pre></div><p>最后执行 <code>kubectl create -f readonly.yaml</code> 创建即可</p><h4 id="2-4、创建-ClusterRoleBinding"><a href="#2-4、创建-ClusterRoleBinding" class="headerlink" title="2.4、创建 ClusterRoleBinding"></a>2.4、创建 ClusterRoleBinding</h4><p>用户已经创建完成，集群权限也有了，接下来使用 ClusterRoleBinding 绑定到一起即可</p><div class="hljs code-wrapper"><pre><code class="hljs yaml"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">rbac.authorization.k8s.io/v1beta1</span><span class="hljs-attr">kind:</span> <span class="hljs-string">ClusterRoleBinding</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">cluster-readonly</span><span class="hljs-attr">roleRef:</span>  <span class="hljs-attr">apiGroup:</span> <span class="hljs-string">rbac.authorization.k8s.io</span>  <span class="hljs-attr">kind:</span> <span class="hljs-string">ClusterRole</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">cluster-readonly</span><span class="hljs-attr">subjects:</span><span class="hljs-bullet">-</span> <span class="hljs-attr">apiGroup:</span> <span class="hljs-string">rbac.authorization.k8s.io</span>  <span class="hljs-attr">kind:</span> <span class="hljs-string">Group</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">develop:readonly</span></code></pre></div><p>将以上保存为 <code>readonly-bind.yaml</code> 执行 <code>kubectl create -f readonly-bind.yaml</code> 即可</p><h4 id="2-5、测试权限"><a href="#2-5、测试权限" class="headerlink" title="2.5、测试权限"></a>2.5、测试权限</h4><p>将最初创建的 kubeconfig 放到 <code>~/.kube/config</code> 或者直接使用 <code>--kubeconfig</code> 选项测试读取、删除 pod 等权限即可，测试后如下所示</p><p><img src="https://cdn.oss.link/markdown/68ukm.png" alt="test readonly"></p>]]></content>
    
    
    <summary type="html">好久没写文章了，过年以后就有点懒... 最近也在学习 golang，再加上不断造轮子所以没太多时间；凑巧最近想控制一下 kubectl 权限，这里便记录一下。</summary>
    
    
    
    <category term="Kubernetes" scheme="https://mritd.com/categories/kubernetes/"/>
    
    
    <category term="Linux" scheme="https://mritd.com/tags/linux/"/>
    
    <category term="Docker" scheme="https://mritd.com/tags/docker/"/>
    
    <category term="Kubernetes" scheme="https://mritd.com/tags/kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>Kubernetes TLS bootstrapping 那点事</title>
    <link href="https://mritd.com/2018/01/07/kubernetes-tls-bootstrapping-note/"/>
    <id>https://mritd.com/2018/01/07/kubernetes-tls-bootstrapping-note/</id>
    <published>2018-01-07T10:06:06.000Z</published>
    <updated>2018-01-07T10:06:06.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>前段时间撸了一会 Kubernetes 官方文档，在查看 TLS bootstrapping 这块是发现已经跟 1.4 的时候完全不一样了；目前所有搭建文档也都保留着 1.4 时代的配置，在看完文档后发现目前配置有很多问题，同时也埋下了 <strong>隐藏炸弹</strong>，这个问题可能会在一年后爆发…..后果就是集群 node 全部掉线；所以仔细的撸了一下这个文档，从元旦到写此文章的时间都在测试这个 TLS bootstrapping，以下记录一下这次的成果</p></blockquote><p>阅读本文章前，请先阅读一下本文参考的相关文档:</p><ul><li><a href="https://kubernetes.io/docs/admin/kubelet-tls-bootstrapping/">TLS bootstrapping</a></li><li><a href="https://github.com/jcbsmpsn/community/blob/a843295a4f7594d41e66a8342e174f48d06b4f9f/contributors/design-proposals/kubelet-server-certificate-bootstrap-rotation.md">Kubelet Server Certificate Bootstrap &amp; Rotation</a></li><li><a href="https://kubernetes.io/docs/admin/authorization/rbac/">Using RBAC Authorization</a></li></ul><h3 id="一、TLS-bootstrapping-简介"><a href="#一、TLS-bootstrapping-简介" class="headerlink" title="一、TLS bootstrapping 简介"></a>一、TLS bootstrapping 简介</h3><p>Kubernetes 在 1.4 版本(我记着是)推出了 TLS bootstrapping 功能；这个功能主要解决了以下问题:</p><p>当集群开启了 TLS 认证后，每个节点的 kubelet 组件都要使用由 apiserver 使用的 CA 签发的有效证书才能与 apiserver 通讯；此时如果节点多起来，为每个节点单独签署证书将是一件非常繁琐的事情；TLS bootstrapping 功能就是让 kubelet 先使用一个预定的低权限用户连接到 apiserver，然后向 apiserver 申请证书，kubelet 的证书由 apiserver 动态签署；在配合 RBAC 授权模型下的工作流程大致如下所示(不完整，下面细说)</p><p><img src="https://cdn.oss.link/markdown/ixtwd.png" alt="tls_bootstrapping"></p><h3 id="二、TLS-bootstrapping-相关术语"><a href="#二、TLS-bootstrapping-相关术语" class="headerlink" title="二、TLS bootstrapping 相关术语"></a>二、TLS bootstrapping 相关术语</h3><h4 id="2-1、kubelet-server"><a href="#2-1、kubelet-server" class="headerlink" title="2.1、kubelet server"></a>2.1、kubelet server</h4><p>在官方 TLS bootstrapping 文档中多次提到过 <code>kubelet server</code> 这个东西；在经过翻阅大量文档以及 TLS bootstrapping 设计文档后得出，**<code>kubelet server</code> 指的应该是 kubelet 的 10250 端口；**</p><p><strong>kubelet 组件在工作时，采用主动的查询机制，即定期请求 apiserver 获取自己所应当处理的任务，如哪些 pod 分配到了自己身上，从而去处理这些任务；同时 kubelet 自己还会暴露出两个本身 api 的端口，用于将自己本身的私有 api 暴露出去，这两个端口分别是 10250 与 10255；对于 10250 端口，kubelet 会在其上采用 TLS 加密以提供适当的鉴权功能；对于 10255 端口，kubelet 会以只读形式暴露组件本身的私有 api，并且不做鉴权处理</strong></p><p><strong>总结一下，就是说 kubelet 上实际上有两个地方用到证书，一个是用于与 API server 通讯所用到的证书，另一个是 kubelet 的 10250 私有 api 端口需要用到的证书</strong></p><h4 id="2-2、CSR-请求类型"><a href="#2-2、CSR-请求类型" class="headerlink" title="2.2、CSR 请求类型"></a>2.2、CSR 请求类型</h4><p>kubelet 发起的 CSR 请求都是由 controller manager 来做实际签署的，对于 controller manager 来说，TLS bootstrapping 下 kubelet 发起的 CSR 请求大致分为以下三种</p><ul><li>nodeclient: kubelet 以 <code>O=system:nodes</code> 和 <code>CN=system:node:(node name)</code> 形式发起的 CSR 请求</li><li>selfnodeclient: kubelet client renew 自己的证书发起的 CSR 请求(与上一个证书就有相同的 O 和 CN)</li><li>selfnodeserver: kubelet server renew 自己的证书发起的 CSR 请求</li></ul><p><strong>大白话加自己测试得出的结果: nodeclient 类型的 CSR 仅在第一次启动时会产生，selfnodeclient 类型的 CSR 请求实际上就是 kubelet renew 自己作为 client 跟 apiserver 通讯时使用的证书产生的，selfnodeserver 类型的 CSR 请求则是 kubelet 首次申请或后续 renew 自己的 10250 api 端口证书时产生的</strong></p><h3 id="三、TLS-bootstrapping-具体引导过程"><a href="#三、TLS-bootstrapping-具体引导过程" class="headerlink" title="三、TLS bootstrapping 具体引导过程"></a>三、TLS bootstrapping 具体引导过程</h3><h4 id="3-1、Kubernetes-TLS-与-RBAC-认证"><a href="#3-1、Kubernetes-TLS-与-RBAC-认证" class="headerlink" title="3.1、Kubernetes TLS 与 RBAC 认证"></a>3.1、Kubernetes TLS 与 RBAC 认证</h4><p>在说具体的引导过程之前先谈一下 TLS 和 RBAC，因为这两个事不整明白下面的都不用谈；</p><ul><li>TLS 作用</li></ul><p>众所周知 TLS 的作用就是对通讯加密，防止中间人窃听；同时如果证书不信任的话根本就无法与 apiserver 建立连接，更不用提有没有权限向 apiserver 请求指定内容</p><ul><li>RBAC 作用</li></ul><p>当 TLS 解决了通讯问题后，那么权限问题就应由 RBAC 解决(可以使用其他权限模型，如 ABAC)；RBAC 中规定了一个用户或者用户组(subject)具有请求哪些 api 的权限；<strong>在配合 TLS 加密的时候，实际上 apiserver 读取客户端证书的 CN 字段作为用户名，读取 O 字段作为用户组</strong></p><p>从以上两点上可以总结出两点: 第一，想要与 apiserver 通讯就必须采用由 apiserver CA 签发的证书，这样才能形成信任关系，建立 TLS 连接；第二，可以通过证书的 CN、O 字段来提供 RBAC 所需的用户与用户组</p><h4 id="3-2、kubelet-首次启动流程"><a href="#3-2、kubelet-首次启动流程" class="headerlink" title="3.2、kubelet 首次启动流程"></a>3.2、kubelet 首次启动流程</h4><p>看完上面的介绍，不知道有没有人想过，既然 TLS bootstrapping 功能是让 kubelet 组件去 apiserver 申请证书，然后用于连接 apiserver；<strong>那么第一次启动时没有证书如何连接 apiserver ?</strong></p><p>这个问题实际上可以去查看一下 <code>bootstrap.kubeconfig</code> 和 <code>token.csv</code> 得到答案: <strong>在 apiserver 配置中指定了一个 <code>token.csv</code> 文件，该文件中是一个预设的用户配置；同时该用户的 Token 和 apiserver 的 CA 证书被写入了 kubelet 所使用的 <code>bootstrap.kubeconfig</code> 配置文件中；这样在首次请求时，kubelet 使用 <code>bootstrap.kubeconfig</code> 中的 apiserver CA 证书来与 apiserver 建立 TLS 通讯，使用 <code>bootstrap.kubeconfig</code> 中的用户 Token 来向 apiserver 声明自己的 RBAC 授权身份</strong>，如下图所示</p><p><img src="https://cdn.oss.link/markdown/ji5ug.png" alt="first_request"></p><p>在有些用户首次启动时，可能与遇到 kubelet 报 401 无权访问 apiserver 的错误；<strong>这是因为在默认情况下，kubelet 通过 <code>bootstrap.kubeconfig</code> 中的预设用户 Token 声明了自己的身份，然后创建 CSR 请求；但是不要忘记这个用户在我们不处理的情况下他没任何权限的，包括创建 CSR 请求；所以需要如下命令创建一个 ClusterRoleBinding，将预设用户 <code>kubelet-bootstrap</code> 与内置的 ClusterRole <code>system:node-bootstrapper</code> 绑定到一起，使其能够发起 CSR 请求</strong></p><div class="hljs code-wrapper"><pre><code class="hljs sh">kubectl create clusterrolebinding kubelet-bootstrap \  --clusterrole=system:node-bootstrapper \  --user=kubelet-bootstrap</code></pre></div><h4 id="3-3、手动签发证书"><a href="#3-3、手动签发证书" class="headerlink" title="3.3、手动签发证书"></a>3.3、手动签发证书</h4><p>在 kubelet 首次启动后，如果用户 Token 没问题，并且 RBAC 也做了相应的设置，那么此时在集群内应该能看到 kubelet 发起的 CSR 请求</p><p><img src="https://cdn.oss.link/markdown/n9bbw.png" alt="bootstrap_csr"></p><p>出现 CSR 请求后，可以使用 kubectl 手动签发(允许) kubelet 的证书</p><p><img src="https://cdn.oss.link/markdown/5ssf8.png" alt="bootstrap_approve_crt"></p><p><strong>当成功签发证书后，目标节点的 kubelet 会将证书写入到 <code>--cert-dir=</code> 选项指定的目录中；注意此时如果不做其他设置应当生成四个文件</strong></p><p><img src="https://cdn.oss.link/markdown/a25ip.png" alt="bootstrap_crt"></p><p><strong>而 kubelet 与 apiserver 通讯所使用的证书为 <code>kubelet-client.crt</code>，剩下的 <code>kubelet.crt</code> 将会被用于 <code>kubelet server</code>(10250) 做鉴权使用；注意，此时 <code>kubelet.crt</code> 这个证书是个独立于 apiserver CA 的自签 CA，并且删除后 kubelet 组件会重新生成它</strong></p><h3 id="四、TLS-bootstrapping-证书自动续期"><a href="#四、TLS-bootstrapping-证书自动续期" class="headerlink" title="四、TLS bootstrapping 证书自动续期"></a>四、TLS bootstrapping 证书自动续期</h3><blockquote><p>单独把这部分拿出来写，是因为个人觉得上面已经有点乱了；这部分实际上更复杂，只好单独写一下了，因为这部分涉及的东西比较多，所以也不想草率的几笔带过</p></blockquote><h4 id="4-1、RBAC-授权"><a href="#4-1、RBAC-授权" class="headerlink" title="4.1、RBAC 授权"></a>4.1、RBAC 授权</h4><p>首先…首先好几次了…嗯，就是说 kubelet 所发起的 CSR 请求是由 controller manager 签署的；如果想要是实现自动续期，就需要让 controller manager 能够在 kubelet 发起证书请求的时候自动帮助其签署证书；那么 controller manager 不可能对所有的 CSR 证书申请都自动签署，这时候就需要配置 RBAC 规则，<strong>保证 controller manager 只对 kubelet 发起的特定 CSR 请求自动批准即可</strong>；在 TLS bootstrapping 官方文档中，针对上面 2.2 章节提出的 3 种 CSR 请求分别给出了 3 种对应的 ClusterRole，如下所示</p><div class="hljs code-wrapper"><pre><code class="hljs yaml"><span class="hljs-comment"># A ClusterRole which instructs the CSR approver to approve a user requesting</span><span class="hljs-comment"># node client credentials.</span><span class="hljs-attr">kind:</span> <span class="hljs-string">ClusterRole</span><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">rbac.authorization.k8s.io/v1</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">approve-node-client-csr</span><span class="hljs-attr">rules:</span><span class="hljs-bullet">-</span> <span class="hljs-attr">apiGroups:</span> [<span class="hljs-string">&quot;certificates.k8s.io&quot;</span>]  <span class="hljs-attr">resources:</span> [<span class="hljs-string">&quot;certificatesigningrequests/nodeclient&quot;</span>]  <span class="hljs-attr">verbs:</span> [<span class="hljs-string">&quot;create&quot;</span>]<span class="hljs-meta">---</span><span class="hljs-comment"># A ClusterRole which instructs the CSR approver to approve a node renewing its</span><span class="hljs-comment"># own client credentials.</span><span class="hljs-attr">kind:</span> <span class="hljs-string">ClusterRole</span><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">rbac.authorization.k8s.io/v1</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">approve-node-client-renewal-csr</span><span class="hljs-attr">rules:</span><span class="hljs-bullet">-</span> <span class="hljs-attr">apiGroups:</span> [<span class="hljs-string">&quot;certificates.k8s.io&quot;</span>]  <span class="hljs-attr">resources:</span> [<span class="hljs-string">&quot;certificatesigningrequests/selfnodeclient&quot;</span>]  <span class="hljs-attr">verbs:</span> [<span class="hljs-string">&quot;create&quot;</span>]<span class="hljs-meta">---</span><span class="hljs-comment"># A ClusterRole which instructs the CSR approver to approve a node requesting a</span><span class="hljs-comment"># serving cert matching its client cert.</span><span class="hljs-attr">kind:</span> <span class="hljs-string">ClusterRole</span><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">rbac.authorization.k8s.io/v1</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">approve-node-server-renewal-csr</span><span class="hljs-attr">rules:</span><span class="hljs-bullet">-</span> <span class="hljs-attr">apiGroups:</span> [<span class="hljs-string">&quot;certificates.k8s.io&quot;</span>]  <span class="hljs-attr">resources:</span> [<span class="hljs-string">&quot;certificatesigningrequests/selfnodeserver&quot;</span>]  <span class="hljs-attr">verbs:</span> [<span class="hljs-string">&quot;create&quot;</span>]</code></pre></div><p>RBAC 中 ClusterRole 只是描述或者说定义一种集群范围内的能力，这三个 ClusterRole 在 1.7 之前需要自己手动创建，在 1.8 后 apiserver 会自动创建前两个(1.8 以后名称有改变，自己查看文档)；以上三个 ClusterRole 含义如下</p><ul><li>approve-node-client-csr: 具有自动批准 nodeclient 类型 CSR 请求的能力</li><li>approve-node-client-renewal-csr: 具有自动批准 selfnodeclient 类型 CSR 请求的能力</li><li>approve-node-server-renewal-csr: 具有自动批准 selfnodeserver 类型 CSR 请求的能力</li></ul><p><strong>所以，如果想要 kubelet 能够自动续期，那么就应当将适当的 ClusterRole 绑定到 kubelet 自动续期时所所采用的用户或者用户组身上</strong></p><h4 id="4-2、自动续期下的引导过程"><a href="#4-2、自动续期下的引导过程" class="headerlink" title="4.2、自动续期下的引导过程"></a>4.2、自动续期下的引导过程</h4><p>在自动续期下引导过程与单纯的手动批准 CSR 有点差异，具体的引导流程地址如下</p><ul><li>kubelet 读取 bootstrap.kubeconfig，使用其 CA 与 Token 向 apiserver 发起第一次 CSR 请求(nodeclient)</li><li>apiserver 根据 RBAC 规则自动批准首次 CSR 请求(approve-node-client-csr)，并下发证书(kubelet-client.crt)</li><li>kubelet **使用刚刚签发的证书(O=system:nodes, CN=system:node:NODE_NAME)**与 apiserver 通讯，并发起申请 10250 server 所使用证书的 CSR 请求</li><li>apiserver 根据 RBAC 规则自动批准 kubelet 为其 10250 端口申请的证书(kubelet-server-current.crt)</li><li>证书即将到期时，kubelet 自动向 apiserver 发起用于与 apiserver 通讯所用证书的 renew CSR 请求和 renew 本身 10250 端口所用证书的 CSR 请求</li><li>apiserver 根据 RBAC 规则自动批准两个证书</li><li>kubelet 拿到新证书后关闭所有连接，reload 新证书，以后便一直如此</li></ul><p><strong>从以上流程我们可以看出，我们如果要创建 RBAC 规则，则至少能满足四种情况:</strong></p><ul><li>自动批准 kubelet 首次用于与 apiserver 通讯证书的 CSR 请求(nodeclient)</li><li>自动批准 kubelet 首次用于 10250 端口鉴权的 CSR 请求(实际上这个请求走的也是 selfnodeserver 类型 CSR)</li><li>自动批准 kubelet 后续 renew 用于与 apiserver 通讯证书的 CSR 请求(selfnodeclient)</li><li>自动批准 kubelet 后续 renew 用于 10250 端口鉴权的 CSR 请求(selfnodeserver)</li></ul><p>基于以上四种情况，我们需要创建 3 个 ClusterRoleBinding，创建如下</p><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment"># 自动批准 kubelet 的首次 CSR 请求(用于与 apiserver 通讯的证书)</span>kubectl create clusterrolebinding node-client-auto-approve-csr --clusterrole=approve-node-client-csr --group=system:bootstrappers<span class="hljs-comment"># 自动批准 kubelet 后续 renew 用于与 apiserver 通讯证书的 CSR 请求</span>kubectl create clusterrolebinding node-client-auto-renew-crt --clusterrole=approve-node-client-renewal-csr --group=system:nodes<span class="hljs-comment"># 自动批准 kubelet 发起的用于 10250 端口鉴权证书的 CSR 请求(包括后续 renew)</span>kubectl create clusterrolebinding node-server-auto-renew-crt --clusterrole=approve-node-server-renewal-csr --group=system:nodes</code></pre></div><h4 id="4-3、开启自动续期"><a href="#4-3、开启自动续期" class="headerlink" title="4.3、开启自动续期"></a>4.3、开启自动续期</h4><p>在 1.7 后，kubelet 启动时增加 <code>--feature-gates=RotateKubeletClientCertificate=true,RotateKubeletServerCertificate=true</code> 选项，则 kubelet 在证书即将到期时会自动发起一个 renew 自己证书的 CSR 请求；同时 controller manager 需要在启动时增加 <code>--feature-gates=RotateKubeletServerCertificate=true</code> 参数，再配合上面创建好的 ClusterRoleBinding，kubelet client 和 kubelet server 证才书会被自动签署；</p><p><strong>注意，1.7 版本设置自动续期参数后，新的 renew 请求不会立即开始，而是在证书总有效期的 <code>70%~90%</code> 的时间时发起；而且经测试 1.7 版本即使自动签发了证书，kubelet 在不重启的情况下不会重新应用新证书；在 1.8 后 kubelet 组件在增加一个 <code>--rotate-certificates</code> 参数后，kubelet 才会自动重载新证书</strong></p><h4 id="4-3、证书过期问题"><a href="#4-3、证书过期问题" class="headerlink" title="4.3、证书过期问题"></a>4.3、证书过期问题</h4><p>需要重复强调一个问题是: <strong>TLS bootstrapping 时的证书实际是由 kube-controller-manager 组件来签署的，也就是说证书有效期是 kube-controller-manager 组件控制的</strong>；所以在 1.7 版本以后(我查文档发现的从1.7开始有) kube-controller-manager 组件提供了一个 <code>--experimental-cluster-signing-duration</code> 参数来设置签署的证书有效时间；默认为 <code>8760h0m0s</code>，将其改为 <code>87600h0m0s</code> 即 10 年后再进行 TLS bootstrapping 签署证书即可。</p><h3 id="五、TLS-bootstrapping-总结以及详细操作"><a href="#五、TLS-bootstrapping-总结以及详细操作" class="headerlink" title="五、TLS bootstrapping 总结以及详细操作"></a>五、TLS bootstrapping 总结以及详细操作</h3><h4 id="5-1、主要流程细节"><a href="#5-1、主要流程细节" class="headerlink" title="5.1、主要流程细节"></a>5.1、主要流程细节</h4><p>kubelet 首次启动通过加载 <code>bootstrap.kubeconfig</code> 中的用户 Token 和 apiserver CA 证书发起首次 CSR 请求，这个 Token 被预先内置在 apiserver 节点的 token.csv 中，其身份为 <code>kubelet-bootstrap</code> 用户和 <code>system:bootstrappers</code> 用户组；想要首次 CSR 请求能成功(成功指的是不会被 apiserver 401 拒绝)，则需要先将 <code>kubelet-bootstrap</code> 用户和 <code>system:node-bootstrapper</code> 内置 ClusterRole 绑定；</p><p>对于首次 CSR 请求可以手动批准，也可以将 <code>system:bootstrappers</code> 用户组与 <code>approve-node-client-csr</code> ClusterRole 绑定实现自动批准(1.8 之前这个 ClusterRole 需要手动创建，1.8 后 apiserver 自动创建，并更名为 <code>system:certificates.k8s.io:certificatesigningrequests:nodeclient</code>)</p><p>默认签署的的证书只有 1 年有效期，如果想要调整证书有效期可以通过设置 kube-controller-manager 的 <code>--experimental-cluster-signing-duration</code> 参数实现，该参数默认值为 <code>8760h0m0s</code></p><p>对于证书自动续签，需要通过协调两个方面实现；第一，想要 kubelet 在证书到期后自动发起续期请求，则需要在 kubelet 启动时增加 <code>--feature-gates=RotateKubeletClientCertificate=true,RotateKubeletServerCertificate=true</code> 来实现；第二，想要让 controller manager 自动批准续签的 CSR 请求需要在 controller manager 启动时增加 <code>--feature-gates=RotateKubeletServerCertificate=true</code> 参数，并绑定对应的 RBAC 规则；<strong>同时需要注意的是 1.7 版本的 kubelet 自动续签后需要手动重启 kubelet 以使其重新加载新证书，而 1.8 后只需要在 kublet 启动时附带 <code>--rotate-certificates</code> 选项就会自动重新加载新证书</strong></p><h4 id="5-2、证书及配置文件作用"><a href="#5-2、证书及配置文件作用" class="headerlink" title="5.2、证书及配置文件作用"></a>5.2、证书及配置文件作用</h4><ul><li>token.csv</li></ul><p>该文件为一个用户的描述文件，基本格式为 <code>Token,用户名,UID,用户组</code>；这个文件在 apiserver 启动时被 apiserver 加载，然后就相当于在集群内创建了一个这个用户；接下来就可以用 RBAC 给他授权；持有这个用户 Token 的组件访问 apiserver 的时候，apiserver 根据 RBAC 定义的该用户应当具有的权限来处理相应请求</p><ul><li>bootstarp.kubeconfig</li></ul><p>该文件中内置了 token.csv 中用户的 Token，以及 apiserver CA 证书；kubelet 首次启动会加载此文件，使用 apiserver CA 证书建立与 apiserver 的 TLS 通讯，使用其中的用户 Token 作为身份标识像 apiserver 发起 CSR 请求</p><ul><li>kubelet-client.crt</li></ul><p>该文件在 kubelet 完成 TLS bootstrapping 后生成，此证书是由 controller manager 签署的，此后 kubelet 将会加载该证书，用于与 apiserver 建立 TLS 通讯，同时使用该证书的 CN 字段作为用户名，O 字段作为用户组向 apiserver 发起其他请求</p><ul><li>kubelet.crt</li></ul><p>该文件在 kubelet 完成 TLS bootstrapping 后并且<strong>没有配置 <code>--feature-gates=RotateKubeletServerCertificate=true</code> 时才会生成</strong>；这种情况下该文件为一个独立于 apiserver CA 的自签 CA 证书，有效期为 1 年；被用作 kubelet 10250 api 端口</p><ul><li>kubelet-server.crt</li></ul><p>该文件在 kubelet 完成 TLS bootstrapping 后并且<strong>配置了 <code>--feature-gates=RotateKubeletServerCertificate=true</code> 时才会生成</strong>；这种情况下该证书由 apiserver CA 签署，默认有效期同样是 1 年，被用作 kubelet 10250 api 端口鉴权</p><ul><li>kubelet-client-current.pem</li></ul><p>这是一个软连接文件，当 kubelet 配置了 <code>--feature-gates=RotateKubeletClientCertificate=true</code> 选项后，会在证书总有效期的 <code>70%~90%</code> 的时间内发起续期请求，请求被批准后会生成一个 <code>kubelet-client-时间戳.pem</code>；<code>kubelet-client-current.pem</code> 文件则始终软连接到最新的真实证书文件，除首次启动外，kubelet 一直会使用这个证书同  apiserver 通讯</p><ul><li>kubelet-server-current.pem</li></ul><p>同样是一个软连接文件，当 kubelet 配置了 <code>--feature-gates=RotateKubeletServerCertificate=true</code> 选项后，会在证书总有效期的 <code>70%~90%</code> 的时间内发起续期请求，请求被批准后会生成一个 <code>kubelet-server-时间戳.pem</code>；<code>kubelet-server-current.pem</code> 文件则始终软连接到最新的真实证书文件，该文件将会一直被用于 kubelet 10250 api 端口鉴权</p><h4 id="5-3、1-7-TLS-bootstrapping-配置"><a href="#5-3、1-7-TLS-bootstrapping-配置" class="headerlink" title="5.3、1.7 TLS bootstrapping 配置"></a>5.3、1.7 TLS bootstrapping 配置</h4><p>apiserver 预先放置 token.csv，内容样例如下</p><div class="hljs code-wrapper"><pre><code class="hljs sh">6df3c701f979cee17732c30958745947,kubelet-bootstrap,10001,<span class="hljs-string">&quot;system:bootstrappers&quot;</span></code></pre></div><p>允许 kubelet-bootstrap 用户创建首次启动的 CSR 请求</p><div class="hljs code-wrapper"><pre><code class="hljs sh">kubectl create clusterrolebinding kubelet-bootstrap \  --clusterrole=system:node-bootstrapper \  --user=kubelet-bootstrap</code></pre></div><p>配置 kubelet 自动续期，<strong>RotateKubeletClientCertificate 用于自动续期 kubelet 连接 apiserver 所用的证书(kubelet-client-xxxx.pem)，RotateKubeletServerCertificate 用于自动续期 kubelet 10250 api 端口所使用的证书(kubelet-server-xxxx.pem)</strong></p><div class="hljs code-wrapper"><pre><code class="hljs sh">KUBELET_ARGS=<span class="hljs-string">&quot;--cgroup-driver=cgroupfs \</span><span class="hljs-string">              --cluster-dns=10.254.0.2 \</span><span class="hljs-string">              --resolv-conf=/etc/resolv.conf \</span><span class="hljs-string">              --experimental-bootstrap-kubeconfig=/etc/kubernetes/bootstrap.kubeconfig \</span><span class="hljs-string">              --feature-gates=RotateKubeletClientCertificate=true,RotateKubeletServerCertificate=true \</span><span class="hljs-string">              --kubeconfig=/etc/kubernetes/kubelet.kubeconfig \</span><span class="hljs-string">              --fail-swap-on=false \</span><span class="hljs-string">              --cert-dir=/etc/kubernetes/ssl \</span><span class="hljs-string">              --cluster-domain=cluster.local. \</span><span class="hljs-string">              --hairpin-mode=promiscuous-bridge \</span><span class="hljs-string">              --serialize-image-pulls=false \</span><span class="hljs-string">              --pod-infra-container-image=gcr.io/google_containers/pause-amd64:3.0&quot;</span></code></pre></div><p>配置 controller manager 自动批准相关 CSR 请求，<strong>如果不配置 <code>--feature-gates=RotateKubeletServerCertificate=true</code> 参数，则即使配置了相关的 RBAC 规则，也只会自动批准 kubelet client 的 renew 请求</strong> </p><div class="hljs code-wrapper"><pre><code class="hljs sh">KUBE_CONTROLLER_MANAGER_ARGS=<span class="hljs-string">&quot;--address=0.0.0.0 \</span><span class="hljs-string">                              --service-cluster-ip-range=10.254.0.0/16 \</span><span class="hljs-string">                              --feature-gates=RotateKubeletServerCertificate=true \</span><span class="hljs-string">                              --cluster-name=kubernetes \</span><span class="hljs-string">                              --cluster-signing-cert-file=/etc/kubernetes/ssl/k8s-root-ca.pem \</span><span class="hljs-string">                              --cluster-signing-key-file=/etc/kubernetes/ssl/k8s-root-ca-key.pem \</span><span class="hljs-string">                              --service-account-private-key-file=/etc/kubernetes/ssl/k8s-root-ca-key.pem \</span><span class="hljs-string">                              --root-ca-file=/etc/kubernetes/ssl/k8s-root-ca.pem \</span><span class="hljs-string">                              --leader-elect=true \</span><span class="hljs-string">                              --node-monitor-grace-period=40s \</span><span class="hljs-string">                              --node-monitor-period=5s \</span><span class="hljs-string">                              --pod-eviction-timeout=5m0s&quot;</span></code></pre></div><p>创建自动批准相关 CSR 请求的 ClusterRole</p><div class="hljs code-wrapper"><pre><code class="hljs yaml"><span class="hljs-comment"># A ClusterRole which instructs the CSR approver to approve a user requesting</span><span class="hljs-comment"># node client credentials.</span><span class="hljs-attr">kind:</span> <span class="hljs-string">ClusterRole</span><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">rbac.authorization.k8s.io/v1</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">approve-node-client-csr</span><span class="hljs-attr">rules:</span><span class="hljs-bullet">-</span> <span class="hljs-attr">apiGroups:</span> [<span class="hljs-string">&quot;certificates.k8s.io&quot;</span>]  <span class="hljs-attr">resources:</span> [<span class="hljs-string">&quot;certificatesigningrequests/nodeclient&quot;</span>]  <span class="hljs-attr">verbs:</span> [<span class="hljs-string">&quot;create&quot;</span>]<span class="hljs-meta">---</span><span class="hljs-comment"># A ClusterRole which instructs the CSR approver to approve a node renewing its</span><span class="hljs-comment"># own client credentials.</span><span class="hljs-attr">kind:</span> <span class="hljs-string">ClusterRole</span><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">rbac.authorization.k8s.io/v1</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">approve-node-client-renewal-csr</span><span class="hljs-attr">rules:</span><span class="hljs-bullet">-</span> <span class="hljs-attr">apiGroups:</span> [<span class="hljs-string">&quot;certificates.k8s.io&quot;</span>]  <span class="hljs-attr">resources:</span> [<span class="hljs-string">&quot;certificatesigningrequests/selfnodeclient&quot;</span>]  <span class="hljs-attr">verbs:</span> [<span class="hljs-string">&quot;create&quot;</span>]<span class="hljs-meta">---</span><span class="hljs-comment"># A ClusterRole which instructs the CSR approver to approve a node requesting a</span><span class="hljs-comment"># serving cert matching its client cert.</span><span class="hljs-attr">kind:</span> <span class="hljs-string">ClusterRole</span><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">rbac.authorization.k8s.io/v1</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">approve-node-server-renewal-csr</span><span class="hljs-attr">rules:</span><span class="hljs-bullet">-</span> <span class="hljs-attr">apiGroups:</span> [<span class="hljs-string">&quot;certificates.k8s.io&quot;</span>]  <span class="hljs-attr">resources:</span> [<span class="hljs-string">&quot;certificatesigningrequests/selfnodeserver&quot;</span>]  <span class="hljs-attr">verbs:</span> [<span class="hljs-string">&quot;create&quot;</span>]</code></pre></div><p>将 ClusterRole 绑定到适当的用户组，以完成自动批准相关 CSR 请求</p><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment"># 自动批准 system:bootstrappers 组用户 TLS bootstrapping 首次申请证书的 CSR 请求</span>kubectl create clusterrolebinding node-client-auto-approve-csr --clusterrole=approve-node-client-csr --group=system:bootstrappers<span class="hljs-comment"># 自动批准 system:nodes 组用户更新 kubelet 自身与 apiserver 通讯证书的 CSR 请求</span>kubectl create clusterrolebinding node-client-auto-renew-crt --clusterrole=approve-node-client-renewal-csr --group=system:nodes<span class="hljs-comment"># 自动批准 system:nodes 组用户更新 kubelet 10250 api 端口证书的 CSR 请求</span>kubectl create clusterrolebinding node-server-auto-renew-crt --clusterrole=approve-node-server-renewal-csr --group=system:nodes</code></pre></div><p><strong>一切就绪后启动 kubelet 组件即可，不过需要注意的是 1.7 版本 kubelet 不会自动重载 renew 的证书，需要自己手动重启</strong></p><h4 id="5-4、1-8-TLS-bootstrapping-配置"><a href="#5-4、1-8-TLS-bootstrapping-配置" class="headerlink" title="5.4、1.8 TLS bootstrapping 配置"></a>5.4、1.8 TLS bootstrapping 配置</h4><p>apiserver 预先放置 token.csv，内容样例如下</p><div class="hljs code-wrapper"><pre><code class="hljs sh">6df3c701f979cee17732c30958745947,kubelet-bootstrap,10001,<span class="hljs-string">&quot;system:bootstrappers&quot;</span></code></pre></div><p>允许 kubelet-bootstrap 用户创建首次启动的 CSR 请求</p><div class="hljs code-wrapper"><pre><code class="hljs sh">kubectl create clusterrolebinding kubelet-bootstrap \  --clusterrole=system:node-bootstrapper \  --user=kubelet-bootstrap</code></pre></div><p>配置 kubelet 自动续期，<strong>RotateKubeletClientCertificate 用于自动续期 kubelet 连接 apiserver 所用的证书(kubelet-client-xxxx.pem)，RotateKubeletServerCertificate 用于自动续期 kubelet 10250 api 端口所使用的证书(kubelet-server-xxxx.pem)，<code>--rotate-certificates</code> 选项使得 kubelet 能够自动重载新证书</strong></p><div class="hljs code-wrapper"><pre><code class="hljs sh">KUBELET_ARGS=<span class="hljs-string">&quot;--cgroup-driver=cgroupfs \</span><span class="hljs-string">              --cluster-dns=10.254.0.2 \</span><span class="hljs-string">              --resolv-conf=/etc/resolv.conf \</span><span class="hljs-string">              --experimental-bootstrap-kubeconfig=/etc/kubernetes/bootstrap.kubeconfig \</span><span class="hljs-string">              --feature-gates=RotateKubeletClientCertificate=true,RotateKubeletServerCertificate=true \</span><span class="hljs-string">              --rotate-certificates \</span><span class="hljs-string">              --kubeconfig=/etc/kubernetes/kubelet.kubeconfig \</span><span class="hljs-string">              --fail-swap-on=false \</span><span class="hljs-string">              --cert-dir=/etc/kubernetes/ssl \</span><span class="hljs-string">              --cluster-domain=cluster.local. \</span><span class="hljs-string">              --hairpin-mode=promiscuous-bridge \</span><span class="hljs-string">              --serialize-image-pulls=false \</span><span class="hljs-string">              --pod-infra-container-image=gcr.io/google_containers/pause-amd64:3.0&quot;</span></code></pre></div><p>配置 controller manager 自动批准相关 CSR 请求，<strong>如果不配置 <code>--feature-gates=RotateKubeletServerCertificate=true</code> 参数，则即使配置了相关的 RBAC 规则，也只会自动批准 kubelet client 的 renew 请求</strong> </p><div class="hljs code-wrapper"><pre><code class="hljs sh">KUBE_CONTROLLER_MANAGER_ARGS=<span class="hljs-string">&quot;--address=0.0.0.0 \</span><span class="hljs-string">                              --service-cluster-ip-range=10.254.0.0/16 \</span><span class="hljs-string">                              --cluster-name=kubernetes \</span><span class="hljs-string">                              --cluster-signing-cert-file=/etc/kubernetes/ssl/k8s-root-ca.pem \</span><span class="hljs-string">                              --cluster-signing-key-file=/etc/kubernetes/ssl/k8s-root-ca-key.pem \</span><span class="hljs-string">                              --service-account-private-key-file=/etc/kubernetes/ssl/k8s-root-ca-key.pem \</span><span class="hljs-string">                              --feature-gates=RotateKubeletServerCertificate=true \</span><span class="hljs-string">                              --root-ca-file=/etc/kubernetes/ssl/k8s-root-ca.pem \</span><span class="hljs-string">                              --leader-elect=true \</span><span class="hljs-string">                              --experimental-cluster-signing-duration 10m0s \</span><span class="hljs-string">                              --node-monitor-grace-period=40s \</span><span class="hljs-string">                              --node-monitor-period=5s \</span><span class="hljs-string">                              --pod-eviction-timeout=5m0s&quot;</span></code></pre></div><p>创建自动批准相关 CSR 请求的 ClusterRole，相对于 1.7 版本，1.8 的 apiserver 自动创建了前两条 ClusterRole，所以只需要创建一条就行了</p><div class="hljs code-wrapper"><pre><code class="hljs yaml"><span class="hljs-comment"># A ClusterRole which instructs the CSR approver to approve a node requesting a</span><span class="hljs-comment"># serving cert matching its client cert.</span><span class="hljs-attr">kind:</span> <span class="hljs-string">ClusterRole</span><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">rbac.authorization.k8s.io/v1</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">system:certificates.k8s.io:certificatesigningrequests:selfnodeserver</span><span class="hljs-attr">rules:</span><span class="hljs-bullet">-</span> <span class="hljs-attr">apiGroups:</span> [<span class="hljs-string">&quot;certificates.k8s.io&quot;</span>]  <span class="hljs-attr">resources:</span> [<span class="hljs-string">&quot;certificatesigningrequests/selfnodeserver&quot;</span>]  <span class="hljs-attr">verbs:</span> [<span class="hljs-string">&quot;create&quot;</span>]</code></pre></div><p>将 ClusterRole 绑定到适当的用户组，以完成自动批准相关 CSR 请求</p><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment"># 自动批准 system:bootstrappers 组用户 TLS bootstrapping 首次申请证书的 CSR 请求</span>kubectl create clusterrolebinding node-client-auto-approve-csr --clusterrole=system:certificates.k8s.io:certificatesigningrequests:nodeclient --group=system:bootstrappers<span class="hljs-comment"># 自动批准 system:nodes 组用户更新 kubelet 自身与 apiserver 通讯证书的 CSR 请求</span>kubectl create clusterrolebinding node-client-auto-renew-crt --clusterrole=system:certificates.k8s.io:certificatesigningrequests:selfnodeclient --group=system:nodes<span class="hljs-comment"># 自动批准 system:nodes 组用户更新 kubelet 10250 api 端口证书的 CSR 请求</span>kubectl create clusterrolebinding node-server-auto-renew-crt --clusterrole=system:certificates.k8s.io:certificatesigningrequests:selfnodeserver --group=system:nodes</code></pre></div><p><strong>一切就绪后启动 kubelet 组件即可，1.8 版本 kubelet 会自动重载证书，以下为 1.8 版本在运行一段时间后的相关证书截图</strong></p><p><img src="https://cdn.oss.link/markdown/570wk.png" alt="tls_bootstrapping_crts"></p>]]></content>
    
    
    <summary type="html">前段时间撸了一会 Kubernetes 官方文档，在查看 TLS bootstrapping 这块是发现已经跟 1.4 的时候完全不一样了；目前所有搭建文档也都保留着 1.4 时代的配置，在看完文档后发现目前配置有很多问题，同时也埋下了 **隐藏炸弹**，这个问题可能会在一年后爆发.....后果就是集群 node 全部掉线；所以仔细的撸了一下这个文档，从元旦到写此文章的时间都在测试这个 TLS bootstrapping，以下记录一下这次的成果</summary>
    
    
    
    <category term="Kubernetes" scheme="https://mritd.com/categories/kubernetes/"/>
    
    
    <category term="Linux" scheme="https://mritd.com/tags/linux/"/>
    
    <category term="Docker" scheme="https://mritd.com/tags/docker/"/>
    
    <category term="Kubernetes" scheme="https://mritd.com/tags/kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>CI/CD 之 GitLab CI</title>
    <link href="https://mritd.com/2017/11/28/ci-cd-gitlab-ci/"/>
    <id>https://mritd.com/2017/11/28/ci-cd-gitlab-ci/</id>
    <published>2017-11-28T09:43:23.000Z</published>
    <updated>2017-11-28T09:43:23.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>接着上篇文章整理，这篇文章主要介绍一下 GitLab CI 相关功能，并通过 GitLab CI 实现自动化构建项目；项目中所用的示例项目已经上传到了 <a href="https://github.com/mritd/GitLabCI-TestProject">GitHub</a></p></blockquote><h3 id="一、环境准备"><a href="#一、环境准备" class="headerlink" title="一、环境准备"></a>一、环境准备</h3><p>首先需要有一台 GitLab 服务器，然后需要有个项目；这里示例项目以 Spring Boot 项目为例，然后最好有一台专门用来 Build 的机器，实际生产中如果 Build 任务不频繁可适当用一些业务机器进行 Build；本文示例所有组件将采用 Docker 启动， GitLab HA 等不在本文阐述范围内</p><ul><li>Docker Version : 1.13.1</li><li>GitLab Version : 10.1.4-ce.0</li><li>GitLab Runner Version : 10.1.0</li><li>GitLab IP : 172.16.0.37</li><li>GitLab Runner IP : 172.16.0.36</li></ul><h3 id="二、GitLab-CI-简介"><a href="#二、GitLab-CI-简介" class="headerlink" title="二、GitLab CI 简介"></a>二、GitLab CI 简介</h3><p>GitLab CI 是 GitLab 默认集成的 CI 功能，GitLab CI 通过在项目内 <code>.gitlab-ci.yaml</code> 配置文件读取 CI 任务并进行相应处理；GitLab CI 通过其称为 GitLab Runner 的 Agent 端进行 build 操作；Runner 本身可以使用多种方式安装，比如使用 Docker 镜像启动等；Runner 在进行 build 操作时也可以选择多种 build 环境提供者；比如直接在 Runner 所在宿主机 build、通过新创建虚拟机(vmware、virtualbox)进行 build等；同时 Runner 支持 Docker 作为 build 提供者，即每次 build 新启动容器进行 build；GitLab CI 其大致架构如下</p><p><img src="https://cdn.oss.link/markdown/wejnz.png" alt="GitLab"></p><h3 id="三、搭建-GitLab-服务器"><a href="#三、搭建-GitLab-服务器" class="headerlink" title="三、搭建 GitLab 服务器"></a>三、搭建 GitLab 服务器</h3><h4 id="3-1、GitLab-搭建"><a href="#3-1、GitLab-搭建" class="headerlink" title="3.1、GitLab 搭建"></a>3.1、GitLab 搭建</h4><p>GitLab 搭建这里直接使用 docker compose 启动，compose 配置如下</p><div class="hljs code-wrapper"><pre><code class="hljs sh">version: <span class="hljs-string">&#x27;2&#x27;</span>services:  gitlab:    image: <span class="hljs-string">&#x27;gitlab/gitlab-ce:10.1.4-ce.0&#x27;</span>    restart: always    container_name: gitlab    hostname: <span class="hljs-string">&#x27;git.mritd.me&#x27;</span>    environment:      GITLAB_OMNIBUS_CONFIG: |        external_url <span class="hljs-string">&#x27;http://git.mritd.me&#x27;</span>        <span class="hljs-comment"># Add any other gitlab.rb configuration here, each on its own line</span>    ports:      - <span class="hljs-string">&#x27;80:80&#x27;</span>      - <span class="hljs-string">&#x27;443:443&#x27;</span>      - <span class="hljs-string">&#x27;8022:22&#x27;</span>    volumes:      - <span class="hljs-string">&#x27;./data/gitlab/config:/etc/gitlab&#x27;</span>      - <span class="hljs-string">&#x27;./data/gitlab/logs:/var/log/gitlab&#x27;</span>      - <span class="hljs-string">&#x27;./data/gitlab/data:/var/opt/gitlab&#x27;</span></code></pre></div><p>直接启动后，首次登陆需要设置初始密码如下，默认用户为 <code>root</code></p><p><img src="https://cdn.oss.link/markdown/5go94.png" alt="gitkab init"></p><p>登陆成功后创建一个用户(该用户最好给予 Admin 权限，以后操作以该用户为例)，并且创建一个测试 Group 和 Project，如下所示</p><p><img src="https://cdn.oss.link/markdown/vtyhi.png" alt="Create User"></p><p><img src="https://cdn.oss.link/markdown/3b7gl.png" alt="Test Project"></p><h4 id="3-2、增加示例项目"><a href="#3-2、增加示例项目" class="headerlink" title="3.2、增加示例项目"></a>3.2、增加示例项目</h4><p>这里示例项目采用 Java 的 SpringBoot 项目，并采用 Gradle 构建，其他语言原理一样；<strong>如果不熟悉 Java 的没必要死磕此步配置，任意语言(最好 Java)整一个能用的 Web 项目就行，并不强求一定 Java 并且使用 Gradle 构建，以下只是一个样例项目</strong>；SpringBoot 可以采用 <a href="https://start.spring.io/">Spring Initializr</a> 直接生成(依赖要加入 WEB)，如下所示</p><p><img src="https://cdn.oss.link/markdown/0wx6d.png" alt="Spring Initializr"></p><p>将项目导入 IDEA，然后创建一个 index 示例页面，主要修改如下</p><ul><li>build.gradle</li></ul><div class="hljs code-wrapper"><pre><code class="hljs groovy">buildscript &#123;    ext &#123;        springBootVersion = <span class="hljs-string">&#x27;1.5.8.RELEASE&#x27;</span>    &#125;    repositories &#123;        mavenCentral()    &#125;    dependencies &#123;        classpath(<span class="hljs-string">&quot;org.springframework.boot:spring-boot-gradle-plugin:$&#123;springBootVersion&#125;&quot;</span>)    &#125;&#125;apply <span class="hljs-attr">plugin:</span> <span class="hljs-string">&#x27;java&#x27;</span>apply <span class="hljs-attr">plugin:</span> <span class="hljs-string">&#x27;eclipse&#x27;</span>apply <span class="hljs-attr">plugin:</span> <span class="hljs-string">&#x27;idea&#x27;</span>apply <span class="hljs-attr">plugin:</span> <span class="hljs-string">&#x27;org.springframework.boot&#x27;</span>group = <span class="hljs-string">&#x27;me.mritd&#x27;</span>version = <span class="hljs-string">&#x27;0.0.1-SNAPSHOT&#x27;</span>sourceCompatibility = <span class="hljs-number">1.8</span>repositories &#123;    mavenCentral()&#125;dependencies &#123;    compile(<span class="hljs-string">&#x27;org.springframework.boot:spring-boot-starter&#x27;</span>)    compile(<span class="hljs-string">&#x27;org.springframework.boot:spring-boot-starter-web&#x27;</span>)    compile(<span class="hljs-string">&#x27;org.springframework.boot:spring-boot-starter-thymeleaf&#x27;</span>)    testCompile(<span class="hljs-string">&#x27;org.springframework.boot:spring-boot-starter-test&#x27;</span>)&#125;</code></pre></div><ul><li>新建一个 HomeController</li></ul><div class="hljs code-wrapper"><pre><code class="hljs java"><span class="hljs-keyword">package</span> me.mritd.TestProject;<span class="hljs-keyword">import</span> org.springframework.stereotype.Controller;<span class="hljs-keyword">import</span> org.springframework.web.bind.annotation.RequestMapping;<span class="hljs-comment">/*******************************************************************************</span><span class="hljs-comment"> * Copyright (c) 2005-2017 Mritd, Inc.</span><span class="hljs-comment"> * TestProject</span><span class="hljs-comment"> * me.mritd.TestProject</span><span class="hljs-comment"> * Created by mritd on 2017/11/24 下午12:23.</span><span class="hljs-comment"> * Description: </span><span class="hljs-comment"> *******************************************************************************/</span><span class="hljs-meta">@Controller</span><span class="hljs-keyword">public</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">HomeController</span> </span>&#123;    <span class="hljs-meta">@RequestMapping(&quot;/&quot;)</span>    <span class="hljs-function"><span class="hljs-keyword">public</span> String <span class="hljs-title">home</span><span class="hljs-params">()</span></span>&#123;        <span class="hljs-keyword">return</span> <span class="hljs-string">&quot;index&quot;</span>;    &#125;&#125;</code></pre></div><ul><li>templates 下新建 index.html</li></ul><div class="hljs code-wrapper"><pre><code class="hljs html"><span class="hljs-meta">&lt;!DOCTYPE <span class="hljs-meta-keyword">html</span>&gt;</span><span class="hljs-tag">&lt;<span class="hljs-name">html</span> <span class="hljs-attr">lang</span>=<span class="hljs-string">&quot;en&quot;</span>&gt;</span><span class="hljs-tag">&lt;<span class="hljs-name">head</span>&gt;</span>    <span class="hljs-tag">&lt;<span class="hljs-name">meta</span> <span class="hljs-attr">charset</span>=<span class="hljs-string">&quot;UTF-8&quot;</span>/&gt;</span>    <span class="hljs-tag">&lt;<span class="hljs-name">title</span>&gt;</span>Title<span class="hljs-tag">&lt;/<span class="hljs-name">title</span>&gt;</span><span class="hljs-tag">&lt;/<span class="hljs-name">head</span>&gt;</span><span class="hljs-tag">&lt;<span class="hljs-name">body</span>&gt;</span><span class="hljs-tag">&lt;<span class="hljs-name">h1</span>&gt;</span>Test...<span class="hljs-tag">&lt;/<span class="hljs-name">h1</span>&gt;</span><span class="hljs-tag">&lt;/<span class="hljs-name">body</span>&gt;</span><span class="hljs-tag">&lt;/<span class="hljs-name">html</span>&gt;</span></code></pre></div><p>最后项目整体结构如下</p><p><img src="https://cdn.oss.link/markdown/5k12p.png" alt="TestProject"></p><p>执行 <code>assemble</code> Task 打包出可执行 jar 包，并运行 <code>java -jar TestProject-0.0.1-SNAPSHOT.jar</code> 测试下能启动访问页面即可</p><p><img src="https://cdn.oss.link/markdown/xoj3d.png" alt="TestProject assemble"></p><p>最后将项目提交到 GitLab 后如下</p><p><img src="https://cdn.oss.link/markdown/1fuex.png" alt="init Project"></p><h3 id="四、GitLab-CI-配置"><a href="#四、GitLab-CI-配置" class="headerlink" title="四、GitLab CI 配置"></a>四、GitLab CI 配置</h3><blockquote><p>针对这一章节创建基础镜像以及项目镜像，这里仅以 Java 项目为例；其他语言原理相通，按照其他语言对应的运行环境修改即可</p></blockquote><h4 id="4-1、增加-Runner"><a href="#4-1、增加-Runner" class="headerlink" title="4.1、增加 Runner"></a>4.1、增加 Runner</h4><p>GitLab CI 在进行构建时会将任务下发给 Runner，让 Runner 去执行；所以先要添加一个 Runner，Runner 这里采用 Docker Compose 启动，build 方式也使用 Docker 方式 Build；compose 文件如下</p><div class="hljs code-wrapper"><pre><code class="hljs yaml"><span class="hljs-attr">version:</span> <span class="hljs-string">&#x27;2&#x27;</span><span class="hljs-attr">services:</span>  <span class="hljs-attr">gitlab-runner:</span>    <span class="hljs-attr">container_name:</span> <span class="hljs-string">gitlab-runner</span>    <span class="hljs-attr">image:</span> <span class="hljs-string">gitlab/gitlab-runner:alpine-v10.1.0</span>    <span class="hljs-attr">restart:</span> <span class="hljs-string">always</span>    <span class="hljs-attr">network_mode:</span> <span class="hljs-string">&quot;host&quot;</span>    <span class="hljs-attr">volumes:</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">/var/run/docker.sock:/var/run/docker.sock</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">./config.toml:/etc/gitlab-runner/config.toml</span>    <span class="hljs-attr">extra_hosts:</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">&quot;git.mritd.me:172.16.0.37&quot;</span></code></pre></div><p><strong>在启动前，我们需要先 touch 一下这个 config.toml 配置文件</strong>；该文件是 Runner 的运行配置，此后 Runner 所有配置都会写入这个文件(不 touch 出来 docker-compose 发现不存在会挂载一个目录进去，导致 Runner 启动失败)；启动 docker-compose 后，<strong>需要进入容器执行注册，让 Runner 主动去连接 GitLab 服务器</strong></p><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment"># 生成 Runner 配置文件</span>touch config.toml<span class="hljs-comment"># 启动 Runner</span>docker-compose up -d<span class="hljs-comment"># 激活 Runner</span>docker <span class="hljs-built_in">exec</span> -it gitlab-runner gitlab-runner register</code></pre></div><p>在执行上一条激活命令后，会按照提示让你输入一些信息；<strong>首先输入 GitLab 地址，然后是 Runner Token，Runner Token 可以从 GitLab 设置中查看</strong>，如下所示</p><p><img src="https://cdn.oss.link/markdown/mfqg7.png" alt="Runner Token"></p><p>整体注册流程如下</p><p><img src="https://cdn.oss.link/markdown/r7xay.png" alt="Runner registry"></p><p>注册完成后，在 GitLab Runner 设置中就可以看到刚刚注册的 Runner，如下所示</p><p><img src="https://cdn.oss.link/markdown/xv03e.png" alt="Runner List"></p><p><strong>Runner 注册成功后会将配置写入到 config.toml 配置文件；由于两个测试宿主机都没有配置内网 DNS，所以为了保证 runner 在使用 docker build 时能正确的找到 GitLab 仓库地址，还需要增加一个 docker 的 host 映射( <code>extra_hosts</code> )；同时为了能调用 宿主机 Docker 和持久化 build 的一些缓存还挂载了一些文件和目录；完整的 配置如下(配置文件可以做一些更高级的配置，具体参考 <a href="https://docs.gitlab.com/runner/configuration/advanced-configuration.html">官方文档</a> )</strong></p><ul><li>config.toml</li></ul><div class="hljs code-wrapper"><pre><code class="hljs toml"><span class="hljs-attr">concurrent</span> = <span class="hljs-number">1</span><span class="hljs-attr">check_interval</span> = <span class="hljs-number">0</span><span class="hljs-section">[[runners]]</span>  <span class="hljs-attr">name</span> = <span class="hljs-string">&quot;Test Runner&quot;</span>  <span class="hljs-attr">url</span> = <span class="hljs-string">&quot;http://git.mritd.me&quot;</span>  <span class="hljs-attr">token</span> = <span class="hljs-string">&quot;c279ec1ac08aec98c7141c7cf2d474&quot;</span>  <span class="hljs-attr">executor</span> = <span class="hljs-string">&quot;docker&quot;</span>  <span class="hljs-attr">builds_dir</span> = <span class="hljs-string">&quot;/gitlab/runner-builds&quot;</span>  <span class="hljs-attr">cache_dir</span> = <span class="hljs-string">&quot;/gitlab/runner-cache&quot;</span>  <span class="hljs-section">[runners.docker]</span>    <span class="hljs-attr">tls_verify</span> = <span class="hljs-literal">false</span>    <span class="hljs-attr">image</span> = <span class="hljs-string">&quot;debian&quot;</span>    <span class="hljs-attr">privileged</span> = <span class="hljs-literal">false</span>    <span class="hljs-attr">disable_cache</span> = <span class="hljs-literal">false</span>    <span class="hljs-attr">shm_size</span> = <span class="hljs-number">0</span>    <span class="hljs-attr">volumes</span> = [<span class="hljs-string">&quot;/data/gitlab-runner:/gitlab&quot;</span>,<span class="hljs-string">&quot;/var/run/docker.sock:/var/run/docker.sock&quot;</span>,<span class="hljs-string">&quot;/data/maven_repo:/data/repo&quot;</span>,<span class="hljs-string">&quot;/data/maven_repo:/data/maven&quot;</span>,<span class="hljs-string">&quot;/data/gradle:/data/gradle&quot;</span>,<span class="hljs-string">&quot;/data/sonar_cache:/root/.sonar&quot;</span>,<span class="hljs-string">&quot;/data/androidsdk:/usr/local/android&quot;</span>,<span class="hljs-string">&quot;/data/node_modules:/data/node_modules&quot;</span>]    <span class="hljs-attr">extra_hosts</span> = [<span class="hljs-string">&quot;git.mritd.me:172.16.0.37&quot;</span>]  <span class="hljs-section">[runners.cache]</span></code></pre></div><p><strong>注意，这里声明的 Volumes 会在每个运行的容器中都生效；也就是说 build 时新开启的每个容器都会被挂载这些目录</strong>；修改完成后重启 runner 容器即可，由于 runner 中没啥可保存的东西，所以可以直接 <code>docker-compose down &amp;&amp; docker-compose up -d</code> 重启</p><h4 id="4-2、创建基础镜像"><a href="#4-2、创建基础镜像" class="headerlink" title="4.2、创建基础镜像"></a>4.2、创建基础镜像</h4><p>由于示例项目是一个 Java 项目，而且是采用 Spring Boot 的，所以该项目想要运行起来只需要一个 java 环境即可，中间件已经被打包到了 jar 包中；以下是一个作为基础运行环境的 openjdk 镜像的 Dockerfile</p><div class="hljs code-wrapper"><pre><code class="hljs sh">FROM alpine:edge LABEL maintainer=<span class="hljs-string">&quot;mritd &lt;mritd1234@gmail.com&gt;&quot;</span>ENV JAVA_HOME /usr/lib/jvm/java-1.8-openjdkENV PATH <span class="hljs-variable">$PATH</span>:/usr/lib/jvm/java-1.8-openjdk/jre/bin:/usr/lib/jvm/java-1.8-openjdk/binRUN apk add --update bash curl tar wget ca-certificates unzip \        openjdk8 font-adobe-100dpi ttf-dejavu fontconfig \    &amp;&amp; rm -rf /var/cache/apk/* \CMD [<span class="hljs-string">&quot;bash&quot;</span>]</code></pre></div><p>**这个 openjdk Dockerfile 升级到了 8.151 版本，并且集成了一些字体相关的软件，以解决在 Java 中某些验证码库无法运行问题，详见 <a href="https://mritd.me/2017/09/27/alpine-3.6-openjdk-8-bug/">Alpine 3.6 OpenJDK 8 Bug</a>**；使用这个 Dockerfile，在当前目录执行 <code>docker build -t mritd/openjdk:8 .</code> build 一个 openjdk8 的基础镜像，然后将其推送到私服，或者 Docker Hub 即可</p><h4 id="4-3、创建项目镜像"><a href="#4-3、创建项目镜像" class="headerlink" title="4.3、创建项目镜像"></a>4.3、创建项目镜像</h4><p>有了基本的 openjdk 的 docker 镜像后，针对于项目每次 build 都应该生成一个包含发布物的 docker 镜像，所以对于项目来说还需要一个项目本身的 Dockerfile；<strong>项目的 Dockerfile 有两种使用方式；一种是动态生成 Dockerfile，然后每次使用新生成的 Dockerfile 去 build；还有一种是写一个通用的 Dockerfile，build 时利用 ARG 参数传入变量</strong>；这里采用第二种方式，以下为一个可以反复使用的 Dockerfile</p><div class="hljs code-wrapper"><pre><code class="hljs sh">FROM mritd/openjdk:8-144-01MAINTAINER mritd &lt;mritd1234@gmail.com&gt;ARG PROJECT_BUILD_FINALNAMEENV TZ <span class="hljs-string">&#x27;Asia/Shanghai&#x27;</span>ENV PROJECT_BUILD_FINALNAME <span class="hljs-variable">$&#123;PROJECT_BUILD_FINALNAME&#125;</span>COPY build/libs/<span class="hljs-variable">$&#123;PROJECT_BUILD_FINALNAME&#125;</span>.jar /<span class="hljs-variable">$&#123;PROJECT_BUILD_FINALNAME&#125;</span>.jarCMD [<span class="hljs-string">&quot;bash&quot;</span>,<span class="hljs-string">&quot;-c&quot;</span>,<span class="hljs-string">&quot;java -jar /<span class="hljs-variable">$&#123;PROJECT_BUILD_FINALNAME&#125;</span>.jar&quot;</span>]</code></pre></div><p><strong>该 Dockerfile 通过声明一个 <code>PROJECT_BUILD_FINALNAME</code> 变量来表示项目的发布物名称；然后将其复制到根目录下，最终利用 java 执行这个 jar 包；所以每次 build 之前只要能拿到项目发布物的名称即可</strong></p><h4 id="4-4、Gradle-修改"><a href="#4-4、Gradle-修改" class="headerlink" title="4.4、Gradle 修改"></a>4.4、Gradle 修改</h4><p>上面已经创建了一个标准的通用型 Dockerfile，每次 build 镜像只要传入 <code>PROJECT_BUILD_FINALNAME</code> 这个最终发布物名称即可；对于发布物名称来说，最好不要固定死；当然不论是 Java 还是其他语言的项目我们都能将最终发布物变成一个固定名字，最不济可以写脚本重命名一下；但是不建议那么干，最好保留版本号信息，以便于异常情况下进入容器能够分辨；对于当前 Java 项目来说，想要拿到 <code>PROJECT_BUILD_FINALNAME</code> 很简单，我们只需要略微修改一下 Gradle 的 build 脚本，让其每次打包 jar 包时将项目的名称及版本号导出到文件中即可；同时这里也加入了镜像版本号的处理，Gradle 脚本修改如下</p><ul><li>build.gradle 最后面增加如下</li></ul><div class="hljs code-wrapper"><pre><code class="hljs groovy">bootRepackage &#123;    mainClass = <span class="hljs-string">&#x27;me.mritd.TestProject.TestProjectApplication&#x27;</span>    executable = <span class="hljs-literal">true</span>    doLast &#123;        File envFile = <span class="hljs-keyword">new</span> File(<span class="hljs-string">&quot;build/tmp/PROJECT_ENV&quot;</span>)        println(<span class="hljs-string">&quot;Create $&#123;archivesBaseName&#125; ENV File ===&gt; &quot;</span> + envFile.createNewFile())        println(<span class="hljs-string">&quot;Export $&#123;archivesBaseName&#125; Build Version ===&gt; $&#123;version&#125;&quot;</span>)        envFile.write(<span class="hljs-string">&quot;export PROJECT_BUILD_FINALNAME=$&#123;archivesBaseName&#125;-$&#123;version&#125;\n&quot;</span>)        println(<span class="hljs-string">&quot;Generate Docker image tag...&quot;</span>)        envFile.append(<span class="hljs-string">&quot;export BUILD_DATE=`date +%Y%m%d%H%M%S`\n&quot;</span>)        envFile.append(<span class="hljs-string">&quot;export IMAGE_NAME=mritd/test:`echo \$&#123;CI_BUILD_REF_NAME&#125; | tr &#x27;/&#x27; &#x27;-&#x27;`-`echo \$&#123;CI_COMMIT_SHA&#125; | cut -c1-8`-\$&#123;BUILD_DATE&#125;\n&quot;</span>)        envFile.append(<span class="hljs-string">&quot;export LATEST_IMAGE_NAME=mritd/test:latest\n&quot;</span>)    &#125;&#125;</code></pre></div><p><strong>这一步操作实际上是修改了 <code>bootRepackage</code> 这个 Task(不了解 Gradle 或者不是 Java 项目的请忽略)，在其结束后创建了一个叫 <code>PROJECT_ENV</code> 的文件，里面实际上就是写入了一些 bash 环境变量声明，以方便后面 source 一下这个文件拿到一些变量，然后用户 build 镜像使用</strong>，<code>PROJECT_ENV</code> 最终生成如下</p><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-built_in">export</span> PROJECT_BUILD_FINALNAME=TestProject-0.0.1-SNAPSHOT<span class="hljs-built_in">export</span> BUILD_DATE=`date +%Y%m%d%H%M%S`<span class="hljs-built_in">export</span> IMAGE_NAME=mritd/<span class="hljs-built_in">test</span>:`<span class="hljs-built_in">echo</span> <span class="hljs-variable">$&#123;CI_BUILD_REF_NAME&#125;</span> | tr <span class="hljs-string">&#x27;/&#x27;</span> <span class="hljs-string">&#x27;-&#x27;</span>`-`<span class="hljs-built_in">echo</span> <span class="hljs-variable">$&#123;CI_COMMIT_SHA&#125;</span> | cut -c1-8`-<span class="hljs-variable">$&#123;BUILD_DATE&#125;</span><span class="hljs-built_in">export</span> LATEST_IMAGE_NAME=mritd/<span class="hljs-built_in">test</span>:latest</code></pre></div><p><img src="https://cdn.oss.link/markdown/gr6kc.png" alt="PROJECT_ENV"></p><h4 id="4-5、创建-CI-配置文件"><a href="#4-5、创建-CI-配置文件" class="headerlink" title="4.5、创建 CI 配置文件"></a>4.5、创建 CI 配置文件</h4><p>一切准备就绪以后，就可以编写 CI 脚本了；GitLab 依靠读取项目根目录下的 <code>.gitlab-ci.yml</code> 文件来执行相应的 CI 操作；以下为测试项目的 <code>.gitlab-ci.yml</code> 配置</p><div class="hljs code-wrapper"><pre><code class="hljs yaml"><span class="hljs-comment"># 调试开启</span><span class="hljs-comment">#before_script:</span><span class="hljs-comment">#  - pwd</span><span class="hljs-comment">#  - env</span><span class="hljs-attr">cache:</span>  <span class="hljs-attr">key:</span> <span class="hljs-string">$CI_PROJECT_NAME/$CI_COMMIT_REF_NAME-$CI_COMMIT_SHA</span>  <span class="hljs-attr">paths:</span>    <span class="hljs-bullet">-</span> <span class="hljs-string">build</span><span class="hljs-attr">stages:</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">build</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">deploy</span><span class="hljs-attr">auto-build:</span>  <span class="hljs-attr">image:</span> <span class="hljs-string">mritd/build:2.1.1</span>  <span class="hljs-attr">stage:</span> <span class="hljs-string">build</span>  <span class="hljs-attr">script:</span>    <span class="hljs-bullet">-</span> <span class="hljs-string">gradle</span> <span class="hljs-string">--no-daemon</span> <span class="hljs-string">clean</span> <span class="hljs-string">assemble</span>  <span class="hljs-attr">tags:</span>    <span class="hljs-bullet">-</span> <span class="hljs-string">test</span><span class="hljs-attr">deploy:</span>  <span class="hljs-attr">image:</span> <span class="hljs-string">mritd/docker-kubectl:v1.7.4</span>  <span class="hljs-attr">stage:</span> <span class="hljs-string">deploy</span>  <span class="hljs-attr">script:</span>    <span class="hljs-bullet">-</span> <span class="hljs-string">source</span> <span class="hljs-string">build/tmp/PROJECT_ENV</span>    <span class="hljs-bullet">-</span> <span class="hljs-string">echo</span> <span class="hljs-string">&quot;Build Docker Image ==&gt; $&#123;IMAGE_NAME&#125;&quot;</span>    <span class="hljs-bullet">-</span> <span class="hljs-string">docker</span> <span class="hljs-string">build</span> <span class="hljs-string">-t</span> <span class="hljs-string">$&#123;IMAGE_NAME&#125;</span> <span class="hljs-string">--build-arg</span> <span class="hljs-string">PROJECT_BUILD_FINALNAME=$&#123;PROJECT_BUILD_FINALNAME&#125;</span> <span class="hljs-string">.</span><span class="hljs-comment">#    - docker push $&#123;IMAGE_NAME&#125;</span>    <span class="hljs-bullet">-</span> <span class="hljs-string">docker</span> <span class="hljs-string">tag</span> <span class="hljs-string">$&#123;IMAGE_NAME&#125;</span> <span class="hljs-string">$&#123;LATEST_IMAGE_NAME&#125;</span><span class="hljs-comment">#    - docker push $&#123;LATEST_IMAGE_NAME&#125;</span><span class="hljs-comment">#    - docker rmi $&#123;IMAGE_NAME&#125; $&#123;LATEST_IMAGE_NAME&#125;</span><span class="hljs-comment">#    - kubectl --kubeconfig $&#123;KUBE_CONFIG&#125; set image deployment/test test=$IMAGE_NAME</span>  <span class="hljs-attr">tags:</span>    <span class="hljs-bullet">-</span> <span class="hljs-string">test</span>  <span class="hljs-attr">only:</span>    <span class="hljs-bullet">-</span> <span class="hljs-string">master</span>    <span class="hljs-bullet">-</span> <span class="hljs-string">develop</span>    <span class="hljs-bullet">-</span> <span class="hljs-string">/^chore.*$/</span></code></pre></div><p><strong>关于 CI 配置的一些简要说明如下</strong></p><h5 id="stages"><a href="#stages" class="headerlink" title="stages"></a>stages</h5><p>stages 字段定义了整个 CI 一共有哪些阶段流程，以上的 CI 配置中，定义了该项目的 CI 总共分为 <code>build</code>、<code>deploy</code> 两个阶段；GitLab CI 会根据其顺序执行对应阶段下的所有任务；<strong>在正常生产环境流程可以定义很多个，比如可以有 <code>test</code>、<code>publish</code>，甚至可能有代码扫描的 <code>sonar</code> 阶段等；这些阶段没有任何限制，完全是自定义的</strong>，上面的阶段定义好后在 CI 中表现如下图</p><p><img src="https://cdn.oss.link/markdown/8c7gs.png" alt="stages"></p><h5 id="task"><a href="#task" class="headerlink" title="task"></a>task</h5><p>task 隶属于 stages 之下；也就是说一个阶段可以有多个任务，任务执行顺序默认不指定会并发执行；对于上面的 CI 配置来说 <code>auto-build</code> 和 <code>deploy</code> 都是 task，他们通过 <code>stage: xxxx</code> 这个标签来指定他们隶属于哪个 stage；当 Runner 使用 Docker 作为 build 提供者时，我们可以在 task 的 <code>image</code> 标签下声明该 task 要使用哪个镜像运行，不指定则默认为 Runner 注册时的镜像(这里是 debian)；<strong>同时 task 还有一个 <code>tags</code> 的标签，该标签指明了这个任务将可以在哪些 Runner 上运行；这个标签可以从 Runner 页面看到，实际上就是 Runner 注册时输入的哪个 tag；对于某些特殊的项目，比如 IOS 项目，则必须在特定机器上执行，所以此时指定 tags 标签很有用</strong>，当 task 运行后如下图所示</p><p><img src="https://cdn.oss.link/markdown/qzvlh.png" alt="Task"></p><p>除此之外 task 还能指定 <code>only</code> 标签用于限定那些分支才能触发这个 task，如果分支名字不满足则不会触发；<strong>默认情况下，这些 task 都是自动执行的，如果感觉某些任务太过危险，则可以通过增加 <code>when: manual</code> 改为手动执行；注意: 手动执行被 GitLab 认为是高权限的写操作，所以只有项目管理员才能手动运行一个 task，直白的说就是管理员才能点击</strong>；手动执行如下图所示</p><p><img src="https://cdn.oss.link/markdown/vcjci.png" alt="manual task"></p><h5 id="cache"><a href="#cache" class="headerlink" title="cache"></a>cache</h5><p>cache 这个参数用于定义全局那些文件将被 cache；<strong>在 GitLab CI 中，跨 stage 是不能保存东西的；也就是说在第一步 build 的操作生成的 jar 包，到第二部打包 docker image 时就会被删除；GitLab 会保证每个 stage 中任务在执行时都将工作目录(Docker 容器 中)还原到跟 GitLab 代码仓库中一模一样，多余文件及变更都会被删除</strong>；正常情况下，第一步 build 生成 jar 包应当立即推送到 nexus 私服；但是这里测试没有搭建，所以只能放到本地；但是放到本地下一个 task 就会删除它，所以利用 <code>cache</code> 这个参数将 <code>build</code> 目录 cache 住，保证其跨 stage 也能存在</p><p><strong>关于 <code>.gitlab-ci.yml</code> 具体配置更完整的请参考 <a href="https://docs.gitlab.com/ee/ci/yaml/">官方文档</a></strong></p><h3 id="五、其他相关"><a href="#五、其他相关" class="headerlink" title="五、其他相关"></a>五、其他相关</h3><h4 id="5-1、GitLab-内置环境变量"><a href="#5-1、GitLab-内置环境变量" class="headerlink" title="5.1、GitLab 内置环境变量"></a>5.1、GitLab 内置环境变量</h4><p>上面已经基本搞定了一个项目的 CI，但是有些变量可能并未说清楚；比如在创建的 <code>PROJECT_ENV</code> 文件中引用了 <code>$&#123;CI_COMMIT_SHA&#125;</code> 变量；这种变量其实是 GitLab CI 的内置隐藏变量，这些变量在每次 CI 调用 Runner 运行某个任务时都会传递到对应的 Runner 的执行环境中；<strong>也就是说这些变量在每次的任务容器 SHELL 环境中都会存在，可以直接引用</strong>，具体的完整环境变量列表可以从 <a href="https://docs.gitlab.com/ee/ci/variables/">官方文档</a> 中获取；如果想知道环境变量具体的值，实际上可以通过在任务执行前用 <code>env</code> 指令打印出来，如下所示</p><p><img src="https://cdn.oss.link/markdown/la9kn.png" alt="env"></p><p><img src="https://cdn.oss.link/markdown/0175j.png" alt="env task"></p><h4 id="5-2、GitLab-自定义环境变量"><a href="#5-2、GitLab-自定义环境变量" class="headerlink" title="5.2、GitLab 自定义环境变量"></a>5.2、GitLab 自定义环境变量</h4><p>在某些情况下，我们希望 CI 能自动的发布或者修改一些东西；比如将 jar 包上传到 nexus、将 docker 镜像 push 到私服；这些动作往往需要一个高权限或者说有可写入对应仓库权限的账户来支持，但是这些账户又不想写到项目的 CI 配置里；因为这样很不安全，谁都能看到；此时我们可以将这些敏感变量写入到 GitLab 自定义环境变量中，GitLab 会像对待内置变量一样将其传送到 Runner 端，以供我们使用；GitLab 中自定义的环境变量可以有两种，一种是项目级别的，只能够在当前项目使用，如下</p><p><img src="https://cdn.oss.link/markdown/ennug.png" alt="project env"></p><p>另一种是组级别的，可以在整个组内的所有项目中使用，如下</p><p><img src="https://cdn.oss.link/markdown/si8ig.png" alt="group env"></p><p>这两种变量添加后都可以在 CI 的脚本中直接引用</p><h4 id="5-3、Kubernetes-集成"><a href="#5-3、Kubernetes-集成" class="headerlink" title="5.3、Kubernetes 集成"></a>5.3、Kubernetes 集成</h4><p>对于 Kubernetes 集成实际上有两种方案，一种是对接 Kubernetes 的 api，纯代码实现；另一种取巧的方案是调用 kubectl 工具，用 kubectl 工具来实现滚动升级；这里采用后一种取巧的方式，将 kubectl 二进制文件封装到镜像中，然后在 deploy 阶段使用这个镜像直接部署就可以</p><p><img src="https://cdn.oss.link/markdown/bu17r.png" alt="kubectl"></p><p>其中 <code>mritd/docker-kubectl:v1.7.4</code> 这个镜像的 Dockerfile 如下</p><div class="hljs code-wrapper"><pre><code class="hljs sh">FROM docker:dind LABEL maintainer=<span class="hljs-string">&quot;mritd &lt;mritd1234@gmail.com&gt;&quot;</span>ARG TZ=<span class="hljs-string">&quot;Asia/Shanghai&quot;</span>ENV TZ <span class="hljs-variable">$&#123;TZ&#125;</span>ENV KUBE_VERSION v1.8.0RUN apk upgrade --update \    &amp;&amp; apk add bash tzdata wget ca-certificates \    &amp;&amp; wget https://storage.googleapis.com/kubernetes-release/release/<span class="hljs-variable">$&#123;KUBE_VERSION&#125;</span>/bin/linux/amd64/kubectl -O /usr/<span class="hljs-built_in">local</span>/bin/kubectl \    &amp;&amp; chmod +x /usr/<span class="hljs-built_in">local</span>/bin/kubectl \    &amp;&amp; ln -sf /usr/share/zoneinfo/<span class="hljs-variable">$&#123;TZ&#125;</span> /etc/localtime \    &amp;&amp; <span class="hljs-built_in">echo</span> <span class="hljs-variable">$&#123;TZ&#125;</span> &gt; /etc/timezone \    &amp;&amp; rm -rf /var/cache/apk/*CMD [<span class="hljs-string">&quot;/bin/bash&quot;</span>]</code></pre></div><p>这里面的 <code>$&#123;KUBE_CONFIG&#125;</code> 是一个自定义的环境变量，对于测试环境我将配置文件直接挂载入了容器中，然后 <code>$&#123;KUBE_CONFIG&#125;</code> 只是指定了一个配置文件位置，实际生产环境中可以选择将配置文件变成自定义环境变量使用</p><h4 id="5-4、GitLab-CI-总结"><a href="#5-4、GitLab-CI-总结" class="headerlink" title="5.4、GitLab CI 总结"></a>5.4、GitLab CI 总结</h4><p>关于 GitLab CI 上面已经讲了很多，但是并不全面，也不算太细致；因为这东西说起来实际太多了，现在目测已经 1W 多字了；以下总结一下 GitLab CI 的总体思想，当思路清晰了以后，我想后面的只是查查文档自己试一试就行了</p><p><strong>CS 架构</strong></p><p>GitLab 作为 Server 端，控制 Runner 端执行一系列的 CI 任务；代码 clone 等无需关心，GitLab 会自动处理好一切；Runner 每次都会启动新的容器执行 CI 任务</p><p><strong>容器即环境</strong></p><p>在 Runner 使用 Docker build 的前提下；<strong>所有依赖切换、环境切换应当由切换不同镜像实现，即 build 那就使用 build 的镜像，deploy 就用带有 deploy 功能的镜像；通过不同镜像容器实现完整的环境隔离</strong></p><p><strong>CI即脚本</strong></p><p>不同的 CI 任务实际上就是在使用不同镜像的容器中执行 SHELL 命令，自动化 CI 就是执行预先写好的一些小脚本</p><p><strong>敏感信息走环境变量</strong></p><p>一切重要的敏感信息，如账户密码等，不要写到 CI 配置中，直接放到 GitLab 的环境变量中；GitLab 会保证将其推送到远端 Runner 的 SHELL 变量中</p>]]></content>
    
    
    <summary type="html">接着上篇文章整理，这篇文章主要介绍一下 GitLab CI 相关功能，并通过 GitLab CI 实现自动化构建项目；项目中所用的示例项目已经上传到了 [GitHub](https://github.com/mritd/GitLabCI-TestProject)</summary>
    
    
    
    <category term="CI/CD" scheme="https://mritd.com/categories/ci-cd/"/>
    
    
    <category term="CI/CD" scheme="https://mritd.com/tags/ci-cd/"/>
    
    <category term="Linux" scheme="https://mritd.com/tags/linux/"/>
    
    <category term="Docker" scheme="https://mritd.com/tags/docker/"/>
    
  </entry>
  
  <entry>
    <title>CI/CD 之 Dockerfile</title>
    <link href="https://mritd.com/2017/11/12/ci-cd-dockerfile/"/>
    <id>https://mritd.com/2017/11/12/ci-cd-dockerfile/</id>
    <published>2017-11-12T14:46:53.000Z</published>
    <updated>2017-11-12T14:46:53.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>最近准备整理一下关于 CI/CD 的相关文档，写一个关于 CI/CD 的系列文章，这篇先从最基本的 Dockerfile 书写开始，本系列文章默认读者已经熟悉 Docker、Kubernetes 相关工具</p></blockquote><h3 id="一、基础镜像选择"><a href="#一、基础镜像选择" class="headerlink" title="一、基础镜像选择"></a>一、基础镜像选择</h3><p>这里的基础镜像指的是实际项目运行时的基础环境镜像，比如 Java 的 JDK 基础镜像、Nodejs 的基础镜像等；在制作项目的基础镜像时，我个人认为应当考虑一下几点因素:</p><h4 id="1-1、可维护性"><a href="#1-1、可维护性" class="headerlink" title="1.1、可维护性"></a>1.1、可维护性</h4><p>可维护性应当放在首要位置，如果在制作基础镜像时，选择了一个你根本不熟悉的基础镜像，或者说你完全不知道这个基础镜像里有哪些环境变量、Entrypoint 脚本做了什么时，请果断放弃这个基础镜像，选择一个你自己更加熟悉的基础镜像，不要为以后挖坑；还有就是如果对应的应用已经有官方镜像，那么尽量采用官方的，因为你可以省去维护 <strong>自己造的轮子</strong> 的精力，<strong>除非你对基础镜像制作已经得心应手，否则请不要造轮子</strong></p><h4 id="1-2、稳定性"><a href="#1-2、稳定性" class="headerlink" title="1.2、稳定性"></a>1.2、稳定性</h4><p>基础镜像稳定性实际上是个很微妙的话题，因为普遍来说成熟的 Linux 发行版都很稳定；但是对于不同发行版镜像之间还是存在差异的，比如 alpine 的镜像用的是 musl libc，而 debian 用的是 glibc，某些依赖 glibc 的程序可能无法在 alpine 上工作；alpine 版本的 nginx 能使用 http2，debian 版本 nginx 则不行，因为 openssl 版本不同；甚至在相同发行版不同版本之间也会有差异，譬如 openjdk alpine 3.6 版本 java 某些图形库无法工作，在 alpine edge 上安装最新的 openjdk 却没问题等；所以稳定性这个话题对于基础镜像自己来说，他永远稳定，但是对于你的应用来说，则不同基础镜像会产生不同的稳定性；<strong>最后，如果你完全熟悉你的应用，甚至应用层代码也是你写的，那么你可以根据你的习惯和喜好去选择基础镜像，因为你能把控应用运行时依赖；否则的话，请尽量选择 debian 这种比较成熟的发行版作为基础镜像，因为它在普遍上兼容性更好一点；还有尽量不要使用 CentOS 作为基础镜像，因为他的体积将会成为大规模网络分发瓶颈</strong></p><h4 id="1-3、易用性"><a href="#1-3、易用性" class="headerlink" title="1.3、易用性"></a>1.3、易用性</h4><p>易用性简单地说就是是否可调试，因为有些极端情况下，并不是应用只要运行起来就没事了；可能出现一些很棘手的问题需要你进入容器进行调试，此时你的镜像易用性就会体现出来；譬如一个 Java 项目你的基础镜像是 JRE，那么 JDK 的调试工具将完全不可用，还有就是如果你的基础镜像选择了 alpine，那么它默认没有 bash，可能你的脚本无法在里面工作；<strong>所有在选择基础镜像的时候最好也考虑一下未来极端情况的可调试性</strong></p><h3 id="二、格式化及注意事项"><a href="#二、格式化及注意事项" class="headerlink" title="二、格式化及注意事项"></a>二、格式化及注意事项</h3><h4 id="2-1、书写格式"><a href="#2-1、书写格式" class="headerlink" title="2.1、书写格式"></a>2.1、书写格式</h4><p>Dockerfile 类似一堆 shell 命令的堆砌，实际上在构建阶段也可以简单的看做是一个 shell 脚本；但是为了更高效的利用缓存层，通常都会在一个 RUN 命令中连续书写大量的脚本命令，这时候一个良好的书写格式可以使 Dockerfile 看起来更加清晰易懂，也方便以后维护；我个人比较推崇的格式是按照 <a href="https://github.com/nginxinc/docker-nginx/blob/master/mainline/alpine/Dockerfile">nginx-alpine官方 Dockerfile</a> 的样式来书写，这个 Dockerfile 大致包括了以下规则:</p><ul><li>换行以 <code>&amp;&amp;</code> 开头保持每行对齐，看起来干净又舒服</li><li>安装大量软件包时，每个包一行并添加换行符，虽然会造成很多行，但是看起来很清晰；也可根据实际需要增加每行软件包个数，但是建议不要超过 5 个</li><li>configure 的配置尽量放在统一的变量里，并做好合理换行，方便以后集中化修改</li><li>注释同样和对应命令对齐，并保持单行长度不超出视野，即不能造成拉动滚动条才能看完你的注释</li><li>alpine 作为基础镜像的话，必要时可以使用 scanelf 来减少安装依赖</li></ul><p>除了以上规则，说下我个人的一些小习惯，仅供参考:</p><ul><li>当需要编译时，尽量避免多次 <code>cd</code> 目录，必须进入目录编译时可以开启子 shell 使其完成后还停留但在当前目录，避免 <code>cd</code> 进去再 <code>cd</code> 回来，如</li></ul><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-built_in">cd</span> xxxx \&amp;&amp; ./configure \&amp;&amp; make \&amp;&amp; make install \&amp;&amp; <span class="hljs-built_in">cd</span> ../</code></pre></div><p>可以变为</p><div class="hljs code-wrapper"><pre><code class="hljs sh">(<span class="hljs-built_in">cd</span> xxx \&amp;&amp; ./configure \&amp;&amp; make \&amp;&amp; make install)</code></pre></div><ul><li>同样意义的操作统一放在相邻行处理，比如镜像需要安装两个软件，做两次 <code>wget</code>，那么没必要安装完一个删除一个安装包，可以在最后统一的进行清理动作，简而言之是 <strong>合并具有相同目的的命令</strong></li><li>尽量使用网络资源，也就是说尽量不要在当前目录下放置那种二进制文件，然后进行 <code>ADD</code>/<code>COPY</code> 操作，因为一般 Dockerfile 都是存放到 git 仓库的，同目录下的二进制变动会给 git 仓库带来很大负担</li><li>调整好镜像时区，最好内置一下 bash，可能以后临时进入容器会处理一些东西</li><li><code>FROM</code> 时指定具体的版本号，防止后续升级或者更换主机 build 造成不可预知的结果</li></ul><h4 id="2-2、合理利用缓存"><a href="#2-2、合理利用缓存" class="headerlink" title="2.2、合理利用缓存"></a>2.2、合理利用缓存</h4><p>Docker 在 build 或者说是拉取镜像时是以层为单位作为缓存的；通俗的讲，一个 Dockerfile 命令就会形成一个镜像层(不绝对)，尤其是 <code>RUN</code> 命令形成的镜像层可能会很大；此时应当合理组织 Dockerfile，以便每次拉取或者 build 时高效的利用缓存层</p><ul><li>重复 build 的缓存利用</li></ul><p>Docker 在进行 build 操作时，对于同一个 Dockerfile 来说，<strong>只要执行过一次 build，那么下次 build 将从命令更改处开始</strong>；简单的例子如下</p><div class="hljs code-wrapper"><pre><code class="hljs sh">FROM alpine:3.6COPY test.jar /test.jarRUN apk add openjdk8 --no-cacheCMD [<span class="hljs-string">&quot;java&quot;</span>,<span class="hljs-string">&quot;-jar&quot;</span>,<span class="hljs-string">&quot;/test.tar&quot;</span>]</code></pre></div><p>假设我们的项目发布物为 <code>test.jar</code>，那么以上 Dockerfile 放到 CI 里每次 build 都会相当慢，原因就是 <strong>每次更改的发布物为 <code>test.jar</code>，那么也就是相当于每次 build 失效位置从 <code>COPY</code> 命令开始，这将导致下面的 <code>RUN</code> 命令每次都会不走缓存重复执行，当 <code>RUN</code> 命令涉及网络下载等复杂动作时这会极大拖慢 build 进度</strong>，解决方案很简单，移动一下 <code>COPY</code> 命令即可</p><div class="hljs code-wrapper"><pre><code class="hljs sh">FROM alpine:3.6RUN apk add openjdk8 --no-cacheCOPY test.jar /test.jarCMD [<span class="hljs-string">&quot;java&quot;</span>,<span class="hljs-string">&quot;-jar&quot;</span>,<span class="hljs-string">&quot;/test.tar&quot;</span>]</code></pre></div><p>此时每次 build 失效位置仍然是 <code>COPY</code> 命令，但是上面的 <code>RUN</code> 命令层已经被 build 过，而且无任何改变，那么每次 build 时 <code>RUN</code> 命令都会命中缓存层从而秒过</p><ul><li>多次拉取的缓存利用</li></ul><p>同上面的 build 一个原理，在 Docker 进行 pull 操作时，也是按照镜像层来进行缓存；当项目进行更新版本，那么只要当前主机 pull 过一次上一个版本的项目，那么下一次将会直接 pull 变更的层，也就是说上面安装 openjdk 的层将会复用；这种情况为了看起来清晰一点也可以将 Dockerfile 拆分成两个</p><p><strong>OpenJDK8 base</strong></p><div class="hljs code-wrapper"><pre><code class="hljs sh">FROM alpine:3.6RUN RUN apk add openjdk8 --no-cache</code></pre></div><p><strong>Java Web image</strong></p><div class="hljs code-wrapper"><pre><code class="hljs sh">FROM xxx.com/base/openjdk8COPY test.jar /test.jarCMD [<span class="hljs-string">&quot;java&quot;</span>,<span class="hljs-string">&quot;-jar&quot;</span>,<span class="hljs-string">&quot;/test.tar&quot;</span>]</code></pre></div><h3 id="三、镜像安全"><a href="#三、镜像安全" class="headerlink" title="三、镜像安全"></a>三、镜像安全</h3><h4 id="3-1、用户切换"><a href="#3-1、用户切换" class="headerlink" title="3.1、用户切换"></a>3.1、用户切换</h4><p>当我们不在 Dockerfile 中指定内部用户时，那么默认以 root 用户运行；由于 Linux 系统权限判定是根据 UID、GID 来进行的，也就是说 <strong>容器里面的 root 用户有权限访问宿主机 root 用户的东西；所以一旦挂载错误(比如将 <code>/root/.ssh</code> 目录挂载进去)，并且里面的用户具有高权限那么就很危险</strong>；通常习惯是遵从最小权限原则，也就是说尽量保证容器里的程序以低权限运行，此时可以在 Dockerfile 中通过 <code>USER</code> 命令指定后续运行命令所使用的账户，通过 <code>WORKDIR</code> 指定后续命令在那个目录下执行</p><div class="hljs code-wrapper"><pre><code class="hljs sh">FROM alpine:3.6RUN apk add openjdk8 --no-cacheCOPY test.jar /test.jarUSER testuser:testuserWORKDIR /tmpCMD [<span class="hljs-string">&quot;java&quot;</span>,<span class="hljs-string">&quot;-jar&quot;</span>,<span class="hljs-string">&quot;/test.tar&quot;</span>]</code></pre></div><p>有时直接使用 <code>USER</code> 指令来切换用户并不算方便，比如你的镜像需要挂载外部存储，如果外部存储中文件权限被意外修改，你的程序接下来可能就会启动失败；此时可以使用一下两个小工具来动态切换用户，巧妙的做法是 <strong>在正式运行程序之前先使用 root 用户进行权限修复，然后使用以下工具切换到具体用户运行</strong></p><ul><li><a href="https://github.com/tianon/gosu">gosu</a> Golang 实现的一个切换用户身份执行其他程序的小工具</li><li><a href="https://github.com/hlovdal/su-exec">su-exec</a> C 实现的一个更轻量级的用户切换工具</li></ul><p>具体的 Dockerfile 可以参见我写的 elasticsearch 的 <a href="https://github.com/mritd/dockerfile/blob/master/elasticsearch/docker-entrypoint.sh">entrypoint 脚本</a></p><h4 id="3-2、容器运行时"><a href="#3-2、容器运行时" class="headerlink" title="3.2、容器运行时"></a>3.2、容器运行时</h4><p>并不是每个容器都一定能切换到低权限用户来运行的，可能某些程序就希望在 root 下运行，此时一定要确认好容器是否需要 <strong>特权模式</strong> 运行；因为一旦开启了特权模式运行的容器将有能力修改宿主机内核参数等重要设置；具体的 Docker 容器运行设置前请参考 <a href="https://docs.docker.com/engine/reference/run/#runtime-privilege-and-linux-capabilities">官方文档</a></p><p>关于 Dockerfile 方面暂时总结出这些，可能也会有遗漏，待后续补充吧；同时欢迎各位提出相关修改意见 😊</p>]]></content>
    
    
    <summary type="html">最近准备整理一下关于 CI/CD 的相关文档，写一个关于 CI/CD 的系列文章，这篇先从最基本的 Dockerfile 书写开始，本系列文章默认读者已经熟悉 Docker、Kubernetes 相关工具</summary>
    
    
    
    <category term="CI/CD" scheme="https://mritd.com/categories/ci-cd/"/>
    
    
    <category term="Linux" scheme="https://mritd.com/tags/linux/"/>
    
    <category term="Docker" scheme="https://mritd.com/tags/docker/"/>
    
    <category term="Kubernetes" scheme="https://mritd.com/tags/kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>Teleport 跳板机部署</title>
    <link href="https://mritd.com/2017/11/09/set-up-teleport/"/>
    <id>https://mritd.com/2017/11/09/set-up-teleport/</id>
    <published>2017-11-09T08:47:51.000Z</published>
    <updated>2017-11-09T08:47:51.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>由于业务需求，以前账号管理混乱，所以很多人有生产服务器的 root 权限；所以目前需要一个能 ssh 登录线上服务器的工具，同时具有简单的审计功能；找了好久找到了这个小工具，以下记录一下搭建教程</p></blockquote><h3 id="一、环境准备"><a href="#一、环境准备" class="headerlink" title="一、环境准备"></a>一、环境准备</h3><p>目前准备了 3 台虚拟机，两台位于内网 NAT 之后，一台位于公网可以直接链接；使用时客户端通过工具连接到公网跳板机上，然后实现自动跳转到内网任意主机；并且具有相应的操作回放审计，通过宿主机账户限制用户权限</p><table><thead><tr><th>ip</th><th>节点</th></tr></thead><tbody><tr><td>92.223.67.84</td><td>公网 Master</td></tr><tr><td>172.16.0.80</td><td>内网 Master</td></tr><tr><td>172.16.0.81</td><td>内网 Node</td></tr></tbody></table><h3 id="二、Teleport-工作模式"><a href="#二、Teleport-工作模式" class="headerlink" title="二、Teleport 工作模式"></a>二、Teleport 工作模式</h3><p>Teleport 工作时从宏观上看是以集群为单位，也就是说<strong>公网算作一个集群，内网算作另一个集群，内网集群通过 ssh 隧道保持跟公网的链接状态，同时内网机群允许公网集群用户连接</strong>，大体工作模式如下</p><p><img src="https://cdn.oss.link/markdown/hsnj8.png" alt="Teleport 工作模式"></p><h3 id="三、搭建公网-Master"><a href="#三、搭建公网-Master" class="headerlink" title="三、搭建公网 Master"></a>三、搭建公网 Master</h3><h4 id="3-1、配置-Systemd"><a href="#3-1、配置-Systemd" class="headerlink" title="3.1、配置 Systemd"></a>3.1、配置 Systemd</h4><p>首先下载相关可执行文件并复制到 Path 目录下，然后创建一下配置目录等</p><div class="hljs code-wrapper"><pre><code class="hljs sh">wget https://github.com/gravitational/teleport/releases/download/v2.3.5/teleport-v2.3.5-linux-amd64-bin.tar.gztar -zxvf teleport-v2.3.5-linux-amd64-bin.tar.gzmv teleport/tctl teleport/teleport teleport/tsh /usr/<span class="hljs-built_in">local</span>/binmkdir -p /etc/teleport /data/teleport</code></pre></div><p>然后为了让服务后台运行创建一个 systemd service 配置文件</p><div class="hljs code-wrapper"><pre><code class="hljs sh">cat &gt; /etc/systemd/system/teleport.service &lt;&lt;<span class="hljs-string">EOF</span><span class="hljs-string">[Unit]</span><span class="hljs-string">Description=Teleport SSH Service</span><span class="hljs-string">After=network.target</span><span class="hljs-string"></span><span class="hljs-string">[Service]</span><span class="hljs-string">Type=simple</span><span class="hljs-string">Restart=always</span><span class="hljs-string">ExecStart=/usr/local/bin/teleport start -c /etc/teleport/teleport.yaml</span><span class="hljs-string"></span><span class="hljs-string">[Install]</span><span class="hljs-string">WantedBy=multi-user.target</span><span class="hljs-string">EOF</span></code></pre></div><h4 id="3-2、配置-Teleport"><a href="#3-2、配置-Teleport" class="headerlink" title="3.2、配置 Teleport"></a>3.2、配置 Teleport</h4><p>Systemd 配置完成后，就需要写一个 Teleport 的配置文件来让 Teleport 启动，具体选项含义可以参考 <a href="https://gravitational.com/teleport/docs/2.3/admin-guide/">官方文档</a>；以下为我的配置样例</p><div class="hljs code-wrapper"><pre><code class="hljs yaml"><span class="hljs-comment"># By default, this file should be stored in /etc/teleport.yaml</span><span class="hljs-comment"># This section of the configuration file applies to all teleport</span><span class="hljs-comment"># services.</span><span class="hljs-attr">teleport:</span>    <span class="hljs-comment"># nodename allows to assign an alternative name this node can be reached by.</span>    <span class="hljs-comment"># by default it&#x27;s equal to hostname</span>    <span class="hljs-attr">nodename:</span> <span class="hljs-string">mritd.master</span>    <span class="hljs-comment"># Data directory where Teleport keeps its data, like keys/users for</span>    <span class="hljs-comment"># authentication (if using the default BoltDB back-end)</span>    <span class="hljs-attr">data_dir:</span> <span class="hljs-string">/data/teleport</span>    <span class="hljs-comment"># one-time invitation token used to join a cluster. it is not used on</span>    <span class="hljs-comment"># subsequent starts</span>    <span class="hljs-attr">auth_token:</span> <span class="hljs-string">jYektagNTmhjv9Dh</span>    <span class="hljs-comment"># when running in multi-homed or NATed environments Teleport nodes need</span>    <span class="hljs-comment"># to know which IP it will be reachable at by other nodes</span>    <span class="hljs-attr">advertise_ip:</span> <span class="hljs-number">92.223</span><span class="hljs-number">.67</span><span class="hljs-number">.84</span>    <span class="hljs-comment"># list of auth servers in a cluster. you will have more than one auth server</span>    <span class="hljs-comment"># if you configure teleport auth to run in HA configuration</span>    <span class="hljs-attr">auth_servers:</span>        <span class="hljs-bullet">-</span> <span class="hljs-number">0.0</span><span class="hljs-number">.0</span><span class="hljs-number">.0</span><span class="hljs-string">:3025</span>        <span class="hljs-bullet">-</span> <span class="hljs-number">0.0</span><span class="hljs-number">.0</span><span class="hljs-number">.0</span><span class="hljs-string">:3025</span>    <span class="hljs-comment"># Teleport throttles all connections to avoid abuse. These settings allow</span>    <span class="hljs-comment"># you to adjust the default limits</span>    <span class="hljs-attr">connection_limits:</span>        <span class="hljs-attr">max_connections:</span> <span class="hljs-number">1000</span>        <span class="hljs-attr">max_users:</span> <span class="hljs-number">250</span>    <span class="hljs-comment"># Logging configuration. Possible output values are &#x27;stdout&#x27;, &#x27;stderr&#x27; and</span>    <span class="hljs-comment"># &#x27;syslog&#x27;. Possible severity values are INFO, WARN and ERROR (default).</span>    <span class="hljs-attr">log:</span>        <span class="hljs-attr">output:</span> <span class="hljs-string">stdout</span>        <span class="hljs-attr">severity:</span> <span class="hljs-string">INFO</span>    <span class="hljs-comment"># Type of storage used for keys. You need to configure this to use etcd</span>    <span class="hljs-comment"># backend if you want to run Teleport in HA configuration.</span>    <span class="hljs-attr">storage:</span>        <span class="hljs-attr">type:</span> <span class="hljs-string">bolt</span>    <span class="hljs-comment"># Cipher algorithms that the server supports. This section only needs to be</span>    <span class="hljs-comment"># set if you want to override the defaults.</span>    <span class="hljs-attr">ciphers:</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">aes128-ctr</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">aes192-ctr</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">aes256-ctr</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">aes128-gcm@openssh.com</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">arcfour256</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">arcfour128</span>    <span class="hljs-comment"># Key exchange algorithms that the server supports. This section only needs</span>    <span class="hljs-comment"># to be set if you want to override the defaults.</span>    <span class="hljs-attr">kex_algos:</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">curve25519-sha256@libssh.org</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">ecdh-sha2-nistp256</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">ecdh-sha2-nistp384</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">ecdh-sha2-nistp521</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">diffie-hellman-group14-sha1</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">diffie-hellman-group1-sha1</span>    <span class="hljs-comment"># Message authentication code (MAC) algorithms that the server supports.</span>    <span class="hljs-comment"># This section only needs to be set if you want to override the defaults.</span>    <span class="hljs-attr">mac_algos:</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">hmac-sha2-256-etm@openssh.com</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">hmac-sha2-256</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">hmac-sha1</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">hmac-sha1-96</span><span class="hljs-comment"># This section configures the &#x27;auth service&#x27;:</span><span class="hljs-attr">auth_service:</span>    <span class="hljs-comment"># Turns &#x27;auth&#x27; role on. Default is &#x27;yes&#x27;</span>    <span class="hljs-attr">enabled:</span> <span class="hljs-literal">yes</span>    <span class="hljs-attr">authentication:</span>        <span class="hljs-comment"># default authentication type. possible values are &#x27;local&#x27;, &#x27;oidc&#x27; and &#x27;saml&#x27;</span>        <span class="hljs-comment"># only local authentication (Teleport&#x27;s own user DB) is supported in the open</span>        <span class="hljs-comment"># source version</span>        <span class="hljs-attr">type:</span> <span class="hljs-string">local</span>        <span class="hljs-comment"># second_factor can be off, otp, or u2f</span>        <span class="hljs-attr">second_factor:</span> <span class="hljs-string">otp</span>        <span class="hljs-comment"># this section is used if second_factor is set to &#x27;u2f&#x27;</span>        <span class="hljs-comment">#u2f:</span>        <span class="hljs-comment">#    # app_id must point to the URL of the Teleport Web UI (proxy) accessible</span>        <span class="hljs-comment">#    # by the end users</span>        <span class="hljs-comment">#    app_id: https://localhost:3080</span>        <span class="hljs-comment">#    # facets must list all proxy servers if there are more than one deployed</span>        <span class="hljs-comment">#    facets:</span>        <span class="hljs-comment">#    - https://localhost:3080</span>    <span class="hljs-comment"># IP and the port to bind to. Other Teleport nodes will be connecting to</span>    <span class="hljs-comment"># this port (AKA &quot;Auth API&quot; or &quot;Cluster API&quot;) to validate client</span>    <span class="hljs-comment"># certificates</span>    <span class="hljs-attr">listen_addr:</span> <span class="hljs-number">0.0</span><span class="hljs-number">.0</span><span class="hljs-number">.0</span><span class="hljs-string">:3025</span>    <span class="hljs-comment"># Pre-defined tokens for adding new nodes to a cluster. Each token specifies</span>    <span class="hljs-comment"># the role a new node will be allowed to assume. The more secure way to</span>    <span class="hljs-comment"># add nodes is to use `ttl node add --ttl` command to generate auto-expiring</span>    <span class="hljs-comment"># tokens.</span>    <span class="hljs-comment">#</span>    <span class="hljs-comment"># We recommend to use tools like `pwgen` to generate sufficiently random</span>    <span class="hljs-comment"># tokens of 32+ byte length.</span>    <span class="hljs-attr">tokens:</span>        <span class="hljs-bullet">-</span> <span class="hljs-string">&quot;proxy,node:jYektagNTmhjv9Dh&quot;</span>        <span class="hljs-bullet">-</span> <span class="hljs-string">&quot;auth:jYektagNTmhjv9Dh&quot;</span>    <span class="hljs-comment"># Optional &quot;cluster name&quot; is needed when configuring trust between multiple</span>    <span class="hljs-comment"># auth servers. A cluster name is used as part of a signature in certificates</span>    <span class="hljs-comment"># generated by this CA.</span>    <span class="hljs-comment">#</span>    <span class="hljs-comment"># By default an automatically generated GUID is used.</span>    <span class="hljs-comment">#</span>    <span class="hljs-comment"># IMPORTANT: if you change cluster_name, it will invalidate all generated</span>    <span class="hljs-comment"># certificates and keys (may need to wipe out /var/lib/teleport directory)</span>    <span class="hljs-attr">cluster_name:</span> <span class="hljs-string">&quot;mritd&quot;</span><span class="hljs-comment"># This section configures the &#x27;node service&#x27;:</span><span class="hljs-attr">ssh_service:</span>    <span class="hljs-comment"># Turns &#x27;ssh&#x27; role on. Default is &#x27;yes&#x27;</span>    <span class="hljs-attr">enabled:</span> <span class="hljs-literal">yes</span>    <span class="hljs-comment"># IP and the port for SSH service to bind to.</span>    <span class="hljs-attr">listen_addr:</span> <span class="hljs-number">0.0</span><span class="hljs-number">.0</span><span class="hljs-number">.0</span><span class="hljs-string">:3022</span>    <span class="hljs-comment"># See explanation of labels in &quot;Labeling Nodes&quot; section below</span>    <span class="hljs-attr">labels:</span>        <span class="hljs-attr">role:</span> <span class="hljs-string">master</span>    <span class="hljs-comment"># List of the commands to periodically execute. Their output will be used as node labels.</span>    <span class="hljs-comment"># See &quot;Labeling Nodes&quot; section below for more information.</span>    <span class="hljs-attr">commands:</span>    <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">arch</span>             <span class="hljs-comment"># this command will add a label like &#x27;arch=x86_64&#x27; to a node</span>      <span class="hljs-attr">command:</span> [<span class="hljs-string">uname</span>, <span class="hljs-string">-p</span>]      <span class="hljs-attr">period:</span> <span class="hljs-string">1h0m0s</span>    <span class="hljs-comment"># enables reading ~/.tsh/environment before creating a session. by default</span>    <span class="hljs-comment"># set to false, can be set true here or as a command line flag.</span>    <span class="hljs-attr">permit_user_env:</span> <span class="hljs-literal">false</span><span class="hljs-comment"># This section configures the &#x27;proxy servie&#x27;</span><span class="hljs-attr">proxy_service:</span>    <span class="hljs-comment"># Turns &#x27;proxy&#x27; role on. Default is &#x27;yes&#x27;</span>    <span class="hljs-attr">enabled:</span> <span class="hljs-literal">yes</span>    <span class="hljs-comment"># SSH forwarding/proxy address. Command line (CLI) clients always begin their</span>    <span class="hljs-comment"># SSH sessions by connecting to this port</span>    <span class="hljs-attr">listen_addr:</span> <span class="hljs-number">0.0</span><span class="hljs-number">.0</span><span class="hljs-number">.0</span><span class="hljs-string">:3023</span>    <span class="hljs-comment"># Reverse tunnel listening address. An auth server (CA) can establish an</span>    <span class="hljs-comment"># outbound (from behind the firewall) connection to this address.</span>    <span class="hljs-comment"># This will allow users of the outside CA to connect to behind-the-firewall</span>    <span class="hljs-comment"># nodes.</span>    <span class="hljs-attr">tunnel_listen_addr:</span> <span class="hljs-number">0.0</span><span class="hljs-number">.0</span><span class="hljs-number">.0</span><span class="hljs-string">:3024</span>    <span class="hljs-comment"># The HTTPS listen address to serve the Web UI and also to authenticate the</span>    <span class="hljs-comment"># command line (CLI) users via password+HOTP</span>    <span class="hljs-attr">web_listen_addr:</span> <span class="hljs-number">0.0</span><span class="hljs-number">.0</span><span class="hljs-number">.0</span><span class="hljs-string">:3080</span>    <span class="hljs-comment"># TLS certificate for the HTTPS connection. Configuring these properly is</span>    <span class="hljs-comment"># critical for Teleport security.</span>    <span class="hljs-comment">#https_key_file: /var/lib/teleport/webproxy_key.pem</span>    <span class="hljs-comment">#https_cert_file: /var/lib/teleport/webproxy_cert.pem</span></code></pre></div><p>然后启动 Teleport 即可</p><div class="hljs code-wrapper"><pre><code class="hljs sh">systemctl <span class="hljs-built_in">enable</span> teleportsystemctl start teleport</code></pre></div><p>如果启动出现如下错误</p><div class="hljs code-wrapper"><pre><code class="hljs sh">error: Could not load host key: /etc/ssh/ssh_host_ecdsa_keyerror: Could not load host key: /etc/ssh/ssh_host_ed25519_key</code></pre></div><p>请执行 ssh-keygen 命令自行生成相关秘钥</p><div class="hljs code-wrapper"><pre><code class="hljs sh">ssh-keygen -t ecdsa -f /etc/ssh/ssh_host_ecdsa_keyssh-keygen -t ed25519 -f /etc/ssh/ssh_host_ed25519_key</code></pre></div><h4 id="3-3、添加用户"><a href="#3-3、添加用户" class="headerlink" title="3.3、添加用户"></a>3.3、添加用户</h4><p>公网这台 Teleport 将会作为主要的接入机器，所以在此节点内添加的用户将有权限登录所有集群，包括内网的另一个集群；所以为了方便以后操作先添加一个用户</p><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment"># 添加一个用户名为 mritd 的用户，该用户在所有集群具有 root 用户权限</span>tctl --config /etc/teleport/teleport.yaml users add mritd root</code></pre></div><p>添加成功后会返回一个 OTP 认证初始化地址，浏览器访问后可以使用 Google 扫描 OTP 二维码从而在登录时增加一层 OTP 认证</p><p><img src="https://cdn.oss.link/markdown/chuyf.png" alt="OTP CMD"></p><p>访问该地址后初始化密码及 OTP</p><p><img src="https://cdn.oss.link/markdown/czwmd.png" alt="init OTP"></p><h3 id="四、搭建内网-Master"><a href="#四、搭建内网-Master" class="headerlink" title="四、搭建内网 Master"></a>四、搭建内网 Master</h3><p>内网搭建 Master 和公网类似，只不过为了安全将所有 <code>0.0.0.0</code> 的地址全部换成内网 IP 即可，以下为内网的配置信息</p><div class="hljs code-wrapper"><pre><code class="hljs yaml"><span class="hljs-comment"># By default, this file should be stored in /etc/teleport.yaml</span><span class="hljs-comment"># This section of the configuration file applies to all teleport</span><span class="hljs-comment"># services.</span><span class="hljs-attr">teleport:</span>    <span class="hljs-comment"># nodename allows to assign an alternative name this node can be reached by.</span>    <span class="hljs-comment"># by default it&#x27;s equal to hostname</span>    <span class="hljs-attr">nodename:</span> <span class="hljs-string">mritd.test1</span>    <span class="hljs-comment"># Data directory where Teleport keeps its data, like keys/users for</span>    <span class="hljs-comment"># authentication (if using the default BoltDB back-end)</span>    <span class="hljs-attr">data_dir:</span> <span class="hljs-string">/data/teleport</span>    <span class="hljs-comment"># one-time invitation token used to join a cluster. it is not used on</span>    <span class="hljs-comment"># subsequent starts</span>    <span class="hljs-attr">auth_token:</span> <span class="hljs-string">jYektagNTmhjv9Dh</span>    <span class="hljs-comment"># when running in multi-homed or NATed environments Teleport nodes need</span>    <span class="hljs-comment"># to know which IP it will be reachable at by other nodes</span>    <span class="hljs-attr">advertise_ip:</span> <span class="hljs-number">172.16</span><span class="hljs-number">.0</span><span class="hljs-number">.80</span>    <span class="hljs-comment"># list of auth servers in a cluster. you will have more than one auth server</span>    <span class="hljs-comment"># if you configure teleport auth to run in HA configuration</span>    <span class="hljs-attr">auth_servers:</span>        <span class="hljs-bullet">-</span> <span class="hljs-number">172.16</span><span class="hljs-number">.0</span><span class="hljs-number">.80</span><span class="hljs-string">:3025</span>    <span class="hljs-comment"># Teleport throttles all connections to avoid abuse. These settings allow</span>    <span class="hljs-comment"># you to adjust the default limits</span>    <span class="hljs-attr">connection_limits:</span>        <span class="hljs-attr">max_connections:</span> <span class="hljs-number">1000</span>        <span class="hljs-attr">max_users:</span> <span class="hljs-number">250</span>    <span class="hljs-comment"># Logging configuration. Possible output values are &#x27;stdout&#x27;, &#x27;stderr&#x27; and</span>    <span class="hljs-comment"># &#x27;syslog&#x27;. Possible severity values are INFO, WARN and ERROR (default).</span>    <span class="hljs-attr">log:</span>        <span class="hljs-attr">output:</span> <span class="hljs-string">stdout</span>        <span class="hljs-attr">severity:</span> <span class="hljs-string">INFO</span>    <span class="hljs-comment"># Type of storage used for keys. You need to configure this to use etcd</span>    <span class="hljs-comment"># backend if you want to run Teleport in HA configuration.</span>    <span class="hljs-attr">storage:</span>        <span class="hljs-attr">type:</span> <span class="hljs-string">bolt</span>    <span class="hljs-comment"># Cipher algorithms that the server supports. This section only needs to be</span>    <span class="hljs-comment"># set if you want to override the defaults. </span>    <span class="hljs-attr">ciphers:</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">aes128-ctr</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">aes192-ctr</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">aes256-ctr</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">aes128-gcm@openssh.com</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">arcfour256</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">arcfour128</span>    <span class="hljs-comment"># Key exchange algorithms that the server supports. This section only needs</span>    <span class="hljs-comment"># to be set if you want to override the defaults.</span>    <span class="hljs-attr">kex_algos:</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">curve25519-sha256@libssh.org</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">ecdh-sha2-nistp256</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">ecdh-sha2-nistp384</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">ecdh-sha2-nistp521</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">diffie-hellman-group14-sha1</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">diffie-hellman-group1-sha1</span>    <span class="hljs-comment"># Message authentication code (MAC) algorithms that the server supports.</span>    <span class="hljs-comment"># This section only needs to be set if you want to override the defaults.</span>    <span class="hljs-attr">mac_algos:</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">hmac-sha2-256-etm@openssh.com</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">hmac-sha2-256</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">hmac-sha1</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">hmac-sha1-96</span><span class="hljs-comment"># This section configures the &#x27;auth service&#x27;:</span><span class="hljs-attr">auth_service:</span>    <span class="hljs-comment"># Turns &#x27;auth&#x27; role on. Default is &#x27;yes&#x27;</span>    <span class="hljs-attr">enabled:</span> <span class="hljs-literal">yes</span>    <span class="hljs-attr">authentication:</span>        <span class="hljs-comment"># default authentication type. possible values are &#x27;local&#x27;, &#x27;oidc&#x27; and &#x27;saml&#x27;</span>        <span class="hljs-comment"># only local authentication (Teleport&#x27;s own user DB) is supported in the open</span>        <span class="hljs-comment"># source version</span>        <span class="hljs-attr">type:</span> <span class="hljs-string">local</span>        <span class="hljs-comment"># second_factor can be off, otp, or u2f</span>        <span class="hljs-attr">second_factor:</span> <span class="hljs-string">otp</span>        <span class="hljs-comment"># this section is used if second_factor is set to &#x27;u2f&#x27;</span>        <span class="hljs-comment">#u2f:</span>        <span class="hljs-comment">#    # app_id must point to the URL of the Teleport Web UI (proxy) accessible</span>        <span class="hljs-comment">#    # by the end users</span>        <span class="hljs-comment">#    app_id: https://localhost:3080</span>        <span class="hljs-comment">#    # facets must list all proxy servers if there are more than one deployed</span>        <span class="hljs-comment">#    facets:</span>        <span class="hljs-comment">#    - https://localhost:3080</span>    <span class="hljs-comment"># IP and the port to bind to. Other Teleport nodes will be connecting to</span>    <span class="hljs-comment"># this port (AKA &quot;Auth API&quot; or &quot;Cluster API&quot;) to validate client</span>    <span class="hljs-comment"># certificates</span>    <span class="hljs-attr">listen_addr:</span> <span class="hljs-number">172.16</span><span class="hljs-number">.0</span><span class="hljs-number">.80</span><span class="hljs-string">:3025</span>    <span class="hljs-comment"># Pre-defined tokens for adding new nodes to a cluster. Each token specifies</span>    <span class="hljs-comment"># the role a new node will be allowed to assume. The more secure way to</span>    <span class="hljs-comment"># add nodes is to use `ttl node add --ttl` command to generate auto-expiring</span>    <span class="hljs-comment"># tokens.</span>    <span class="hljs-comment">#</span>    <span class="hljs-comment"># We recommend to use tools like `pwgen` to generate sufficiently random</span>    <span class="hljs-comment"># tokens of 32+ byte length.</span>    <span class="hljs-attr">tokens:</span>        <span class="hljs-bullet">-</span> <span class="hljs-string">&quot;proxy,node:jYektagNTmhjv9Dh&quot;</span>        <span class="hljs-bullet">-</span> <span class="hljs-string">&quot;auth:jYektagNTmhjv9Dh&quot;</span>    <span class="hljs-comment"># Optional &quot;cluster name&quot; is needed when configuring trust between multiple</span>    <span class="hljs-comment"># auth servers. A cluster name is used as part of a signature in certificates</span>    <span class="hljs-comment"># generated by this CA.</span>    <span class="hljs-comment">#</span>    <span class="hljs-comment"># By default an automatically generated GUID is used.</span>    <span class="hljs-comment">#</span>    <span class="hljs-comment"># IMPORTANT: if you change cluster_name, it will invalidate all generated</span>    <span class="hljs-comment"># certificates and keys (may need to wipe out /var/lib/teleport directory)</span>    <span class="hljs-attr">cluster_name:</span> <span class="hljs-string">&quot;nat&quot;</span><span class="hljs-comment"># This section configures the &#x27;node service&#x27;:</span><span class="hljs-attr">ssh_service:</span>    <span class="hljs-comment"># Turns &#x27;ssh&#x27; role on. Default is &#x27;yes&#x27;</span>    <span class="hljs-attr">enabled:</span> <span class="hljs-literal">yes</span>    <span class="hljs-comment"># IP and the port for SSH service to bind to.</span>    <span class="hljs-attr">listen_addr:</span> <span class="hljs-number">172.16</span><span class="hljs-number">.0</span><span class="hljs-number">.80</span><span class="hljs-string">:3022</span>    <span class="hljs-comment"># See explanation of labels in &quot;Labeling Nodes&quot; section below</span>    <span class="hljs-attr">labels:</span>        <span class="hljs-attr">role:</span> <span class="hljs-string">master</span>    <span class="hljs-comment"># List of the commands to periodically execute. Their output will be used as node labels.</span>    <span class="hljs-comment"># See &quot;Labeling Nodes&quot; section below for more information.</span>    <span class="hljs-attr">commands:</span>    <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">arch</span>             <span class="hljs-comment"># this command will add a label like &#x27;arch=x86_64&#x27; to a node</span>      <span class="hljs-attr">command:</span> [<span class="hljs-string">uname</span>, <span class="hljs-string">-p</span>]      <span class="hljs-attr">period:</span> <span class="hljs-string">1h0m0s</span>    <span class="hljs-comment"># enables reading ~/.tsh/environment before creating a session. by default</span>    <span class="hljs-comment"># set to false, can be set true here or as a command line flag.</span>    <span class="hljs-attr">permit_user_env:</span> <span class="hljs-literal">false</span><span class="hljs-comment"># This section configures the &#x27;proxy servie&#x27;</span><span class="hljs-attr">proxy_service:</span>    <span class="hljs-comment"># Turns &#x27;proxy&#x27; role on. Default is &#x27;yes&#x27;</span>    <span class="hljs-attr">enabled:</span> <span class="hljs-literal">yes</span>    <span class="hljs-comment"># SSH forwarding/proxy address. Command line (CLI) clients always begin their</span>    <span class="hljs-comment"># SSH sessions by connecting to this port</span>    <span class="hljs-attr">listen_addr:</span> <span class="hljs-number">172.16</span><span class="hljs-number">.0</span><span class="hljs-number">.80</span><span class="hljs-string">:3023</span>    <span class="hljs-comment"># Reverse tunnel listening address. An auth server (CA) can establish an</span>    <span class="hljs-comment"># outbound (from behind the firewall) connection to this address.</span>    <span class="hljs-comment"># This will allow users of the outside CA to connect to behind-the-firewall</span>    <span class="hljs-comment"># nodes.</span>    <span class="hljs-attr">tunnel_listen_addr:</span> <span class="hljs-number">172.16</span><span class="hljs-number">.0</span><span class="hljs-number">.80</span><span class="hljs-string">:3024</span>    <span class="hljs-comment"># The HTTPS listen address to serve the Web UI and also to authenticate the</span>    <span class="hljs-comment"># command line (CLI) users via password+HOTP</span>    <span class="hljs-attr">web_listen_addr:</span> <span class="hljs-number">172.16</span><span class="hljs-number">.0</span><span class="hljs-number">.80</span><span class="hljs-string">:3080</span>    <span class="hljs-comment"># TLS certificate for the HTTPS connection. Configuring these properly is</span>    <span class="hljs-comment"># critical for Teleport security.</span>    <span class="hljs-comment">#https_key_file: /var/lib/teleport/webproxy_key.pem</span>    <span class="hljs-comment">#https_cert_file: /var/lib/teleport/webproxy_cert.pem</span></code></pre></div><p>配置完成后直接启动即可</p><div class="hljs code-wrapper"><pre><code class="hljs sh">systemctl <span class="hljs-built_in">enable</span> teleportsystemctl start teleport</code></pre></div><h3 id="五、将内网集群链接至公网"><a href="#五、将内网集群链接至公网" class="headerlink" title="五、将内网集群链接至公网"></a>五、将内网集群链接至公网</h3><p>上文已经讲过，Teleport 通过公网链接内网主机的方式是让内网集群向公网打通一条 ssh 隧道，然后再进行通讯；具体配置如下</p><h4 id="5-1、公网-Master-开启授信集群"><a href="#5-1、公网-Master-开启授信集群" class="headerlink" title="5.1、公网 Master 开启授信集群"></a>5.1、公网 Master 开启授信集群</h4><p>在公网 Master 增加 Token 配置，以允许持有该 Token 的其他内网集群连接到此，修改 <code>/etc/teleport/teleport.yaml</code> 增加一个 token 即可</p><div class="hljs code-wrapper"><pre><code class="hljs sh">tokens:    - <span class="hljs-string">&quot;proxy,node:jYektagNTmhjv9Dh&quot;</span>    - <span class="hljs-string">&quot;auth:jYektagNTmhjv9Dh&quot;</span>    - <span class="hljs-string">&quot;trusted_cluster:xiomwWcrKinFw4Vs&quot;</span></code></pre></div><p>然后重启 Teleport</p><div class="hljs code-wrapper"><pre><code class="hljs sh">systemctl restart teleport</code></pre></div><h4 id="5-2、内网-Master-链接公网-Master"><a href="#5-2、内网-Master-链接公网-Master" class="headerlink" title="5.2、内网 Master 链接公网 Master"></a>5.2、内网 Master 链接公网 Master</h4><p>当公网集群开启了允许其他集群链接后，内网集群只需要创建配置进行连接即可，创建配置(cluster.yaml)如下</p><div class="hljs code-wrapper"><pre><code class="hljs yaml"><span class="hljs-comment"># cluster.yaml</span><span class="hljs-attr">kind:</span> <span class="hljs-string">trusted_cluster</span><span class="hljs-attr">version:</span> <span class="hljs-string">v2</span><span class="hljs-attr">metadata:</span>  <span class="hljs-comment"># the trusted cluster name MUST match the &#x27;cluster_name&#x27; setting of the</span>  <span class="hljs-comment"># cluster</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">local_cluster</span><span class="hljs-attr">spec:</span>  <span class="hljs-comment"># this field allows to create tunnels that are disabled, but can be enabled later.</span>  <span class="hljs-attr">enabled:</span> <span class="hljs-literal">true</span>  <span class="hljs-comment"># the token expected by the &quot;main&quot; cluster:</span>  <span class="hljs-attr">token:</span> <span class="hljs-string">xiomwWcrKinFw4Vs</span>  <span class="hljs-comment"># the address in &#x27;host:port&#x27; form of the reverse tunnel listening port on the</span>  <span class="hljs-comment"># &quot;master&quot; proxy server:</span>  <span class="hljs-attr">tunnel_addr:</span> <span class="hljs-number">92.223</span><span class="hljs-number">.67</span><span class="hljs-number">.84</span><span class="hljs-string">:3024</span>  <span class="hljs-comment"># the address in &#x27;host:port&#x27; form of the web listening port on the</span>  <span class="hljs-comment"># &quot;master&quot; proxy server:</span>  <span class="hljs-attr">web_proxy_addr:</span> <span class="hljs-number">92.223</span><span class="hljs-number">.67</span><span class="hljs-number">.84</span><span class="hljs-string">:3080</span></code></pre></div><p>执行以下命令使内网集群通过 ssh 隧道连接到公网集群</p><div class="hljs code-wrapper"><pre><code class="hljs sh">tctl --config /etc/teleport/teleport.yaml create /etc/teleport/cluster.yaml</code></pre></div><p><strong>注意，如果在启动公网和内网集群时没有指定受信的证书( <code>https_cert_file</code>、<code>https_key_file</code> )，那么默认 Teleport 将会生成一个自签名证书，此时在 create 受信集群时将会产生如下错误:</strong></p><div class="hljs code-wrapper"><pre><code class="hljs sh">the trusted cluster uses misconfigured HTTP/TLS certificate</code></pre></div><p>此时需要在 <strong>待添加集群(内网)</strong> 启动时增加 <code>--insecure</code> 参数，即 Systemd 配置修改如下</p><div class="hljs code-wrapper"><pre><code class="hljs sh">[Unit]Description=Teleport SSH ServiceAfter=network.target[Service]Type=simpleRestart=alwaysExecStart=/usr/<span class="hljs-built_in">local</span>/bin/teleport start --insecure -c /etc/teleport/teleport.yaml[Install]WantedBy=multi-user.target</code></pre></div><p>然后再进行 create 就不会报错</p><h3 id="六、添加其他节点"><a href="#六、添加其他节点" class="headerlink" title="六、添加其他节点"></a>六、添加其他节点</h3><p>两台节点打通后，此时如果有其他机器则可以将其加入到对应集群中，以下以另一台内网机器为例</p><p>由于在主节点 <code>auth_service</code> 中已经预先指定了一个 static Token 用于其他节点加入( <code>proxy,node:jYektagNTmhjv9Dh</code> )，所以其他节点只需要使用这个 Token 加入即可，在另一台内网主机上修改 Systemd 配置如下，然后启动即可</p><div class="hljs code-wrapper"><pre><code class="hljs sh">[Unit]Description=Teleport SSH ServiceAfter=network.target[Service]Type=simpleRestart=alwaysExecStart=/usr/<span class="hljs-built_in">local</span>/bin/teleport start --roles=node,proxy \                                        --token=jYektagNTmhjv9Dh \                                        --auth-server=172.16.0.80[Install]WantedBy=multi-user.target</code></pre></div><p>此时在内网的 Master 上可以查看到 Node 已经加入</p><div class="hljs code-wrapper"><pre><code class="hljs sh">test1.node ➜ tctl --config /etc/teleport/teleport.yaml nodes lsHostname    UUID                                 Address          Labels----------- ------------------------------------ ---------------- -----------------------test2.node  abc786fe-9a60-4480-80f7-8edc20710e58 172.16.0.81:3022mritd.test1 be9080fb-bdba-4823-9fb6-294e0b0dcce3 172.16.0.80:3022 arch=x86_64,role=master</code></pre></div><h3 id="七、连接测试"><a href="#七、连接测试" class="headerlink" title="七、连接测试"></a>七、连接测试</h3><h4 id="7-1、Web-测试"><a href="#7-1、Web-测试" class="headerlink" title="7.1、Web 测试"></a>7.1、Web 测试</h4><p>Teleport 支持 Web 页面访问，直接访问 <code>https://公网IP:3080</code>，然后登陆即可，登陆后如下</p><p><img src="https://cdn.oss.link/markdown/9yf6k.png" alt="Web login"></p><p>通过 Cluster 选项可以切换不同集群，点击后面的用户名可以选择不同用户登录到不同主机(用户授权在添加用户时控制)，登陆成功后如下</p><p><img src="https://cdn.oss.link/markdown/m7hz5.png" alt="Login Success"></p><p>通过 Teleport 进行的所有操作可以通过审计菜单进行操作回放</p><p><img src="https://cdn.oss.link/markdown/c8a74.png" alt="Audit"></p><h4 id="7-2、命令行测试"><a href="#7-2、命令行测试" class="headerlink" title="7.2、命令行测试"></a>7.2、命令行测试</h4><p>类 Uninx 系统下我们还是习惯使用终端登录，终端登录需要借助 Teleport 的命令行工具 <code>tsh</code>，<code>tsh</code> 在下载的 release 压缩版中已经有了，具体使用文档请自行 help 和参考官方文档，以下为简单的使用示例</p><ul><li>登录跳板机: 短时间内只需要登录一次即可，登录时需要输入密码及 OTP 口令</li></ul><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-built_in">export</span> TELEPORT_PROXY=92.223.67.84<span class="hljs-built_in">export</span> TELEPORT_USER=mritdtsh login --insecure</code></pre></div><ul><li>登录主机: 完成上一步 login 后就可以免密码登录任意主机</li></ul><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment"># cluster 名字是上面设置的，在 web 界面也能看到</span>tsh ssh --cluster nat root@test2.node</code></pre></div><ul><li>复制文件: <strong>复制文件时不显示进度，并非卡死</strong></li></ul><div class="hljs code-wrapper"><pre><code class="hljs sh">tsh scp --cluster nat teleport-v2.3.5-linux-amd64-bin.tar.gz root@test2.node:/-&gt; teleport-v2.3.5-linux-amd64-bin.tar.gz (16797035)</code></pre></div>]]></content>
    
    
    <summary type="html">由于业务需求，以前账号管理混乱，所以很多人有生产服务器的 root 权限；所以目前需要一个能 ssh 登录线上服务器的工具，同时具有简单的审计功能；找了好久找到了这个小工具，以下记录一下搭建教程</summary>
    
    
    
    <category term="Linux" scheme="https://mritd.com/categories/linux/"/>
    
    
    <category term="Linux" scheme="https://mritd.com/tags/linux/"/>
    
  </entry>
  
  <entry>
    <title>Kubernetes 深度学习笔记</title>
    <link href="https://mritd.com/2017/11/03/deep-learning-on-kubernetes/"/>
    <id>https://mritd.com/2017/11/03/deep-learning-on-kubernetes/</id>
    <published>2017-11-03T09:37:13.000Z</published>
    <updated>2017-11-03T09:37:13.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>本文主要记录下 Kubernetes 下运行深度学习框架如 Tensorflow、Caffe2 等一些坑，纯总结性文档</p></blockquote><h3 id="一、先决条件"><a href="#一、先决条件" class="headerlink" title="一、先决条件"></a>一、先决条件</h3><p>Kubernetes 运行深度学习应用实际上要解决的唯一问题就是 GPU 调用，以下只描述 Nvidia 相关的问题以及解决方法；要想完成 Kubernetes 对 GPU 调用，首先要满足以下条件:</p><ul><li>Nvidia 显卡驱动安装正确</li><li>CUDA 安装正确</li><li>Nvidia Docker 安装正确</li></ul><p>关于 Nvidia 驱动和 CUDA 请自行查找安装方法，如果这两部都搞不定，那么不用继续了</p><p><strong>还有一点需要注意: <code>/var/lib</code> 这个目录不能处于单独分区中，具体原因下面阐述</strong></p><h3 id="二、Nvidia-Docker-安装"><a href="#二、Nvidia-Docker-安装" class="headerlink" title="二、Nvidia Docker 安装"></a>二、Nvidia Docker 安装</h3><p>在安装 Nvidia Docker 之前，请确保 Nvidia 驱动以及 CUDA 安装成功，并且 <code>nvidia-smi</code> 能正确显示，如下图所示(来源于网络)</p><p><img src="https://cdn.oss.link/markdown/tdpbk.jpg" alt="nvidia-smi"></p><p>Nvidia Docker 安装极其简单，具体可参考 <a href="https://github.com/NVIDIA/nvidia-docker">官方文档</a>，安装完成后请自行按照官方文档描述进行测试，这一步一般不会出现问题</p><p>如果测试成功后，<strong>请查看 <code>/var/lib/nvidia-docker/volumes</code></strong> 目录下是否有文件，<strong>如果没有，那就意味着 Nvidia Docker 并未生成相关的驱动文件成功，需要单独执行 <code>docker volume create --driver=nvidia-docker --name=nvidia_driver_$(modinfo -F version nvidia)</code> 以生成该文件；该命令生成的方式是将已经安装到系统的相关文件硬链接至此，所以要求 <code>/var/lib</code> 目录不能在单独的分区</strong>；驱动生成完成后应该会产生类似 <code>/var/lib/nvidia-docker/volumes/nvidia_driver/375.66</code> 的目录结构</p><h3 id="三、Kubernetes-配置"><a href="#三、Kubernetes-配置" class="headerlink" title="三、Kubernetes 配置"></a>三、Kubernetes 配置</h3><p>当所有基础环境就绪后，最后需要开启 Kubernetes 对 GPU 支持；Kubernetes GPU 文档可以参考 <a href="https://kubernetes.io/docs/tasks/manage-gpus/scheduling-gpus">这里</a>，实际主要就是在 kubelet 启动时增加 <code>--feature-gates=&quot;Accelerators=true&quot;</code> 参数，如下所示</p><p><img src="https://cdn.oss.link/markdown/gifs3.jpg" alt="Accelerators"></p><p>所有节点全部修改完成后重启 kubelet 即可，<strong>如果一台机器上有不同型号的显卡，同时希望 Pod 能区别使用不同的 GPU 则可以按照 <a href="https://kubernetes.io/docs/tasks/manage-gpus/scheduling-gpus/#api">官方文档</a> 增加相应设置</strong></p><h3 id="四、Deployment-设置"><a href="#四、Deployment-设置" class="headerlink" title="四、Deployment 设置"></a>四、Deployment 设置</h3><p>Deployment 部署采用一个 Tensorflow 镜像作为示例，部署配置如下</p><div class="hljs code-wrapper"><pre><code class="hljs sh">apiVersion: apps/v1beta1kind: Deploymentmetadata:  name: tensorflow  labels:    name: tensorflowspec:  replicas: 1  template:    metadata:      labels:        name: tensorflow    spec:      containers:        - name: tensorflow          image: tensorflow/tensorflow:1.4.0-rc0-gpu          imagePullPolicy: IfNotPresent          <span class="hljs-built_in">command</span>: [<span class="hljs-string">&quot;bash&quot;</span>,<span class="hljs-string">&quot;-c&quot;</span>,<span class="hljs-string">&quot;sleep 999999&quot;</span>]          ports:            - name: tensorflow              containerPort: 8888          resources:             limits:               alpha.kubernetes.io/nvidia-gpu: 1          volumeMounts:            - mountPath: /usr/<span class="hljs-built_in">local</span>/nvidia              name: nvidia-driver            - mountPath: /dev/nvidia0              name: nvidia0            - mountPath: /dev/nvidia-uvm              name: nvidia-uvm            - mountPath: /dev/nvidia-uvm-tools              name: nvidia-uvm-tools            - mountPath: /dev/nvidiactl              name: nvidiactl      volumes:        - name: nvidia-driver          hostPath:            path: /var/lib/nvidia-docker/volumes/nvidia_driver/375.66        - name: nvidia0          hostPath:            path: /dev/nvidia0        - name: nvidia-uvm          hostPath:            path: /dev/nvidia-uvm        - name: nvidia-uvm-tools          hostPath:            path: /dev/nvidia-uvm-tools        - name: nvidiactl          hostPath:            path: /dev/nvidiactl</code></pre></div><p><strong>Deployment 中运行的 Pod 需要挂载对应的宿主机设备文件以及驱动文件才能正确的调用宿主机 GPU，所以一定要确保前几步生成的相关驱动文件等没问题；如果有多个 nvidia 显卡的话可能需要挂载多个 nvidia 设备</strong></p><p>Pod 运行成功后可执行以下代码测试 GPU 调用</p><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tfhello = tf.constant(<span class="hljs-string">&#x27;Hello, TensorFlow!&#x27;</span>)sess = tf.Session()print(sess.run(hello))a = tf.constant(<span class="hljs-number">10</span>)b = tf.constant(<span class="hljs-number">32</span>)print(sess.run(a + b))</code></pre></div><p>成功后截图如下</p><p><img src="https://cdn.oss.link/markdown/l7ufl.jpg" alt="Tensorflow"></p>]]></content>
    
    
    <summary type="html">本文主要记录下 Kubernetes 下运行深度学习框架如 Tensorflow、Caffe2 等一些坑，纯总结性文档</summary>
    
    
    
    <category term="Kubernetes" scheme="https://mritd.com/categories/kubernetes/"/>
    
    <category term="Docker" scheme="https://mritd.com/categories/kubernetes/docker/"/>
    
    
    <category term="Linux" scheme="https://mritd.com/tags/linux/"/>
    
    <category term="Docker" scheme="https://mritd.com/tags/docker/"/>
    
    <category term="Kubernetes" scheme="https://mritd.com/tags/kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>Kubernetes 1.8 kube-proxy 开启 ipvs</title>
    <link href="https://mritd.com/2017/10/10/kube-proxy-use-ipvs-on-kubernetes-1.8/"/>
    <id>https://mritd.com/2017/10/10/kube-proxy-use-ipvs-on-kubernetes-1.8/</id>
    <published>2017-10-10T09:19:04.000Z</published>
    <updated>2017-10-10T09:19:04.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>Kubernetes 1.8 发布已经好几天，1.8 对于 kube-proxy 组件增加了 ipvs 支持，以下记录一下 kube-proxy ipvs 开启教程</p></blockquote><h3 id="一、环境准备"><a href="#一、环境准备" class="headerlink" title="一、环境准备"></a>一、环境准备</h3><p>目前测试为 5 台虚拟机，CentOS 系统，etcd、kubernetes 全部采用 rpm 安装，使用 systemd 来做管理，网络组件采用 calico，Master 实现了 HA；基本环境如下</p><table><thead><tr><th>IP</th><th>组件</th></tr></thead><tbody><tr><td>10.10.1.5</td><td>Master、Node、etcd</td></tr><tr><td>10.10.1.6</td><td>Master、Node、etcd</td></tr><tr><td>10.10.1.7</td><td>Master、Node、etcd</td></tr><tr><td>10.10.1.8</td><td>Node</td></tr><tr><td>10.10.1.9</td><td>Node</td></tr></tbody></table><h3 id="二、注意事项"><a href="#二、注意事项" class="headerlink" title="二、注意事项"></a>二、注意事项</h3><p>之所以把这个单独写一个标题是因为坑有点多，为了避免下面出现问题，先说一下注意事项:</p><h4 id="2-1、SELinux"><a href="#2-1、SELinux" class="headerlink" title="2.1、SELinux"></a>2.1、SELinux</h4><p>如果对 SELinux 玩的不溜的朋友，我建议先关闭  SELinux，关闭方法如下</p><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment"># 编辑 /etc/selinux/config 文件；确保 SELINUX=disabled</span>docker1.node ➜  ~ cat /etc/selinux/config<span class="hljs-comment"># This file controls the state of SELinux on the system.</span><span class="hljs-comment"># SELINUX= can take one of these three values:</span><span class="hljs-comment">#     enforcing - SELinux security policy is enforced.</span><span class="hljs-comment">#     permissive - SELinux prints warnings instead of enforcing.</span><span class="hljs-comment">#     disabled - No SELinux policy is loaded.</span>SELINUX=disabled<span class="hljs-comment"># SELINUXTYPE= can take one of three two values:</span><span class="hljs-comment">#     targeted - Targeted processes are protected,</span><span class="hljs-comment">#     minimum - Modification of targeted policy. Only selected processes are protected.</span><span class="hljs-comment">#     mls - Multi Level Security protection.</span>SELINUXTYPE=targeted</code></pre></div><p><strong>然后重启机器并验证</strong></p><div class="hljs code-wrapper"><pre><code class="hljs sh">docker1.node ➜  ~ sestatusSELinux status:                 disabled</code></pre></div><h4 id="2-2、Firewall"><a href="#2-2、Firewall" class="headerlink" title="2.2、Firewall"></a>2.2、Firewall</h4><p>搭建时尽量关闭防火墙，如果你玩的很溜，那么请在测试没问题后再开启防火墙</p><div class="hljs code-wrapper"><pre><code class="hljs sh">systemctl stop firewalldsystemctl <span class="hljs-built_in">disable</span> firewalld</code></pre></div><h4 id="2-3、内核参数调整"><a href="#2-3、内核参数调整" class="headerlink" title="2.3、内核参数调整"></a>2.3、内核参数调整</h4><p>确保内核已经开启如下参数，或者说确保 <code>/etc/sysctl.conf</code> 有如下配置</p><div class="hljs code-wrapper"><pre><code class="hljs sh">docker1.node ➜  ~ cat /etc/sysctl.conf<span class="hljs-comment"># sysctl settings are defined through files in</span><span class="hljs-comment"># /usr/lib/sysctl.d/, /run/sysctl.d/, and /etc/sysctl.d/.</span><span class="hljs-comment">#</span><span class="hljs-comment"># Vendors settings live in /usr/lib/sysctl.d/.</span><span class="hljs-comment"># To override a whole file, create a new file with the same in</span><span class="hljs-comment"># /etc/sysctl.d/ and put new settings there. To override</span><span class="hljs-comment"># only specific settings, add a file with a lexically later</span><span class="hljs-comment"># name in /etc/sysctl.d/ and put new settings there.</span><span class="hljs-comment">#</span><span class="hljs-comment"># For more information, see sysctl.conf(5) and sysctl.d(5).</span>net.ipv4.ip_forward=1net.bridge.bridge-nf-call-iptables=1net.bridge.bridge-nf-call-ip6tables=1</code></pre></div><p>然后执行 <code>sysctl -p</code> 使之生效</p><div class="hljs code-wrapper"><pre><code class="hljs sh">docker1.node ➜  ~ sysctl -pnet.ipv4.ip_forward = 1net.bridge.bridge-nf-call-iptables = 1net.bridge.bridge-nf-call-ip6tables = 1</code></pre></div><h4 id="2-4、内核模块加载"><a href="#2-4、内核模块加载" class="headerlink" title="2.4、内核模块加载"></a>2.4、内核模块加载</h4><p>由于 ipvs 已经加入到内核主干，所以需要内核模块支持，请确保内核已经加载了相应模块；如不确定，执行以下脚本，以确保内核加载相应模块，<strong>否则会出现 <code>failed to load kernel modules: [ip_vs_rr ip_vs_sh ip_vs_wrr]</code> 错误</strong></p><div class="hljs code-wrapper"><pre><code class="hljs sh">cat &gt; /etc/sysconfig/modules/ipvs.modules &lt;&lt;<span class="hljs-string">EOF</span><span class="hljs-string">#!/bin/bash</span><span class="hljs-string">ipvs_modules=&quot;ip_vs ip_vs_lc ip_vs_wlc ip_vs_rr ip_vs_wrr ip_vs_lblc ip_vs_lblcr ip_vs_dh ip_vs_sh ip_vs_fo ip_vs_nq ip_vs_sed ip_vs_ftp nf_conntrack_ipv4&quot;</span><span class="hljs-string">for kernel_module in \$&#123;ipvs_modules&#125;; do</span><span class="hljs-string">    /sbin/modinfo -F filename \$&#123;kernel_module&#125; &gt; /dev/null 2&gt;&amp;1</span><span class="hljs-string">    if [ $? -eq 0 ]; then</span><span class="hljs-string">        /sbin/modprobe \$&#123;kernel_module&#125;</span><span class="hljs-string">    fi</span><span class="hljs-string">done</span><span class="hljs-string">EOF</span>chmod 755 /etc/sysconfig/modules/ipvs.modules &amp;&amp; bash /etc/sysconfig/modules/ipvs.modules &amp;&amp; lsmod | grep ip_vs</code></pre></div><p>执行后应该如下图所示，<strong>如果 <code>lsmod | grep ip_vs</code> 并未出现 <code>ip_vs_rr</code> 等模块；那么请更换内核(一般不会，2.6 以后 ipvs 好像已经就合并进主干了)</strong></p><p><img src="https://cdn.oss.link/markdown/49wbb.jpg" alt="Load kernel modules"></p><h3 id="三、开启-ipvs-支持"><a href="#三、开启-ipvs-支持" class="headerlink" title="三、开启 ipvs 支持"></a>三、开启 ipvs 支持</h3><h4 id="3-1、修改配置"><a href="#3-1、修改配置" class="headerlink" title="3.1、修改配置"></a>3.1、修改配置</h4><p>修改 <code>/etc/kubernetes/proxy</code> 配置如下</p><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment">###</span><span class="hljs-comment"># kubernetes proxy config</span><span class="hljs-comment"># default config should be adequate</span><span class="hljs-comment"># Add your own!</span>KUBE_PROXY_ARGS=<span class="hljs-string">&quot;--bind-address=10.10.1.8 \</span><span class="hljs-string">                 --hostname-override=docker4.node \</span><span class="hljs-string">                 --masquerade-all \</span><span class="hljs-string">                 --feature-gates=SupportIPVSProxyMode=true \</span><span class="hljs-string">                 --proxy-mode=ipvs \</span><span class="hljs-string">                 --ipvs-min-sync-period=5s \</span><span class="hljs-string">                 --ipvs-sync-period=5s \</span><span class="hljs-string">                 --ipvs-scheduler=rr \</span><span class="hljs-string">                 --kubeconfig=/etc/kubernetes/kube-proxy.kubeconfig \</span><span class="hljs-string">                 --cluster-cidr=10.254.0.0/16&quot;</span></code></pre></div><p><strong>启用 ipvs 后与 1.7 版本的配置差异如下：</strong></p><ul><li>增加 <code>--feature-gates=SupportIPVSProxyMode=true</code> 选项，用于告诉 kube-proxy 开启 ipvs 支持，因为目前 ipvs 并未稳定</li><li>增加 <code>ipvs-min-sync-period</code>、<code>--ipvs-sync-period</code>、<code>--ipvs-scheduler</code> 三个参数用于调整 ipvs，具体参数值请自行查阅 ipvs 文档</li><li><strong>增加 <code>--masquerade-all</code> 选项，以确保反向流量通过</strong></li></ul><p><strong>重点说一下 <code>--masquerade-all</code> 选项: kube-proxy ipvs 是基于 NAT 实现的，当创建一个 service 后，kubernetes 会在每个节点上创建一个网卡，同时帮你将 Service IP(VIP) 绑定上，此时相当于每个 Node 都是一个 ds，而其他任何 Node 上的 Pod，甚至是宿主机服务(比如 kube-apiserver 的 6443)都可能成为 rs；按照正常的 lvs nat 模型，所有 rs 应该将 ds 设置成为默认网关，以便数据包在返回时能被 ds 正确修改；在 kubernetes 将 vip 设置到每个 Node 后，默认路由显然不可行，所以要设置 <code>--masquerade-all</code> 选项，以便反向数据包能通过</strong></p><p>以上描述可能并不精准，具体请看 <a href="https://docs.google.com/document/d/1YEBWR4EWeCEWwxufXzRM0e82l_lYYzIXQiSayGaVQ8M/edit?usp=sharing">Google 文档</a></p><h4 id="3-2、测试-ipvs"><a href="#3-2、测试-ipvs" class="headerlink" title="3.2、测试 ipvs"></a>3.2、测试 ipvs</h4><p>修改完成后，重启 kube-proxy 使其生效</p><div class="hljs code-wrapper"><pre><code class="hljs sh">systemctl daemon-reloadsystemctl restart kube-proxy</code></pre></div><p>重启后日志中应该能看到如下输出，不应该有其他提示 ipvs 的错误信息出现</p><p><img src="https://cdn.oss.link/markdown/o05rq.jpg" alt="kube-proxy ipvs log"></p><p>同时使用 ipvsadm 命令应该能看到相应的 service 的 ipvs 规则(ipvsadm 自己安装一下)</p><p><img src="https://cdn.oss.link/markdown/d1ilk.jpg" alt="ipvs role"></p><p>然后进入 Pod 测试</p><p><img src="https://cdn.oss.link/markdown/42pjm.jpg" alt="test ipvs1"></p><p><strong>最后说一点: ipvs 尚未稳定，请慎用；而且 <code>--masquerade-all</code> 选项与 Calico 安全策略控制不兼容，请酌情考虑使用(Calico 在做网络策略限制的时候要求不能开启此选项)</strong></p>]]></content>
    
    
    <summary type="html">Kubernetes 1.8 发布已经好几天，1.8 对于 kube-proxy 组件增加了 ipvs 支持，以下记录一下 kube-proxy ipvs 开启教程</summary>
    
    
    
    <category term="Kubernetes" scheme="https://mritd.com/categories/kubernetes/"/>
    
    
    <category term="Linux" scheme="https://mritd.com/tags/linux/"/>
    
    <category term="Docker" scheme="https://mritd.com/tags/docker/"/>
    
    <category term="Kubernetes" scheme="https://mritd.com/tags/kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>Kubernetes 1.8 集群搭建</title>
    <link href="https://mritd.com/2017/10/09/set-up-kubernetes-1.8-ha-cluster/"/>
    <id>https://mritd.com/2017/10/09/set-up-kubernetes-1.8-ha-cluster/</id>
    <published>2017-10-09T14:48:03.000Z</published>
    <updated>2017-10-09T14:48:03.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>目前 Kubernetes 1.8.0 已经发布，1.8.0增加了很多新特性，比如 kube-proxy 组建的 ipvs 模式等，同时 RBAC 授权也做了一些调整，国庆没事干，所以试了一下；以下记录了 Kubernetes 1.8.0 的搭建过程</p></blockquote><h3 id="一、环境准备"><a href="#一、环境准备" class="headerlink" title="一、环境准备"></a>一、环境准备</h3><p>目前测试为 5 台虚拟机，etcd、kubernetes 全部采用 rpm 安装，使用 systemd 来做管理，网络组件采用 calico，Master 实现了 HA；基本环境如下</p><table><thead><tr><th>IP</th><th>组件</th></tr></thead><tbody><tr><td>10.10.1.5</td><td>Master、Node、etcd</td></tr><tr><td>10.10.1.6</td><td>Master、Node、etcd</td></tr><tr><td>10.10.1.7</td><td>Master、Node、etcd</td></tr><tr><td>10.10.1.8</td><td>Node</td></tr><tr><td>10.10.1.9</td><td>Node</td></tr></tbody></table><p><strong>本文尽量以实际操作为主，因为写过一篇 <a href="https://mritd.me/2017/07/21/set-up-kubernetes-ha-cluster-by-binary/">Kubernetes 1.7 搭建文档</a>，所以以下细节部分不在详细阐述，不懂得可以参考上一篇文章；本文所有安装工具均已打包上传到了 <a href="https://pan.baidu.com/s/1nvwZCfv">百度云</a> 密码: <code>4zaz</code>，可直接下载重复搭建过程，搭建前请自行 load 好 images 目录下的相关 docker 镜像</strong></p><h3 id="二、搭建-Etcd-集群"><a href="#二、搭建-Etcd-集群" class="headerlink" title="二、搭建 Etcd 集群"></a>二、搭建 Etcd 集群</h3><h4 id="2-1、生成-Etcd-证书"><a href="#2-1、生成-Etcd-证书" class="headerlink" title="2.1、生成 Etcd 证书"></a>2.1、生成 Etcd 证书</h4><p>同样证书工具仍使用的是 <a href="https://pkg.cfssl.org/">cfssl</a>，百度云的压缩包里已经包含了，下面直接上配置(<strong>注意，所有证书生成只需要在任意一台主机上生成一遍即可，我这里在 Master 上操作的</strong>)</p><h5 id="etcd-csr-json"><a href="#etcd-csr-json" class="headerlink" title="etcd-csr.json"></a>etcd-csr.json</h5><div class="hljs code-wrapper"><pre><code class="hljs json">&#123;  <span class="hljs-attr">&quot;key&quot;</span>: &#123;    <span class="hljs-attr">&quot;algo&quot;</span>: <span class="hljs-string">&quot;rsa&quot;</span>,    <span class="hljs-attr">&quot;size&quot;</span>: <span class="hljs-number">2048</span>  &#125;,  <span class="hljs-attr">&quot;names&quot;</span>: [    &#123;      <span class="hljs-attr">&quot;O&quot;</span>: <span class="hljs-string">&quot;etcd&quot;</span>,      <span class="hljs-attr">&quot;OU&quot;</span>: <span class="hljs-string">&quot;etcd Security&quot;</span>,      <span class="hljs-attr">&quot;L&quot;</span>: <span class="hljs-string">&quot;Beijing&quot;</span>,      <span class="hljs-attr">&quot;ST&quot;</span>: <span class="hljs-string">&quot;Beijing&quot;</span>,      <span class="hljs-attr">&quot;C&quot;</span>: <span class="hljs-string">&quot;CN&quot;</span>    &#125;  ],  <span class="hljs-attr">&quot;CN&quot;</span>: <span class="hljs-string">&quot;etcd&quot;</span>,  <span class="hljs-attr">&quot;hosts&quot;</span>: [    <span class="hljs-string">&quot;127.0.0.1&quot;</span>,    <span class="hljs-string">&quot;localhost&quot;</span>,    <span class="hljs-string">&quot;10.10.1.5&quot;</span>,    <span class="hljs-string">&quot;10.10.1.6&quot;</span>,    <span class="hljs-string">&quot;10.10.1.7&quot;</span>,    <span class="hljs-string">&quot;10.10.1.8&quot;</span>,    <span class="hljs-string">&quot;10.10.1.9&quot;</span>  ]&#125;</code></pre></div><h5 id="etcd-gencert-json"><a href="#etcd-gencert-json" class="headerlink" title="etcd-gencert.json"></a>etcd-gencert.json</h5><div class="hljs code-wrapper"><pre><code class="hljs json">&#123;  <span class="hljs-attr">&quot;signing&quot;</span>: &#123;    <span class="hljs-attr">&quot;default&quot;</span>: &#123;        <span class="hljs-attr">&quot;usages&quot;</span>: [          <span class="hljs-string">&quot;signing&quot;</span>,          <span class="hljs-string">&quot;key encipherment&quot;</span>,          <span class="hljs-string">&quot;server auth&quot;</span>,          <span class="hljs-string">&quot;client auth&quot;</span>        ],        <span class="hljs-attr">&quot;expiry&quot;</span>: <span class="hljs-string">&quot;87600h&quot;</span>    &#125;  &#125;&#125;</code></pre></div><h5 id="etcd-root-ca-csr-json"><a href="#etcd-root-ca-csr-json" class="headerlink" title="etcd-root-ca-csr.json"></a>etcd-root-ca-csr.json</h5><div class="hljs code-wrapper"><pre><code class="hljs json">&#123;  <span class="hljs-attr">&quot;key&quot;</span>: &#123;    <span class="hljs-attr">&quot;algo&quot;</span>: <span class="hljs-string">&quot;rsa&quot;</span>,    <span class="hljs-attr">&quot;size&quot;</span>: <span class="hljs-number">4096</span>  &#125;,  <span class="hljs-attr">&quot;names&quot;</span>: [    &#123;      <span class="hljs-attr">&quot;O&quot;</span>: <span class="hljs-string">&quot;etcd&quot;</span>,      <span class="hljs-attr">&quot;OU&quot;</span>: <span class="hljs-string">&quot;etcd Security&quot;</span>,      <span class="hljs-attr">&quot;L&quot;</span>: <span class="hljs-string">&quot;Beijing&quot;</span>,      <span class="hljs-attr">&quot;ST&quot;</span>: <span class="hljs-string">&quot;Beijing&quot;</span>,      <span class="hljs-attr">&quot;C&quot;</span>: <span class="hljs-string">&quot;CN&quot;</span>    &#125;  ],  <span class="hljs-attr">&quot;CN&quot;</span>: <span class="hljs-string">&quot;etcd-root-ca&quot;</span>&#125;</code></pre></div><p><strong>最后生成证书</strong></p><div class="hljs code-wrapper"><pre><code class="hljs sh">cfssl gencert --initca=<span class="hljs-literal">true</span> etcd-root-ca-csr.json | cfssljson --bare etcd-root-cacfssl gencert --ca etcd-root-ca.pem --ca-key etcd-root-ca-key.pem --config etcd-gencert.json etcd-csr.json | cfssljson --bare etcd</code></pre></div><p>证书生成后截图如下</p><p><img src="https://cdn.oss.link/markdown/6mn6y.jpg" alt="Gen Etcd Cert"></p><h4 id="2-2、搭建集群"><a href="#2-2、搭建集群" class="headerlink" title="2.2、搭建集群"></a>2.2、搭建集群</h4><p>首先分发证书及 rpm 包</p><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment"># 分发 rpm</span><span class="hljs-keyword">for</span> IP <span class="hljs-keyword">in</span> `seq 5 7`; <span class="hljs-keyword">do</span>    scp etcd-3.2.7-1.fc28.x86_64.rpm root@10.10.1.<span class="hljs-variable">$IP</span>:~    ssh root@10.10.1.<span class="hljs-variable">$IP</span> rpm -ivh etcd-3.2.7-1.fc28.x86_64.rpm<span class="hljs-keyword">done</span><span class="hljs-comment"># 分发证书</span><span class="hljs-keyword">for</span> IP <span class="hljs-keyword">in</span> `seq 5 7`;<span class="hljs-keyword">do</span>    ssh root@10.10.1.<span class="hljs-variable">$IP</span> mkdir /etc/etcd/ssl    scp *.pem root@10.10.1.<span class="hljs-variable">$IP</span>:/etc/etcd/ssl    ssh root@10.10.1.<span class="hljs-variable">$IP</span> chown -R etcd:etcd /etc/etcd/ssl    ssh root@10.10.1.<span class="hljs-variable">$IP</span> chmod -R 644 /etc/etcd/ssl/*    ssh root@10.10.1.<span class="hljs-variable">$IP</span> chmod 755 /etc/etcd/ssl<span class="hljs-keyword">done</span></code></pre></div><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment"># 修改 etcd 数据目录权限组</span><span class="hljs-keyword">for</span> IP <span class="hljs-keyword">in</span> `seq 5 7`;<span class="hljs-keyword">do</span>    ssh root@10.10.1.<span class="hljs-variable">$IP</span> chown -R etcd:etcd /var/lib/etcd<span class="hljs-keyword">done</span></code></pre></div><p><strong>然后修改配置如下(其他两个节点类似，只需要改监听地址和 Etcd Name 即可)</strong></p><div class="hljs code-wrapper"><pre><code class="hljs sh">docker1.node ➜  ~ cat /etc/etcd/etcd.conf<span class="hljs-comment"># [member]</span>ETCD_NAME=etcd1ETCD_DATA_DIR=<span class="hljs-string">&quot;/var/lib/etcd/etcd1.etcd&quot;</span>ETCD_WAL_DIR=<span class="hljs-string">&quot;/var/lib/etcd/wal&quot;</span>ETCD_SNAPSHOT_COUNT=<span class="hljs-string">&quot;100&quot;</span>ETCD_HEARTBEAT_INTERVAL=<span class="hljs-string">&quot;100&quot;</span>ETCD_ELECTION_TIMEOUT=<span class="hljs-string">&quot;1000&quot;</span>ETCD_LISTEN_PEER_URLS=<span class="hljs-string">&quot;https://10.10.1.5:2380&quot;</span>ETCD_LISTEN_CLIENT_URLS=<span class="hljs-string">&quot;https://10.10.1.5:2379,http://127.0.0.1:2379&quot;</span>ETCD_MAX_SNAPSHOTS=<span class="hljs-string">&quot;5&quot;</span>ETCD_MAX_WALS=<span class="hljs-string">&quot;5&quot;</span><span class="hljs-comment">#ETCD_CORS=&quot;&quot;</span><span class="hljs-comment"># [cluster]</span>ETCD_INITIAL_ADVERTISE_PEER_URLS=<span class="hljs-string">&quot;https://10.10.1.5:2380&quot;</span><span class="hljs-comment"># if you use different ETCD_NAME (e.g. test), set ETCD_INITIAL_CLUSTER value for this name, i.e. &quot;test=http://...&quot;</span>ETCD_INITIAL_CLUSTER=<span class="hljs-string">&quot;etcd1=https://10.10.1.5:2380,etcd2=https://10.10.1.6:2380,etcd3=https://10.10.1.7:2380&quot;</span>ETCD_INITIAL_CLUSTER_STATE=<span class="hljs-string">&quot;new&quot;</span>ETCD_INITIAL_CLUSTER_TOKEN=<span class="hljs-string">&quot;etcd-cluster&quot;</span>ETCD_ADVERTISE_CLIENT_URLS=<span class="hljs-string">&quot;https://10.10.1.5:2379&quot;</span><span class="hljs-comment">#ETCD_DISCOVERY=&quot;&quot;</span><span class="hljs-comment">#ETCD_DISCOVERY_SRV=&quot;&quot;</span><span class="hljs-comment">#ETCD_DISCOVERY_FALLBACK=&quot;proxy&quot;</span><span class="hljs-comment">#ETCD_DISCOVERY_PROXY=&quot;&quot;</span><span class="hljs-comment">#ETCD_STRICT_RECONFIG_CHECK=&quot;false&quot;</span><span class="hljs-comment">#ETCD_AUTO_COMPACTION_RETENTION=&quot;0&quot;</span><span class="hljs-comment"># [proxy]</span><span class="hljs-comment">#ETCD_PROXY=&quot;off&quot;</span><span class="hljs-comment">#ETCD_PROXY_FAILURE_WAIT=&quot;5000&quot;</span><span class="hljs-comment">#ETCD_PROXY_REFRESH_INTERVAL=&quot;30000&quot;</span><span class="hljs-comment">#ETCD_PROXY_DIAL_TIMEOUT=&quot;1000&quot;</span><span class="hljs-comment">#ETCD_PROXY_WRITE_TIMEOUT=&quot;5000&quot;</span><span class="hljs-comment">#ETCD_PROXY_READ_TIMEOUT=&quot;0&quot;</span><span class="hljs-comment"># [security]</span>ETCD_CERT_FILE=<span class="hljs-string">&quot;/etc/etcd/ssl/etcd.pem&quot;</span>ETCD_KEY_FILE=<span class="hljs-string">&quot;/etc/etcd/ssl/etcd-key.pem&quot;</span>ETCD_CLIENT_CERT_AUTH=<span class="hljs-string">&quot;true&quot;</span>ETCD_TRUSTED_CA_FILE=<span class="hljs-string">&quot;/etc/etcd/ssl/etcd-root-ca.pem&quot;</span>ETCD_AUTO_TLS=<span class="hljs-string">&quot;true&quot;</span>ETCD_PEER_CERT_FILE=<span class="hljs-string">&quot;/etc/etcd/ssl/etcd.pem&quot;</span>ETCD_PEER_KEY_FILE=<span class="hljs-string">&quot;/etc/etcd/ssl/etcd-key.pem&quot;</span>ETCD_PEER_CLIENT_CERT_AUTH=<span class="hljs-string">&quot;true&quot;</span>ETCD_PEER_TRUSTED_CA_FILE=<span class="hljs-string">&quot;/etc/etcd/ssl/etcd-root-ca.pem&quot;</span>ETCD_PEER_AUTO_TLS=<span class="hljs-string">&quot;true&quot;</span><span class="hljs-comment"># [logging]</span><span class="hljs-comment">#ETCD_DEBUG=&quot;false&quot;</span><span class="hljs-comment"># examples for -log-package-levels etcdserver=WARNING,security=DEBUG</span><span class="hljs-comment">#ETCD_LOG_PACKAGE_LEVELS=&quot;&quot;</span></code></pre></div><p>最后启动集群并测试如下</p><div class="hljs code-wrapper"><pre><code class="hljs sh">systemctl daemon-reloadsystemctl start etcdsystemctl <span class="hljs-built_in">enable</span> etcd<span class="hljs-built_in">export</span> ETCDCTL_API=3etcdctl --cacert=/etc/etcd/ssl/etcd-root-ca.pem --cert=/etc/etcd/ssl/etcd.pem --key=/etc/etcd/ssl/etcd-key.pem --endpoints=https://10.10.1.5:2379,https://10.10.1.6:2379,https://10.10.1.7:2379 endpoint health</code></pre></div><p><img src="https://cdn.oss.link/markdown/ecrgr.jpg" alt="check etcd"></p><h3 id="三、搭建-Master-节点"><a href="#三、搭建-Master-节点" class="headerlink" title="三、搭建 Master 节点"></a>三、搭建 Master 节点</h3><h4 id="3-1、生成-Kubernetes-证书"><a href="#3-1、生成-Kubernetes-证书" class="headerlink" title="3.1、生成 Kubernetes 证书"></a>3.1、生成 Kubernetes 证书</h4><p><strong>生成证书配置文件需要借助 kubectl，所以先要安装一下 kubernetes-client 包</strong></p><div class="hljs code-wrapper"><pre><code class="hljs sh">rpm -ivh kubernetes-client-1.8.0-1.el7.centos.x86_64.rpm</code></pre></div><p>生成证书配置如下</p><h5 id="admin-csr-json"><a href="#admin-csr-json" class="headerlink" title="admin-csr.json"></a>admin-csr.json</h5><div class="hljs code-wrapper"><pre><code class="hljs json">&#123;  <span class="hljs-attr">&quot;CN&quot;</span>: <span class="hljs-string">&quot;admin&quot;</span>,  <span class="hljs-attr">&quot;hosts&quot;</span>: [],  <span class="hljs-attr">&quot;key&quot;</span>: &#123;    <span class="hljs-attr">&quot;algo&quot;</span>: <span class="hljs-string">&quot;rsa&quot;</span>,    <span class="hljs-attr">&quot;size&quot;</span>: <span class="hljs-number">2048</span>  &#125;,  <span class="hljs-attr">&quot;names&quot;</span>: [    &#123;      <span class="hljs-attr">&quot;C&quot;</span>: <span class="hljs-string">&quot;CN&quot;</span>,      <span class="hljs-attr">&quot;ST&quot;</span>: <span class="hljs-string">&quot;BeiJing&quot;</span>,      <span class="hljs-attr">&quot;L&quot;</span>: <span class="hljs-string">&quot;BeiJing&quot;</span>,      <span class="hljs-attr">&quot;O&quot;</span>: <span class="hljs-string">&quot;system:masters&quot;</span>,      <span class="hljs-attr">&quot;OU&quot;</span>: <span class="hljs-string">&quot;System&quot;</span>    &#125;  ]&#125;</code></pre></div><h5 id="k8s-gencert-json"><a href="#k8s-gencert-json" class="headerlink" title="k8s-gencert.json"></a>k8s-gencert.json</h5><div class="hljs code-wrapper"><pre><code class="hljs json">&#123;  <span class="hljs-attr">&quot;signing&quot;</span>: &#123;    <span class="hljs-attr">&quot;default&quot;</span>: &#123;      <span class="hljs-attr">&quot;expiry&quot;</span>: <span class="hljs-string">&quot;87600h&quot;</span>    &#125;,    <span class="hljs-attr">&quot;profiles&quot;</span>: &#123;      <span class="hljs-attr">&quot;kubernetes&quot;</span>: &#123;        <span class="hljs-attr">&quot;usages&quot;</span>: [            <span class="hljs-string">&quot;signing&quot;</span>,            <span class="hljs-string">&quot;key encipherment&quot;</span>,            <span class="hljs-string">&quot;server auth&quot;</span>,            <span class="hljs-string">&quot;client auth&quot;</span>        ],        <span class="hljs-attr">&quot;expiry&quot;</span>: <span class="hljs-string">&quot;87600h&quot;</span>      &#125;    &#125;  &#125;&#125;</code></pre></div><h5 id="k8s-root-ca-csr-json"><a href="#k8s-root-ca-csr-json" class="headerlink" title="k8s-root-ca-csr.json"></a>k8s-root-ca-csr.json</h5><div class="hljs code-wrapper"><pre><code class="hljs json">&#123;  <span class="hljs-attr">&quot;CN&quot;</span>: <span class="hljs-string">&quot;kubernetes&quot;</span>,  <span class="hljs-attr">&quot;key&quot;</span>: &#123;    <span class="hljs-attr">&quot;algo&quot;</span>: <span class="hljs-string">&quot;rsa&quot;</span>,    <span class="hljs-attr">&quot;size&quot;</span>: <span class="hljs-number">4096</span>  &#125;,  <span class="hljs-attr">&quot;names&quot;</span>: [    &#123;      <span class="hljs-attr">&quot;C&quot;</span>: <span class="hljs-string">&quot;CN&quot;</span>,      <span class="hljs-attr">&quot;ST&quot;</span>: <span class="hljs-string">&quot;BeiJing&quot;</span>,      <span class="hljs-attr">&quot;L&quot;</span>: <span class="hljs-string">&quot;BeiJing&quot;</span>,      <span class="hljs-attr">&quot;O&quot;</span>: <span class="hljs-string">&quot;k8s&quot;</span>,      <span class="hljs-attr">&quot;OU&quot;</span>: <span class="hljs-string">&quot;System&quot;</span>    &#125;  ]&#125;</code></pre></div><h5 id="kube-proxy-csr-json"><a href="#kube-proxy-csr-json" class="headerlink" title="kube-proxy-csr.json"></a>kube-proxy-csr.json</h5><div class="hljs code-wrapper"><pre><code class="hljs json">&#123;  <span class="hljs-attr">&quot;CN&quot;</span>: <span class="hljs-string">&quot;system:kube-proxy&quot;</span>,  <span class="hljs-attr">&quot;hosts&quot;</span>: [],  <span class="hljs-attr">&quot;key&quot;</span>: &#123;    <span class="hljs-attr">&quot;algo&quot;</span>: <span class="hljs-string">&quot;rsa&quot;</span>,    <span class="hljs-attr">&quot;size&quot;</span>: <span class="hljs-number">2048</span>  &#125;,  <span class="hljs-attr">&quot;names&quot;</span>: [    &#123;      <span class="hljs-attr">&quot;C&quot;</span>: <span class="hljs-string">&quot;CN&quot;</span>,      <span class="hljs-attr">&quot;ST&quot;</span>: <span class="hljs-string">&quot;BeiJing&quot;</span>,      <span class="hljs-attr">&quot;L&quot;</span>: <span class="hljs-string">&quot;BeiJing&quot;</span>,      <span class="hljs-attr">&quot;O&quot;</span>: <span class="hljs-string">&quot;k8s&quot;</span>,      <span class="hljs-attr">&quot;OU&quot;</span>: <span class="hljs-string">&quot;System&quot;</span>    &#125;  ]&#125;</code></pre></div><h5 id="kubernetes-csr-json"><a href="#kubernetes-csr-json" class="headerlink" title="kubernetes-csr.json"></a>kubernetes-csr.json</h5><div class="hljs code-wrapper"><pre><code class="hljs json">&#123;    <span class="hljs-attr">&quot;CN&quot;</span>: <span class="hljs-string">&quot;kubernetes&quot;</span>,    <span class="hljs-attr">&quot;hosts&quot;</span>: [        <span class="hljs-string">&quot;127.0.0.1&quot;</span>,        <span class="hljs-string">&quot;10.254.0.1&quot;</span>,        <span class="hljs-string">&quot;10.10.1.5&quot;</span>,        <span class="hljs-string">&quot;10.10.1.6&quot;</span>,        <span class="hljs-string">&quot;10.10.1.7&quot;</span>,        <span class="hljs-string">&quot;10.10.1.8&quot;</span>,        <span class="hljs-string">&quot;10.10.1.9&quot;</span>,        <span class="hljs-string">&quot;localhost&quot;</span>,        <span class="hljs-string">&quot;kubernetes&quot;</span>,        <span class="hljs-string">&quot;kubernetes.default&quot;</span>,        <span class="hljs-string">&quot;kubernetes.default.svc&quot;</span>,        <span class="hljs-string">&quot;kubernetes.default.svc.cluster&quot;</span>,        <span class="hljs-string">&quot;kubernetes.default.svc.cluster.local&quot;</span>    ],    <span class="hljs-attr">&quot;key&quot;</span>: &#123;        <span class="hljs-attr">&quot;algo&quot;</span>: <span class="hljs-string">&quot;rsa&quot;</span>,        <span class="hljs-attr">&quot;size&quot;</span>: <span class="hljs-number">2048</span>    &#125;,    <span class="hljs-attr">&quot;names&quot;</span>: [        &#123;            <span class="hljs-attr">&quot;C&quot;</span>: <span class="hljs-string">&quot;CN&quot;</span>,            <span class="hljs-attr">&quot;ST&quot;</span>: <span class="hljs-string">&quot;BeiJing&quot;</span>,            <span class="hljs-attr">&quot;L&quot;</span>: <span class="hljs-string">&quot;BeiJing&quot;</span>,            <span class="hljs-attr">&quot;O&quot;</span>: <span class="hljs-string">&quot;k8s&quot;</span>,            <span class="hljs-attr">&quot;OU&quot;</span>: <span class="hljs-string">&quot;System&quot;</span>        &#125;    ]&#125;</code></pre></div><p>最后生成证书及配置文件</p><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment"># 生成证书</span>cfssl gencert --initca=<span class="hljs-literal">true</span> k8s-root-ca-csr.json | cfssljson --bare k8s-root-ca<span class="hljs-keyword">for</span> targetName <span class="hljs-keyword">in</span> kubernetes admin kube-proxy; <span class="hljs-keyword">do</span>    cfssl gencert --ca k8s-root-ca.pem --ca-key k8s-root-ca-key.pem --config k8s-gencert.json --profile kubernetes <span class="hljs-variable">$targetName</span>-csr.json | cfssljson --bare <span class="hljs-variable">$targetName</span><span class="hljs-keyword">done</span><span class="hljs-comment"># 生成配置</span><span class="hljs-built_in">export</span> KUBE_APISERVER=<span class="hljs-string">&quot;https://127.0.0.1:6443&quot;</span><span class="hljs-built_in">export</span> BOOTSTRAP_TOKEN=$(head -c 16 /dev/urandom | od -An -t x | tr -d <span class="hljs-string">&#x27; &#x27;</span>)<span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;Tokne: <span class="hljs-variable">$&#123;BOOTSTRAP_TOKEN&#125;</span>&quot;</span>cat &gt; token.csv &lt;&lt;<span class="hljs-string">EOF</span><span class="hljs-string">$&#123;BOOTSTRAP_TOKEN&#125;,kubelet-bootstrap,10001,&quot;system:kubelet-bootstrap&quot;</span><span class="hljs-string">EOF</span><span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;Create kubelet bootstrapping kubeconfig...&quot;</span>kubectl config set-cluster kubernetes \  --certificate-authority=k8s-root-ca.pem \  --embed-certs=<span class="hljs-literal">true</span> \  --server=<span class="hljs-variable">$&#123;KUBE_APISERVER&#125;</span> \  --kubeconfig=bootstrap.kubeconfigkubectl config set-credentials kubelet-bootstrap \  --token=<span class="hljs-variable">$&#123;BOOTSTRAP_TOKEN&#125;</span> \  --kubeconfig=bootstrap.kubeconfigkubectl config set-context default \  --cluster=kubernetes \  --user=kubelet-bootstrap \  --kubeconfig=bootstrap.kubeconfigkubectl config use-context default --kubeconfig=bootstrap.kubeconfig<span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;Create kube-proxy kubeconfig...&quot;</span>kubectl config set-cluster kubernetes \  --certificate-authority=k8s-root-ca.pem \  --embed-certs=<span class="hljs-literal">true</span> \  --server=<span class="hljs-variable">$&#123;KUBE_APISERVER&#125;</span> \  --kubeconfig=kube-proxy.kubeconfigkubectl config set-credentials kube-proxy \  --client-certificate=kube-proxy.pem \  --client-key=kube-proxy-key.pem \  --embed-certs=<span class="hljs-literal">true</span> \  --kubeconfig=kube-proxy.kubeconfigkubectl config set-context default \  --cluster=kubernetes \  --user=kube-proxy \  --kubeconfig=kube-proxy.kubeconfigkubectl config use-context default --kubeconfig=kube-proxy.kubeconfig<span class="hljs-comment"># 生成高级审计配置</span>cat &gt;&gt; audit-policy.yaml &lt;&lt;<span class="hljs-string">EOF</span><span class="hljs-string"># Log all requests at the Metadata level.</span><span class="hljs-string">apiVersion: audit.k8s.io/v1beta1</span><span class="hljs-string">kind: Policy</span><span class="hljs-string">rules:</span><span class="hljs-string">- level: Metadata</span><span class="hljs-string">EOF</span></code></pre></div><h4 id="3-2、分发-rpm-及证书"><a href="#3-2、分发-rpm-及证书" class="headerlink" title="3.2、分发 rpm 及证书"></a>3.2、分发 rpm 及证书</h4><p>创建好证书以后就要进行分发，同时由于 Master 也作为 Node 使用，所以以下命令中在 Master 上也安装了 kubelet、kube-proxy 组件</p><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment"># 分发并安装 rpm</span><span class="hljs-keyword">for</span> IP <span class="hljs-keyword">in</span> `seq 5 7`; <span class="hljs-keyword">do</span>    scp kubernetes*.rpm root@10.10.1.<span class="hljs-variable">$IP</span>:~;     ssh root@10.10.1.<span class="hljs-variable">$IP</span> yum install -y kubernetes*.rpm<span class="hljs-keyword">done</span><span class="hljs-comment"># 分发证书</span><span class="hljs-keyword">for</span> IP <span class="hljs-keyword">in</span> `seq 5 7`;<span class="hljs-keyword">do</span>    ssh root@10.10.1.<span class="hljs-variable">$IP</span> mkdir /etc/kubernetes/ssl    scp *.pem root@10.10.1.<span class="hljs-variable">$IP</span>:/etc/kubernetes/ssl    scp *.kubeconfig token.csv audit-policy.yaml root@10.10.1.<span class="hljs-variable">$IP</span>:/etc/kubernetes    ssh root@10.10.1.<span class="hljs-variable">$IP</span> chown -R kube:kube /etc/kubernetes/ssl<span class="hljs-keyword">done</span><span class="hljs-comment"># 设置 log 目录权限</span><span class="hljs-keyword">for</span> IP <span class="hljs-keyword">in</span> `seq 5 7`;<span class="hljs-keyword">do</span>    ssh root@10.10.1.<span class="hljs-variable">$IP</span> mkdir -p /var/<span class="hljs-built_in">log</span>/kube-audit /usr/libexec/kubernetes    ssh root@10.10.1.<span class="hljs-variable">$IP</span> chown -R kube:kube /var/<span class="hljs-built_in">log</span>/kube-audit /usr/libexec/kubernetes    ssh root@10.10.1.<span class="hljs-variable">$IP</span> chmod -R 755 /var/<span class="hljs-built_in">log</span>/kube-audit /usr/libexec/kubernetes<span class="hljs-keyword">done</span></code></pre></div><h4 id="3-3、-搭建-Master-节点"><a href="#3-3、-搭建-Master-节点" class="headerlink" title="3.3、 搭建 Master 节点"></a>3.3、 搭建 Master 节点</h4><p>证书与 rpm 都安装完成后，只需要修改配置(配置位于 <code>/etc/kubernetes</code> 目录)后启动相关组件即可</p><ul><li>config 通用配置</li></ul><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment">###</span><span class="hljs-comment"># kubernetes system config</span><span class="hljs-comment">#</span><span class="hljs-comment"># The following values are used to configure various aspects of all</span><span class="hljs-comment"># kubernetes services, including</span><span class="hljs-comment">#</span><span class="hljs-comment">#   kube-apiserver.service</span><span class="hljs-comment">#   kube-controller-manager.service</span><span class="hljs-comment">#   kube-scheduler.service</span><span class="hljs-comment">#   kubelet.service</span><span class="hljs-comment">#   kube-proxy.service</span><span class="hljs-comment"># logging to stderr means we get it in the systemd journal</span>KUBE_LOGTOSTDERR=<span class="hljs-string">&quot;--logtostderr=true&quot;</span><span class="hljs-comment"># journal message level, 0 is debug</span>KUBE_LOG_LEVEL=<span class="hljs-string">&quot;--v=2&quot;</span><span class="hljs-comment"># Should this cluster be allowed to run privileged docker containers</span>KUBE_ALLOW_PRIV=<span class="hljs-string">&quot;--allow-privileged=true&quot;</span><span class="hljs-comment"># How the controller-manager, scheduler, and proxy find the apiserver</span>KUBE_MASTER=<span class="hljs-string">&quot;--master=http://127.0.0.1:8080&quot;</span></code></pre></div><h5 id="apiserver-配置"><a href="#apiserver-配置" class="headerlink" title="apiserver 配置"></a>apiserver 配置</h5><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment">###</span><span class="hljs-comment"># kubernetes system config</span><span class="hljs-comment">#</span><span class="hljs-comment"># The following values are used to configure the kube-apiserver</span><span class="hljs-comment">#</span><span class="hljs-comment"># The address on the local server to listen to.</span>KUBE_API_ADDRESS=<span class="hljs-string">&quot;--advertise-address=10.10.1.5 --insecure-bind-address=127.0.0.1 --bind-address=10.10.1.5&quot;</span><span class="hljs-comment"># The port on the local server to listen on.</span>KUBE_API_PORT=<span class="hljs-string">&quot;--insecure-port=8080 --secure-port=6443&quot;</span><span class="hljs-comment"># Port minions listen on</span><span class="hljs-comment"># KUBELET_PORT=&quot;--kubelet-port=10250&quot;</span><span class="hljs-comment"># Comma separated list of nodes in the etcd cluster</span>KUBE_ETCD_SERVERS=<span class="hljs-string">&quot;--etcd-servers=https://10.10.1.5:2379,https://10.10.1.6:2379,https://10.10.1.7:2379&quot;</span><span class="hljs-comment"># Address range to use for services</span>KUBE_SERVICE_ADDRESSES=<span class="hljs-string">&quot;--service-cluster-ip-range=10.254.0.0/16&quot;</span><span class="hljs-comment"># default admission control policies</span>KUBE_ADMISSION_CONTROL=<span class="hljs-string">&quot;--admission-control=NamespaceLifecycle,LimitRanger,SecurityContextDeny,ServiceAccount,ResourceQuota,NodeRestriction&quot;</span><span class="hljs-comment"># Add your own!</span>KUBE_API_ARGS=<span class="hljs-string">&quot;--authorization-mode=RBAC,Node \</span><span class="hljs-string">               --anonymous-auth=false \</span><span class="hljs-string">               --kubelet-https=true \</span><span class="hljs-string">               --enable-bootstrap-token-auth \</span><span class="hljs-string">               --token-auth-file=/etc/kubernetes/token.csv \</span><span class="hljs-string">               --service-node-port-range=30000-50000 \</span><span class="hljs-string">               --tls-cert-file=/etc/kubernetes/ssl/kubernetes.pem \</span><span class="hljs-string">               --tls-private-key-file=/etc/kubernetes/ssl/kubernetes-key.pem \</span><span class="hljs-string">               --client-ca-file=/etc/kubernetes/ssl/k8s-root-ca.pem \</span><span class="hljs-string">               --service-account-key-file=/etc/kubernetes/ssl/k8s-root-ca.pem \</span><span class="hljs-string">               --etcd-quorum-read=true \</span><span class="hljs-string">               --storage-backend=etcd3 \</span><span class="hljs-string">               --etcd-cafile=/etc/etcd/ssl/etcd-root-ca.pem \</span><span class="hljs-string">               --etcd-certfile=/etc/etcd/ssl/etcd.pem \</span><span class="hljs-string">               --etcd-keyfile=/etc/etcd/ssl/etcd-key.pem \</span><span class="hljs-string">               --enable-swagger-ui=true \</span><span class="hljs-string">               --apiserver-count=3 \</span><span class="hljs-string">               --audit-policy-file=/etc/kubernetes/audit-policy.yaml \</span><span class="hljs-string">               --audit-log-maxage=30 \</span><span class="hljs-string">               --audit-log-maxbackup=3 \</span><span class="hljs-string">               --audit-log-maxsize=100 \</span><span class="hljs-string">               --audit-log-path=/var/log/kube-audit/audit.log \</span><span class="hljs-string">               --event-ttl=1h&quot;</span></code></pre></div><p><strong>注意：API SERVER 对比 1.7 配置出现几项变动:</strong></p><ul><li>移除了 <code>--runtime-config=rbac.authorization.k8s.io/v1beta1</code> 配置，因为 RBAC 已经稳定，被纳入了 v1 api，不再需要指定开启</li><li><code>--authorization-mode</code> 授权模型增加了 <code>Node</code> 参数，因为 1.8 后默认 <code>system:node</code> role 不会自动授予 <code>system:nodes</code> 组，具体请参看 <a href="https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG.md#before-upgrading">CHANGELOG</a>(before-upgrading 段最后一条说明)</li><li>由于以上原因，<code>--admission-control</code> 同时增加了 <code>NodeRestriction</code> 参数，关于关于节点授权器请参考 <a href="https://kubernetes.io/docs/admin/authorization/node/">Using Node Authorization</a></li><li>增加 <code>--audit-policy-file</code> 参数用于指定高级审计配置，具体可参考 <a href="https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG.md#before-upgrading">CHANGELOG</a>(before-upgrading 第四条)、<a href="https://kubernetes.io/docs/tasks/debug-application-cluster/audit/#advanced-audit">Advanced audit</a></li><li>移除 <code>--experimental-bootstrap-token-auth</code> 参数，更换为 <code>--enable-bootstrap-token-auth</code>，详情参考 <a href="https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG.md#auth">CHANGELOG</a>(Auth 第二条)</li></ul><h5 id="controller-manager-配置"><a href="#controller-manager-配置" class="headerlink" title="controller-manager 配置"></a>controller-manager 配置</h5><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment">###</span><span class="hljs-comment"># The following values are used to configure the kubernetes controller-manager</span><span class="hljs-comment"># defaults from config and apiserver should be adequate</span><span class="hljs-comment"># Add your own!</span>KUBE_CONTROLLER_MANAGER_ARGS=<span class="hljs-string">&quot;--address=0.0.0.0 \</span><span class="hljs-string">                              --service-cluster-ip-range=10.254.0.0/16 \</span><span class="hljs-string">                              --cluster-name=kubernetes \</span><span class="hljs-string">                              --cluster-signing-cert-file=/etc/kubernetes/ssl/k8s-root-ca.pem \</span><span class="hljs-string">                              --cluster-signing-key-file=/etc/kubernetes/ssl/k8s-root-ca-key.pem \</span><span class="hljs-string">                              --experimental-cluster-signing-duration=87600h0m0s \</span><span class="hljs-string">                              --service-account-private-key-file=/etc/kubernetes/ssl/k8s-root-ca-key.pem \</span><span class="hljs-string">                              --root-ca-file=/etc/kubernetes/ssl/k8s-root-ca.pem \</span><span class="hljs-string">                              --leader-elect=true \</span><span class="hljs-string">                              --node-monitor-grace-period=40s \</span><span class="hljs-string">                              --node-monitor-period=5s \</span><span class="hljs-string">                              --pod-eviction-timeout=5m0s&quot;</span></code></pre></div><h5 id="scheduler-配置"><a href="#scheduler-配置" class="headerlink" title="scheduler 配置"></a>scheduler 配置</h5><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment">###</span><span class="hljs-comment"># kubernetes scheduler config</span><span class="hljs-comment"># default config should be adequate</span><span class="hljs-comment"># Add your own!</span>KUBE_SCHEDULER_ARGS=<span class="hljs-string">&quot;--leader-elect=true --address=0.0.0.0&quot;</span></code></pre></div><p>最后启动 Master 相关组件并验证</p><div class="hljs code-wrapper"><pre><code class="hljs sh">systemctl daemon-reloadsystemctl start kube-apiserversystemctl start kube-controller-managersystemctl start kube-schedulersystemctl <span class="hljs-built_in">enable</span> kube-apiserversystemctl <span class="hljs-built_in">enable</span> kube-controller-managersystemctl <span class="hljs-built_in">enable</span> kube-scheduler</code></pre></div><p><img src="https://cdn.oss.link/markdown/klnwa.jpg" alt="Master Success"></p><h3 id="四、搭建-Node-节点"><a href="#四、搭建-Node-节点" class="headerlink" title="四、搭建 Node 节点"></a>四、搭建 Node 节点</h3><h4 id="4-1、分发-rpm-及证书"><a href="#4-1、分发-rpm-及证书" class="headerlink" title="4.1、分发 rpm 及证书"></a>4.1、分发 rpm 及证书</h4><p>对于 Node 节点，只需要安装 <code>kubernetes-node</code> 即可，同时为了方便使用，这里也安装了 <code>kubernetes-client</code>，如下</p><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-keyword">for</span> IP <span class="hljs-keyword">in</span> `seq 8 9`;<span class="hljs-keyword">do</span>    scp kubernetes-node-1.8.0-1.el7.centos.x86_64.rpm kubernetes-client-1.8.0-1.el7.centos.x86_64.rpm root@10.10.1.<span class="hljs-variable">$IP</span>:~    ssh root@10.10.1.<span class="hljs-variable">$IP</span> yum install -y kubernetes-node-1.8.0-1.el7.centos.x86_64.rpm kubernetes-client-1.8.0-1.el7.centos.x86_64.rpm<span class="hljs-keyword">done</span></code></pre></div><p>同时还要分发相关证书；这里将 Etcd 证书已进行了分发，是因为 <strong>虽然 Node 节点上没有 Etcd，但是如果部署网络组件，如 calico、flannel 等时，网络组件需要联通 Etcd 就会用到 Etcd 的相关证书。</strong></p><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment"># 分发 Kubernetes 证书</span><span class="hljs-keyword">for</span> IP <span class="hljs-keyword">in</span> `seq 8 9`;<span class="hljs-keyword">do</span>    ssh root@10.10.1.<span class="hljs-variable">$IP</span> mkdir /etc/kubernetes/ssl    scp *.pem root@10.10.1.<span class="hljs-variable">$IP</span>:/etc/kubernetes/ssl    scp *.kubeconfig token.csv audit-policy.yaml root@10.10.1.<span class="hljs-variable">$IP</span>:/etc/kubernetes    ssh root@10.10.1.<span class="hljs-variable">$IP</span> chown -R kube:kube /etc/kubernetes/ssl<span class="hljs-keyword">done</span><span class="hljs-comment"># 分发 Etcd 证书</span><span class="hljs-keyword">for</span> IP <span class="hljs-keyword">in</span> `seq 8 9`;<span class="hljs-keyword">do</span>    ssh root@10.10.1.<span class="hljs-variable">$IP</span> mkdir -p /etc/etcd/ssl    scp *.pem root@10.10.1.<span class="hljs-variable">$IP</span>:/etc/etcd/ssl    ssh root@10.10.1.<span class="hljs-variable">$IP</span> chmod -R 644 /etc/etcd/ssl/*    ssh root@10.10.1.<span class="hljs-variable">$IP</span> chmod 755 /etc/etcd/ssl<span class="hljs-keyword">done</span></code></pre></div><h4 id="4-2、修改-Node-配置"><a href="#4-2、修改-Node-配置" class="headerlink" title="4.2、修改 Node 配置"></a>4.2、修改 Node 配置</h4><p>Node 上只需要修改 kubelet 和 kube-proxy 的配置即可</p><h5 id="config-通用配置"><a href="#config-通用配置" class="headerlink" title="config 通用配置"></a>config 通用配置</h5><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment">###</span><span class="hljs-comment"># kubernetes system config</span><span class="hljs-comment">#</span><span class="hljs-comment"># The following values are used to configure various aspects of all</span><span class="hljs-comment"># kubernetes services, including</span><span class="hljs-comment">#</span><span class="hljs-comment">#   kube-apiserver.service</span><span class="hljs-comment">#   kube-controller-manager.service</span><span class="hljs-comment">#   kube-scheduler.service</span><span class="hljs-comment">#   kubelet.service</span><span class="hljs-comment">#   kube-proxy.service</span><span class="hljs-comment"># logging to stderr means we get it in the systemd journal</span>KUBE_LOGTOSTDERR=<span class="hljs-string">&quot;--logtostderr=true&quot;</span><span class="hljs-comment"># journal message level, 0 is debug</span>KUBE_LOG_LEVEL=<span class="hljs-string">&quot;--v=2&quot;</span><span class="hljs-comment"># Should this cluster be allowed to run privileged docker containers</span>KUBE_ALLOW_PRIV=<span class="hljs-string">&quot;--allow-privileged=true&quot;</span><span class="hljs-comment"># How the controller-manager, scheduler, and proxy find the apiserver</span><span class="hljs-comment"># KUBE_MASTER=&quot;--master=http://127.0.0.1:8080&quot;</span></code></pre></div><h5 id="kubelet-配置"><a href="#kubelet-配置" class="headerlink" title="kubelet 配置"></a>kubelet 配置</h5><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment">###</span><span class="hljs-comment"># kubernetes kubelet (minion) config</span><span class="hljs-comment"># The address for the info server to serve on (set to 0.0.0.0 or &quot;&quot; for all interfaces)</span>KUBELET_ADDRESS=<span class="hljs-string">&quot;--address=10.10.1.8&quot;</span><span class="hljs-comment"># The port for the info server to serve on</span><span class="hljs-comment"># KUBELET_PORT=&quot;--port=10250&quot;</span><span class="hljs-comment"># You may leave this blank to use the actual hostname</span>KUBELET_HOSTNAME=<span class="hljs-string">&quot;--hostname-override=docker4.node&quot;</span><span class="hljs-comment"># location of the api-server</span><span class="hljs-comment"># KUBELET_API_SERVER=&quot;&quot;</span><span class="hljs-comment"># Add your own!</span>KUBELET_ARGS=<span class="hljs-string">&quot;--cgroup-driver=cgroupfs \</span><span class="hljs-string">              --cluster-dns=10.254.0.2 \</span><span class="hljs-string">              --resolv-conf=/etc/resolv.conf \</span><span class="hljs-string">              --experimental-bootstrap-kubeconfig=/etc/kubernetes/bootstrap.kubeconfig \</span><span class="hljs-string">              --kubeconfig=/etc/kubernetes/kubelet.kubeconfig \</span><span class="hljs-string">              --fail-swap-on=false \</span><span class="hljs-string">              --cert-dir=/etc/kubernetes/ssl \</span><span class="hljs-string">              --cluster-domain=cluster.local. \</span><span class="hljs-string">              --hairpin-mode=promiscuous-bridge \</span><span class="hljs-string">              --serialize-image-pulls=false \</span><span class="hljs-string">              --pod-infra-container-image=gcr.io/google_containers/pause-amd64:3.0&quot;</span></code></pre></div><p><strong>注意: kubelet 配置与 1.7 版本有一定改动</strong></p><ul><li>增加 <code>--fail-swap-on=false</code> 选项，否则可能导致在开启 swap 分区的机器上无法启动 kubelet，详细可参考 <a href="https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG.md#before-upgrading">CHANGELOG</a>(before-upgrading 第一条)</li><li>移除 <code>--require-kubeconfig</code> 选项，已经过时废弃</li></ul><h5 id="proxy-配置"><a href="#proxy-配置" class="headerlink" title="proxy 配置"></a>proxy 配置</h5><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment">###</span><span class="hljs-comment"># kubernetes proxy config</span><span class="hljs-comment"># default config should be adequate</span><span class="hljs-comment"># Add your own!</span>KUBE_PROXY_ARGS=<span class="hljs-string">&quot;--bind-address=10.10.1.8 \</span><span class="hljs-string">                 --hostname-override=docker4.node \</span><span class="hljs-string">                 --kubeconfig=/etc/kubernetes/kube-proxy.kubeconfig \</span><span class="hljs-string">                 --cluster-cidr=10.254.0.0/16&quot;</span></code></pre></div><p><strong>kube-proxy 配置与 1.7 并无改变，最新 1.8 的 ipvs 模式将单独写一篇文章，这里不做介绍</strong></p><h4 id="4-3、创建-Nginx-代理"><a href="#4-3、创建-Nginx-代理" class="headerlink" title="4.3、创建 Nginx 代理"></a>4.3、创建 Nginx 代理</h4><p>由于 HA 方案基于 Nginx 反代实现，所以每个 Node 要启动一个 Nginx 负载均衡 Master，具体参考 <a href="https://mritd.me/2017/07/21/set-up-kubernetes-ha-cluster-by-binary/#41ha-master-%E7%AE%80%E8%BF%B0">HA Master 简述</a></p><h5 id="nginx-conf"><a href="#nginx-conf" class="headerlink" title="nginx.conf"></a>nginx.conf</h5><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment"># 创建配置目录</span>mkdir -p /etc/nginx<span class="hljs-comment"># 写入代理配置</span>cat &lt;&lt; <span class="hljs-string">EOF &gt;&gt; /etc/nginx/nginx.conf</span><span class="hljs-string">error_log stderr notice;</span><span class="hljs-string"></span><span class="hljs-string">worker_processes auto;</span><span class="hljs-string">events &#123;</span><span class="hljs-string">  multi_accept on;</span><span class="hljs-string">  use epoll;</span><span class="hljs-string">  worker_connections 1024;</span><span class="hljs-string">&#125;</span><span class="hljs-string"></span><span class="hljs-string">stream &#123;</span><span class="hljs-string">    upstream kube_apiserver &#123;</span><span class="hljs-string">        least_conn;</span><span class="hljs-string">        server 10.10.1.5:6443;</span><span class="hljs-string">        server 10.10.1.6:6443;</span><span class="hljs-string">        server 10.10.1.7:6443;</span><span class="hljs-string">    &#125;</span><span class="hljs-string"></span><span class="hljs-string">    server &#123;</span><span class="hljs-string">        listen        0.0.0.0:6443;</span><span class="hljs-string">        proxy_pass    kube_apiserver;</span><span class="hljs-string">        proxy_timeout 10m;</span><span class="hljs-string">        proxy_connect_timeout 1s;</span><span class="hljs-string">    &#125;</span><span class="hljs-string">&#125;</span><span class="hljs-string">EOF</span><span class="hljs-comment"># 更新权限</span>chmod +r /etc/nginx/nginx.conf</code></pre></div><h5 id="nginx-proxy-service"><a href="#nginx-proxy-service" class="headerlink" title="nginx-proxy.service"></a>nginx-proxy.service</h5><div class="hljs code-wrapper"><pre><code class="hljs sh">cat &lt;&lt; <span class="hljs-string">EOF &gt;&gt; /etc/systemd/system/nginx-proxy.service</span><span class="hljs-string">[Unit]</span><span class="hljs-string">Description=kubernetes apiserver docker wrapper</span><span class="hljs-string">Wants=docker.socket</span><span class="hljs-string">After=docker.service</span><span class="hljs-string"></span><span class="hljs-string">[Service]</span><span class="hljs-string">User=root</span><span class="hljs-string">PermissionsStartOnly=true</span><span class="hljs-string">ExecStart=/usr/bin/docker run -p 127.0.0.1:6443:6443 \\</span><span class="hljs-string">                              -v /etc/nginx:/etc/nginx \\</span><span class="hljs-string">                              --name nginx-proxy \\</span><span class="hljs-string">                              --net=host \\</span><span class="hljs-string">                              --restart=on-failure:5 \\</span><span class="hljs-string">                              --memory=512M \\</span><span class="hljs-string">                              nginx:1.13.5-alpine</span><span class="hljs-string">ExecStartPre=-/usr/bin/docker rm -f nginx-proxy</span><span class="hljs-string">ExecStop=/usr/bin/docker stop nginx-proxy</span><span class="hljs-string">Restart=always</span><span class="hljs-string">RestartSec=15s</span><span class="hljs-string">TimeoutStartSec=30s</span><span class="hljs-string"></span><span class="hljs-string">[Install]</span><span class="hljs-string">WantedBy=multi-user.target</span><span class="hljs-string">EOF</span></code></pre></div><p><strong>最后启动 Nginx 代理即可</strong></p><div class="hljs code-wrapper"><pre><code class="hljs sh">systemctl daemon-reloadsystemctl start nginx-proxysystemctl <span class="hljs-built_in">enable</span> nginx-proxy</code></pre></div><h4 id="4-4、添加-Node"><a href="#4-4、添加-Node" class="headerlink" title="4.4、添加 Node"></a>4.4、添加 Node</h4><p>一切准备就绪后就可以添加 Node 了，首先由于我们采用了 <a href="https://kubernetes.io/docs/admin/kubelet-tls-bootstrapping/">TLS Bootstrapping</a>，所以需要先创建一个 ClusterRoleBinding</p><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment"># 在任意 master 执行即可</span>kubectl create clusterrolebinding kubelet-bootstrap \  --clusterrole=system:node-bootstrapper \  --user=kubelet-bootstrap</code></pre></div><p>然后启动 kubelet</p><div class="hljs code-wrapper"><pre><code class="hljs sh">systemctl daemon-reloadsystemctl start kubeletsystemctl <span class="hljs-built_in">enable</span> kubelet</code></pre></div><p>由于采用了 TLS Bootstrapping，所以 kubelet 启动后不会立即加入集群，而是进行证书申请，从日志中可以看到如下输出</p><div class="hljs code-wrapper"><pre><code class="hljs sh">10月 06 19:53:23 docker4.node kubelet[3797]: I1006 19:53:23.917261    3797 bootstrap.go:57] Using bootstrap kubeconfig to generate TLS client cert, key and kubeconfig file</code></pre></div><p>此时只需要在 master 允许其证书申请即可</p><div class="hljs code-wrapper"><pre><code class="hljs sh">kubectl get csr | grep Pending | awk <span class="hljs-string">&#x27;&#123;print $1&#125;&#x27;</span> | xargs kubectl certificate approve</code></pre></div><p>此时可以看到 Node 已经加入了</p><div class="hljs code-wrapper"><pre><code class="hljs sh">docker1.node ➜  ~ kubectl get nodeNAME           STATUS    ROLES     AGE       VERSIONdocker4.node   Ready     &lt;none&gt;    14m       v1.8.0docker5.node   Ready     &lt;none&gt;    3m        v1.8.0</code></pre></div><p>最后再启动 kube-proxy 即可</p><div class="hljs code-wrapper"><pre><code class="hljs sh">systemctl start kube-proxysystemctl <span class="hljs-built_in">enable</span> kube-proxy</code></pre></div><p><strong>再次提醒: 如果 kubelet 启动出现了类似 <code>system:node:xxxx</code> 用户没有权限访问 API 的 RBAC 错误，那么一定是 API Server 授权控制器、准入控制配置有问题，请仔细阅读上面的文档进行更改</strong></p><h4 id="4-5、Master-作为-Node"><a href="#4-5、Master-作为-Node" class="headerlink" title="4.5、Master 作为 Node"></a>4.5、Master 作为 Node</h4><p>如果想讲 Master 也作为 Node 的话，请在 Master 上安装 kubernete-node rpm 包，配置与上面基本一致；<strong>区别于 Master 上不需要启动 nginx 做负载均衡，同时 <code>bootstrap.kubeconfig</code>、<code>kube-proxy.kubeconfig</code> 中的 API Server 地址改成当前 Master IP 即可。</strong></p><p>最终成功后如下图所示</p><p><img src="https://cdn.oss.link/markdown/c4dde.jpg" alt="cluster success"></p><h3 id="五、部署-Calico"><a href="#五、部署-Calico" class="headerlink" title="五、部署 Calico"></a>五、部署 Calico</h3><h4 id="5-1、修改-Calico-配置"><a href="#5-1、修改-Calico-配置" class="headerlink" title="5.1、修改 Calico 配置"></a>5.1、修改 Calico 配置</h4><p>Calico 部署仍然采用 “混搭” 方式，即 Systemd 控制 calico node，cni 等由 kubernetes daemonset 安装，具体请参考 <a href="https://mritd.me/2017/07/31/calico-yml-bug/">Calico 部署踩坑记录</a>，以下直接上代码</p><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment"># 获取 calico.yaml</span>wget https://docs.projectcalico.org/v2.6/getting-started/kubernetes/installation/hosted/calico.yaml<span class="hljs-comment"># 替换 Etcd 地址</span>sed -i <span class="hljs-string">&#x27;s@.*etcd_endpoints:.*@\ \ etcd_endpoints:\ \&quot;https://10.10.1.5:2379,https://10.10.1.6:2379,https://10.10.1.7:2379\&quot;@gi&#x27;</span> calico.yaml<span class="hljs-comment"># 替换 Etcd 证书</span><span class="hljs-built_in">export</span> ETCD_CERT=`cat /etc/etcd/ssl/etcd.pem | base64 | tr -d <span class="hljs-string">&#x27;\n&#x27;</span>`<span class="hljs-built_in">export</span> ETCD_KEY=`cat /etc/etcd/ssl/etcd-key.pem | base64 | tr -d <span class="hljs-string">&#x27;\n&#x27;</span>`<span class="hljs-built_in">export</span> ETCD_CA=`cat /etc/etcd/ssl/etcd-root-ca.pem | base64 | tr -d <span class="hljs-string">&#x27;\n&#x27;</span>`sed -i <span class="hljs-string">&quot;s@.*etcd-cert:.*@\ \ etcd-cert:\ <span class="hljs-variable">$&#123;ETCD_CERT&#125;</span>@gi&quot;</span> calico.yamlsed -i <span class="hljs-string">&quot;s@.*etcd-key:.*@\ \ etcd-key:\ <span class="hljs-variable">$&#123;ETCD_KEY&#125;</span>@gi&quot;</span> calico.yamlsed -i <span class="hljs-string">&quot;s@.*etcd-ca:.*@\ \ etcd-ca:\ <span class="hljs-variable">$&#123;ETCD_CA&#125;</span>@gi&quot;</span> calico.yamlsed -i <span class="hljs-string">&#x27;s@.*etcd_ca:.*@\ \ etcd_ca:\ &quot;/calico-secrets/etcd-ca&quot;@gi&#x27;</span> calico.yamlsed -i <span class="hljs-string">&#x27;s@.*etcd_cert:.*@\ \ etcd_cert:\ &quot;/calico-secrets/etcd-cert&quot;@gi&#x27;</span> calico.yamlsed -i <span class="hljs-string">&#x27;s@.*etcd_key:.*@\ \ etcd_key:\ &quot;/calico-secrets/etcd-key&quot;@gi&#x27;</span> calico.yaml<span class="hljs-comment"># 注释掉 calico-node 部分(由 Systemd 接管)</span>sed -i <span class="hljs-string">&#x27;103,189s@.*@#&amp;@gi&#x27;</span> calico.yaml</code></pre></div><h4 id="5-2、创建-Systemd-文件"><a href="#5-2、创建-Systemd-文件" class="headerlink" title="5.2、创建 Systemd 文件"></a>5.2、创建 Systemd 文件</h4><p>上一步注释了 <code>calico.yaml</code> 中 Calico Node 相关内容，为了防止自动获取 IP 出现问题，将其移动到 Systemd，Systemd service 配置如下，<strong>每个节点都要安装 calico-node 的 Service</strong>，其他节点请自行修改 ip(被问我为啥是两个反引号 <code>\\</code>，自己试就知道了)</p><div class="hljs code-wrapper"><pre><code class="hljs sh">cat &gt; /usr/lib/systemd/system/calico-node.service &lt;&lt;<span class="hljs-string">EOF</span><span class="hljs-string">[Unit]</span><span class="hljs-string">Description=calico node</span><span class="hljs-string">After=docker.service</span><span class="hljs-string">Requires=docker.service</span><span class="hljs-string"></span><span class="hljs-string">[Service]</span><span class="hljs-string">User=root</span><span class="hljs-string">PermissionsStartOnly=true</span><span class="hljs-string">ExecStart=/usr/bin/docker run   --net=host --privileged --name=calico-node \\</span><span class="hljs-string">                                -e ETCD_ENDPOINTS=https://10.10.1.5:2379,https://10.10.1.6:2379,https://10.10.1.7:2379 \\</span><span class="hljs-string">                                -e ETCD_CA_CERT_FILE=/etc/etcd/ssl/etcd-root-ca.pem \\</span><span class="hljs-string">                                -e ETCD_CERT_FILE=/etc/etcd/ssl/etcd.pem \\</span><span class="hljs-string">                                -e ETCD_KEY_FILE=/etc/etcd/ssl/etcd-key.pem \\</span><span class="hljs-string">                                -e NODENAME=docker1.node \\</span><span class="hljs-string">                                -e IP=10.10.1.5 \\</span><span class="hljs-string">                                -e IP6= \\</span><span class="hljs-string">                                -e AS= \\</span><span class="hljs-string">                                -e CALICO_IPV4POOL_CIDR=10.20.0.0/16 \\</span><span class="hljs-string">                                -e CALICO_IPV4POOL_IPIP=always \\</span><span class="hljs-string">                                -e CALICO_LIBNETWORK_ENABLED=true \\</span><span class="hljs-string">                                -e CALICO_NETWORKING_BACKEND=bird \\</span><span class="hljs-string">                                -e CALICO_DISABLE_FILE_LOGGING=true \\</span><span class="hljs-string">                                -e FELIX_IPV6SUPPORT=false \\</span><span class="hljs-string">                                -e FELIX_DEFAULTENDPOINTTOHOSTACTION=ACCEPT \\</span><span class="hljs-string">                                -e FELIX_LOGSEVERITYSCREEN=info \\</span><span class="hljs-string">                                -v /etc/etcd/ssl/etcd-root-ca.pem:/etc/etcd/ssl/etcd-root-ca.pem \\</span><span class="hljs-string">                                -v /etc/etcd/ssl/etcd.pem:/etc/etcd/ssl/etcd.pem \\</span><span class="hljs-string">                                -v /etc/etcd/ssl/etcd-key.pem:/etc/etcd/ssl/etcd-key.pem \\</span><span class="hljs-string">                                -v /var/run/calico:/var/run/calico \\</span><span class="hljs-string">                                -v /lib/modules:/lib/modules \\</span><span class="hljs-string">                                -v /run/docker/plugins:/run/docker/plugins \\</span><span class="hljs-string">                                -v /var/run/docker.sock:/var/run/docker.sock \\</span><span class="hljs-string">                                -v /var/log/calico:/var/log/calico \\</span><span class="hljs-string">                                quay.io/calico/node:v2.6.1</span><span class="hljs-string">ExecStop=/usr/bin/docker rm -f calico-node</span><span class="hljs-string">Restart=always</span><span class="hljs-string">RestartSec=10</span><span class="hljs-string"></span><span class="hljs-string">[Install]</span><span class="hljs-string">WantedBy=multi-user.target</span><span class="hljs-string">EOF</span></code></pre></div><h4 id="5-3、修改-kubelet-配置"><a href="#5-3、修改-kubelet-配置" class="headerlink" title="5.3、修改 kubelet 配置"></a>5.3、修改 kubelet 配置</h4><p>根据官方文档要求 <code>kubelet</code> 配置必须增加 <code>--network-plugin=cni</code> 选项，所以需要修改 kubelet 配置</p><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment">###</span><span class="hljs-comment"># kubernetes kubelet (minion) config</span><span class="hljs-comment"># The address for the info server to serve on (set to 0.0.0.0 or &quot;&quot; for all interfaces)</span>KUBELET_ADDRESS=<span class="hljs-string">&quot;--address=10.10.1.5&quot;</span><span class="hljs-comment"># The port for the info server to serve on</span><span class="hljs-comment"># KUBELET_PORT=&quot;--port=10250&quot;</span><span class="hljs-comment"># You may leave this blank to use the actual hostname</span>KUBELET_HOSTNAME=<span class="hljs-string">&quot;--hostname-override=docker1.node&quot;</span><span class="hljs-comment"># location of the api-server</span><span class="hljs-comment"># KUBELET_API_SERVER=&quot;&quot;</span><span class="hljs-comment"># Add your own!</span>KUBELET_ARGS=<span class="hljs-string">&quot;--cgroup-driver=cgroupfs \</span><span class="hljs-string">              --network-plugin=cni \</span><span class="hljs-string">              --cluster-dns=10.254.0.2 \</span><span class="hljs-string">              --resolv-conf=/etc/resolv.conf \</span><span class="hljs-string">              --experimental-bootstrap-kubeconfig=/etc/kubernetes/bootstrap.kubeconfig \</span><span class="hljs-string">              --kubeconfig=/etc/kubernetes/kubelet.kubeconfig \</span><span class="hljs-string">              --fail-swap-on=false \</span><span class="hljs-string">              --cert-dir=/etc/kubernetes/ssl \</span><span class="hljs-string">              --cluster-domain=cluster.local. \</span><span class="hljs-string">              --hairpin-mode=promiscuous-bridge \</span><span class="hljs-string">              --serialize-image-pulls=false \</span><span class="hljs-string">              --pod-infra-container-image=gcr.io/google_containers/pause-amd64:3.0&quot;</span></code></pre></div><p>然后重启即可</p><div class="hljs code-wrapper"><pre><code class="hljs sh">systemctl daemon-reloadsystemctl restart kubelet</code></pre></div><p>此时执行 <code>kubectl get node</code> 会看到 Node 为 <code>NotReady</code> 状态，属于正常情况</p><h4 id="5-4、创建-Calico-Daemonset"><a href="#5-4、创建-Calico-Daemonset" class="headerlink" title="5.4、创建 Calico Daemonset"></a>5.4、创建 Calico Daemonset</h4><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment"># 先创建 RBAC</span>kubectl apply -f https://docs.projectcalico.org/v2.6/getting-started/kubernetes/installation/rbac.yaml<span class="hljs-comment"># 再创建 Calico Daemonset</span>kubectl create -f calico.yaml</code></pre></div><h4 id="5-5、创建-Calico-Node"><a href="#5-5、创建-Calico-Node" class="headerlink" title="5.5、创建 Calico Node"></a>5.5、创建 Calico Node</h4><p>Calico Node 采用 Systemd 方式启动，在每个节点配置好 Systemd service后，<strong>每个节点修改对应的 <code>calico-node.service</code> 中的 IP 和节点名称，然后启动即可</strong></p><div class="hljs code-wrapper"><pre><code class="hljs sh">systemctl daemon-reloadsystemctl restart calico-nodesleep 5systemctl restart kubelet</code></pre></div><p>此时检查 Node 应该都处于 Ready 状态</p><p><img src="https://cdn.oss.link/markdown/agxp3.jpg" alt="Node Ready"></p><p><strong>最后测试一下跨主机通讯</strong></p><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment"># 创建 deployment</span>cat &lt;&lt; <span class="hljs-string">EOF &gt;&gt; demo.deploy.yml</span><span class="hljs-string">apiVersion: apps/v1beta2</span><span class="hljs-string">kind: Deployment</span><span class="hljs-string">metadata:</span><span class="hljs-string">  name: demo-deployment</span><span class="hljs-string">spec:</span><span class="hljs-string">  replicas: 5</span><span class="hljs-string">  selector:</span><span class="hljs-string">    matchLabels:</span><span class="hljs-string">      app: demo</span><span class="hljs-string">  template:</span><span class="hljs-string">    metadata:</span><span class="hljs-string">      labels:</span><span class="hljs-string">        app: demo</span><span class="hljs-string">    spec:</span><span class="hljs-string">      containers:</span><span class="hljs-string">      - name: demo</span><span class="hljs-string">        image: mritd/demo</span><span class="hljs-string">        imagePullPolicy: IfNotPresent</span><span class="hljs-string">        ports:</span><span class="hljs-string">        - containerPort: 80</span><span class="hljs-string">EOF</span>kubectl create -f demo.deploy.yml</code></pre></div><p><strong>进入其中一个 Pod，ping 另一个 Pod 的 IP 测试即可</strong></p><p><img src="https://cdn.oss.link/markdown/00krx.jpg" alt="Test Calico"></p><h3 id="六、部署-DNS"><a href="#六、部署-DNS" class="headerlink" title="六、部署 DNS"></a>六、部署 DNS</h3><h4 id="6-1、部署集群-DNS"><a href="#6-1、部署集群-DNS" class="headerlink" title="6.1、部署集群 DNS"></a>6.1、部署集群 DNS</h4><p>DNS 组件部署非常简单，直接创建相应的 deployment 等即可；但是有一个事得说一嘴，Kubernets 一直在推那个 <code>Addon Manager</code> 的工具来管理 DNS 啥的，文档说的条条是道，就是不希望我们手动搞这些东西，防止意外修改云云… 但问题是关于那个 <code>Addon Manager</code> 咋用一句没提，虽然说里面就一个小脚本，看看也能懂；但是我还是选择手动 😌… 还有这个 DNS 配置文件好像又挪地方了，以前在 <code>contrib</code> 项目下的…</p><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment"># 获取文件</span>wget https://raw.githubusercontent.com/kubernetes/kubernetes/master/cluster/addons/dns/kube-dns.yaml.sedmv kube-dns.yaml.sed kube-dns.yaml<span class="hljs-comment"># 修改配置</span>sed -i <span class="hljs-string">&#x27;s/$DNS_DOMAIN/cluster.local/gi&#x27;</span> kube-dns.yamlsed -i <span class="hljs-string">&#x27;s/$DNS_SERVER_IP/10.254.0.2/gi&#x27;</span> kube-dns.yaml<span class="hljs-comment"># 创建</span>kubectl create -f kube-dns.yaml</code></pre></div><p>创建好以后如下所示</p><p><img src="https://cdn.oss.link/markdown/vg95n.jpg" alt="DNS"></p><p>然后创建两组 Pod 和 Service，进入 Pod 中 curl 另一个 Service 名称看看是否能解析；同时还要测试一下外网能否解析</p><p><img src="https://cdn.oss.link/markdown/x185c.jpg" alt="Test DNS1"></p><p>测试外网</p><p><img src="https://cdn.oss.link/markdown/3k9gz.jpg" alt="Test DNS2"></p><h4 id="6-2、部署-DNS-自动扩容部署"><a href="#6-2、部署-DNS-自动扩容部署" class="headerlink" title="6.2、部署 DNS 自动扩容部署"></a>6.2、部署 DNS 自动扩容部署</h4><p>这个同样下载 yaml，然后创建一下即可，不需要修改任何配置</p><div class="hljs code-wrapper"><pre><code class="hljs sh">wget https://raw.githubusercontent.com/kubernetes/kubernetes/master/cluster/addons/dns-horizontal-autoscaler/dns-horizontal-autoscaler.yamlkubectl create -f dns-horizontal-autoscaler.yaml</code></pre></div><p>部署完成后如下</p><p><img src="https://cdn.oss.link/markdown/mid1u.jpg" alt="DNS autoscaler"></p><p>自动扩容这里不做测试了，虚拟机吃不消了，详情自己参考 <a href="https://kubernetes.io/docs/tasks/administer-cluster/dns-horizontal-autoscaling/">Autoscale the DNS Service in a Cluster</a></p><p><strong>kube-proxy ipvs 下一篇写，坑有点多，虽然搞定了，但是一篇写有点囫囵吞枣，后来想一想还是分开吧</strong></p>]]></content>
    
    
    <summary type="html">目前 Kubernetes 1.8.0 已经发布，1.8.0增加了很多新特性，比如 kube-proxy 组建的 ipvs 模式等，同时 RBAC 授权也做了一些调整，国庆没事干，所以试了一下；以下记录了 Kubernetes 1.8.0 的搭建过程</summary>
    
    
    
    <category term="Kubernetes" scheme="https://mritd.com/categories/kubernetes/"/>
    
    
    <category term="Linux" scheme="https://mritd.com/tags/linux/"/>
    
    <category term="Docker" scheme="https://mritd.com/tags/docker/"/>
    
    <category term="Kubernetes" scheme="https://mritd.com/tags/kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>Alpine 3.6 OpenJDK 8 Bug</title>
    <link href="https://mritd.com/2017/09/27/alpine-3.6-openjdk-8-bug/"/>
    <id>https://mritd.com/2017/09/27/alpine-3.6-openjdk-8-bug/</id>
    <published>2017-09-27T12:43:12.000Z</published>
    <updated>2017-09-27T12:43:12.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>最近切换项目基础镜像踩到一个大坑，由于 alpine 基础镜像和 OpenJDK8 Bug 导致鼓捣了2天才解决，故记录一下这个问题</p></blockquote><h3 id="一、问题环境"><a href="#一、问题环境" class="headerlink" title="一、问题环境"></a>一、问题环境</h3><p>出现问题的基本环境如下</p><ul><li>OpneJDK 8u131</li><li>Alpine 3.6</li><li>Kaptcha (Java 验证码库)</li></ul><h3 id="二、问题描述"><a href="#二、问题描述" class="headerlink" title="二、问题描述"></a>二、问题描述</h3><p>出现问题表象为 <strong>Spring Boot 项目启动后，访问注册页(有验证码)时，验证码不显示，后台报错信息大意为缺失字体库，安装字体后会报错说 <code>libfontmanager.so: AWTFontDefaultChar: symbol not found</code></strong></p><h3 id="三、解决方案"><a href="#三、解决方案" class="headerlink" title="三、解决方案"></a>三、解决方案</h3><p>当出现字体找不到这种错误时，原因是 <strong>Alpine 太过精简，导致里面没有字体，只需要安装字体即可</strong>，在 Dockerfile 中添加如下命令即可:</p><div class="hljs code-wrapper"><pre><code class="hljs sh">apk add --update font-adobe-100dpi ttf-dejavu fontconfig</code></pre></div><p>当安装字体后，可能会出现如下错误:</p><div class="hljs code-wrapper"><pre><code class="hljs sh">Caused by: java.lang.UnsatisfiedLinkError: /usr/lib/jvm/java-1.8-openjdk/jre/lib/amd64/libfontmanager.so: Error relocating /usr/lib/jvm/java-1.8-openjdk/jre/lib/amd64/libfontmanager.so: AWTFontDefaultChar: symbol not found    at java.lang.ClassLoader<span class="hljs-variable">$NativeLibrary</span>.load(Native Method)    at java.lang.ClassLoader.loadLibrary0(ClassLoader.java:1941)    at java.lang.ClassLoader.loadLibrary(ClassLoader.java:1845)    at java.lang.Runtime.loadLibrary0(Runtime.java:870)    at java.lang.System.loadLibrary(System.java:1122)    at sun.font.FontManagerNativeLibrary<span class="hljs-variable">$1</span>.run(FontManagerNativeLibrary.java:61)    at java.security.AccessController.doPrivileged(Native Method)    at sun.font.FontManagerNativeLibrary.&lt;clinit&gt;(FontManagerNativeLibrary.java:32)    at sun.font.SunFontManager<span class="hljs-variable">$1</span>.run(SunFontManager.java:339)    at java.security.AccessController.doPrivileged(Native Method)    at sun.font.SunFontManager.&lt;clinit&gt;(SunFontManager.java:335)    at java.lang.Class.forName0(Native Method)    at java.lang.Class.forName(Class.java:348)    at sun.font.FontManagerFactory<span class="hljs-variable">$1</span>.run(FontManagerFactory.java:82)    at java.security.AccessController.doPrivileged(Native Method)    at sun.font.FontManagerFactory.getInstance(FontManagerFactory.java:74)    at java.awt.Font.getFont2D(Font.java:491)    at java.awt.Font.getFamily(Font.java:1220)    at java.awt.Font.getFamily_NoClientCode(Font.java:1194)    at java.awt.Font.getFamily(Font.java:1186)    at java.awt.Font.toString(Font.java:1683)    at hudson.util.ChartUtil.&lt;clinit&gt;(ChartUtil.java:260)    at hudson.WebAppMain.contextInitialized(WebAppMain.java:194)    ... 23 more</code></pre></div><p>Google 半天，最后找到了 <a href="https://bugs.alpinelinux.org/issues/7372">Alpine 官方 Bug 列表</a>，在最后面做了回复，其中大意是: <strong>Alpine 3.6 版本的 Docker 镜像中安装的是 OpenJDK 8u131，这个版本有 BUG，并且在 3.6.3 的 OpenJDK 8.141.15 版本做了修复</strong>；从上面可知我们解决方案有两个:</p><ul><li>降级到 Alpine 3.5，其内的 OpneJDK 是 8u121 版本，没有这个 Bug</li><li>升级到 Alpine Edge，其内部 OpenJDK 版本为 8.144.01，已经修复了这个 Bug</li></ul><p>当然我选择浪一波，做了升级，最终基础镜像的 Dockerfile 如下</p><div class="hljs code-wrapper"><pre><code class="hljs sh">FROM alpine:edgeLABEL maintainer=<span class="hljs-string">&quot;mritd &lt;mritd1234@gmail.com&gt;&quot;</span>ENV JAVA_HOME /usr/lib/jvm/java-1.8-openjdkENV PATH <span class="hljs-variable">$PATH</span>:/usr/lib/jvm/java-1.8-openjdk/jre/bin:/usr/lib/jvm/java-1.8-openjdk/binENV JAVA_VERSION 8u144ENV JAVA_ALPINE_VERSION 8.144.01-r0RUN apk add --update bash curl tar wget ca-certificates unzip \        openjdk8=<span class="hljs-variable">$&#123;JAVA_ALPINE_VERSION&#125;</span> font-adobe-100dpi ttf-dejavu fontconfig \    &amp;&amp; rm -rf /var/cache/apk/* \CMD [<span class="hljs-string">&quot;bash&quot;</span>]</code></pre></div>]]></content>
    
    
    <summary type="html">记录一下 Alpine 3.6 OpneJDK 8u131 的 BUG</summary>
    
    
    
    <category term="Docker" scheme="https://mritd.com/categories/docker/"/>
    
    
    <category term="Java" scheme="https://mritd.com/tags/java/"/>
    
    <category term="Linux" scheme="https://mritd.com/tags/linux/"/>
    
  </entry>
  
  <entry>
    <title>Consul 集群搭建</title>
    <link href="https://mritd.com/2017/09/21/set-up-ha-consul-cluster/"/>
    <id>https://mritd.com/2017/09/21/set-up-ha-consul-cluster/</id>
    <published>2017-09-21T14:50:28.000Z</published>
    <updated>2017-09-21T14:50:28.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>不知道 Consul 用的人多还是少，最近有人问怎么搭建 Consul 集群，这里顺手记录一下吧</p></blockquote><h3 id="一、简介"><a href="#一、简介" class="headerlink" title="一、简介"></a>一、简介</h3><p>Consul 与 Etcd 一样，都属于分布式一致性数据库，其主要特性就是在分布式系统中出现意外情况如节点宕机的情况下保证数据的一致性；相对于 Etcd 来说，Consul 提供了更加实用的其他功能特性，如 DNS、健康检查、服务发现、多数据中心等，同时还有 web ui 界面，体验相对于更加友好</p><h3 id="二、环境准备"><a href="#二、环境准备" class="headerlink" title="二、环境准备"></a>二、环境准备</h3><p>同 Etcd 一样，Consul 最少也需要 3 台机器，这里测试实用 5 台机器进行部署集群，具体环境如下</p><table><thead><tr><th>节点</th><th>IP</th><th>Version</th></tr></thead><tbody><tr><td>server</td><td>192.168.1.11</td><td>v0.9.3</td></tr><tr><td>server</td><td>192.168.1.12</td><td>v0.9.3</td></tr><tr><td>server</td><td>192.168.1.13</td><td>v0.9.3</td></tr><tr><td>client</td><td>192.168.1.14</td><td>v0.9.3</td></tr><tr><td>client</td><td>192.168.1.15</td><td>v0.9.3</td></tr></tbody></table><p>其中 consul 采用 rpm 包的形式进行安装，这里并没有使用 docker 方式启动是因为个人习惯重要的数据存储服务交给 systemd管理；因为 docker 存在 docker daemon 的原因，如果用 docker 启动这种存储核心数据的组件，一但 daemon 出现问题那么所有容器都将出现问题；所以个人还是比较习惯将 etcd 和 consul 以二进制装在宿主机，由 systemd 直接管理。</p><h3 id="三、部署集群"><a href="#三、部署集群" class="headerlink" title="三、部署集群"></a>三、部署集群</h3><h4 id="3-1、Consul-集群模式"><a href="#3-1、Consul-集群模式" class="headerlink" title="3.1、Consul 集群模式"></a>3.1、Consul 集群模式</h4><p>Consul 集群与 Etcd 略有区别，<strong>Consul 在启动后分为两种模式:</strong></p><ul><li>Server 模式: 一个 Server 是一个有一组扩展功能的代理，这些功能包括参与 Raft 选举，维护集群状态，响应 RPC 查询，与其他数据中心交互 WAN gossip 和转发查询给 leader 或者远程数据中心。</li><li>Client 模式: 一个 Client 是一个转发所有 RPC 到 Server 的代理。这个 Client 是相对无状态的；Client 唯一执行的后台活动是加入 LAN gossip 池，这有一个最低的资源开销并且仅消耗少量的网络带宽。</li></ul><p><strong>其集群后如下所示:</strong></p><p><img src="https://cdn.oss.link/markdown/n4mdw.jpg" alt="Consul Cluster"></p><h4 id="3-2、集群搭建"><a href="#3-2、集群搭建" class="headerlink" title="3.2、集群搭建"></a>3.2、集群搭建</h4><p>Consul 集群搭建时一般提供两种模式:</p><ul><li><strong>手动模式: 启动第一个节点后，此时此节点处于 bootstrap 模式，其节点手动执行加入</strong></li><li><strong>自动模式: 启动第一个节点后，在其他节点配置好尝试加入的目标节点，然后等待其自动加入(不需要人为命令加入)</strong></li></ul><p>这里采用自动加入模式，搭建过程如下:</p><p><strong>首先获取 Consul 的 rpm 包，鉴于官方并未提供 rpm 安装包，所以我自己造了一个轮子，打包脚本见 <a href="https://github.com/mritd/consul-rpm">Github</a>，以下直接从我的 yum 源中安装</strong></p><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment"># 安装 yum 源</span>tee /etc/yum.repos.d/mritd.repo &lt;&lt; <span class="hljs-string">EOF</span><span class="hljs-string">[mritdrepo]</span><span class="hljs-string">name=Mritd Repository</span><span class="hljs-string">baseurl=https://yumrepo.b0.upaiyun.com/centos/7/x86_64</span><span class="hljs-string">enabled=1</span><span class="hljs-string">gpgcheck=1</span><span class="hljs-string">gpgkey=https://cdn.oss.link/keys/rpm.public.key</span><span class="hljs-string">EOF</span><span class="hljs-comment"># 安装 Consul，请不要在大规模部署时使用此 yum 源，CDN 流量不多请手下留情，</span><span class="hljs-comment"># 如需大规模部署 请使用 yumdonwloader 工具下载 rpm 后手动分发安装</span>yum install -y consul</code></pre></div><p><strong>5 台机器安装好后修改其中三台为 Server 模式并启动</strong></p><div class="hljs code-wrapper"><pre><code class="hljs sh">vim /etc/consul/consul.json<span class="hljs-comment"># 配置如下</span>&#123;    <span class="hljs-string">&quot;datacenter&quot;</span>: <span class="hljs-string">&quot;dc1&quot;</span>,                // 数据中心名称    <span class="hljs-string">&quot;data_dir&quot;</span>: <span class="hljs-string">&quot;/var/lib/consul&quot;</span>,      // Server 节点数据目录    <span class="hljs-string">&quot;log_level&quot;</span>: <span class="hljs-string">&quot;INFO&quot;</span>,                // 日志级别    <span class="hljs-string">&quot;node_name&quot;</span>: <span class="hljs-string">&quot;docker1.node&quot;</span>,        // 当前节点名称    <span class="hljs-string">&quot;server&quot;</span>: <span class="hljs-literal">true</span>,                     // 是否为 Server 模式，<span class="hljs-literal">false</span> 为 Client 模式    <span class="hljs-string">&quot;ui&quot;</span>: <span class="hljs-literal">true</span>,                         // 是否开启 UI 访问    <span class="hljs-string">&quot;bootstrap_expect&quot;</span>: 1,              // 启动时期望的就绪节点，1 代表启动为 bootstrap 模式，等待其他节点加入    <span class="hljs-string">&quot;bind_addr&quot;</span>: <span class="hljs-string">&quot;192.168.1.11&quot;</span>,        // 绑定的 IP    <span class="hljs-string">&quot;client_addr&quot;</span>: <span class="hljs-string">&quot;192.168.1.11&quot;</span>,      // 同时作为 Client 接受请求的绑定 IP    <span class="hljs-string">&quot;retry_join&quot;</span>: [<span class="hljs-string">&quot;192.168.1.12&quot;</span>,<span class="hljs-string">&quot;192.168.1.13&quot;</span>],  // 尝试加入的其他节点    <span class="hljs-string">&quot;retry_interval&quot;</span>: <span class="hljs-string">&quot;3s&quot;</span>,             // 每次尝试间隔    <span class="hljs-string">&quot;raft_protocol&quot;</span>: 3,                 // Raft 协议版本    <span class="hljs-string">&quot;enable_debug&quot;</span>: <span class="hljs-literal">false</span>,              // 是否开启 Debug 模式    <span class="hljs-string">&quot;rejoin_after_leave&quot;</span>: <span class="hljs-literal">true</span>,         // 允许重新加入集群    <span class="hljs-string">&quot;enable_syslog&quot;</span>: <span class="hljs-literal">false</span>              // 是否开启 syslog&#125;</code></pre></div><p><strong>另外两个节点与以上配置大致相同，差别在于其他两个 Server 节点 <code>bootstrap_expect</code> 值为 2，即期望启动时已经有两个节点就绪；然后依次启动三个 Server 节点即可</strong></p><div class="hljs code-wrapper"><pre><code class="hljs sh">systemctl start consulsystemctl <span class="hljs-built_in">enable</span> consulsystemctl status consul</code></pre></div><p><strong>此时可访问任意一台 Server 节点的 UI 界面，地址为 <code>http://serverIP:8500</code>，截图如下</strong></p><p><img src="https://cdn.oss.link/markdown/t9cxf.jpg" alt="Server Success"></p><p>接下来修改其他两个节点配置，使其作为 Client 加入到集群即可，<strong>注意的是当处于 Client 模式时，<code>bootstrap_expect</code> 必须为 0，即关闭状态；具体配置如下</strong></p><div class="hljs code-wrapper"><pre><code class="hljs json">&#123;    <span class="hljs-attr">&quot;datacenter&quot;</span>: <span class="hljs-string">&quot;dc1&quot;</span>,    <span class="hljs-attr">&quot;data_dir&quot;</span>: <span class="hljs-string">&quot;/var/lib/consul&quot;</span>,    <span class="hljs-attr">&quot;log_level&quot;</span>: <span class="hljs-string">&quot;INFO&quot;</span>,    <span class="hljs-attr">&quot;node_name&quot;</span>: <span class="hljs-string">&quot;docker4.node&quot;</span>,    <span class="hljs-attr">&quot;server&quot;</span>: <span class="hljs-literal">false</span>,    <span class="hljs-attr">&quot;ui&quot;</span>: <span class="hljs-literal">true</span>,    <span class="hljs-attr">&quot;bootstrap_expect&quot;</span>: <span class="hljs-number">0</span>,    <span class="hljs-attr">&quot;bind_addr&quot;</span>: <span class="hljs-string">&quot;192.168.1.14&quot;</span>,    <span class="hljs-attr">&quot;client_addr&quot;</span>: <span class="hljs-string">&quot;192.168.1.14&quot;</span>,    <span class="hljs-attr">&quot;retry_join&quot;</span>: [<span class="hljs-string">&quot;192.168.1.11&quot;</span>,<span class="hljs-string">&quot;192.168.1.12&quot;</span>,<span class="hljs-string">&quot;192.168.1.13&quot;</span>],    <span class="hljs-attr">&quot;retry_interval&quot;</span>: <span class="hljs-string">&quot;3s&quot;</span>,    <span class="hljs-attr">&quot;raft_protocol&quot;</span>: <span class="hljs-number">3</span>,    <span class="hljs-attr">&quot;enable_debug&quot;</span>: <span class="hljs-literal">false</span>,    <span class="hljs-attr">&quot;rejoin_after_leave&quot;</span>: <span class="hljs-literal">true</span>,    <span class="hljs-attr">&quot;enable_syslog&quot;</span>: <span class="hljs-literal">false</span>&#125;</code></pre></div><p>另外一个 Client 配置与以上相同，最终集群成功后如下所示</p><p><img src="https://cdn.oss.link/markdown/j1zrc.jpg" alt="Cluster ok"></p><p><img src="https://cdn.oss.link/markdown/kq4cz.jpg" alt="Command Line"></p><h3 id="四、其他说明"><a href="#四、其他说明" class="headerlink" title="四、其他说明"></a>四、其他说明</h3><p>关于 Consul 的其他各种参数说明，中文版可参考 <a href="http://www.10tiao.com/html/357/201705/2247485185/1.html">Consul集群部署</a>；这个文章对大体上讲的基本很全了，但是随着版本变化，有些参数还是需要参考一下 <a href="https://www.consul.io/docs/agent/options.html">官方配置文档</a></p>]]></content>
    
    
    <summary type="html">不知道 Consul 用的人多还是少，最近有人问怎么搭建 Consul 集群，这里顺手记录一下吧</summary>
    
    
    
    <category term="Kubernetes" scheme="https://mritd.com/categories/kubernetes/"/>
    
    <category term="Docker" scheme="https://mritd.com/categories/kubernetes/docker/"/>
    
    
    <category term="Linux" scheme="https://mritd.com/tags/linux/"/>
    
    <category term="Docker" scheme="https://mritd.com/tags/docker/"/>
    
    <category term="Kubernetes" scheme="https://mritd.com/tags/kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>阿里云部署 Kubernetes</title>
    <link href="https://mritd.com/2017/09/20/set-up-ha-kubernetes-cluster-on-aliyun-ecs/"/>
    <id>https://mritd.com/2017/09/20/set-up-ha-kubernetes-cluster-on-aliyun-ecs/</id>
    <published>2017-09-20T03:02:24.000Z</published>
    <updated>2017-09-20T03:02:24.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>公司有点小需求，在阿里云上开了几台机器，然后部署了一个 Kubernetes 集群，以下记录一下阿里云踩坑问题，主要是网络组件的坑。</p></blockquote><h3 id="一、部署环境"><a href="#一、部署环境" class="headerlink" title="一、部署环境"></a>一、部署环境</h3><p>部署时开启了 4 台 ECS 实例，基本部署环境与裸机部署相似，其中区别是，阿里云网络采用 VPC 网络，不过以下流程适用于经典网络；以下为各个组件版本:</p><ul><li>OS CentOS</li><li>Kernel 4.4.88-1.el7.elrepo.x86_64</li><li>docker 1.13.1</li><li>Kubernetes 1.7.5</li><li>flannel v0.8.0-amd64</li></ul><p>flannel 采用 vxlan 模式，虽然性能不太好，但是兼容度高一点；在阿里云上 flannel 可以采用 vpc 方式，具体可参考 <a href="https://coreos.com/flannel/docs/latest/alicloud-vpc-backend.html">官方文档</a>(这个文档中描述的方法应该更适合 CNM 方式，我用的是 CNI，所以没去折腾他)</p><h3 id="二、基本部署流程"><a href="#二、基本部署流程" class="headerlink" title="二、基本部署流程"></a>二、基本部署流程</h3><p>关于 Master HA 等基本部署流程可以参考 <a href="https://mritd.me/2017/07/21/set-up-kubernetes-ha-cluster-by-binary/">手动档搭建 Kubernetes HA 集群</a> 这篇文章，在部署网络组件之前的流程是相同的，这里不再阐述</p><h3 id="三、Flannel-部署"><a href="#三、Flannel-部署" class="headerlink" title="三、Flannel 部署"></a>三、Flannel 部署</h3><p>关于 Flannel 部署，基本上有两种模式，一种是 vxlan，一种是采用 VPC，VPC 相关的部署上面已经提了，可以参考官方文档；以下说一下 Flannel 的 vxlan 部署方式:</p><h4 id="3-1、CNI-配置"><a href="#3-1、CNI-配置" class="headerlink" title="3.1、CNI 配置"></a>3.1、CNI 配置</h4><p>首先保证集群在不开启 CNI 插件的情况下所有 Node Ready 状态，然后修改 <code>/etc/kubernetes/kubelet</code> 配置文件，加入 CNI 支持( <code>--network-plugin</code> )，配置如下</p><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment"># kubernetes kubelet (minion) config</span><span class="hljs-comment"># The address for the info server to serve on (set to 0.0.0.0 or &quot;&quot; for all interfaces)</span>KUBELET_ADDRESS=<span class="hljs-string">&quot;--address=192.168.1.77&quot;</span><span class="hljs-comment"># The port for the info server to serve on</span><span class="hljs-comment"># KUBELET_PORT=&quot;--port=10250&quot;</span><span class="hljs-comment"># You may leave this blank to use the actual hostname</span>KUBELET_HOSTNAME=<span class="hljs-string">&quot;--hostname-override=docker77.node&quot;</span><span class="hljs-comment"># location of the api-server</span><span class="hljs-comment"># KUBELET_API_SERVER=&quot;&quot;</span><span class="hljs-comment"># Add your own!</span>KUBELET_ARGS=<span class="hljs-string">&quot;--cgroup-driver=cgroupfs \</span><span class="hljs-string">              --cluster-dns=10.254.0.2 \</span><span class="hljs-string">              --network-plugin=cni \</span><span class="hljs-string">              --resolv-conf=/etc/resolv.conf \</span><span class="hljs-string">              --experimental-bootstrap-kubeconfig=/etc/kubernetes/bootstrap.kubeconfig \</span><span class="hljs-string">              --kubeconfig=/etc/kubernetes/kubelet.kubeconfig \</span><span class="hljs-string">              --require-kubeconfig \</span><span class="hljs-string">              --cert-dir=/etc/kubernetes/ssl \</span><span class="hljs-string">              --cluster-domain=cluster.local. \</span><span class="hljs-string">              --hairpin-mode promiscuous-bridge \</span><span class="hljs-string">              --serialize-image-pulls=false \</span><span class="hljs-string">              --pod-infra-container-image=gcr.io/google_containers/pause-amd64:3.0&quot;</span></code></pre></div><h4 id="3-2、Cluster-CIDR-配置"><a href="#3-2、Cluster-CIDR-配置" class="headerlink" title="3.2、Cluster CIDR 配置"></a>3.2、Cluster CIDR 配置</h4><p><strong>在开启 CNI 时使用 Flannel，要设置 <code>--allocate-node-cidrs</code> 和 <code>--cluster-cidr</code> 以保证 Flannel 能正确进行 IP 分配，这两个配置需要加入到 <code>/etc/kubernetes/controller-manager</code> 配置中，完整配置如下</strong></p><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment">###</span><span class="hljs-comment"># The following values are used to configure the kubernetes controller-manager</span><span class="hljs-comment"># defaults from config and apiserver should be adequate</span><span class="hljs-comment"># Add your own!</span>KUBE_CONTROLLER_MANAGER_ARGS=<span class="hljs-string">&quot;--address=192.168.1.77 \</span><span class="hljs-string">                              --allocate-node-cidrs=true \</span><span class="hljs-string">                              --cluster-cidr=10.244.0.0/16 \</span><span class="hljs-string">                              --service-cluster-ip-range=10.254.0.0/16 \</span><span class="hljs-string">                              --cluster-name=kubernetes \</span><span class="hljs-string">                              --cluster-signing-cert-file=/etc/kubernetes/ssl/k8s-root-ca.pem \</span><span class="hljs-string">                              --cluster-signing-key-file=/etc/kubernetes/ssl/k8s-root-ca-key.pem \</span><span class="hljs-string">                              --service-account-private-key-file=/etc/kubernetes/ssl/k8s-root-ca-key.pem \</span><span class="hljs-string">                              --root-ca-file=/etc/kubernetes/ssl/k8s-root-ca.pem \</span><span class="hljs-string">                              --leader-elect=true \</span><span class="hljs-string">                              --node-monitor-grace-period=40s \</span><span class="hljs-string">                              --node-monitor-period=5s \</span><span class="hljs-string">                              --pod-eviction-timeout=5m0s&quot;</span></code></pre></div><h4 id="3-3、CNI-插件配置"><a href="#3-3、CNI-插件配置" class="headerlink" title="3.3、CNI 插件配置"></a>3.3、CNI 插件配置</h4><p>开启 CNI 后，kubelet 创建的 POD 则需要 CNI 插件支持，这里让我感觉奇怪的是 Flannel 的 yaml 中对于 <code>install-cni</code> 这个容器只进行了配置复制，没有做插件复制；所以我们需要手动安装 CNI 插件，CNI 插件最新版本请留意 <a href="https://github.com/containernetworking/plugins/releases">Github</a>；安装过程如下:</p><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment"># 创建 CNI 目录</span>mkdir -p /opt/cni/bin<span class="hljs-comment"># 下载 CNI 插件</span>wget https://github.com/containernetworking/plugins/releases/download/v0.6.0/cni-plugins-amd64-v0.6.0.tgztar -zxvf cni-plugins-amd64-v0.6.0.tgz<span class="hljs-comment"># 移动 CNI 插件</span>mv bridge flannel host-local loopback /opt/cni/bin</code></pre></div><h4 id="3-4、安装-Flannel"><a href="#3-4、安装-Flannel" class="headerlink" title="3.4、安装 Flannel"></a>3.4、安装 Flannel</h4><p>当上面所有配置和 CNI 插件安装完成后，应当重启 kube-controller-manager 和 kubelet</p><div class="hljs code-wrapper"><pre><code class="hljs sh">systemctl daemon-reloadsystemctl restart kube-controller-manager kubelet</code></pre></div><p>然后安装 Flannel 并配置 RBAC 即可</p><div class="hljs code-wrapper"><pre><code class="hljs sh">kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.ymlkubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel-rbac.yml</code></pre></div><p><strong>其他部署如 dns 等与原流程相同，不在阐述</strong></p>]]></content>
    
    
    <summary type="html">公司有点小需求，在阿里云上开了几台机器，然后部署了一个 Kubernetes 集群，以下记录一下阿里云踩坑问题，主要是网络组件的坑</summary>
    
    
    
    <category term="Kubernetes" scheme="https://mritd.com/categories/kubernetes/"/>
    
    
    <category term="Linux" scheme="https://mritd.com/tags/linux/"/>
    
    <category term="Docker" scheme="https://mritd.com/tags/docker/"/>
    
    <category term="Kubernetes" scheme="https://mritd.com/tags/kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>CI/CD Git Flow</title>
    <link href="https://mritd.com/2017/09/05/git-flow-note/"/>
    <id>https://mritd.com/2017/09/05/git-flow-note/</id>
    <published>2017-09-05T06:00:55.000Z</published>
    <updated>2017-09-05T06:00:55.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>由于 git 代码管理比较混乱，所以记录一下 Git Flow + GitLab 的整体工作流程</p></blockquote><h3 id="一、Git-Flow-简介"><a href="#一、Git-Flow-简介" class="headerlink" title="一、Git Flow 简介"></a>一、Git Flow 简介</h3><p>Git Flow 定义了一个围绕项目开发发布的严格 git 分支模型，用于管理多人协作的大型项目中实现高效的协作开发；Git Flow 分支模型最早起源于 <a href="http://nvie.com/about/">Vincent Driessen</a> 的 <a href="http://nvie.com/posts/a-successful-git-branching-model/">A successful Git branching model</a> 文章；随着时间发展，Git Flow 大致分为三种:</p><ul><li>Git Flow: 最原始的 Git Flow 分支模型</li><li>Github Flow: Git Flow 的简化版，专门配合持续发布</li><li>GitLab Flow: Git Flow 与 Github Flow 的结合版</li></ul><p>关于三种 Git Flow 区别详情可参考 <a href="http://www.ruanyifeng.com/blog/2015/12/git-workflow.html">Git 工作流程</a></p><h3 id="二、-Git-Flow-流程"><a href="#二、-Git-Flow-流程" class="headerlink" title="二、 Git Flow 流程"></a>二、 Git Flow 流程</h3><p>Github Flow 和 GitLab Flow 对于持续发布支持比较好，但是原始版本的 Git Flow 对于传统的按照版本发布更加友好一些，所以以下主要说明以下 Git Flow 的工作流程；Git Flow 主要分支模型如下</p><p><img src="https://cdn.oss.link/markdown/80dio.jpg" alt="git flow"></p><p>在整个分支模型中 <strong>存在两个长期分支: develop 和 master</strong>，其中 develop 分支为开发分支，master 为生产分支；<strong>master 代码始终保持随时可以部署到线上的状态；develop 分支用于合并最新提交的功能性代码</strong>；具体的分支定义如下</p><ul><li>master: 生产代码，始终保持可以直接部署生产的状态</li><li>develop: 开发分支，每次合并最新功能代码到此分支</li><li>feature: 新功能分支，所有新开发的功能将采用 <code>feature/xxxx</code> 形式命名分支</li><li>hotfixes: 紧急修复补丁分支，当新功能部署到了线上出现了严重 bug 需要紧急修复时，则创建 <code>hotfixes/xxxx</code> 形式命名的分支</li><li>release: 稳定版分支，当完成大版本变动后，应该创建 <code>release/xxxx</code> 分支</li></ul><p>在整个分支模型中，develop 分支为最上游分支，会不断有新的 feature 合并入 develop 分支，当功能开发达到完成所有版本需求时，则从 develop 分支创建 release 分支，release 后如没有发现其他问题，最终 release 会被合并到 master 分支以完成线上部署</p><h3 id="三、Git-Flow-工具"><a href="#三、Git-Flow-工具" class="headerlink" title="三、Git Flow 工具"></a>三、Git Flow 工具</h3><p>针对于 Git Flow，其手动操作 git 命令可能过于繁琐，所以后来有了 git-flow 工具；git-flow 是一个 git 扩展集，按 Vincent Driessen 的分支模型提供高层次的库操作；使用 git-flow 工具可以以更加简单的命令完成对 Vincent Driessen 分支模型的实践；<br>git-flow 安装以及使用具体请参考 <a href="https://danielkummer.github.io/git-flow-cheatsheet/index.zh_CN.html">git-flow 备忘清单</a>，该文章详细描述了 git-flow 工具的使用方式</p><p>还有另一个工具是 <a href="https://github.com/tj/git-extras">git-extras</a>，该工具没有 git-flow 那么简单化，不过其提供更加强大的命令支持</p><h3 id="四、Git-Commit-Message"><a href="#四、Git-Commit-Message" class="headerlink" title="四、Git Commit Message"></a>四、Git Commit Message</h3><p>在整个 Git Flow 中，commit message 也是必不可少的一部分；一个良好且统一的 commit message 有助于代码审计以及 review 等；目前使用最广泛的写法是 <a href="https://docs.google.com/document/d/1QrDFcIiPjSLDn3EL15IJygNPiHORgU1_OOAqWjiDU5Y/edit#heading=h.greljkmo14y0">Angular 社区规范</a>，该规范大中 commit message 格式大致如下:</p><div class="hljs code-wrapper"><pre><code class="hljs sh">&lt;<span class="hljs-built_in">type</span>&gt;(&lt;scope&gt;): &lt;subject&gt;&lt;BLANK LINE&gt;&lt;body&gt;&lt;BLANK LINE&gt;&lt;footer&gt;</code></pre></div><p>总体格式大致分为 3 部分，首行主要 3 个组成部分:</p><ul><li>type: 本次提交类型</li><li>scope: 本次提交影响范围，一般标明影响版本号或者具体的范围如 <code>$browser, $compile, $rootScope, ngHref, ngClick, ngView, etc...</code></li><li>subject: 本次提交简短说明</li></ul><p>关于 type 提交类型，有如下几种值:</p><ul><li>feat：新功能(feature)</li><li>fix：修补 bug</li><li>docs：文档(documentation)</li><li>style： 格式(不影响代码运行的变动)</li><li>refactor：重构(即不是新增功能，也不是修改 bug 的代码变动)</li><li>test：增加测试</li><li>chore：构建过程或辅助工具的变动</li></ul><p>中间的 body 部分是对本次提交的详细描述信息，底部的 footer 部分一般分为两种情况:</p><ul><li>不兼容变动: 如果出现不兼容变动，则以 <code>BREAKING CHANGE:</code> 开头，后面跟上不兼容变动的具体描述和解决办法</li><li>关闭 issue: 如果该 commit 针对某个 issue，并且可以将其关闭，则可以在其中指定关闭的 issue，如 <code>Close #9527,#9528</code></li></ul><p>不过 footer 部分也有特殊情况，如回滚某次提交，则以 <code>revert:</code> 开头，后面紧跟 commit 信息和具体描述；还有时某些 commit 只是解决了 某个 issue 的一部分问题，这是可以使用 <code>refs ISSUE</code> 的方式来引用该 issue </p><h3 id="五、Git-Commit-Message-工具"><a href="#五、Git-Commit-Message-工具" class="headerlink" title="五、Git Commit Message 工具"></a>五、Git Commit Message 工具</h3><p>针对 Git 的 commit message 目前已经有了成熟的生成工具，比较有名的为 <a href="https://github.com/commitizen/cz-cli">commitizen-cli</a> 工具，其采用 node.js 编写，执行 <code>git cz</code> 命令能够自动生成符合 Angular 社区规范的 commit message；不过由于其使用 node.js 编写，所以安装前需要安装 node.js，因此可能不适合其他非 node.js 的项目使用；这里推荐一个基于 shell 编写的 <a href="https://cimhealth.github.io/git-toolkit">Git-toolkit</a>，安装此工具后执行 <code>git ci</code> 命令进行提交将会产生交互式生成 Angular git commit message 格式的提交说明，截图如下:</p><p><img src="https://cdn.oss.link/markdown/xnonb.jpg" alt="git ci"></p><h3 id="六、GitLab-整合"><a href="#六、GitLab-整合" class="headerlink" title="六、GitLab 整合"></a>六、GitLab 整合</h3><p>以上 Git Flow 所有操作介绍的都是在本地操作，而正常我们在工作中都是基于 GitLab 搭建私有 Git 仓库来进行协同开发的，以下简述以下 Git Flow 配合 GitLab 的流程</p><h4 id="6-1、开发-features"><a href="#6-1、开发-features" class="headerlink" title="6.1、开发 features"></a>6.1、开发 features</h4><p>当开发一个新功能时流程如下:</p><ul><li>本地 <code>git flow feature start xxxx</code> 开启一个 feature 新分支</li><li><code>git flow feature publish xxxx</code> 将此分支推送到远端以便他人获取</li><li>完成开发后 GitLab 上向 <code>develop</code> 分支发起合并请求</li><li>CI sonar 等质量检测工具扫描，其他用户 review 代码</li><li>确认无误后 <code>master</code> 权限用户合并其到 <code>develop</code> 分支</li><li>部署到测试环境以便测试组测试</li><li>如果测试不通过，则继续基于此分支开发，直到该功能开发完成</li></ul><h4 id="6-2、创建-release"><a href="#6-2、创建-release" class="headerlink" title="6.2、创建 release"></a>6.2、创建 release</h4><p>当一定量的 feature 开发完成并合并到 develop 后，如所有 feature 都测试通过并满足版本需求，则可以创建 release 版本分支；release 分支流程如下</p><ul><li>本地 <code>git flow release start xxxx</code> 开启 release 分支</li><li><code>git flow release publish xxxx</code> 将其推送到远端以便他人获取</li><li>继续进行完整性测试，出现问题继续修复，直到 release 完全稳定</li><li>从 release 分支向 master、develop 分支分别发起合并请求</li><li>master 合并后创建对应的 release 标签，并部署生产环境</li><li>develop 合并 release 的后期修改</li></ul><h4 id="6-3、紧急修复"><a href="#6-3、紧急修复" class="headerlink" title="6.3、紧急修复"></a>6.3、紧急修复</h4><p>当 master 某个 tag 部署到生产环境后，也可能出现不符合预期的问题出现；此时应该基于 master 创建 hotfix 分支进行修复，流程如下</p><ul><li>本地 <code>git flow hotfix start xxxx</code> 创建紧急修复分支</li><li>修改代码后将其推送到远端，并像 master、develop 分支发起合并</li><li>develop 合并紧急修复补丁，如果必要最好再做一下测试</li><li>master 合并紧急修复补丁，创建紧急修复 tag，并部署生产环境</li></ul>]]></content>
    
    
    <summary type="html">由于 git 代码管理比较混乱，所以记录一下 Git Flow + GitLab 的整体工作流程</summary>
    
    
    
    <category term="CI/CD" scheme="https://mritd.com/categories/ci-cd/"/>
    
    
    <category term="CI/CD" scheme="https://mritd.com/tags/ci-cd/"/>
    
    <category term="Git" scheme="https://mritd.com/tags/git/"/>
    
  </entry>
  
  <entry>
    <title>Calico 部署踩坑记录</title>
    <link href="https://mritd.com/2017/07/31/calico-yml-bug/"/>
    <id>https://mritd.com/2017/07/31/calico-yml-bug/</id>
    <published>2017-07-31T07:39:23.000Z</published>
    <updated>2017-07-31T07:39:23.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>自从上次在虚拟机中手动了部署了 Kubernetes 1.7.2 以后，自己在测试环境就来了一下，结果网络组件死活起不来，最后找到原因记录一下</p></blockquote><h3 id="一、Calico-部署注意事项"><a href="#一、Calico-部署注意事项" class="headerlink" title="一、Calico 部署注意事项"></a>一、Calico 部署注意事项</h3><p>在使用 Calico 前当然最好撸一下官方文档，地址在这里 <a href="http://docs.projectcalico.org/v2.3/getting-started/kubernetes/installation/">Calico 官方文档</a>，其中部署前需要注意以下几点</p><ul><li><strong>官方文档中要求 <code>kubelet</code> 配置必须增加 <code>--network-plugin=cni</code> 选项</strong></li><li><strong><code>kube-proxy</code> 组件必须采用 <code>iptables</code> proxy mode 模式(1.2 以后是默认模式)</strong></li><li><strong><code>kubec-proxy</code> 组件不能采用 <code>--masquerade-all</code> 启动，因为会与 Calico policy 冲突</strong></li><li><strong><code>NetworkPolicy API</code> 只要需要 Kubernetes 1.3 以上</strong></li><li><strong>启用 RBAC 后需要设置对应的 RoleBinding，参考 <a href="http://docs.projectcalico.org/v2.3/getting-started/kubernetes/installation/hosted/">官方文档 RBAC 部分</a></strong></li></ul><h3 id="二、Calico-官方部署方式"><a href="#二、Calico-官方部署方式" class="headerlink" title="二、Calico 官方部署方式"></a>二、Calico 官方部署方式</h3><p>在已经有了一个 Kubernetes 集群的情况下，官方部署方式描述的很简单，只需要改一改 yml 配置，然后 create 一下即可，具体描述见 <a href="http://docs.projectcalico.org/v2.3/getting-started/kubernetes/installation/hosted/">官方文档</a></p><p>官方文档中大致给出了三种部署方案: </p><ul><li><strong>Standard Hosted Install:</strong> 修改 calico.yml etcd 相关配置，直接创建，证书配置等参考 <a href="https://mritd.me/2017/07/21/set-up-kubernetes-ha-cluster-by-binary/#%E5%85%AD%E9%83%A8%E7%BD%B2-calico">手动部署 Kubernetes 文档</a></li><li><strong>Kubeadm Hosted Install:</strong> 根据 <code>1.6 or high</code> 和 <code>1.5</code> 区分两个 yml 配置，直接创建即可</li><li><strong>Kubernetes Datastore:</strong> 不使用 Etcd 存储数据，不推荐，这里也不做说明</li></ul><h3 id="三、Standard-Hosted-Install-的坑"><a href="#三、Standard-Hosted-Install-的坑" class="headerlink" title="三、Standard Hosted Install 的坑"></a>三、Standard Hosted Install 的坑</h3><p>当我从虚拟机中测试完全没问题以后，就在测试环境尝试创建 Calico 网络，结果出现的问题是<strong>某个(几个) Calico 节点无法启动，同时创建 deployment 后，执行 <code>route -n</code> 会发现每个 node 只有自己节点 Pod 的路由，正常每个 node 上会有所有 node 上 Pod 网段的路由，如下(正常情况)</strong></p><p><img src="https://cdn.oss.link/markdown/c44e7.jpg" alt="calico route"></p><p>此时观察每个 node 上 Calico Pod 日志，会有提示 <strong>未知节点 xxxx</strong> 等错误日志，大体意思就是 <strong>未知的一个(几个)节点在进行 BGP 协议时被拒绝</strong>，偶尔某些 node 上还可能出现 <strong>IP 已经被占用</strong> 的神奇错误提示</p><p>后来经过翻查 <a href="http://docs.projectcalico.org/v2.3/getting-started/kubernetes/installation/integration">Calico 自定义部署文档</a> 和 <a href="https://github.com/kubernetes-incubator/kubespray">Kargo 项目源码</a> 发现了主要问题在于 <strong>官方文档中直接创建的 calico.yml 文件中，使用 DaemonSet 方式启动 calico-node，同时 calico-node 的 IP 设置和 NODENAME 设置均为空，此时 calico-node 会进行自动获取，网络复杂情况下获取会出现问题；比如 IP 拿到了 docker 网桥的 IP，NODENAME 获取不正确等，最终导致出现很奇怪的错误</strong></p><h3 id="四、解决方案"><a href="#四、解决方案" class="headerlink" title="四、解决方案"></a>四、解决方案</h3><p>一开始想到的解决方案很简单，直接照着 Kargo 抄，使用 Systemd 来启动 calico-node，然后在拆分过程中需要各种配置信息直接也根据 Kargo 的做法生成；当然鼓捣了 1/3 的时候就炸了，Kargo 是 ansible 批量部署的，有些变量找起来要人命；最后选择了一个折中(偷懒)的方案: <strong>使用官方的 calico.yml 创建相关组件，这样 ConfigMap、Etcd 配置、Calico policy 啥的直接创建好，然后把 DaemonSet 中 calico-node 容器单独搞出来，使用 Systemd 启动，这样就即方便又简单(我真特么机智)；最终操作如下:</strong></p><h4 id="4-1、首先修改-calico-yml"><a href="#4-1、首先修改-calico-yml" class="headerlink" title="4.1、首先修改 calico.yml"></a>4.1、首先修改 calico.yml</h4><p>在进行网络组件部署前，请确保集群已经满足 Calico 部署要求(本文第一部分)；然后获取 calico.yml，注释掉 DaemonSet 中 calico-node 部分，如下所示</p><div class="hljs code-wrapper"><pre><code class="hljs yml"><span class="hljs-comment"># Calico Version v2.3.0</span><span class="hljs-comment"># http://docs.projectcalico.org/v2.3/releases#v2.3.0</span><span class="hljs-comment"># This manifest includes the following component versions:</span><span class="hljs-comment">#   calico/node:v1.3.0</span><span class="hljs-comment">#   calico/cni:v1.9.1</span><span class="hljs-comment">#   calico/kube-policy-controller:v0.6.0</span><span class="hljs-comment"># This ConfigMap is used to configure a self-hosted Calico installation.</span><span class="hljs-attr">kind:</span> <span class="hljs-string">ConfigMap</span><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">calico-config</span>  <span class="hljs-attr">namespace:</span> <span class="hljs-string">kube-system</span><span class="hljs-attr">data:</span>  <span class="hljs-comment"># Configure this with the location of your etcd cluster.</span>  <span class="hljs-attr">etcd_endpoints:</span> <span class="hljs-string">&quot;https://192.168.1.11:2379,https://192.168.1.12:2379,https://192.168.1.13:2379&quot;</span>  <span class="hljs-comment"># Configure the Calico backend to use.</span>  <span class="hljs-attr">calico_backend:</span> <span class="hljs-string">&quot;bird&quot;</span>  <span class="hljs-comment"># The CNI network configuration to install on each node.</span>  <span class="hljs-attr">cni_network_config:</span> <span class="hljs-string">|-</span>    &#123;        <span class="hljs-attr">&quot;name&quot;:</span> <span class="hljs-string">&quot;k8s-pod-network&quot;</span>,        <span class="hljs-attr">&quot;cniVersion&quot;:</span> <span class="hljs-string">&quot;0.1.0&quot;</span>,        <span class="hljs-attr">&quot;type&quot;:</span> <span class="hljs-string">&quot;calico&quot;</span>,        <span class="hljs-attr">&quot;etcd_endpoints&quot;:</span> <span class="hljs-string">&quot;__ETCD_ENDPOINTS__&quot;</span>,        <span class="hljs-attr">&quot;etcd_key_file&quot;:</span> <span class="hljs-string">&quot;__ETCD_KEY_FILE__&quot;</span>,        <span class="hljs-attr">&quot;etcd_cert_file&quot;:</span> <span class="hljs-string">&quot;__ETCD_CERT_FILE__&quot;</span>,        <span class="hljs-attr">&quot;etcd_ca_cert_file&quot;:</span> <span class="hljs-string">&quot;__ETCD_CA_CERT_FILE__&quot;</span>,        <span class="hljs-attr">&quot;log_level&quot;:</span> <span class="hljs-string">&quot;info&quot;</span>,        <span class="hljs-attr">&quot;ipam&quot;:</span> &#123;            <span class="hljs-attr">&quot;type&quot;:</span> <span class="hljs-string">&quot;calico-ipam&quot;</span>        &#125;,        <span class="hljs-attr">&quot;policy&quot;:</span> &#123;            <span class="hljs-attr">&quot;type&quot;:</span> <span class="hljs-string">&quot;k8s&quot;</span>,            <span class="hljs-attr">&quot;k8s_api_root&quot;:</span> <span class="hljs-string">&quot;https://__KUBERNETES_SERVICE_HOST__:__KUBERNETES_SERVICE_PORT__&quot;</span>,            <span class="hljs-attr">&quot;k8s_auth_token&quot;:</span> <span class="hljs-string">&quot;__SERVICEACCOUNT_TOKEN__&quot;</span>        &#125;,        <span class="hljs-attr">&quot;kubernetes&quot;:</span> &#123;            <span class="hljs-attr">&quot;kubeconfig&quot;:</span> <span class="hljs-string">&quot;__KUBECONFIG_FILEPATH__&quot;</span>        &#125;    &#125;  <span class="hljs-comment"># If you&#x27;re using TLS enabled etcd uncomment the following.</span>  <span class="hljs-comment"># You must also populate the Secret below with these files.</span>  <span class="hljs-attr">etcd_ca:</span> <span class="hljs-string">&quot;/calico-secrets/etcd-ca&quot;</span>  <span class="hljs-attr">etcd_cert:</span> <span class="hljs-string">&quot;/calico-secrets/etcd-cert&quot;</span>  <span class="hljs-attr">etcd_key:</span> <span class="hljs-string">&quot;/calico-secrets/etcd-key&quot;</span><span class="hljs-meta">---</span><span class="hljs-comment"># The following contains k8s Secrets for use with a TLS enabled etcd cluster.</span><span class="hljs-comment"># For information on populating Secrets, see http://kubernetes.io/docs/user-guide/secrets/</span><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span><span class="hljs-attr">kind:</span> <span class="hljs-string">Secret</span><span class="hljs-attr">type:</span> <span class="hljs-string">Opaque</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">calico-etcd-secrets</span>  <span class="hljs-attr">namespace:</span> <span class="hljs-string">kube-system</span><span class="hljs-attr">data:</span>  <span class="hljs-comment"># Populate the following files with etcd TLS configuration if desired, but leave blank if</span>  <span class="hljs-comment"># not using TLS for etcd.</span>  <span class="hljs-comment"># This self-hosted install expects three files with the following names.  The values</span>  <span class="hljs-comment"># should be base64 encoded strings of the entire contents of each file.</span>  <span class="hljs-attr">etcd-key:</span> <span class="hljs-string">这块自己对</span> <span class="hljs-string">etcd</span> <span class="hljs-string">相关证书做</span> <span class="hljs-string">base64</span>  <span class="hljs-attr">etcd-cert:</span> <span class="hljs-string">这块自己对</span> <span class="hljs-string">etcd</span> <span class="hljs-string">相关证书做</span> <span class="hljs-string">base64</span>  <span class="hljs-attr">etcd-ca:</span> <span class="hljs-string">这块自己对</span> <span class="hljs-string">etcd</span> <span class="hljs-string">相关证书做</span> <span class="hljs-string">base64</span><span class="hljs-meta">---</span><span class="hljs-comment"># This manifest installs the calico/node container, as well</span><span class="hljs-comment"># as the Calico CNI plugins and network config on</span><span class="hljs-comment"># each master and worker node in a Kubernetes cluster.</span><span class="hljs-attr">kind:</span> <span class="hljs-string">DaemonSet</span><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">extensions/v1beta1</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">calico-node</span>  <span class="hljs-attr">namespace:</span> <span class="hljs-string">kube-system</span>  <span class="hljs-attr">labels:</span>    <span class="hljs-attr">k8s-app:</span> <span class="hljs-string">calico-node</span><span class="hljs-attr">spec:</span>  <span class="hljs-attr">selector:</span>    <span class="hljs-attr">matchLabels:</span>      <span class="hljs-attr">k8s-app:</span> <span class="hljs-string">calico-node</span>  <span class="hljs-attr">template:</span>    <span class="hljs-attr">metadata:</span>      <span class="hljs-attr">labels:</span>        <span class="hljs-attr">k8s-app:</span> <span class="hljs-string">calico-node</span>      <span class="hljs-attr">annotations:</span>        <span class="hljs-attr">scheduler.alpha.kubernetes.io/critical-pod:</span> <span class="hljs-string">&#x27;&#x27;</span>        <span class="hljs-attr">scheduler.alpha.kubernetes.io/tolerations:</span> <span class="hljs-string">|</span>          [&#123;<span class="hljs-attr">&quot;key&quot;:</span> <span class="hljs-string">&quot;dedicated&quot;</span>, <span class="hljs-attr">&quot;value&quot;:</span> <span class="hljs-string">&quot;master&quot;</span>, <span class="hljs-attr">&quot;effect&quot;:</span> <span class="hljs-string">&quot;NoSchedule&quot;</span> &#125;,           &#123;<span class="hljs-string">&quot;key&quot;</span><span class="hljs-string">:&quot;CriticalAddonsOnly&quot;</span>, <span class="hljs-string">&quot;operator&quot;</span><span class="hljs-string">:&quot;Exists&quot;</span>&#125;]    <span class="hljs-attr">spec:</span>      <span class="hljs-attr">hostNetwork:</span> <span class="hljs-literal">true</span>      <span class="hljs-attr">serviceAccountName:</span> <span class="hljs-string">calico-node</span>      <span class="hljs-attr">containers:</span>        <span class="hljs-comment"># Runs calico/node container on each Kubernetes node.  This</span>        <span class="hljs-comment"># container programs network policy and routes on each</span>        <span class="hljs-comment"># host.</span><span class="hljs-comment"># calico-node 注释掉，移动到 Systemd 中</span><span class="hljs-comment">#        - name: calico-node</span><span class="hljs-comment">#          image: quay.io/calico/node:v1.3.0</span><span class="hljs-comment">#          env:</span><span class="hljs-comment">#            # The location of the Calico etcd cluster.</span><span class="hljs-comment">#            - name: ETCD_ENDPOINTS</span><span class="hljs-comment">#              valueFrom:</span><span class="hljs-comment">#                configMapKeyRef:</span><span class="hljs-comment">#                  name: calico-config</span><span class="hljs-comment">#                  key: etcd_endpoints</span><span class="hljs-comment">#            # Choose the backend to use.</span><span class="hljs-comment">#            - name: CALICO_NETWORKING_BACKEND</span><span class="hljs-comment">#              valueFrom:</span><span class="hljs-comment">#                configMapKeyRef:</span><span class="hljs-comment">#                  name: calico-config</span><span class="hljs-comment">#                  key: calico_backend</span><span class="hljs-comment">#            # Disable file logging so `kubectl logs` works.</span><span class="hljs-comment">#            - name: CALICO_DISABLE_FILE_LOGGING</span><span class="hljs-comment">#              value: &quot;true&quot;</span><span class="hljs-comment">#            # Set Felix endpoint to host default action to ACCEPT.</span><span class="hljs-comment">#            - name: FELIX_DEFAULTENDPOINTTOHOSTACTION</span><span class="hljs-comment">#              value: &quot;ACCEPT&quot;</span><span class="hljs-comment">#            # Configure the IP Pool from which Pod IPs will be chosen.</span><span class="hljs-comment">#            - name: CALICO_IPV4POOL_CIDR</span><span class="hljs-comment">#              value: &quot;10.254.64.0/18&quot;</span><span class="hljs-comment">#            - name: CALICO_IPV4POOL_IPIP</span><span class="hljs-comment">#              value: &quot;always&quot;</span><span class="hljs-comment">#            # Disable IPv6 on Kubernetes.</span><span class="hljs-comment">#            - name: FELIX_IPV6SUPPORT</span><span class="hljs-comment">#              value: &quot;false&quot;</span><span class="hljs-comment">#            # Set Felix logging to &quot;info&quot;</span><span class="hljs-comment">#            - name: FELIX_LOGSEVERITYSCREEN</span><span class="hljs-comment">#              value: &quot;info&quot;</span><span class="hljs-comment">#            # Location of the CA certificate for etcd.</span><span class="hljs-comment">#            - name: ETCD_CA_CERT_FILE</span><span class="hljs-comment">#              valueFrom:</span><span class="hljs-comment">#                configMapKeyRef:</span><span class="hljs-comment">#                  name: calico-config</span><span class="hljs-comment">#                  key: etcd_ca</span><span class="hljs-comment">#            # Location of the client key for etcd.</span><span class="hljs-comment">#            - name: ETCD_KEY_FILE</span><span class="hljs-comment">#              valueFrom:</span><span class="hljs-comment">#                configMapKeyRef:</span><span class="hljs-comment">#                  name: calico-config</span><span class="hljs-comment">#                  key: etcd_key</span><span class="hljs-comment">#            # Location of the client certificate for etcd.</span><span class="hljs-comment">#            - name: ETCD_CERT_FILE</span><span class="hljs-comment">#              valueFrom:</span><span class="hljs-comment">#                configMapKeyRef:</span><span class="hljs-comment">#                  name: calico-config</span><span class="hljs-comment">#                  key: etcd_cert</span><span class="hljs-comment">#            # Auto-detect the BGP IP address.</span><span class="hljs-comment">#            - name: IP</span><span class="hljs-comment">#              value: &quot;&quot;</span><span class="hljs-comment">#          securityContext:</span><span class="hljs-comment">#            privileged: true</span><span class="hljs-comment">#          resources:</span><span class="hljs-comment">#            requests:</span><span class="hljs-comment">#              cpu: 250m</span><span class="hljs-comment">#          volumeMounts:</span><span class="hljs-comment">#            - mountPath: /lib/modules</span><span class="hljs-comment">#              name: lib-modules</span><span class="hljs-comment">#              readOnly: true</span><span class="hljs-comment">#            - mountPath: /var/run/calico</span><span class="hljs-comment">#              name: var-run-calico</span><span class="hljs-comment">#              readOnly: false</span><span class="hljs-comment">#            - mountPath: /calico-secrets</span><span class="hljs-comment">#              name: etcd-certs</span><span class="hljs-comment">#        # This container installs the Calico CNI binaries</span><span class="hljs-comment">#        # and CNI network config file on each node.</span>        <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">install-cni</span>          <span class="hljs-attr">image:</span> <span class="hljs-string">quay.io/calico/cni:v1.9.1</span>          <span class="hljs-attr">command:</span> [<span class="hljs-string">&quot;/install-cni.sh&quot;</span>]          <span class="hljs-attr">env:</span>            <span class="hljs-comment"># The location of the Calico etcd cluster.</span>            <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">ETCD_ENDPOINTS</span>              <span class="hljs-attr">valueFrom:</span>                <span class="hljs-attr">configMapKeyRef:</span>                  <span class="hljs-attr">name:</span> <span class="hljs-string">calico-config</span>                  <span class="hljs-attr">key:</span> <span class="hljs-string">etcd_endpoints</span>            <span class="hljs-comment"># The CNI network config to install on each node.</span>            <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">CNI_NETWORK_CONFIG</span>              <span class="hljs-attr">valueFrom:</span>                <span class="hljs-attr">configMapKeyRef:</span>                  <span class="hljs-attr">name:</span> <span class="hljs-string">calico-config</span>                  <span class="hljs-attr">key:</span> <span class="hljs-string">cni_network_config</span>          <span class="hljs-attr">volumeMounts:</span>            <span class="hljs-bullet">-</span> <span class="hljs-attr">mountPath:</span> <span class="hljs-string">/host/opt/cni/bin</span>              <span class="hljs-attr">name:</span> <span class="hljs-string">cni-bin-dir</span>            <span class="hljs-bullet">-</span> <span class="hljs-attr">mountPath:</span> <span class="hljs-string">/host/etc/cni/net.d</span>              <span class="hljs-attr">name:</span> <span class="hljs-string">cni-net-dir</span>            <span class="hljs-bullet">-</span> <span class="hljs-attr">mountPath:</span> <span class="hljs-string">/calico-secrets</span>              <span class="hljs-attr">name:</span> <span class="hljs-string">etcd-certs</span>      <span class="hljs-attr">volumes:</span>        <span class="hljs-comment"># Used by calico/node.</span>        <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">lib-modules</span>          <span class="hljs-attr">hostPath:</span>            <span class="hljs-attr">path:</span> <span class="hljs-string">/lib/modules</span>        <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">var-run-calico</span>          <span class="hljs-attr">hostPath:</span>            <span class="hljs-attr">path:</span> <span class="hljs-string">/var/run/calico</span>        <span class="hljs-comment"># Used to install CNI.</span>        <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">cni-bin-dir</span>          <span class="hljs-attr">hostPath:</span>            <span class="hljs-attr">path:</span> <span class="hljs-string">/opt/cni/bin</span>        <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">cni-net-dir</span>          <span class="hljs-attr">hostPath:</span>            <span class="hljs-attr">path:</span> <span class="hljs-string">/etc/cni/net.d</span>        <span class="hljs-comment"># Mount in the etcd TLS secrets.</span>        <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">etcd-certs</span>          <span class="hljs-attr">secret:</span>            <span class="hljs-attr">secretName:</span> <span class="hljs-string">calico-etcd-secrets</span><span class="hljs-meta">---</span><span class="hljs-comment"># This manifest deploys the Calico policy controller on Kubernetes.</span><span class="hljs-comment"># See https://github.com/projectcalico/k8s-policy</span><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">extensions/v1beta1</span><span class="hljs-attr">kind:</span> <span class="hljs-string">Deployment</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">calico-policy-controller</span>  <span class="hljs-attr">namespace:</span> <span class="hljs-string">kube-system</span>  <span class="hljs-attr">labels:</span>    <span class="hljs-attr">k8s-app:</span> <span class="hljs-string">calico-policy</span>  <span class="hljs-attr">annotations:</span>    <span class="hljs-attr">scheduler.alpha.kubernetes.io/critical-pod:</span> <span class="hljs-string">&#x27;&#x27;</span>    <span class="hljs-attr">scheduler.alpha.kubernetes.io/tolerations:</span> <span class="hljs-string">|</span>      [&#123;<span class="hljs-attr">&quot;key&quot;:</span> <span class="hljs-string">&quot;dedicated&quot;</span>, <span class="hljs-attr">&quot;value&quot;:</span> <span class="hljs-string">&quot;master&quot;</span>, <span class="hljs-attr">&quot;effect&quot;:</span> <span class="hljs-string">&quot;NoSchedule&quot;</span> &#125;,       &#123;<span class="hljs-string">&quot;key&quot;</span><span class="hljs-string">:&quot;CriticalAddonsOnly&quot;</span>, <span class="hljs-string">&quot;operator&quot;</span><span class="hljs-string">:&quot;Exists&quot;</span>&#125;]<span class="hljs-attr">spec:</span>  <span class="hljs-comment"># The policy controller can only have a single active instance.</span>  <span class="hljs-attr">replicas:</span> <span class="hljs-number">1</span>  <span class="hljs-attr">strategy:</span>    <span class="hljs-attr">type:</span> <span class="hljs-string">Recreate</span>  <span class="hljs-attr">template:</span>    <span class="hljs-attr">metadata:</span>      <span class="hljs-attr">name:</span> <span class="hljs-string">calico-policy-controller</span>      <span class="hljs-attr">namespace:</span> <span class="hljs-string">kube-system</span>      <span class="hljs-attr">labels:</span>        <span class="hljs-attr">k8s-app:</span> <span class="hljs-string">calico-policy</span>    <span class="hljs-attr">spec:</span>      <span class="hljs-comment"># The policy controller must run in the host network namespace so that</span>      <span class="hljs-comment"># it isn&#x27;t governed by policy that would prevent it from working.</span>      <span class="hljs-attr">hostNetwork:</span> <span class="hljs-literal">true</span>      <span class="hljs-attr">serviceAccountName:</span> <span class="hljs-string">calico-policy-controller</span>      <span class="hljs-attr">containers:</span>        <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">calico-policy-controller</span>          <span class="hljs-attr">image:</span> <span class="hljs-string">quay.io/calico/kube-policy-controller:v0.6.0</span>          <span class="hljs-attr">env:</span>            <span class="hljs-comment"># The location of the Calico etcd cluster.</span>            <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">ETCD_ENDPOINTS</span>              <span class="hljs-attr">valueFrom:</span>                <span class="hljs-attr">configMapKeyRef:</span>                  <span class="hljs-attr">name:</span> <span class="hljs-string">calico-config</span>                  <span class="hljs-attr">key:</span> <span class="hljs-string">etcd_endpoints</span>            <span class="hljs-comment"># Location of the CA certificate for etcd.</span>            <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">ETCD_CA_CERT_FILE</span>              <span class="hljs-attr">valueFrom:</span>                <span class="hljs-attr">configMapKeyRef:</span>                  <span class="hljs-attr">name:</span> <span class="hljs-string">calico-config</span>                  <span class="hljs-attr">key:</span> <span class="hljs-string">etcd_ca</span>            <span class="hljs-comment"># Location of the client key for etcd.</span>            <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">ETCD_KEY_FILE</span>              <span class="hljs-attr">valueFrom:</span>                <span class="hljs-attr">configMapKeyRef:</span>                  <span class="hljs-attr">name:</span> <span class="hljs-string">calico-config</span>                  <span class="hljs-attr">key:</span> <span class="hljs-string">etcd_key</span>            <span class="hljs-comment"># Location of the client certificate for etcd.</span>            <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">ETCD_CERT_FILE</span>              <span class="hljs-attr">valueFrom:</span>                <span class="hljs-attr">configMapKeyRef:</span>                  <span class="hljs-attr">name:</span> <span class="hljs-string">calico-config</span>                  <span class="hljs-attr">key:</span> <span class="hljs-string">etcd_cert</span>            <span class="hljs-comment"># The location of the Kubernetes API.  Use the default Kubernetes</span>            <span class="hljs-comment"># service for API access.</span>            <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">K8S_API</span>              <span class="hljs-attr">value:</span> <span class="hljs-string">&quot;https://kubernetes.default:443&quot;</span>            <span class="hljs-comment"># Since we&#x27;re running in the host namespace and might not have KubeDNS</span>            <span class="hljs-comment"># access, configure the container&#x27;s /etc/hosts to resolve</span>            <span class="hljs-comment"># kubernetes.default to the correct service clusterIP.</span>            <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">CONFIGURE_ETC_HOSTS</span>              <span class="hljs-attr">value:</span> <span class="hljs-string">&quot;true&quot;</span>          <span class="hljs-attr">volumeMounts:</span>            <span class="hljs-comment"># Mount in the etcd TLS secrets.</span>            <span class="hljs-bullet">-</span> <span class="hljs-attr">mountPath:</span> <span class="hljs-string">/calico-secrets</span>              <span class="hljs-attr">name:</span> <span class="hljs-string">etcd-certs</span>      <span class="hljs-attr">volumes:</span>        <span class="hljs-comment"># Mount in the etcd TLS secrets.</span>        <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">etcd-certs</span>          <span class="hljs-attr">secret:</span>            <span class="hljs-attr">secretName:</span> <span class="hljs-string">calico-etcd-secrets</span><span class="hljs-meta">---</span><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span><span class="hljs-attr">kind:</span> <span class="hljs-string">ServiceAccount</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">calico-policy-controller</span>  <span class="hljs-attr">namespace:</span> <span class="hljs-string">kube-system</span><span class="hljs-meta">---</span><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span><span class="hljs-attr">kind:</span> <span class="hljs-string">ServiceAccount</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">calico-node</span>  <span class="hljs-attr">namespace:</span> <span class="hljs-string">kube-system</span></code></pre></div><p><strong>修改完成后直接 create 即可</strong></p><h4 id="4-2、增加-calico-node-Systemd-配置"><a href="#4-2、增加-calico-node-Systemd-配置" class="headerlink" title="4.2、增加 calico-node Systemd 配置"></a>4.2、增加 calico-node Systemd 配置</h4><p>最后写一个 service 文件(我放到了 <code>/etc/systemd/system/calico-node.service</code>)，使用 Systemd 启动即可；<strong>注意以下配置中 <code>IP</code>、<code>NODENAME</code> 是自己手动定义的，IP 为宿主机 IP，NODENAME 最好与 hostname 相同</strong></p><div class="hljs code-wrapper"><pre><code class="hljs sh">[Unit]Description=calico nodeAfter=docker.serviceRequires=docker.service[Service]User=rootPermissionsStartOnly=<span class="hljs-literal">true</span>ExecStart=/usr/bin/docker run   --net=host --privileged --name=calico-node \                                -e ETCD_ENDPOINTS=https://192.168.1.11:2379,https://192.168.1.12:2379,https://192.168.1.13:2379 \                                -e ETCD_CA_CERT_FILE=/etc/etcd/ssl/etcd-root-ca.pem \                                -e ETCD_CERT_FILE=/etc/etcd/ssl/etcd.pem \                                -e ETCD_KEY_FILE=/etc/etcd/ssl/etcd-key.pem \                                -e NODENAME=docker1.node \                                -e IP=192.168.1.11 \                                -e IP6= \                                -e AS= \                                -e CALICO_IPV4POOL_CIDR=10.20.0.0/16 \                                -e CALICO_IPV4POOL_IPIP=always \                                -e CALICO_LIBNETWORK_ENABLED=<span class="hljs-literal">true</span> \                                -e CALICO_NETWORKING_BACKEND=bird \                                -e CALICO_DISABLE_FILE_LOGGING=<span class="hljs-literal">true</span> \                                -e FELIX_IPV6SUPPORT=<span class="hljs-literal">false</span> \                                -e FELIX_DEFAULTENDPOINTTOHOSTACTION=ACCEPT \                                -e FELIX_LOGSEVERITYSCREEN=info \                                -v /etc/etcd/ssl/etcd-root-ca.pem:/etc/etcd/ssl/etcd-root-ca.pem \                                -v /etc/etcd/ssl/etcd.pem:/etc/etcd/ssl/etcd.pem \                                -v /etc/etcd/ssl/etcd-key.pem:/etc/etcd/ssl/etcd-key.pem \                                -v /var/run/calico:/var/run/calico \                                -v /lib/modules:/lib/modules \                                -v /run/docker/plugins:/run/docker/plugins \                                -v /var/run/docker.sock:/var/run/docker.sock \                                -v /var/<span class="hljs-built_in">log</span>/calico:/var/<span class="hljs-built_in">log</span>/calico \                                quay.io/calico/node:v1.3.0ExecStop=/usr/bin/docker rm -f calico-nodeRestart=alwaysRestartSec=10[Install]WantedBy=multi-user.target</code></pre></div>]]></content>
    
    
    <summary type="html">自从上次在虚拟机中手动了部署了 Kubernetes 1.7.2 以后，自己在测试环境就来了一下，结果网络组件死活起不来，最后找到原因记录一下</summary>
    
    
    
    <category term="Kubernetes" scheme="https://mritd.com/categories/kubernetes/"/>
    
    
    <category term="Kubernetes" scheme="https://mritd.com/tags/kubernetes/"/>
    
    <category term="Calico" scheme="https://mritd.com/tags/calico/"/>
    
  </entry>
  
  <entry>
    <title>手动档搭建 Kubernetes HA 集群</title>
    <link href="https://mritd.com/2017/07/21/set-up-kubernetes-ha-cluster-by-binary/"/>
    <id>https://mritd.com/2017/07/21/set-up-kubernetes-ha-cluster-by-binary/</id>
    <published>2017-07-21T08:23:50.000Z</published>
    <updated>2017-07-21T08:23:50.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>以前一直用 Kargo(基于 ansible) 来搭建 Kubernetes 集群，最近发现 ansible 部署的时候有些东西有点 bug，而且 Kargo 对 rkt 等也做了适配，感觉问题已经有点复杂化了；在 2.2 release 没出来这个时候，准备自己纯手动挡部署一下，Master HA 直接抄 Kargo 的就行了，以下记录一下;<strong>本文以下部分所有用到的 rpm 、配置文件等全部已经上传到了 <a href="http://pan.baidu.com/s/1o8PZLKA">百度云</a>  密码: x5v4</strong></p></blockquote><h3 id="一、环境准备"><a href="#一、环境准备" class="headerlink" title="一、环境准备"></a>一、环境准备</h3><blockquote><p>以下文章本着 <strong>多写代码少哔哔</strong> 的原则，会主要以实际操作为主，不会过多介绍每步细节动作，如果纯小白想要更详细的了解，可以参考 <a href="https://github.com/rootsongjc/kubernetes-handbook">这里</a></p></blockquote><p><strong>环境总共 5 台虚拟机，2 个 master，3 个 etcd 节点，master 同时也作为 node 负载 pod，在分发证书等阶段将在另外一台主机上执行，该主机对集群内所有节点配置了 ssh 秘钥登录</strong></p><table><thead><tr><th>IP</th><th>节点</th></tr></thead><tbody><tr><td>192.168.1.11</td><td>master、node、etcd</td></tr><tr><td>192.168.1.12</td><td>master、node、etcd</td></tr><tr><td>192.168.1.13</td><td>master、node、etcd</td></tr><tr><td>192.168.1.14</td><td>node</td></tr><tr><td>192.168.1.15</td><td>node</td></tr></tbody></table><p>网络方案这里采用性能比较好的 Calico，集群开启 RBAC，RBAC 相关可参考 <a href="https://mritd.me/2017/07/17/kubernetes-rbac-chinese-translation/">这里的胡乱翻译版本</a></p><h3 id="二、证书相关处理"><a href="#二、证书相关处理" class="headerlink" title="二、证书相关处理"></a>二、证书相关处理</h3><h4 id="2-1、证书说明"><a href="#2-1、证书说明" class="headerlink" title="2.1、证书说明"></a>2.1、证书说明</h4><p>由于 Etcd 和 Kubernetes 全部采用 TLS 通讯，所以先要生成 TLS 证书，<strong>证书生成工具采用 <a href="https://github.com/cloudflare/cfssl/releases">cfssl</a>，具体使用方法这里不再详细阐述，生成证书时可在任一节点完成，这里在宿主机执行</strong>，证书列表如下</p><table><thead><tr><th>证书名称</th><th>配置文件</th><th>用途</th></tr></thead><tbody><tr><td>etcd-root-ca.pem</td><td>etcd-root-ca-csr.json</td><td>etcd 根 CA 证书</td></tr><tr><td>etcd.pem</td><td>etcd-gencert.json、etcd-csr.json</td><td>etcd 集群证书</td></tr><tr><td>k8s-root-ca.pem</td><td>k8s-root-ca-csr.json</td><td>k8s 根 CA 证书</td></tr><tr><td>kube-proxy.pem</td><td>k8s-gencert.json、kube-proxy-csr.json</td><td>kube-proxy 使用的证书</td></tr><tr><td>admin.pem</td><td>k8s-gencert.json、admin-csr.json</td><td>kubectl 使用的证书</td></tr><tr><td>kubernetes.pem</td><td>k8s-gencert.json、kubernetes-csr.json</td><td>kube-apiserver 使用的证书</td></tr></tbody></table><h4 id="2-2、CFSSL-工具安装"><a href="#2-2、CFSSL-工具安装" class="headerlink" title="2.2、CFSSL 工具安装"></a>2.2、CFSSL 工具安装</h4><p><strong>首先下载 cfssl，并给予可执行权限，然后扔到 PATH 目录下</strong></p><div class="hljs code-wrapper"><pre><code class="hljs sh">wget https://pkg.cfssl.org/R1.2/cfssl_linux-amd64wget https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64chmod +x cfssl_linux-amd64 cfssljson_linux-amd64mv cfssl_linux-amd64 /usr/<span class="hljs-built_in">local</span>/bin/cfsslmv cfssljson_linux-amd64 /usr/<span class="hljs-built_in">local</span>/bin/cfssljson</code></pre></div><h4 id="2-3、生成-Etcd-证书"><a href="#2-3、生成-Etcd-证书" class="headerlink" title="2.3、生成 Etcd 证书"></a>2.3、生成 Etcd 证书</h4><p>Etcd 证书生成所需配置文件如下: </p><ul><li>etcd-root-ca-csr.json</li></ul><div class="hljs code-wrapper"><pre><code class="hljs json">&#123;  <span class="hljs-attr">&quot;key&quot;</span>: &#123;    <span class="hljs-attr">&quot;algo&quot;</span>: <span class="hljs-string">&quot;rsa&quot;</span>,    <span class="hljs-attr">&quot;size&quot;</span>: <span class="hljs-number">4096</span>  &#125;,  <span class="hljs-attr">&quot;names&quot;</span>: [    &#123;      <span class="hljs-attr">&quot;O&quot;</span>: <span class="hljs-string">&quot;etcd&quot;</span>,      <span class="hljs-attr">&quot;OU&quot;</span>: <span class="hljs-string">&quot;etcd Security&quot;</span>,      <span class="hljs-attr">&quot;L&quot;</span>: <span class="hljs-string">&quot;Beijing&quot;</span>,      <span class="hljs-attr">&quot;ST&quot;</span>: <span class="hljs-string">&quot;Beijing&quot;</span>,      <span class="hljs-attr">&quot;C&quot;</span>: <span class="hljs-string">&quot;CN&quot;</span>    &#125;  ],  <span class="hljs-attr">&quot;CN&quot;</span>: <span class="hljs-string">&quot;etcd-root-ca&quot;</span>&#125;</code></pre></div><ul><li>etcd-gencert.json</li></ul><div class="hljs code-wrapper"><pre><code class="hljs json">&#123;  <span class="hljs-attr">&quot;signing&quot;</span>: &#123;    <span class="hljs-attr">&quot;default&quot;</span>: &#123;        <span class="hljs-attr">&quot;usages&quot;</span>: [          <span class="hljs-string">&quot;signing&quot;</span>,          <span class="hljs-string">&quot;key encipherment&quot;</span>,          <span class="hljs-string">&quot;server auth&quot;</span>,          <span class="hljs-string">&quot;client auth&quot;</span>        ],        <span class="hljs-attr">&quot;expiry&quot;</span>: <span class="hljs-string">&quot;87600h&quot;</span>    &#125;  &#125;&#125;</code></pre></div><ul><li>etcd-csr.json</li></ul><div class="hljs code-wrapper"><pre><code class="hljs json">&#123;  <span class="hljs-attr">&quot;key&quot;</span>: &#123;    <span class="hljs-attr">&quot;algo&quot;</span>: <span class="hljs-string">&quot;rsa&quot;</span>,    <span class="hljs-attr">&quot;size&quot;</span>: <span class="hljs-number">4096</span>  &#125;,  <span class="hljs-attr">&quot;names&quot;</span>: [    &#123;      <span class="hljs-attr">&quot;O&quot;</span>: <span class="hljs-string">&quot;etcd&quot;</span>,      <span class="hljs-attr">&quot;OU&quot;</span>: <span class="hljs-string">&quot;etcd Security&quot;</span>,      <span class="hljs-attr">&quot;L&quot;</span>: <span class="hljs-string">&quot;Beijing&quot;</span>,      <span class="hljs-attr">&quot;ST&quot;</span>: <span class="hljs-string">&quot;Beijing&quot;</span>,      <span class="hljs-attr">&quot;C&quot;</span>: <span class="hljs-string">&quot;CN&quot;</span>    &#125;  ],  <span class="hljs-attr">&quot;CN&quot;</span>: <span class="hljs-string">&quot;etcd&quot;</span>,  <span class="hljs-attr">&quot;hosts&quot;</span>: [    <span class="hljs-string">&quot;127.0.0.1&quot;</span>,    <span class="hljs-string">&quot;localhost&quot;</span>,    <span class="hljs-string">&quot;192.168.1.11&quot;</span>,    <span class="hljs-string">&quot;192.168.1.12&quot;</span>,    <span class="hljs-string">&quot;192.168.1.13&quot;</span>,    <span class="hljs-string">&quot;192.168.1.14&quot;</span>,    <span class="hljs-string">&quot;192.168.1.15&quot;</span>  ]&#125;</code></pre></div><p>最后生成 Etcd 证书</p><div class="hljs code-wrapper"><pre><code class="hljs sh">cfssl gencert --initca=<span class="hljs-literal">true</span> etcd-root-ca-csr.json | cfssljson --bare etcd-root-cacfssl gencert --ca etcd-root-ca.pem --ca-key etcd-root-ca-key.pem --config etcd-gencert.json etcd-csr.json | cfssljson --bare etcd</code></pre></div><p>生成的证书列表如下</p><p><img src="https://cdn.oss.link/markdown/2x0ja.jpg" alt="Etcd Certs"></p><h4 id="2-4、生成-Kubernetes-证书"><a href="#2-4、生成-Kubernetes-证书" class="headerlink" title="2.4、生成 Kubernetes 证书"></a>2.4、生成 Kubernetes 证书</h4><p>Kubernetes 证书生成所需配置文件如下:</p><ul><li>k8s-root-ca-csr.json</li></ul><div class="hljs code-wrapper"><pre><code class="hljs json">&#123;  <span class="hljs-attr">&quot;CN&quot;</span>: <span class="hljs-string">&quot;kubernetes&quot;</span>,  <span class="hljs-attr">&quot;key&quot;</span>: &#123;    <span class="hljs-attr">&quot;algo&quot;</span>: <span class="hljs-string">&quot;rsa&quot;</span>,    <span class="hljs-attr">&quot;size&quot;</span>: <span class="hljs-number">4096</span>  &#125;,  <span class="hljs-attr">&quot;names&quot;</span>: [    &#123;      <span class="hljs-attr">&quot;C&quot;</span>: <span class="hljs-string">&quot;CN&quot;</span>,      <span class="hljs-attr">&quot;ST&quot;</span>: <span class="hljs-string">&quot;BeiJing&quot;</span>,      <span class="hljs-attr">&quot;L&quot;</span>: <span class="hljs-string">&quot;BeiJing&quot;</span>,      <span class="hljs-attr">&quot;O&quot;</span>: <span class="hljs-string">&quot;k8s&quot;</span>,      <span class="hljs-attr">&quot;OU&quot;</span>: <span class="hljs-string">&quot;System&quot;</span>    &#125;  ]&#125;</code></pre></div><ul><li>k8s-gencert.json</li></ul><div class="hljs code-wrapper"><pre><code class="hljs json">&#123;  <span class="hljs-attr">&quot;signing&quot;</span>: &#123;    <span class="hljs-attr">&quot;default&quot;</span>: &#123;      <span class="hljs-attr">&quot;expiry&quot;</span>: <span class="hljs-string">&quot;87600h&quot;</span>    &#125;,    <span class="hljs-attr">&quot;profiles&quot;</span>: &#123;      <span class="hljs-attr">&quot;kubernetes&quot;</span>: &#123;        <span class="hljs-attr">&quot;usages&quot;</span>: [            <span class="hljs-string">&quot;signing&quot;</span>,            <span class="hljs-string">&quot;key encipherment&quot;</span>,            <span class="hljs-string">&quot;server auth&quot;</span>,            <span class="hljs-string">&quot;client auth&quot;</span>        ],        <span class="hljs-attr">&quot;expiry&quot;</span>: <span class="hljs-string">&quot;87600h&quot;</span>      &#125;    &#125;  &#125;&#125;</code></pre></div><ul><li>kubernetes-csr.json</li></ul><div class="hljs code-wrapper"><pre><code class="hljs json">&#123;    <span class="hljs-attr">&quot;CN&quot;</span>: <span class="hljs-string">&quot;kubernetes&quot;</span>,    <span class="hljs-attr">&quot;hosts&quot;</span>: [        <span class="hljs-string">&quot;127.0.0.1&quot;</span>,        <span class="hljs-string">&quot;10.254.0.1&quot;</span>,        <span class="hljs-string">&quot;192.168.1.11&quot;</span>,        <span class="hljs-string">&quot;192.168.1.12&quot;</span>,        <span class="hljs-string">&quot;192.168.1.13&quot;</span>,        <span class="hljs-string">&quot;192.168.1.14&quot;</span>,        <span class="hljs-string">&quot;192.168.1.15&quot;</span>,        <span class="hljs-string">&quot;localhost&quot;</span>,        <span class="hljs-string">&quot;kubernetes&quot;</span>,        <span class="hljs-string">&quot;kubernetes.default&quot;</span>,        <span class="hljs-string">&quot;kubernetes.default.svc&quot;</span>,        <span class="hljs-string">&quot;kubernetes.default.svc.cluster&quot;</span>,        <span class="hljs-string">&quot;kubernetes.default.svc.cluster.local&quot;</span>    ],    <span class="hljs-attr">&quot;key&quot;</span>: &#123;        <span class="hljs-attr">&quot;algo&quot;</span>: <span class="hljs-string">&quot;rsa&quot;</span>,        <span class="hljs-attr">&quot;size&quot;</span>: <span class="hljs-number">2048</span>    &#125;,    <span class="hljs-attr">&quot;names&quot;</span>: [        &#123;            <span class="hljs-attr">&quot;C&quot;</span>: <span class="hljs-string">&quot;CN&quot;</span>,            <span class="hljs-attr">&quot;ST&quot;</span>: <span class="hljs-string">&quot;BeiJing&quot;</span>,            <span class="hljs-attr">&quot;L&quot;</span>: <span class="hljs-string">&quot;BeiJing&quot;</span>,            <span class="hljs-attr">&quot;O&quot;</span>: <span class="hljs-string">&quot;k8s&quot;</span>,            <span class="hljs-attr">&quot;OU&quot;</span>: <span class="hljs-string">&quot;System&quot;</span>        &#125;    ]&#125;</code></pre></div><ul><li>kube-proxy-csr.json</li></ul><div class="hljs code-wrapper"><pre><code class="hljs json">&#123;  <span class="hljs-attr">&quot;CN&quot;</span>: <span class="hljs-string">&quot;system:kube-proxy&quot;</span>,  <span class="hljs-attr">&quot;hosts&quot;</span>: [],  <span class="hljs-attr">&quot;key&quot;</span>: &#123;    <span class="hljs-attr">&quot;algo&quot;</span>: <span class="hljs-string">&quot;rsa&quot;</span>,    <span class="hljs-attr">&quot;size&quot;</span>: <span class="hljs-number">2048</span>  &#125;,  <span class="hljs-attr">&quot;names&quot;</span>: [    &#123;      <span class="hljs-attr">&quot;C&quot;</span>: <span class="hljs-string">&quot;CN&quot;</span>,      <span class="hljs-attr">&quot;ST&quot;</span>: <span class="hljs-string">&quot;BeiJing&quot;</span>,      <span class="hljs-attr">&quot;L&quot;</span>: <span class="hljs-string">&quot;BeiJing&quot;</span>,      <span class="hljs-attr">&quot;O&quot;</span>: <span class="hljs-string">&quot;k8s&quot;</span>,      <span class="hljs-attr">&quot;OU&quot;</span>: <span class="hljs-string">&quot;System&quot;</span>    &#125;  ]&#125;</code></pre></div><ul><li>admin-csr.json</li></ul><div class="hljs code-wrapper"><pre><code class="hljs json">&#123;  <span class="hljs-attr">&quot;CN&quot;</span>: <span class="hljs-string">&quot;admin&quot;</span>,  <span class="hljs-attr">&quot;hosts&quot;</span>: [],  <span class="hljs-attr">&quot;key&quot;</span>: &#123;    <span class="hljs-attr">&quot;algo&quot;</span>: <span class="hljs-string">&quot;rsa&quot;</span>,    <span class="hljs-attr">&quot;size&quot;</span>: <span class="hljs-number">2048</span>  &#125;,  <span class="hljs-attr">&quot;names&quot;</span>: [    &#123;      <span class="hljs-attr">&quot;C&quot;</span>: <span class="hljs-string">&quot;CN&quot;</span>,      <span class="hljs-attr">&quot;ST&quot;</span>: <span class="hljs-string">&quot;BeiJing&quot;</span>,      <span class="hljs-attr">&quot;L&quot;</span>: <span class="hljs-string">&quot;BeiJing&quot;</span>,      <span class="hljs-attr">&quot;O&quot;</span>: <span class="hljs-string">&quot;system:masters&quot;</span>,      <span class="hljs-attr">&quot;OU&quot;</span>: <span class="hljs-string">&quot;System&quot;</span>    &#125;  ]&#125;</code></pre></div><p>生成 Kubernetes 证书</p><div class="hljs code-wrapper"><pre><code class="hljs sh">cfssl gencert --initca=<span class="hljs-literal">true</span> k8s-root-ca-csr.json | cfssljson --bare k8s-root-ca<span class="hljs-keyword">for</span> targetName <span class="hljs-keyword">in</span> kubernetes admin kube-proxy; <span class="hljs-keyword">do</span>    cfssl gencert --ca k8s-root-ca.pem --ca-key k8s-root-ca-key.pem --config k8s-gencert.json --profile kubernetes <span class="hljs-variable">$targetName</span>-csr.json | cfssljson --bare <span class="hljs-variable">$targetName</span><span class="hljs-keyword">done</span></code></pre></div><p>生成后证书列表如下</p><p><img src="https://cdn.oss.link/markdown/uj9q0.jpg" alt="Kubernetes Certs"></p><h4 id="2-5、生成-token-及-kubeconfig"><a href="#2-5、生成-token-及-kubeconfig" class="headerlink" title="2.5、生成 token 及 kubeconfig"></a>2.5、生成 token 及 kubeconfig</h4><p>生成 token 如下</p><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-built_in">export</span> BOOTSTRAP_TOKEN=$(head -c 16 /dev/urandom | od -An -t x | tr -d <span class="hljs-string">&#x27; &#x27;</span>)cat &gt; token.csv &lt;&lt;<span class="hljs-string">EOF</span><span class="hljs-string">$&#123;BOOTSTRAP_TOKEN&#125;,kubelet-bootstrap,10001,&quot;system:kubelet-bootstrap&quot;</span><span class="hljs-string">EOF</span></code></pre></div><p>创建 kubelet bootstrapping kubeconfig 配置(需要提前安装 kubectl 命令)，<strong>对于 node 节点，api server 地址为本地 nginx 监听的 127.0.0.1:6443，如果想把 master 也当做 node 使用，那么 master 上 api server 地址应该为 masterIP:6443，因为在 master 上没必要也无法启动 nginx 来监听 127.0.0.1:6443(6443 已经被 master 上的 api server 占用了)</strong></p><p><strong>所以以下配置只适合 node 节点，如果想把 master 也当做 node，那么需要重新生成下面的 kubeconfig 配置，并把 api server 地址修改为当前 master 的 api server 地址</strong></p><p><strong>没看懂上面这段话，照着下面的操作就行，看完下面的 Master HA 示意图就懂了</strong></p><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment"># Master 上该地址应为 https://MasterIP:6443</span><span class="hljs-built_in">export</span> KUBE_APISERVER=<span class="hljs-string">&quot;https://127.0.0.1:6443&quot;</span><span class="hljs-comment"># 设置集群参数</span>kubectl config set-cluster kubernetes \  --certificate-authority=k8s-root-ca.pem \  --embed-certs=<span class="hljs-literal">true</span> \  --server=<span class="hljs-variable">$&#123;KUBE_APISERVER&#125;</span> \  --kubeconfig=bootstrap.kubeconfig<span class="hljs-comment"># 设置客户端认证参数</span>kubectl config set-credentials kubelet-bootstrap \  --token=<span class="hljs-variable">$&#123;BOOTSTRAP_TOKEN&#125;</span> \  --kubeconfig=bootstrap.kubeconfig<span class="hljs-comment"># 设置上下文参数</span>kubectl config set-context default \  --cluster=kubernetes \  --user=kubelet-bootstrap \  --kubeconfig=bootstrap.kubeconfig<span class="hljs-comment"># 设置默认上下文</span>kubectl config use-context default --kubeconfig=bootstrap.kubeconfig</code></pre></div><p>创建 kube-proxy kubeconfig 配置，同上面一样，如果想要把 master 当 node 使用，需要修改 api server</p><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment"># 设置集群参数</span>kubectl config set-cluster kubernetes \  --certificate-authority=k8s-root-ca.pem \  --embed-certs=<span class="hljs-literal">true</span> \  --server=<span class="hljs-variable">$&#123;KUBE_APISERVER&#125;</span> \  --kubeconfig=kube-proxy.kubeconfig<span class="hljs-comment"># 设置客户端认证参数</span>kubectl config set-credentials kube-proxy \  --client-certificate=kube-proxy.pem \  --client-key=kube-proxy-key.pem \  --embed-certs=<span class="hljs-literal">true</span> \  --kubeconfig=kube-proxy.kubeconfig<span class="hljs-comment"># 设置上下文参数</span>kubectl config set-context default \  --cluster=kubernetes \  --user=kube-proxy \  --kubeconfig=kube-proxy.kubeconfig<span class="hljs-comment"># 设置默认上下文</span>kubectl config use-context default --kubeconfig=kube-proxy.kubeconfig</code></pre></div><h3 id="三、部署-HA-ETCD"><a href="#三、部署-HA-ETCD" class="headerlink" title="三、部署 HA ETCD"></a>三、部署 HA ETCD</h3><h4 id="3-1、安装-Etcd"><a href="#3-1、安装-Etcd" class="headerlink" title="3.1、安装 Etcd"></a>3.1、安装 Etcd</h4><p>ETCD 直接采用 rpm 安装，RPM 可以从 <a href="https://src.fedoraproject.org/cgit/rpms/etcd.git/">Fedora 官方仓库</a> 获取 spec 文件自己 build，或者直接从 <a href="https://www.rpmfind.net/">rpmFind 网站</a> 搜索</p><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment"># 下载 rpm 包</span>wget ftp://195.220.108.108/linux/fedora/linux/development/rawhide/Everything/x86_64/os/Packages/e/etcd-3.1.9-1.fc27.x86_64.rpm<span class="hljs-comment"># 分发并安装 rpm</span><span class="hljs-keyword">for</span> IP <span class="hljs-keyword">in</span> `seq 1 3`; <span class="hljs-keyword">do</span>    scp etcd-3.1.9-1.fc27.x86_64.rpm root@192.168.1.1<span class="hljs-variable">$IP</span>:~    ssh root@192.168.1.1<span class="hljs-variable">$IP</span> rpm -ivh etcd-3.1.9-1.fc27.x86_64.rpm<span class="hljs-keyword">done</span></code></pre></div><h4 id="3-2、分发证书"><a href="#3-2、分发证书" class="headerlink" title="3.2、分发证书"></a>3.2、分发证书</h4><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-keyword">for</span> IP <span class="hljs-keyword">in</span> `seq 1 3`;<span class="hljs-keyword">do</span>    ssh root@192.168.1.1<span class="hljs-variable">$IP</span> mkdir /etc/etcd/ssl    scp *.pem root@192.168.1.1<span class="hljs-variable">$IP</span>:/etc/etcd/ssl    ssh root@192.168.1.1<span class="hljs-variable">$IP</span> chown -R etcd:etcd /etc/etcd/ssl    ssh root@192.168.1.1<span class="hljs-variable">$IP</span> chmod -R 755 /etc/etcd<span class="hljs-keyword">done</span></code></pre></div><h4 id="3-3、修改配置"><a href="#3-3、修改配置" class="headerlink" title="3.3、修改配置"></a>3.3、修改配置</h4><p>rpm 安装好以后直接修改 <code>/etc/etcd/etcd.conf</code> 配置文件即可，其中单个节点配置如下(其他节点只是名字和 IP 不同)</p><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment"># [member]</span>ETCD_NAME=etcd1ETCD_DATA_DIR=<span class="hljs-string">&quot;/var/lib/etcd/etcd1.etcd&quot;</span>ETCD_WAL_DIR=<span class="hljs-string">&quot;/var/lib/etcd/wal&quot;</span>ETCD_SNAPSHOT_COUNT=<span class="hljs-string">&quot;100&quot;</span>ETCD_HEARTBEAT_INTERVAL=<span class="hljs-string">&quot;100&quot;</span>ETCD_ELECTION_TIMEOUT=<span class="hljs-string">&quot;1000&quot;</span>ETCD_LISTEN_PEER_URLS=<span class="hljs-string">&quot;https://192.168.1.11:2380&quot;</span>ETCD_LISTEN_CLIENT_URLS=<span class="hljs-string">&quot;https://192.168.1.11:2379,http://127.0.0.1:2379&quot;</span>ETCD_MAX_SNAPSHOTS=<span class="hljs-string">&quot;5&quot;</span>ETCD_MAX_WALS=<span class="hljs-string">&quot;5&quot;</span><span class="hljs-comment">#ETCD_CORS=&quot;&quot;</span><span class="hljs-comment"># [cluster]</span>ETCD_INITIAL_ADVERTISE_PEER_URLS=<span class="hljs-string">&quot;https://192.168.1.11:2380&quot;</span><span class="hljs-comment"># if you use different ETCD_NAME (e.g. test), set ETCD_INITIAL_CLUSTER value for this name, i.e. &quot;test=http://...&quot;</span>ETCD_INITIAL_CLUSTER=<span class="hljs-string">&quot;etcd1=https://192.168.1.11:2380,etcd2=https://192.168.1.12:2380,etcd3=https://192.168.1.13:2380&quot;</span>ETCD_INITIAL_CLUSTER_STATE=<span class="hljs-string">&quot;new&quot;</span>ETCD_INITIAL_CLUSTER_TOKEN=<span class="hljs-string">&quot;etcd-cluster&quot;</span>ETCD_ADVERTISE_CLIENT_URLS=<span class="hljs-string">&quot;https://192.168.1.11:2379&quot;</span><span class="hljs-comment">#ETCD_DISCOVERY=&quot;&quot;</span><span class="hljs-comment">#ETCD_DISCOVERY_SRV=&quot;&quot;</span><span class="hljs-comment">#ETCD_DISCOVERY_FALLBACK=&quot;proxy&quot;</span><span class="hljs-comment">#ETCD_DISCOVERY_PROXY=&quot;&quot;</span><span class="hljs-comment">#ETCD_STRICT_RECONFIG_CHECK=&quot;false&quot;</span><span class="hljs-comment">#ETCD_AUTO_COMPACTION_RETENTION=&quot;0&quot;</span><span class="hljs-comment"># [proxy]</span><span class="hljs-comment">#ETCD_PROXY=&quot;off&quot;</span><span class="hljs-comment">#ETCD_PROXY_FAILURE_WAIT=&quot;5000&quot;</span><span class="hljs-comment">#ETCD_PROXY_REFRESH_INTERVAL=&quot;30000&quot;</span><span class="hljs-comment">#ETCD_PROXY_DIAL_TIMEOUT=&quot;1000&quot;</span><span class="hljs-comment">#ETCD_PROXY_WRITE_TIMEOUT=&quot;5000&quot;</span><span class="hljs-comment">#ETCD_PROXY_READ_TIMEOUT=&quot;0&quot;</span><span class="hljs-comment"># [security]</span>ETCD_CERT_FILE=<span class="hljs-string">&quot;/etc/etcd/ssl/etcd.pem&quot;</span>ETCD_KEY_FILE=<span class="hljs-string">&quot;/etc/etcd/ssl/etcd-key.pem&quot;</span>ETCD_CLIENT_CERT_AUTH=<span class="hljs-string">&quot;true&quot;</span>ETCD_TRUSTED_CA_FILE=<span class="hljs-string">&quot;/etc/etcd/ssl/etcd-root-ca.pem&quot;</span>ETCD_AUTO_TLS=<span class="hljs-string">&quot;true&quot;</span>ETCD_PEER_CERT_FILE=<span class="hljs-string">&quot;/etc/etcd/ssl/etcd.pem&quot;</span>ETCD_PEER_KEY_FILE=<span class="hljs-string">&quot;/etc/etcd/ssl/etcd-key.pem&quot;</span>ETCD_PEER_CLIENT_CERT_AUTH=<span class="hljs-string">&quot;true&quot;</span>ETCD_PEER_TRUSTED_CA_FILE=<span class="hljs-string">&quot;/etc/etcd/ssl/etcd-root-ca.pem&quot;</span>ETCD_PEER_AUTO_TLS=<span class="hljs-string">&quot;true&quot;</span><span class="hljs-comment"># [logging]</span><span class="hljs-comment">#ETCD_DEBUG=&quot;false&quot;</span><span class="hljs-comment"># examples for -log-package-levels etcdserver=WARNING,security=DEBUG</span><span class="hljs-comment">#ETCD_LOG_PACKAGE_LEVELS=&quot;&quot;</span></code></pre></div><h4 id="3-4、启动及验证"><a href="#3-4、启动及验证" class="headerlink" title="3.4、启动及验证"></a>3.4、启动及验证</h4><p>配置修改后在每个节点进行启动即可，<strong>注意，Etcd 各个节点间必须保证时钟同步，否则会造成启动失败等错误</strong></p><div class="hljs code-wrapper"><pre><code class="hljs sh">systemctl daemon-reloadsystemctl start etcdsystemctl <span class="hljs-built_in">enable</span> etcd</code></pre></div><p>启动成功后验证节点状态</p><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-built_in">export</span> ETCDCTL_API=3etcdctl --cacert=/etc/etcd/ssl/etcd-root-ca.pem --cert=/etc/etcd/ssl/etcd.pem --key=/etc/etcd/ssl/etcd-key.pem --endpoints=https://192.168.1.11:2379,https://192.168.1.12:2379,https://192.168.1.13:2379 endpoint health</code></pre></div><p>最后截图如下，警告可忽略</p><p><img src="https://cdn.oss.link/markdown/yr6k4.jpg" alt="Etcd Healthy"></p><h3 id="四、部署-HA-Master"><a href="#四、部署-HA-Master" class="headerlink" title="四、部署 HA Master"></a>四、部署 HA Master</h3><h4 id="4-1、HA-Master-简述"><a href="#4-1、HA-Master-简述" class="headerlink" title="4.1、HA Master 简述"></a>4.1、HA Master 简述</h4><p>目前所谓的 Kubernetes HA 其实主要的就是 API Server 的 HA，master 上其他组件比如 controller-manager 等都是可以通过 Etcd 做选举；而 API Server 只是提供一个请求接收服务，所以对于 API Server 一般有两种方式做 HA；一种是对多个 API Server 做 vip，另一种使用 nginx 反向代理，本文采用 nginx 方式，以下为 HA 示意图</p><p><img src="https://cdn.oss.link/markdown/m2sug.jpg" alt="master ha"></p><p><strong>master 之间除 api server 以外其他组件通过 etcd 选举，api server 默认不作处理；在每个 node 上启动一个 nginx，每个 nginx 反向代理所有 api server，node 上 kubelet、kube-proxy 连接本地的 nginx 代理端口，当 nginx 发现无法连接后端时会自动踢掉出问题的 api server，从而实现 api server 的 HA</strong></p><h4 id="4-2、部署前预处理"><a href="#4-2、部署前预处理" class="headerlink" title="4.2、部署前预处理"></a>4.2、部署前预处理</h4><p>一切以偷懒为主，所以我们仍然采用 rpm 的方式来安装 kubernetes 各个组件，关于 rpm 获取方式可以参考 <a href="https://mritd.me/2017/07/12/how-to-build-kubernetes-rpm/">How to build Kubernetes RPM</a>，以下文章默认认为你已经搞定了 rpm</p><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment"># 分发 rpm</span><span class="hljs-keyword">for</span> IP <span class="hljs-keyword">in</span> `seq 1 3`; <span class="hljs-keyword">do</span>    scp kubernetes*.rpm root@192.168.1.1<span class="hljs-variable">$IP</span>:~;     ssh root@192.168.1.1<span class="hljs-variable">$IP</span> yum install -y conntrack-tools socat    ssh root@192.168.1.1<span class="hljs-variable">$IP</span> rpm -ivh kubernetes*.rpm<span class="hljs-keyword">done</span></code></pre></div><p>rpm 安装好以后还需要进行分发证书配置等</p><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-keyword">for</span> IP <span class="hljs-keyword">in</span> `seq 1 3`;<span class="hljs-keyword">do</span>    ssh root@192.168.1.1<span class="hljs-variable">$IP</span> mkdir /etc/kubernetes/ssl    scp *.pem root@192.168.1.1<span class="hljs-variable">$IP</span>:/etc/kubernetes/ssl    scp *.kubeconfig root@192.168.1.1<span class="hljs-variable">$IP</span>:/etc/kubernetes    scp token.csv root@192.168.1.1<span class="hljs-variable">$IP</span>:/etc/kubernetes    ssh root@192.168.1.1<span class="hljs-variable">$IP</span> chown -R kube:kube /etc/kubernetes/ssl<span class="hljs-keyword">done</span></code></pre></div><p><strong>最后由于 api server 会写入一些日志，所以先创建好相关目录，并做好授权，防止因为权限错误导致 api server 无法启动</strong></p><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-keyword">for</span> IP <span class="hljs-keyword">in</span> `seq 1 3`;<span class="hljs-keyword">do</span>    ssh root@192.168.1.1<span class="hljs-variable">$IP</span> mkdir /var/<span class="hljs-built_in">log</span>/kube-audit      ssh root@192.168.1.1<span class="hljs-variable">$IP</span> chown -R kube:kube /var/<span class="hljs-built_in">log</span>/kube-audit    ssh root@192.168.1.1<span class="hljs-variable">$IP</span> chmod -R 755 /var/<span class="hljs-built_in">log</span>/kube-audit<span class="hljs-keyword">done</span></code></pre></div><h4 id="4-3、修改-master-配置"><a href="#4-3、修改-master-配置" class="headerlink" title="4.3、修改 master 配置"></a>4.3、修改 master 配置</h4><p>rpm 安装好以后，默认会生成 <code>/etc/kubernetes</code> 目录，并且该目录中会有很多配置，其中 config 配置文件为通用配置，具体文件如下</p><div class="hljs code-wrapper"><pre><code class="hljs sh">➜  kubernetes tree.├── apiserver├── config├── controller-manager├── kubelet├── proxy└── scheduler0 directories, 6 files</code></pre></div><p><strong>master 需要编辑 <code>config</code>、<code>apiserver</code>、<code>controller-manager</code>、<code>scheduler</code>这四个文件，具体修改如下</strong></p><ul><li>config 通用配置</li></ul><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment">###</span><span class="hljs-comment"># kubernetes system config</span><span class="hljs-comment">#</span><span class="hljs-comment"># The following values are used to configure various aspects of all</span><span class="hljs-comment"># kubernetes services, including</span><span class="hljs-comment">#</span><span class="hljs-comment">#   kube-apiserver.service</span><span class="hljs-comment">#   kube-controller-manager.service</span><span class="hljs-comment">#   kube-scheduler.service</span><span class="hljs-comment">#   kubelet.service</span><span class="hljs-comment">#   kube-proxy.service</span><span class="hljs-comment"># logging to stderr means we get it in the systemd journal</span>KUBE_LOGTOSTDERR=<span class="hljs-string">&quot;--logtostderr=true&quot;</span><span class="hljs-comment"># journal message level, 0 is debug</span>KUBE_LOG_LEVEL=<span class="hljs-string">&quot;--v=2&quot;</span><span class="hljs-comment"># Should this cluster be allowed to run privileged docker containers</span>KUBE_ALLOW_PRIV=<span class="hljs-string">&quot;--allow-privileged=true&quot;</span><span class="hljs-comment"># How the controller-manager, scheduler, and proxy find the apiserver</span>KUBE_MASTER=<span class="hljs-string">&quot;--master=http://127.0.0.1:8080&quot;</span></code></pre></div><ul><li>apiserver 配置(其他节点只有 IP 不同)</li></ul><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment">###</span><span class="hljs-comment"># kubernetes system config</span><span class="hljs-comment">#</span><span class="hljs-comment"># The following values are used to configure the kube-apiserver</span><span class="hljs-comment">#</span><span class="hljs-comment"># The address on the local server to listen to.</span>KUBE_API_ADDRESS=<span class="hljs-string">&quot;--advertise-address=192.168.1.11 --insecure-bind-address=127.0.0.1 --bind-address=192.168.1.11&quot;</span><span class="hljs-comment"># The port on the local server to listen on.</span>KUBE_API_PORT=<span class="hljs-string">&quot;--insecure-port=8080 --secure-port=6443&quot;</span><span class="hljs-comment"># Port minions listen on</span><span class="hljs-comment"># KUBELET_PORT=&quot;--kubelet-port=10250&quot;</span><span class="hljs-comment"># Comma separated list of nodes in the etcd cluster</span>KUBE_ETCD_SERVERS=<span class="hljs-string">&quot;--etcd-servers=https://192.168.1.11:2379,https://192.168.1.12:2379,https://192.168.1.13:2379&quot;</span><span class="hljs-comment"># Address range to use for services</span>KUBE_SERVICE_ADDRESSES=<span class="hljs-string">&quot;--service-cluster-ip-range=10.254.0.0/16&quot;</span><span class="hljs-comment"># default admission control policies</span>KUBE_ADMISSION_CONTROL=<span class="hljs-string">&quot;--admission-control=NamespaceLifecycle,LimitRanger,SecurityContextDeny,ServiceAccount,ResourceQuota&quot;</span><span class="hljs-comment"># Add your own!</span>KUBE_API_ARGS=<span class="hljs-string">&quot;--authorization-mode=RBAC \</span><span class="hljs-string">               --runtime-config=rbac.authorization.k8s.io/v1beta1 \</span><span class="hljs-string">               --anonymous-auth=false \</span><span class="hljs-string">               --kubelet-https=true \</span><span class="hljs-string">               --experimental-bootstrap-token-auth \</span><span class="hljs-string">               --token-auth-file=/etc/kubernetes/token.csv \</span><span class="hljs-string">               --service-node-port-range=30000-50000 \</span><span class="hljs-string">               --tls-cert-file=/etc/kubernetes/ssl/kubernetes.pem \</span><span class="hljs-string">               --tls-private-key-file=/etc/kubernetes/ssl/kubernetes-key.pem \</span><span class="hljs-string">               --client-ca-file=/etc/kubernetes/ssl/k8s-root-ca.pem \</span><span class="hljs-string">               --service-account-key-file=/etc/kubernetes/ssl/k8s-root-ca.pem \</span><span class="hljs-string">               --etcd-quorum-read=true \</span><span class="hljs-string">               --storage-backend=etcd3 \</span><span class="hljs-string">               --etcd-cafile=/etc/etcd/ssl/etcd-root-ca.pem \</span><span class="hljs-string">               --etcd-certfile=/etc/etcd/ssl/etcd.pem \</span><span class="hljs-string">               --etcd-keyfile=/etc/etcd/ssl/etcd-key.pem \</span><span class="hljs-string">               --enable-swagger-ui=true \</span><span class="hljs-string">               --apiserver-count=3 \</span><span class="hljs-string">               --audit-log-maxage=30 \</span><span class="hljs-string">               --audit-log-maxbackup=3 \</span><span class="hljs-string">               --audit-log-maxsize=100 \</span><span class="hljs-string">               --audit-log-path=/var/log/kube-audit/audit.log \</span><span class="hljs-string">               --event-ttl=1h&quot;</span></code></pre></div><ul><li>controller-manager 配置</li></ul><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment">###</span><span class="hljs-comment"># The following values are used to configure the kubernetes controller-manager</span><span class="hljs-comment"># defaults from config and apiserver should be adequate</span><span class="hljs-comment"># Add your own!</span>KUBE_CONTROLLER_MANAGER_ARGS=<span class="hljs-string">&quot;--address=0.0.0.0 \</span><span class="hljs-string">                              --service-cluster-ip-range=10.254.0.0/16 \</span><span class="hljs-string">                              --cluster-name=kubernetes \</span><span class="hljs-string">                              --cluster-signing-cert-file=/etc/kubernetes/ssl/k8s-root-ca.pem \</span><span class="hljs-string">                              --cluster-signing-key-file=/etc/kubernetes/ssl/k8s-root-ca-key.pem \</span><span class="hljs-string">                              --service-account-private-key-file=/etc/kubernetes/ssl/k8s-root-ca-key.pem \</span><span class="hljs-string">                              --root-ca-file=/etc/kubernetes/ssl/k8s-root-ca.pem \</span><span class="hljs-string">                              --experimental-cluster-signing-duration=87600h0m0s \</span><span class="hljs-string">                              --leader-elect=true \</span><span class="hljs-string">                              --node-monitor-grace-period=40s \</span><span class="hljs-string">                              --node-monitor-period=5s \</span><span class="hljs-string">                              --pod-eviction-timeout=5m0s&quot;</span></code></pre></div><ul><li>scheduler 配置</li></ul><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment">###</span><span class="hljs-comment"># kubernetes scheduler config</span><span class="hljs-comment"># default config should be adequate</span><span class="hljs-comment"># Add your own!</span>KUBE_SCHEDULER_ARGS=<span class="hljs-string">&quot;--leader-elect=true --address=0.0.0.0&quot;</span></code></pre></div><p>其他 master 节点配置相同，只需要修改以下 IP 地址即可，修改完成后启动 api server</p><div class="hljs code-wrapper"><pre><code class="hljs sh">systemctl daemon-reloadsystemctl start kube-apiserversystemctl start kube-controller-managersystemctl start kube-schedulersystemctl <span class="hljs-built_in">enable</span> kube-apiserversystemctl <span class="hljs-built_in">enable</span> kube-controller-managersystemctl <span class="hljs-built_in">enable</span> kube-scheduler</code></pre></div><p>各个节点启动成功后，验证组件状态(kubectl 在不做任何配置的情况下默认链接本地 8080 端口)如下，<strong>其中 etcd 全部为 Unhealthy 状态，并且提示 <code>remote error: tls: bad certificate</code> 这是个 bug，不影响实际使用，具体可参考 <a href="https://github.com/kubernetes/kubernetes/issues/29330">issue</a></strong></p><p><img src="https://cdn.oss.link/markdown/0j7k2.jpg" alt="api status"></p><h3 id="五、部署-Node"><a href="#五、部署-Node" class="headerlink" title="五、部署 Node"></a>五、部署 Node</h3><h4 id="5-1、部署前预处理"><a href="#5-1、部署前预处理" class="headerlink" title="5.1、部署前预处理"></a>5.1、部署前预处理</h4><p>部署前分发 rpm 以及证书、token 等配置</p><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment"># 分发 rpm</span><span class="hljs-keyword">for</span> IP <span class="hljs-keyword">in</span> `seq 4 5`;<span class="hljs-keyword">do</span>    scp kubernetes-node-1.6.7-1.el7.centos.x86_64.rpm kubernetes-client-1.6.7-1.el7.centos.x86_64.rpm root@192.168.1.1<span class="hljs-variable">$IP</span>:~;     ssh root@192.168.1.1<span class="hljs-variable">$IP</span> yum install -y conntrack-tools socat    ssh root@192.168.1.1<span class="hljs-variable">$IP</span> rpm -ivh kubernetes-node-1.6.7-1.el7.centos.x86_64.rpm kubernetes-client-1.6.7-1.el7.centos.x86_64.rpm<span class="hljs-keyword">done</span><span class="hljs-comment"># 分发证书等配置文件</span><span class="hljs-keyword">for</span> IP <span class="hljs-keyword">in</span> `seq 4 5`;<span class="hljs-keyword">do</span>    ssh root@192.168.1.1<span class="hljs-variable">$IP</span> mkdir /etc/kubernetes/ssl    scp *.pem root@192.168.1.1<span class="hljs-variable">$IP</span>:/etc/kubernetes/ssl    scp *.kubeconfig root@192.168.1.1<span class="hljs-variable">$IP</span>:/etc/kubernetes    scp token.csv root@192.168.1.1<span class="hljs-variable">$IP</span>:/etc/kubernetes    ssh root@192.168.1.1<span class="hljs-variable">$IP</span> chown -R kube:kube /etc/kubernetes/ssl<span class="hljs-keyword">done</span></code></pre></div><h4 id="5-2、修改-node-配置"><a href="#5-2、修改-node-配置" class="headerlink" title="5.2、修改 node 配置"></a>5.2、修改 node 配置</h4><p>node 节点上配置文件同样位于 <code>/etc/kubernetes</code> 目录，node 节点只需要修改 <code>config</code>、<code>kubelet</code>、<code>proxy</code> 这三个配置文件，修改如下</p><ul><li>config 通用配置</li></ul><p><strong>注意: config 配置文件(包括下面的 kubelet、proxy)中全部未 定义 API Server 地址，因为 kubelet 和 kube-proxy 组件启动时使用了 <code>--require-kubeconfig</code> 选项，该选项会使其从 <code>*.kubeconfig</code> 中读取 API Server 地址，而忽略配置文件中设置的；所以配置文件中设置的地址其实是无效的</strong></p><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment">###</span><span class="hljs-comment"># kubernetes system config</span><span class="hljs-comment">#</span><span class="hljs-comment"># The following values are used to configure various aspects of all</span><span class="hljs-comment"># kubernetes services, including</span><span class="hljs-comment">#</span><span class="hljs-comment">#   kube-apiserver.service</span><span class="hljs-comment">#   kube-controller-manager.service</span><span class="hljs-comment">#   kube-scheduler.service</span><span class="hljs-comment">#   kubelet.service</span><span class="hljs-comment">#   kube-proxy.service</span><span class="hljs-comment"># logging to stderr means we get it in the systemd journal</span>KUBE_LOGTOSTDERR=<span class="hljs-string">&quot;--logtostderr=true&quot;</span><span class="hljs-comment"># journal message level, 0 is debug</span>KUBE_LOG_LEVEL=<span class="hljs-string">&quot;--v=2&quot;</span><span class="hljs-comment"># Should this cluster be allowed to run privileged docker containers</span>KUBE_ALLOW_PRIV=<span class="hljs-string">&quot;--allow-privileged=true&quot;</span><span class="hljs-comment"># How the controller-manager, scheduler, and proxy find the apiserver</span><span class="hljs-comment"># KUBE_MASTER=&quot;--master=http://127.0.0.1:8080&quot;</span></code></pre></div><ul><li>kubelet 配置</li></ul><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment">###</span><span class="hljs-comment"># kubernetes kubelet (minion) config</span><span class="hljs-comment"># The address for the info server to serve on (set to 0.0.0.0 or &quot;&quot; for all interfaces)</span>KUBELET_ADDRESS=<span class="hljs-string">&quot;--address=192.168.1.14&quot;</span><span class="hljs-comment"># The port for the info server to serve on</span><span class="hljs-comment"># KUBELET_PORT=&quot;--port=10250&quot;</span><span class="hljs-comment"># You may leave this blank to use the actual hostname</span>KUBELET_HOSTNAME=<span class="hljs-string">&quot;--hostname-override=docker4.node&quot;</span><span class="hljs-comment"># location of the api-server</span><span class="hljs-comment"># KUBELET_API_SERVER=&quot;&quot;</span><span class="hljs-comment"># Add your own!</span>KUBELET_ARGS=<span class="hljs-string">&quot;--cgroup-driver=cgroupfs \</span><span class="hljs-string">              --cluster-dns=10.254.0.2 \</span><span class="hljs-string">              --resolv-conf=/etc/resolv.conf \</span><span class="hljs-string">              --experimental-bootstrap-kubeconfig=/etc/kubernetes/bootstrap.kubeconfig \</span><span class="hljs-string">              --kubeconfig=/etc/kubernetes/kubelet.kubeconfig \</span><span class="hljs-string">              --require-kubeconfig \</span><span class="hljs-string">              --cert-dir=/etc/kubernetes/ssl \</span><span class="hljs-string">              --cluster-domain=cluster.local. \</span><span class="hljs-string">              --hairpin-mode promiscuous-bridge \</span><span class="hljs-string">              --serialize-image-pulls=false \</span><span class="hljs-string">              --pod-infra-container-image=gcr.io/google_containers/pause-amd64:3.0&quot;</span></code></pre></div><ul><li>proxy 配置</li></ul><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment">###</span><span class="hljs-comment"># kubernetes proxy config</span><span class="hljs-comment"># default config should be adequate</span><span class="hljs-comment"># Add your own!</span>KUBE_PROXY_ARGS=<span class="hljs-string">&quot;--bind-address=192.168.1.14 \</span><span class="hljs-string">                 --hostname-override=docker4.node \</span><span class="hljs-string">                 --kubeconfig=/etc/kubernetes/kube-proxy.kubeconfig \</span><span class="hljs-string">                 --cluster-cidr=10.254.0.0/16&quot;</span></code></pre></div><h4 id="5-3、创建-ClusterRoleBinding"><a href="#5-3、创建-ClusterRoleBinding" class="headerlink" title="5.3、创建 ClusterRoleBinding"></a>5.3、创建 ClusterRoleBinding</h4><p>由于 kubelet 采用了 <a href="https://kubernetes.io/docs/admin/kubelet-tls-bootstrapping/">TLS Bootstrapping</a>，所有根绝 RBAC 控制策略，kubelet 使用的用户 <code>kubelet-bootstrap</code> 是不具备任何访问 API 权限的，这是需要预先在集群内创建 ClusterRoleBinding 授予其 <code>system:node-bootstrapper</code> Role</p><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment"># 在任意 master 执行即可</span>kubectl create clusterrolebinding kubelet-bootstrap \  --clusterrole=system:node-bootstrapper \  --user=kubelet-bootstrap</code></pre></div><h4 id="5-4、创建-nginx-代理"><a href="#5-4、创建-nginx-代理" class="headerlink" title="5.4、创建 nginx 代理"></a>5.4、创建 nginx 代理</h4><p><strong>根据上面描述的 master HA 架构，此时所有 node 应该连接本地的 nginx 代理，然后 nginx 来负载所有 api server；以下为 nginx 代理相关配置</strong></p><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment"># 创建配置目录</span>mkdir -p /etc/nginx<span class="hljs-comment"># 写入代理配置</span>cat &lt;&lt; <span class="hljs-string">EOF &gt;&gt; /etc/nginx/nginx.conf</span><span class="hljs-string">error_log stderr notice;</span><span class="hljs-string"></span><span class="hljs-string">worker_processes auto;</span><span class="hljs-string">events &#123;</span><span class="hljs-string">  multi_accept on;</span><span class="hljs-string">  use epoll;</span><span class="hljs-string">  worker_connections 1024;</span><span class="hljs-string">&#125;</span><span class="hljs-string"></span><span class="hljs-string">stream &#123;</span><span class="hljs-string">    upstream kube_apiserver &#123;</span><span class="hljs-string">        least_conn;</span><span class="hljs-string">        server 192.168.1.11:6443;</span><span class="hljs-string">        server 192.168.1.12:6443;</span><span class="hljs-string">        server 192.168.1.13:6443;</span><span class="hljs-string">    &#125;</span><span class="hljs-string"></span><span class="hljs-string">    server &#123;</span><span class="hljs-string">        listen        0.0.0.0:6443;</span><span class="hljs-string">        proxy_pass    kube_apiserver;</span><span class="hljs-string">        proxy_timeout 10m;</span><span class="hljs-string">        proxy_connect_timeout 1s;</span><span class="hljs-string">    &#125;</span><span class="hljs-string">&#125;</span><span class="hljs-string">EOF</span><span class="hljs-comment"># 更新权限</span>chmod +r /etc/nginx/nginx.conf</code></pre></div><p>为了保证 nginx 的可靠性，综合便捷性考虑，<strong>node 节点上的 nginx 使用 docker 启动，同时 使用 systemd 来守护，</strong> systemd 配置如下</p><div class="hljs code-wrapper"><pre><code class="hljs sh">cat &lt;&lt; <span class="hljs-string">EOF &gt;&gt; /etc/systemd/system/nginx-proxy.service</span><span class="hljs-string">[Unit]</span><span class="hljs-string">Description=kubernetes apiserver docker wrapper</span><span class="hljs-string">Wants=docker.socket</span><span class="hljs-string">After=docker.service</span><span class="hljs-string"></span><span class="hljs-string">[Service]</span><span class="hljs-string">User=root</span><span class="hljs-string">PermissionsStartOnly=true</span><span class="hljs-string">ExecStart=/usr/bin/docker run -p 127.0.0.1:6443:6443 \\</span><span class="hljs-string">                              -v /etc/nginx:/etc/nginx \\</span><span class="hljs-string">                              --name nginx-proxy \\</span><span class="hljs-string">                              --net=host \\</span><span class="hljs-string">                              --restart=on-failure:5 \\</span><span class="hljs-string">                              --memory=512M \\</span><span class="hljs-string">                              nginx:1.13.3-alpine</span><span class="hljs-string">ExecStartPre=-/usr/bin/docker rm -f nginx-proxy</span><span class="hljs-string">ExecStop=/usr/bin/docker stop nginx-proxy</span><span class="hljs-string">Restart=always</span><span class="hljs-string">RestartSec=15s</span><span class="hljs-string">TimeoutStartSec=30s</span><span class="hljs-string"></span><span class="hljs-string">[Install]</span><span class="hljs-string">WantedBy=multi-user.target</span><span class="hljs-string">EOF</span></code></pre></div><p><strong>最后启动 nginx，同时在每个 node 安装 kubectl，然后使用 kubectl 测试 api server 负载情况</strong></p><div class="hljs code-wrapper"><pre><code class="hljs sh">systemctl daemon-reloadsystemctl start nginx-proxysystemctl <span class="hljs-built_in">enable</span> nginx-proxy</code></pre></div><p>启动成功后如下</p><p><img src="https://cdn.oss.link/markdown/0shgz.jpg" alt="nginx-proxy"></p><p>kubectl 测试联通性如下</p><p><img src="https://cdn.oss.link/markdown/maiz2.jpg" alt="test nginx-proxy"></p><h4 id="5-5、添加-Node"><a href="#5-5、添加-Node" class="headerlink" title="5.5、添加 Node"></a>5.5、添加 Node</h4><p>一起准备就绪以后就可以启动 node 相关组件了</p><div class="hljs code-wrapper"><pre><code class="hljs sh">systemctl daemon-reloadsystemctl start kubeletsystemctl <span class="hljs-built_in">enable</span> kubelet</code></pre></div><p>由于采用了 <a href="https://kubernetes.io/docs/admin/kubelet-tls-bootstrapping/">TLS Bootstrapping</a>，所以 kubelet 启动后不会立即加入集群，而是进行证书申请，从日志中可以看到如下输出</p><div class="hljs code-wrapper"><pre><code class="hljs sh">Jul 19 14:15:31 docker4.node kubelet[18213]: I0719 14:15:31.810914   18213 feature_gate.go:144] feature gates: map[]Jul 19 14:15:31 docker4.node kubelet[18213]: I0719 14:15:31.811025   18213 bootstrap.go:58] Using bootstrap kubeconfig to generate TLS client cert, key and kubeconfig file</code></pre></div><p><strong>此时只需要在 master 允许其证书申请即可</strong></p><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment"># 查看 csr</span>➜  kubectl get csrNAME        AGE       REQUESTOR           CONDITIONcsr-l9d25   2m        kubelet-bootstrap   Pending<span class="hljs-comment"># 签发证书</span>➜  kubectl certificate approve csr-l9d25certificatesigningrequest <span class="hljs-string">&quot;csr-l9d25&quot;</span> approved<span class="hljs-comment"># 查看 node</span>➜  kubectl get nodeNAME           STATUS    AGE       VERSIONdocker4.node   Ready     26s       v1.6.7</code></pre></div><p>最后再启动 kube-proxy 组件即可</p><div class="hljs code-wrapper"><pre><code class="hljs sh">systemctl start kube-proxysystemctl <span class="hljs-built_in">enable</span> kube-proxy</code></pre></div><h4 id="5-6、Master-开启-Pod-负载"><a href="#5-6、Master-开启-Pod-负载" class="headerlink" title="5.6、Master 开启 Pod 负载"></a>5.6、Master 开启 Pod 负载</h4><p>Master 上部署 Node 与单独 Node 部署大致相同，<strong>只需要修改 <code>bootstrap.kubeconfig</code>、<code>kube-proxy.kubeconfig</code> 中的 API Server 地址即可</strong></p><p><img src="https://cdn.oss.link/markdown/2gkyj.jpg" alt="modify api server"></p><p>然后修改 <code>kubelet</code>、<code>proxy</code> 配置启动即可</p><div class="hljs code-wrapper"><pre><code class="hljs sh">systemctl daemon-reloadsystemctl start kubeletsystemctl <span class="hljs-built_in">enable</span> kubeletsystemctl start kube-proxysystemctl <span class="hljs-built_in">enable</span> kube-proxy</code></pre></div><p>最后在 master 签发一下相关证书</p><div class="hljs code-wrapper"><pre><code class="hljs sh">kubectl certificate approve csr-z090b</code></pre></div><p>整体部署完成后如下</p><p><img src="https://cdn.oss.link/markdown/fsddv.jpg" alt="read"></p><h3 id="六、部署-Calico"><a href="#六、部署-Calico" class="headerlink" title="六、部署 Calico"></a>六、部署 Calico</h3><p>网路组件这里采用 Calico，Calico 目前部署也相对比较简单，只需要创建一下 yml 文件即可，具体可参考 <a href="http://docs.projectcalico.org/v2.3/getting-started/kubernetes/">Calico 官方文档</a></p><p><strong>Cliaco 官方文档要求 kubelet 启动时要配置使用 cni 插件 <code>--network-plugin=cni</code>，同时 kube-proxy 不能使用 <code>--masquerade-all</code> 启动(会与 Calico policy 冲突)，所以需要修改所有 kubelet 和 proxy 配置文件，以下默认为这两项已经调整完毕，这里不做演示</strong></p><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment"># 获取相关 Cliaco.yml</span>wget http://docs.projectcalico.org/v2.3/getting-started/kubernetes/installation/hosted/calico.yaml<span class="hljs-comment"># 修改 Etcd 相关配置，以下列出主要修改部分(etcd 证书内容需要被 base64 转码)</span>sed -i <span class="hljs-string">&#x27;s@.*etcd_endpoints:.*@\ \ etcd_endpoints:\ \&quot;https://192.168.1.11:2379,https://192.168.1.12:2379,https://192.168.1.13:2379\&quot;@gi&#x27;</span> calico.yaml<span class="hljs-built_in">export</span> ETCD_CERT=`cat /etc/etcd/ssl/etcd.pem | base64 | tr -d <span class="hljs-string">&#x27;\n&#x27;</span>`<span class="hljs-built_in">export</span> ETCD_KEY=`cat /etc/etcd/ssl/etcd-key.pem | base64 | tr -d <span class="hljs-string">&#x27;\n&#x27;</span>`<span class="hljs-built_in">export</span> ETCD_CA=`cat /etc/etcd/ssl/etcd-root-ca.pem | base64 | tr -d <span class="hljs-string">&#x27;\n&#x27;</span>`sed -i <span class="hljs-string">&quot;s@.*etcd-cert:.*@\ \ etcd-cert:\ <span class="hljs-variable">$&#123;ETCD_CERT&#125;</span>@gi&quot;</span> calico.yamlsed -i <span class="hljs-string">&quot;s@.*etcd-key:.*@\ \ etcd-key:\ <span class="hljs-variable">$&#123;ETCD_KEY&#125;</span>@gi&quot;</span> calico.yamlsed -i <span class="hljs-string">&quot;s@.*etcd-ca:.*@\ \ etcd-ca:\ <span class="hljs-variable">$&#123;ETCD_CA&#125;</span>@gi&quot;</span> calico.yamlsed -i <span class="hljs-string">&#x27;s@.*etcd_ca:.*@\ \ etcd_ca:\ &quot;/calico-secrets/etcd-ca&quot;@gi&#x27;</span> calico.yamlsed -i <span class="hljs-string">&#x27;s@.*etcd_cert:.*@\ \ etcd_cert:\ &quot;/calico-secrets/etcd-cert&quot;@gi&#x27;</span> calico.yamlsed -i <span class="hljs-string">&#x27;s@.*etcd_key:.*@\ \ etcd_key:\ &quot;/calico-secrets/etcd-key&quot;@gi&#x27;</span> calico.yamlsed -i <span class="hljs-string">&#x27;s@192.168.0.0/16@10.254.64.0/18@gi&#x27;</span> calico.yaml</code></pre></div><p>执行部署操作，<strong>注意，在开启 RBAC 的情况下需要单独创建 ClusterRole 和 ClusterRoleBinding</strong></p><div class="hljs code-wrapper"><pre><code class="hljs sh">kubectl create -f calico.yamlkubectl apply -f http://docs.projectcalico.org/v2.3/getting-started/kubernetes/installation/rbac.yaml</code></pre></div><p>部署完成后如下 </p><p><img src="https://cdn.oss.link/markdown/ocqsf.jpg" alt="caliaco"></p><p><strong>最后测试一下跨主机通讯</strong></p><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment"># 创建 deployment</span>cat &lt;&lt; <span class="hljs-string">EOF &gt;&gt; demo.deploy.yml</span><span class="hljs-string">apiVersion: apps/v1beta1</span><span class="hljs-string">kind: Deployment</span><span class="hljs-string">metadata:</span><span class="hljs-string">  name: demo-deployment</span><span class="hljs-string">spec:</span><span class="hljs-string">  replicas: 3</span><span class="hljs-string">  template:</span><span class="hljs-string">    metadata:</span><span class="hljs-string">      labels:</span><span class="hljs-string">        app: demo</span><span class="hljs-string">    spec:</span><span class="hljs-string">      containers:</span><span class="hljs-string">      - name: demo</span><span class="hljs-string">        image: mritd/demo</span><span class="hljs-string">        ports:</span><span class="hljs-string">        - containerPort: 80</span><span class="hljs-string">EOF</span>kubectl create -f demo.deploy.yml</code></pre></div><p>exec 到一台主机 pod 内 ping 另一个不同 node 上的 pod 如下</p><p><img src="https://cdn.oss.link/markdown/phkgr.jpg" alt="ping"></p><h3 id="七、部署-DNS"><a href="#七、部署-DNS" class="headerlink" title="七、部署 DNS"></a>七、部署 DNS</h3><h4 id="7-1、DNS-组件部署"><a href="#7-1、DNS-组件部署" class="headerlink" title="7.1、DNS 组件部署"></a>7.1、DNS 组件部署</h4><p>DNS 部署目前有两种方式，一种是纯手动，另一种是使用 <a href="https://github.com/kubernetes/kubernetes/blob/master/cluster/addons/addon-manager/README.md">Addon-manager</a>，目前个人感觉 Addon-manager 有点繁琐，所以以下采取纯手动部署 DNS 组件</p><p>DNS 组件相关文件位于 <a href="https://github.com/kubernetes/kubernetes/blob/master/cluster/addons/dns/README.md">kubernetes addons</a> 目录下，把相关文件下载下来然后稍作修改即可</p><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment"># 获取文件</span>mkdir dns &amp;&amp; <span class="hljs-built_in">cd</span> dnswget https://raw.githubusercontent.com/kubernetes/kubernetes/master/cluster/addons/dns/kubedns-cm.yamlwget https://raw.githubusercontent.com/kubernetes/kubernetes/master/cluster/addons/dns/kubedns-sa.yamlwget https://raw.githubusercontent.com/kubernetes/kubernetes/master/cluster/addons/dns/kubedns-svc.yaml.sedwget https://raw.githubusercontent.com/kubernetes/kubernetes/master/cluster/addons/dns/kubedns-controller.yaml.sedmv kubedns-controller.yaml.sed kubedns-controller.yamlmv kubedns-svc.yaml.sed kubedns-svc.yaml<span class="hljs-comment"># 修改配置</span>sed -i <span class="hljs-string">&#x27;s/$DNS_DOMAIN/cluster.local/gi&#x27;</span> kubedns-controller.yamlsed -i <span class="hljs-string">&#x27;s/$DNS_SERVER_IP/10.254.0.2/gi&#x27;</span> kubedns-svc.yaml<span class="hljs-comment"># 创建(我把所有 yml 放到的 dns 目录中)</span>kubectl create -f ../dns</code></pre></div><p><strong>接下来测试 DNS，</strong>测试方法创建两个 deployment 和 svc，通过在 pod 内通过 svc 域名方式访问另一个 deployment 下的 pod，相关测试的 deploy、svc 配置在这里不在展示，基本情况如下图所示</p><p><img src="https://cdn.oss.link/markdown/o94qb.jpg" alt="deployment"></p><p><img src="https://cdn.oss.link/markdown/586mm.jpg" alt="test dns"></p><h4 id="7-2、DNS-自动扩容部署"><a href="#7-2、DNS-自动扩容部署" class="headerlink" title="7.2、DNS 自动扩容部署"></a>7.2、DNS 自动扩容部署</h4><p>关于 DNS 自动扩容详细可参考 <a href="https://kubernetes.io/docs/tasks/administer-cluster/dns-horizontal-autoscaling/">Autoscale the DNS Service in a Cluster</a>，以下直接操作</p><p>首先获取 Dns horizontal autoscaler 配置文件</p><div class="hljs code-wrapper"><pre><code class="hljs sh">wget https://raw.githubusercontent.com/kubernetes/kubernetes/master/cluster/addons/dns-horizontal-autoscaler/dns-horizontal-autoscaler-rbac.yamlwget https://raw.githubusercontent.com/kubernetes/kubernetes/master/cluster/addons/dns-horizontal-autoscaler/dns-horizontal-autoscaler.yaml</code></pre></div><p>然后直接 <code>kubectl create -f </code> 即可，<strong>DNS 自动扩容计算公式为 <code>replicas = max( ceil( cores * 1/coresPerReplica ) , ceil( nodes * 1/nodesPerReplica ) )</code>，如果想调整 DNS 数量(负载因子)，只需要调整 ConfigMap 中对应参数即可，具体计算细节参考上面的官方文档</strong></p><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment"># 编辑 Config Map</span>kubectl edit cm kube-dns-autoscaler --namespace=kube-system</code></pre></div>]]></content>
    
    
    <summary type="html">以前一直用 Kargo(基于 ansible) 来搭建 Kubernetes 集群，最近发现 ansible 部署的时候有些东西有点 bug，而且 Kargo 对 rkt 等也做了适配，感觉问题已经有点复杂化了；在 2.2 release 没出来这个时候，准备自己纯手动挡部署一下，Master HA 直接抄 Kargo 的就行了，以下记录一下;**本文以下部分所有用到的 rpm 、配置文件等全部已经上传到了 [百度云](http://pan.baidu.com/s/1o8PZLKA)  密码: x5v4**</summary>
    
    
    
    <category term="Kubernetes" scheme="https://mritd.com/categories/kubernetes/"/>
    
    
    <category term="Linux" scheme="https://mritd.com/tags/linux/"/>
    
    <category term="Docker" scheme="https://mritd.com/tags/docker/"/>
    
    <category term="Kubernetes" scheme="https://mritd.com/tags/kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>Kubernetes RBAC</title>
    <link href="https://mritd.com/2017/07/17/kubernetes-rbac-chinese-translation/"/>
    <id>https://mritd.com/2017/07/17/kubernetes-rbac-chinese-translation/</id>
    <published>2017-07-17T12:44:45.000Z</published>
    <updated>2017-07-17T12:44:45.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>基于角色的访问控制使用 <code>rbac.authorization.k8s.io</code> API 组来实现权限控制，RBAC 允许管理员通过 Kubernetes API 动态的配置权限策略。<strong>在 1.6 版本中 RBAC 还处于 Beat 阶段</strong>，如果想要开启 RBAC 授权模式需要在 apiserver 组件中指定 <code>--authorization-mode=RBAC</code> 选项。</p></blockquote><h3 id="一、API-Overview"><a href="#一、API-Overview" class="headerlink" title="一、API Overview"></a>一、API Overview</h3><p>本节介绍了 RBAC 的四个顶级类型，用户可以像与其他 Kubernetes API 资源一样通过 kubectl、API 调用方式与其交互；例如使用 <code>kubectl create -f (resource).yml</code> 命令创建资源对象，跟随本文档操作前最好先阅读引导部分。</p><h4 id="1-1、Role-and-ClusterRole"><a href="#1-1、Role-and-ClusterRole" class="headerlink" title="1.1、Role and ClusterRole"></a>1.1、Role and ClusterRole</h4><p>在 RBAC API 中，Role 表示一组规则权限，权限只会增加(累加权限)，不存在一个资源一开始就有很多权限而通过 RBAC 对其进行减少的操作；Role 可以定义在一个 namespace 中，如果想要跨 namespace 则可以创建 ClusterRole。</p><p><strong>Role 只能用于授予对单个命名空间中的资源访问权限，</strong> 以下是一个对默认命名空间中 Pods 具有访问权限的样例:</p><div class="hljs code-wrapper"><pre><code class="hljs yml"><span class="hljs-attr">kind:</span> <span class="hljs-string">Role</span><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">rbac.authorization.k8s.io/v1beta1</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">namespace:</span> <span class="hljs-string">default</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">pod-reader</span><span class="hljs-attr">rules:</span><span class="hljs-bullet">-</span> <span class="hljs-attr">apiGroups:</span> [<span class="hljs-string">&quot;&quot;</span>] <span class="hljs-comment"># &quot;&quot; indicates the core API group</span>  <span class="hljs-attr">resources:</span> [<span class="hljs-string">&quot;pods&quot;</span>]  <span class="hljs-attr">verbs:</span> [<span class="hljs-string">&quot;get&quot;</span>, <span class="hljs-string">&quot;watch&quot;</span>, <span class="hljs-string">&quot;list&quot;</span>]</code></pre></div><p>ClusterRole 具有与 Role 相同的权限角色控制能力，不同的是 ClusterRole 是集群级别的，ClusterRole 可以用于:</p><ul><li>集群级别的资源控制(例如 node 访问权限)</li><li>非资源型 endpoints(例如 <code>/healthz</code> 访问)</li><li>所有命名空间资源控制(例如 pods)</li></ul><p>以下是 ClusterRole 授权某个特定命名空间或全部命名空间(取决于<a href="https://kubernetes.io/docs/admin/authorization/rbac/#rolebinding-and-clusterrolebinding">绑定方式</a>)访问 secrets 的样例</p><div class="hljs code-wrapper"><pre><code class="hljs yml"><span class="hljs-attr">kind:</span> <span class="hljs-string">ClusterRole</span><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">rbac.authorization.k8s.io/v1beta1</span><span class="hljs-attr">metadata:</span>  <span class="hljs-comment"># &quot;namespace&quot; omitted since ClusterRoles are not namespaced</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">secret-reader</span><span class="hljs-attr">rules:</span><span class="hljs-bullet">-</span> <span class="hljs-attr">apiGroups:</span> [<span class="hljs-string">&quot;&quot;</span>]  <span class="hljs-attr">resources:</span> [<span class="hljs-string">&quot;secrets&quot;</span>]  <span class="hljs-attr">verbs:</span> [<span class="hljs-string">&quot;get&quot;</span>, <span class="hljs-string">&quot;watch&quot;</span>, <span class="hljs-string">&quot;list&quot;</span>]</code></pre></div><h4 id="1-2、RoleBinding-and-ClusterRoleBinding"><a href="#1-2、RoleBinding-and-ClusterRoleBinding" class="headerlink" title="1.2、RoleBinding and ClusterRoleBinding"></a>1.2、RoleBinding and ClusterRoleBinding</h4><p>RoloBinding 可以将角色中定义的权限授予用户或用户组，RoleBinding 包含一组权限列表(subjects)，权限列表中包含有不同形式的待授予权限资源类型(users, groups, or service accounts)；RoloBinding 同样包含对被 Bind 的 Role 引用；RoleBinding 适用于某个命名空间内授权，而 ClusterRoleBinding 适用于集群范围内的授权。</p><p>RoleBinding 可以在同一命名空间中引用对应的 Role，以下 RoleBinding 样例将 default 命名空间的 <code>pod-reader</code> Role 授予 jane 用户，此后 jane 用户在 default 命名空间中将具有 <code>pod-reader</code> 的权限</p><div class="hljs code-wrapper"><pre><code class="hljs yml"><span class="hljs-comment"># This role binding allows &quot;jane&quot; to read pods in the &quot;default&quot; namespace.</span><span class="hljs-attr">kind:</span> <span class="hljs-string">RoleBinding</span><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">rbac.authorization.k8s.io/v1beta1</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">read-pods</span>  <span class="hljs-attr">namespace:</span> <span class="hljs-string">default</span><span class="hljs-attr">subjects:</span><span class="hljs-bullet">-</span> <span class="hljs-attr">kind:</span> <span class="hljs-string">User</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">jane</span>  <span class="hljs-attr">apiGroup:</span> <span class="hljs-string">rbac.authorization.k8s.io</span><span class="hljs-attr">roleRef:</span>  <span class="hljs-attr">kind:</span> <span class="hljs-string">Role</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">pod-reader</span>  <span class="hljs-attr">apiGroup:</span> <span class="hljs-string">rbac.authorization.k8s.io</span></code></pre></div><p><strong>RoleBinding 同样可以引用 ClusterRole 来对当前 namespace 内用户、用户组或 ServiceAccount 进行授权，这种操作允许集群管理员在整个集群内定义一些通用的 ClusterRole，然后在不同的 namespace 中使用 RoleBinding 来引用</strong></p><p>例如，以下 RoleBinding 引用了一个 ClusterRole，这个 ClusterRole 具有整个集群内对 secrets 的访问权限；但是其授权用户 <code>dave</code> 只能访问 development 空间中的 secrets(因为 RoleBinding 定义在 development 命名空间)</p><div class="hljs code-wrapper"><pre><code class="hljs yml"><span class="hljs-comment"># This role binding allows &quot;dave&quot; to read secrets in the &quot;development&quot; namespace.</span><span class="hljs-attr">kind:</span> <span class="hljs-string">RoleBinding</span><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">rbac.authorization.k8s.io/v1beta1</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">read-secrets</span>  <span class="hljs-attr">namespace:</span> <span class="hljs-string">development</span> <span class="hljs-comment"># This only grants permissions within the &quot;development&quot; namespace.</span><span class="hljs-attr">subjects:</span><span class="hljs-bullet">-</span> <span class="hljs-attr">kind:</span> <span class="hljs-string">User</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">dave</span>  <span class="hljs-attr">apiGroup:</span> <span class="hljs-string">rbac.authorization.k8s.io</span><span class="hljs-attr">roleRef:</span>  <span class="hljs-attr">kind:</span> <span class="hljs-string">ClusterRole</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">secret-reader</span>  <span class="hljs-attr">apiGroup:</span> <span class="hljs-string">rbac.authorization.k8s.io</span></code></pre></div><p>最后，使用 ClusterRoleBinding 可以对整个集群中的所有命名空间资源权限进行授权；以下 ClusterRoleBinding 样例展示了授权 manager 组内所有用户在全部命名空间中对 secrets 进行访问</p><div class="hljs code-wrapper"><pre><code class="hljs yml"><span class="hljs-comment"># This cluster role binding allows anyone in the &quot;manager&quot; group to read secrets in any namespace.</span><span class="hljs-attr">kind:</span> <span class="hljs-string">ClusterRoleBinding</span><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">rbac.authorization.k8s.io/v1beta1</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">read-secrets-global</span><span class="hljs-attr">subjects:</span><span class="hljs-bullet">-</span> <span class="hljs-attr">kind:</span> <span class="hljs-string">Group</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">manager</span>  <span class="hljs-attr">apiGroup:</span> <span class="hljs-string">rbac.authorization.k8s.io</span><span class="hljs-attr">roleRef:</span>  <span class="hljs-attr">kind:</span> <span class="hljs-string">ClusterRole</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">secret-reader</span>  <span class="hljs-attr">apiGroup:</span> <span class="hljs-string">rbac.authorization.k8s.io</span></code></pre></div><h4 id="1-3、Referring-to-Resources"><a href="#1-3、Referring-to-Resources" class="headerlink" title="1.3、Referring to Resources"></a>1.3、Referring to Resources</h4><p>Kubernetes 集群内一些资源一般以其名称字符串来表示，这些字符串一般会在 API 的 URL 地址中出现；同时某些资源也会包含子资源，例如 logs 资源就属于 pods 的子资源，API 中 URL 样例如下</p><div class="hljs code-wrapper"><pre><code class="hljs sh">GET /api/v1/namespaces/&#123;namespace&#125;/pods/&#123;name&#125;/<span class="hljs-built_in">log</span></code></pre></div><p><strong>如果要在 RBAC 授权模型中控制这些子资源的访问权限，可以通过 <code>/</code> 分隔符来实现</strong>，以下是一个定义 pods 资资源 logs 访问权限的 Role 定义样例</p><div class="hljs code-wrapper"><pre><code class="hljs yml"><span class="hljs-attr">kind:</span> <span class="hljs-string">Role</span><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">rbac.authorization.k8s.io/v1beta1</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">namespace:</span> <span class="hljs-string">default</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">pod-and-pod-logs-reader</span><span class="hljs-attr">rules:</span><span class="hljs-bullet">-</span> <span class="hljs-attr">apiGroups:</span> [<span class="hljs-string">&quot;&quot;</span>]  <span class="hljs-attr">resources:</span> [<span class="hljs-string">&quot;pods&quot;</span>, <span class="hljs-string">&quot;pods/log&quot;</span>]  <span class="hljs-attr">verbs:</span> [<span class="hljs-string">&quot;get&quot;</span>, <span class="hljs-string">&quot;list&quot;</span>]</code></pre></div><p>具体的资源引用可以通过 <code>resourceNames</code> 来定义，当指定 <code>get</code>、<code>delete</code>、<code>update</code>、<code>patch</code> 四个动词时，可以控制对其目标资源的相应动作；以下为限制一个 subject 对名称为 my-configmap 的 configmap 只能具有 <code>get</code> 和 <code>update</code> 权限的样例</p><div class="hljs code-wrapper"><pre><code class="hljs yml"><span class="hljs-attr">kind:</span> <span class="hljs-string">Role</span><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">rbac.authorization.k8s.io/v1beta1</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">namespace:</span> <span class="hljs-string">default</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">configmap-updater</span><span class="hljs-attr">rules:</span><span class="hljs-bullet">-</span> <span class="hljs-attr">apiGroups:</span> [<span class="hljs-string">&quot;&quot;</span>]  <span class="hljs-attr">resources:</span> [<span class="hljs-string">&quot;configmap&quot;</span>]  <span class="hljs-attr">resourceNames:</span> [<span class="hljs-string">&quot;my-configmap&quot;</span>]  <span class="hljs-attr">verbs:</span> [<span class="hljs-string">&quot;update&quot;</span>, <span class="hljs-string">&quot;get&quot;</span>]</code></pre></div><p><strong>值得注意的是，当设定了 resourceNames 后，verbs 动词不能指定为 <code>list</code>、<code>watch</code>、<code>create</code> 和 <code>deletecollection</code>；因为这个具体的资源名称不在上面四个动词限定的请求 URL 地址中匹配到，最终会因为 URL 地址不匹配导致 Role 无法创建成功</strong></p><h5 id="1-3-1、Role-Examples"><a href="#1-3-1、Role-Examples" class="headerlink" title="1.3.1、Role Examples"></a>1.3.1、Role Examples</h5><p>以下样例只给出了 role 部分</p><p>在核心 API 组中允许读取 <code>pods</code> 资源</p><div class="hljs code-wrapper"><pre><code class="hljs yml"><span class="hljs-attr">rules:</span><span class="hljs-bullet">-</span> <span class="hljs-attr">apiGroups:</span> [<span class="hljs-string">&quot;&quot;</span>]  <span class="hljs-attr">resources:</span> [<span class="hljs-string">&quot;pods&quot;</span>]  <span class="hljs-attr">verbs:</span> [<span class="hljs-string">&quot;get&quot;</span>, <span class="hljs-string">&quot;list&quot;</span>, <span class="hljs-string">&quot;watch&quot;</span>]</code></pre></div><p>在 <code>extensions</code> 和 <code>apps</code> API 组中允许读取/写入 <code>deployments</code></p><div class="hljs code-wrapper"><pre><code class="hljs yml"><span class="hljs-attr">rules:</span><span class="hljs-bullet">-</span> <span class="hljs-attr">apiGroups:</span> [<span class="hljs-string">&quot;extensions&quot;</span>, <span class="hljs-string">&quot;apps&quot;</span>]  <span class="hljs-attr">resources:</span> [<span class="hljs-string">&quot;deployments&quot;</span>]  <span class="hljs-attr">verbs:</span> [<span class="hljs-string">&quot;get&quot;</span>, <span class="hljs-string">&quot;list&quot;</span>, <span class="hljs-string">&quot;watch&quot;</span>, <span class="hljs-string">&quot;create&quot;</span>, <span class="hljs-string">&quot;update&quot;</span>, <span class="hljs-string">&quot;patch&quot;</span>, <span class="hljs-string">&quot;delete&quot;</span>]</code></pre></div><p>允许读取 <code>pods</code> 资源，允许读取/写入 <code>jobs</code> 资源</p><div class="hljs code-wrapper"><pre><code class="hljs yml"><span class="hljs-attr">rules:</span><span class="hljs-bullet">-</span> <span class="hljs-attr">apiGroups:</span> [<span class="hljs-string">&quot;&quot;</span>]  <span class="hljs-attr">resources:</span> [<span class="hljs-string">&quot;pods&quot;</span>]  <span class="hljs-attr">verbs:</span> [<span class="hljs-string">&quot;get&quot;</span>, <span class="hljs-string">&quot;list&quot;</span>, <span class="hljs-string">&quot;watch&quot;</span>]<span class="hljs-bullet">-</span> <span class="hljs-attr">apiGroups:</span> [<span class="hljs-string">&quot;batch&quot;</span>, <span class="hljs-string">&quot;extensions&quot;</span>]  <span class="hljs-attr">resources:</span> [<span class="hljs-string">&quot;jobs&quot;</span>]  <span class="hljs-attr">verbs:</span> [<span class="hljs-string">&quot;get&quot;</span>, <span class="hljs-string">&quot;list&quot;</span>, <span class="hljs-string">&quot;watch&quot;</span>, <span class="hljs-string">&quot;create&quot;</span>, <span class="hljs-string">&quot;update&quot;</span>, <span class="hljs-string">&quot;patch&quot;</span>, <span class="hljs-string">&quot;delete&quot;</span>]</code></pre></div><p>允许读取名称为 <code>my-config</code> 的 ConfigMap(需要与 RoleBinding 绑定来限制某个特定命名空间和指定名字的 ConfigMap)</p><div class="hljs code-wrapper"><pre><code class="hljs yml"><span class="hljs-attr">rules:</span><span class="hljs-bullet">-</span> <span class="hljs-attr">apiGroups:</span> [<span class="hljs-string">&quot;&quot;</span>]  <span class="hljs-attr">resources:</span> [<span class="hljs-string">&quot;configmaps&quot;</span>]  <span class="hljs-attr">resourceNames:</span> [<span class="hljs-string">&quot;my-config&quot;</span>]  <span class="hljs-attr">verbs:</span> [<span class="hljs-string">&quot;get&quot;</span>]</code></pre></div><p>允许在核心组中读取 <code>nodes</code> 资源( Node 是集群范围内的资源，需要使用 ClusterRole 并且与 ClusterRoleBinding 绑定才能进行限制)</p><div class="hljs code-wrapper"><pre><code class="hljs yml"><span class="hljs-attr">rules:</span><span class="hljs-bullet">-</span> <span class="hljs-attr">apiGroups:</span> [<span class="hljs-string">&quot;&quot;</span>]  <span class="hljs-attr">resources:</span> [<span class="hljs-string">&quot;nodes&quot;</span>]  <span class="hljs-attr">verbs:</span> [<span class="hljs-string">&quot;get&quot;</span>, <span class="hljs-string">&quot;list&quot;</span>, <span class="hljs-string">&quot;watch&quot;</span>]</code></pre></div><p>允许对非资源型 endpoint <code>/healthz</code> 和其子路径 <code>/healthz/*</code> 进行 <code>GET</code> 和 <code>POST</code> 请求(同样需要使用 ClusterRole 和 ClusterRoleBinding 才能生效)</p><div class="hljs code-wrapper"><pre><code class="hljs yml"><span class="hljs-attr">rules:</span><span class="hljs-bullet">-</span> <span class="hljs-attr">nonResourceURLs:</span> [<span class="hljs-string">&quot;/healthz&quot;</span>, <span class="hljs-string">&quot;/healthz/*&quot;</span>] <span class="hljs-comment"># &#x27;*&#x27; in a nonResourceURL is a suffix glob match</span>  <span class="hljs-attr">verbs:</span> [<span class="hljs-string">&quot;get&quot;</span>, <span class="hljs-string">&quot;post&quot;</span>]</code></pre></div><h4 id="1-4、Referring-to-Subjects"><a href="#1-4、Referring-to-Subjects" class="headerlink" title="1.4、Referring to Subjects"></a>1.4、Referring to Subjects</h4><p>RoleBinding 和 ClusterRoleBinding 可以将 Role 绑定到 Subjects；Subjects 可以是 groups、users 或者 service accounts。</p><p>Subjects 中 Users 使用字符串表示，它可以是一个普通的名字字符串，如 “alice”；也可以是 email 格式的邮箱地址，如 <code>bob@example.com</code>；甚至是一组字符串形式的数字 ID。Users 的格式必须满足集群管理员配置的<a href="https://kubernetes.io/docs/admin/authentication/">验证模块</a>，RBAC 授权系统中没有对其做任何格式限定；<strong>但是 Users 的前缀 <code>system:</code> 是系统保留的，集群管理员应该确保普通用户不会使用这个前缀格式</strong></p><p>Kubernetes 的 Group 信息目前由 Authenticator 模块提供，Groups 书写格式与 Users 相同，都为一个字符串，并且没有特定的格式要求；<strong>同样 <code>system:</code> 前缀为系统保留</strong></p><p>具有 <code>system:serviceaccount:</code> 前缀的用户名和 <code>system:serviceaccounts:</code> 前缀的组为 <a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/">Service Accounts</a></p><h5 id="1-4-1、Role-Binding-Examples"><a href="#1-4-1、Role-Binding-Examples" class="headerlink" title="1.4.1、Role Binding Examples"></a>1.4.1、Role Binding Examples</h5><p>以下示例仅展示 RoleBinding 的 subjects 部分</p><p>指定一个名字为 <code>alice@example.com</code> 的用户</p><div class="hljs code-wrapper"><pre><code class="hljs yml"><span class="hljs-attr">subjects:</span><span class="hljs-bullet">-</span> <span class="hljs-attr">kind:</span> <span class="hljs-string">User</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">&quot;alice@example.com&quot;</span>  <span class="hljs-attr">apiGroup:</span> <span class="hljs-string">rbac.authorization.k8s.io</span></code></pre></div><p>指定一个名字为 <code>frontend-admins</code> 的组</p><div class="hljs code-wrapper"><pre><code class="hljs yml"><span class="hljs-attr">subjects:</span><span class="hljs-bullet">-</span> <span class="hljs-attr">kind:</span> <span class="hljs-string">Group</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">&quot;frontend-admins&quot;</span>  <span class="hljs-attr">apiGroup:</span> <span class="hljs-string">rbac.authorization.k8s.io</span></code></pre></div><p>指定 kube-system namespace 中默认的 Service Account</p><div class="hljs code-wrapper"><pre><code class="hljs yml"><span class="hljs-attr">subjects:</span><span class="hljs-bullet">-</span> <span class="hljs-attr">kind:</span> <span class="hljs-string">ServiceAccount</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">default</span>  <span class="hljs-attr">namespace:</span> <span class="hljs-string">kube-system</span></code></pre></div><p>指定在 qa namespace 中全部的 Service Account</p><div class="hljs code-wrapper"><pre><code class="hljs yml"><span class="hljs-attr">subjects:</span><span class="hljs-bullet">-</span> <span class="hljs-attr">kind:</span> <span class="hljs-string">Group</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">system:serviceaccounts:qa</span>  <span class="hljs-attr">apiGroup:</span> <span class="hljs-string">rbac.authorization.k8s.io</span></code></pre></div><p>指定全部 namspace 中的全部 Service Account</p><div class="hljs code-wrapper"><pre><code class="hljs yml"><span class="hljs-attr">subjects:</span><span class="hljs-bullet">-</span> <span class="hljs-attr">kind:</span> <span class="hljs-string">Group</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">system:serviceaccounts</span>  <span class="hljs-attr">apiGroup:</span> <span class="hljs-string">rbac.authorization.k8s.io</span></code></pre></div><p>指定全部的 authenticated 用户(1.5+)</p><div class="hljs code-wrapper"><pre><code class="hljs yml"><span class="hljs-attr">subjects:</span><span class="hljs-bullet">-</span> <span class="hljs-attr">kind:</span> <span class="hljs-string">Group</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">system:authenticated</span>  <span class="hljs-attr">apiGroup:</span> <span class="hljs-string">rbac.authorization.k8s.io</span></code></pre></div><p>指定全部的 unauthenticated 用户(1.5+)</p><div class="hljs code-wrapper"><pre><code class="hljs yml"><span class="hljs-attr">subjects:</span><span class="hljs-bullet">-</span> <span class="hljs-attr">kind:</span> <span class="hljs-string">Group</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">system:unauthenticated</span>  <span class="hljs-attr">apiGroup:</span> <span class="hljs-string">rbac.authorization.k8s.io</span></code></pre></div><p>指定全部用户</p><div class="hljs code-wrapper"><pre><code class="hljs yml"><span class="hljs-attr">subjects:</span><span class="hljs-bullet">-</span> <span class="hljs-attr">kind:</span> <span class="hljs-string">Group</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">system:authenticated</span>  <span class="hljs-attr">apiGroup:</span> <span class="hljs-string">rbac.authorization.k8s.io</span><span class="hljs-bullet">-</span> <span class="hljs-attr">kind:</span> <span class="hljs-string">Group</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">system:unauthenticated</span>  <span class="hljs-attr">apiGroup:</span> <span class="hljs-string">rbac.authorization.k8s.io</span></code></pre></div><h3 id="二、Default-Roles-and-Role-Bindings"><a href="#二、Default-Roles-and-Role-Bindings" class="headerlink" title="二、Default Roles and Role Bindings"></a>二、Default Roles and Role Bindings</h3><p>集群创建后 API Server 默认会创建一些 ClusterRole 和 ClusterRoleBinding 对象；这些对象以 <code>system:</code> 为前缀，这表明这些资源对象由集群基础设施拥有；<strong>修改这些集群基础设施拥有的对象可能导致集群不可用。</strong> 一个简单的例子是 <code>system:node</code> ClusterRole，这个 ClusterRole 定义了 kubelet 的相关权限，如果该 ClusterRole 被修改可能导致 ClusterRole 不可用。</p><p><strong>所有的默认 ClusterRole 和 RoleBinding 都具有 <code>kubernetes.io/bootstrapping=rbac-defaults</code> lable</strong></p><h4 id="2-1、Auto-reconciliation"><a href="#2-1、Auto-reconciliation" class="headerlink" title="2.1、Auto-reconciliation"></a>2.1、Auto-reconciliation</h4><p>API Server 在每次启动后都会更新已经丢失的默认 ClusterRole 和 其绑定的相关 Subjects；这将允许集群自动修复因为意外更改导致的 RBAC 授权错误，同时能够使在升级集群后基础设施的 RBAC 授权得以自动更新。</p><p><strong>如果想要关闭 API Server 的自动修复功能，只需要将默认创建的 ClusterRole 和其 RoleBind 的 <code>rbac.authorization.kubernetes.io/autoupdate</code> 注解设置为 false 即可，这样做会有很大风险导致集群因为意外修改 RBAC 而无法工作</strong></p><p><strong>Auto-reconciliation 在 1.6+ 版本被默认启用(当 RBAC 授权被激活时)</strong></p><h4 id="2-2、Discovery-Roles"><a href="#2-2、Discovery-Roles" class="headerlink" title="2.2、Discovery Roles"></a>2.2、Discovery Roles</h4><table><thead><tr><th>Default ClusterRole</th><th>Default ClusterRoleBinding</th><th>Description</th></tr></thead><tbody><tr><td>system:basic-user</td><td>system:authenticated and system:unauthenticated groups</td><td>允许用户以只读的方式读取其基础信息</td></tr><tr><td>system:discovery</td><td>system:authenticated and system:unauthenticated groups</td><td>允许以只读的形式访问 发现和协商 API Level 所需的 API discovery endpoints</td></tr></tbody></table><h4 id="2-3、User-facing-Roles"><a href="#2-3、User-facing-Roles" class="headerlink" title="2.3、User-facing Roles"></a>2.3、User-facing Roles</h4><p>一些默认的 Role 并未以 <code>system:</code> 前缀开头，这表明这些默认的 Role 是面向用户级别的。这其中包括超级用户的一些 Role( <code>cluster-admin</code> )，和为面向集群范围授权的 RoleBinding( <code>cluster-status</code> )，以及在特定命名空间中授权的 RoleBinding( <code>admin</code>，<code>edit</code>，<code>view</code> )</p><table><thead><tr><th>Default ClusterRole</th><th>Default ClusterRoleBinding</th><th>Description</th></tr></thead><tbody><tr><td>cluster-admin</td><td>system:masters group</td><td>允许超级用户对集群内任意资源执行任何动作。当该 Role 绑定到 ClusterRoleBinding 时，将授予目标 subject 在任意 namespace 内对任何 resource 执行任何动作的权限；当绑定到 RoleBinding 时，将授予目标 subject 在当前 namespace 内对任意 resource 执行任何动作的权限，当然也包括 namespace 自己</td></tr><tr><td>admin</td><td>None</td><td>管理员权限，用于在单个 namespace 内授权；在与某个 RoleBinding 绑定后提供在单个 namesapce 中对资源的读写权限，包括在单个 namesapce 内创建 Role 和进行 RoleBinding 的权限。<strong>该 ClusterRole 不允许对资源配额和 namespace 本身进行修改</strong></td></tr><tr><td>edit</td><td>None</td><td>允许读写指定 namespace 中的大多数资源对象；<strong>该 ClusterRole 不允许查看或修改 Role 和 RoleBinding</strong></td></tr><tr><td>view</td><td>None</td><td>允许以只读方式访问特定 namespace 中的大多数资源对象；<strong>该 ClusterRole 不允许查看 Role 或 RoleBinding，同时不允许查看 secrets，因为他们会不断更新</strong></td></tr></tbody></table><h4 id="2-4、Core-Component-Roles"><a href="#2-4、Core-Component-Roles" class="headerlink" title="2.4、Core Component Roles"></a>2.4、Core Component Roles</h4><table><thead><tr><th>Default ClusterRole</th><th>Default ClusterRoleBinding</th><th>Description</th></tr></thead><tbody><tr><td>system:kube-scheduler</td><td>system:kube-scheduler user</td><td>允许访问 kube-scheduler 所需资源</td></tr><tr><td>system:kube-controller-manager</td><td>system:kube-controller-manager user</td><td>允许访问 kube-controller-manager 所需资源；<a href="https://kubernetes.io/docs/admin/authorization/rbac/#controller-roles">该 ClusterRole</a> 包含每个控制循环所需要的权限</td></tr><tr><td>system:node</td><td>system:nodes group (deprecated in 1.7)</td><td>允许访问 kubelet 所需资源；包括对所有的 secrets 读访问权限和对所有 pod 的写权限；在 1.7 中更推荐使用 <a href="/docs/admin/authorization/node/">Node authorizer</a> 和 <a href="/docs/admin/admission-controllers#NodeRestriction">NodeRestriction admission plugin</a> 而非本 ClusterRole；Node authorizer 和 NodeRestriction admission plugin 可以授权当前 node 上运行的具体 pod 对 kubelet API 的访问权限，<strong>在 1.7 版本中，如果开启了 <code>Node authorization mode</code>，那么 <code>system:nodes</code> group将不会被创建和自动绑定</strong></td></tr><tr><td>system:node-proxier</td><td>system:kube-proxy user</td><td>允许访问 kube-proxy 所需资源</td></tr></tbody></table><h4 id="2-5、Other-Component-Roles"><a href="#2-5、Other-Component-Roles" class="headerlink" title="2.5、Other Component Roles"></a>2.5、Other Component Roles</h4><table><thead><tr><th>Default ClusterRole</th><th>Default ClusterRoleBinding</th><th>Description</th></tr></thead><tbody><tr><td>system:auth-delegator</td><td>None</td><td>允许委托认证和授权检查；此情况下通常由附加的 API Server 来进行统一认证和授权</td></tr><tr><td>system:heapster</td><td>None</td><td><a href="https://github.com/kubernetes/heapster">Heapster</a> 组件相关权限</td></tr><tr><td>system:kube-aggregator</td><td>None</td><td><a href="https://github.com/kubernetes/kube-aggregator">kube-aggregator</a> 相关权限</td></tr><tr><td>system:kube-dns</td><td>kube-dns service account in the kube-system namespace</td><td><a href="https://kubernetes.io/docs/admin/dns/">kube-dns</a> 相关权限</td></tr><tr><td>system:node-bootstrapper</td><td>None</td><td>允许访问 <a href="https://kubernetes.io/docs/admin/kubelet-tls-bootstrapping/">Kubelet TLS bootstrapping</a> 相关资源权限</td></tr><tr><td>system:node-problem-detector</td><td>None</td><td><a href="https://github.com/kubernetes/node-problem-detector">node-problem-detector</a> 相关权限</td></tr><tr><td>system:persistent-volume-provisioner</td><td>Node</td><td>允许访问 <a href="https://kubernetes.io/docs/user-guide/persistent-volumes/#provisioner">dynamic volume provisioners</a> 相关资源权限</td></tr></tbody></table><h4 id="2-6、Controller-Roles"><a href="#2-6、Controller-Roles" class="headerlink" title="2.6、Controller Roles"></a>2.6、Controller Roles</h4><p><a href="https://kubernetes.io/docs/admin/kube-controller-manager/">Kubernetes controller manager</a> 运行着一些核心的 <code>control loops</code>，<strong>当使用 <code>--use-service-account-credentials</code> 参数启动时，每个 <code>control loop</code> 都会使用独立的 <code>Service Account</code> 启动；相应的 roles 会以 <code>system:controller</code> 前缀存在于每个 control loop 中；如果不指定该选项，那么 Kubernetes controller manager 将会使用自己的凭据来运行所有 <code>control loops</code>，此时必须保证 RBAC 授权模型中授予了其所有相关 Role，如下:</strong></p><ul><li>system:controller:attachdetach-controller</li><li>system:controller:certificate-controller</li><li>system:controller:cronjob-controller</li><li>system:controller:daemon-set-controller</li><li>system:controller:deployment-controller</li><li>system:controller:disruption-controller</li><li>system:controller:endpoint-controller</li><li>system:controller:generic-garbage-collector</li><li>system:controller:horizontal-pod-autoscaler</li><li>system:controller:job-controller</li><li>system:controller:namespace-controller</li><li>system:controller:node-controller</li><li>system:controller:persistent-volume-binder</li><li>system:controller:pod-garbage-collector</li><li>system:controller:replicaset-controller</li><li>system:controller:replication-controller</li><li>system:controller:resourcequota-controller</li><li>system:controller:route-controller</li><li>system:controller:service-account-controller</li><li>system:controller:service-controller</li><li>system:controller:statefulset-controller</li><li>system:controller:ttl-controller</li></ul><h3 id="三、Privilege-Escalation-Prevention-and-Bootstrapping"><a href="#三、Privilege-Escalation-Prevention-and-Bootstrapping" class="headerlink" title="三、Privilege Escalation Prevention and Bootstrapping"></a>三、Privilege Escalation Prevention and Bootstrapping</h3><p>RBAC API 会通过阻止用户编辑 Role 或 RoleBinding 来进行特权升级，RBAC 在 API 级别实现了这一机制，所以即使 RBAC authorizer 不被使用也适用。</p><p><strong>用户即使在对某个 Role 拥有全部权限的情况下也仅能在其作用范围内(ClusterRole -&gt; 集群范围内，Role -&gt; 当前 namespace 或 集群范围)对其进行 create 和 update 操作；</strong> 例如 “user-1” 用户不具有在集群范围内列出 secrets 的权限，那么他也无法在集群范围内创建具有该权限的 ClusterRole，也就是说想传递权限必须先获得该权限；想要允许用户 cretae/update Role 有两种方式:</p><ul><li>1、授予一个该用户期望 create/update 的 Role 或者 ClusterRole</li><li>2、授予一个包含该用户期望 create/update 的 Role 或者 ClusterRole 的 Role 或者 ClusterRole(有点绕…)；如果用户尝试 crate/update 一个其不拥有的 Role 或者 ClusterRole，则 API 会禁止</li></ul><p><strong>用户只有拥有了一个 RoleBind 引用的 Role 全部权限，或者被显示授予了对其具有 bind 的权限下，才能在其作用范围(范围同上)内对其进行 create/update 操作；</strong> 例如 “user-1” 在不具有列出集群内 secrets 权限的情况下，也不可能为具有该权限的 Role 创建 ClusterRoleBinding；如果想要用户具有 create/update ClusterRoleBinding 的权限有以下两种方式:</p><ul><li>1、授予一个该用户期望 create/update 的 RoleBinding 或者 ClusterRoleBinding 的 Role 或 ClusterRole 的 Role 或 ClusterRole(汉语专8)</li><li>2、通过其他方式授予一个该用户 期望 create/update 的 RoleBinding 或者 ClusterRoleBinding 的权限:<ul><li>2.1、授予一个包含用户期望 create/update 的 RoleBinding 或者 ClusterRoleBinding 的 Role 或 ClusterRole 的 Role 或 ClusterRole(我汉语10级)</li><li>2.2、明确的授予用户一个在对特定 Role 或 ClusterRole 进行 bind 的权限</li></ul></li></ul><p>以下样例中，ClusterRole 和 RoleBinding 将允许 “user-1” 用户具有授予其他用户在 “user-1-namespace” namespace 下具有 admin、edit 和 view roles 的权限</p><div class="hljs code-wrapper"><pre><code class="hljs yml"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">rbac.authorization.k8s.io/v1beta1</span><span class="hljs-attr">kind:</span> <span class="hljs-string">ClusterRole</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">role-grantor</span><span class="hljs-attr">rules:</span><span class="hljs-bullet">-</span> <span class="hljs-attr">apiGroups:</span> [<span class="hljs-string">&quot;rbac.authorization.k8s.io&quot;</span>]  <span class="hljs-attr">resources:</span> [<span class="hljs-string">&quot;rolebindings&quot;</span>]  <span class="hljs-attr">verbs:</span> [<span class="hljs-string">&quot;create&quot;</span>]<span class="hljs-bullet">-</span> <span class="hljs-attr">apiGroups:</span> [<span class="hljs-string">&quot;rbac.authorization.k8s.io&quot;</span>]  <span class="hljs-attr">resources:</span> [<span class="hljs-string">&quot;clusterroles&quot;</span>]  <span class="hljs-attr">verbs:</span> [<span class="hljs-string">&quot;bind&quot;</span>]  <span class="hljs-attr">resourceNames:</span> [<span class="hljs-string">&quot;admin&quot;</span>,<span class="hljs-string">&quot;edit&quot;</span>,<span class="hljs-string">&quot;view&quot;</span>]<span class="hljs-meta">---</span><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">rbac.authorization.k8s.io/v1beta1</span><span class="hljs-attr">kind:</span> <span class="hljs-string">RoleBinding</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">role-grantor-binding</span>  <span class="hljs-attr">namespace:</span> <span class="hljs-string">user-1-namespace</span><span class="hljs-attr">roleRef:</span>  <span class="hljs-attr">apiGroup:</span> <span class="hljs-string">rbac.authorization.k8s.io</span>  <span class="hljs-attr">kind:</span> <span class="hljs-string">ClusterRole</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">role-grantor</span><span class="hljs-attr">subjects:</span><span class="hljs-bullet">-</span> <span class="hljs-attr">apiGroup:</span> <span class="hljs-string">rbac.authorization.k8s.io</span>  <span class="hljs-attr">kind:</span> <span class="hljs-string">User</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">user-1</span></code></pre></div><p>当使用 bootstrapping 时，初始用户尚没有访问 API 的权限，此时想要授予他们一些尚未拥有的权限是不可能的，此时可以有两种解决方案:</p><ul><li>1、通过使用系统级的 <code>system:masters</code> 组从而通过默认绑定绑定到 <code>cluster-admin</code> 超级用户，这样就可以直接沟通 API Server</li><li>2、如果 API Server 开启了 <code>--insecure-port</code> 端口，那么可以通过此端口调用完成第一次授权动作</li></ul><h3 id="四、Command-line-Utilities"><a href="#四、Command-line-Utilities" class="headerlink" title="四、Command-line Utilities"></a>四、Command-line Utilities</h3><p>通过两个 <code>kubectl</code> 的子命令完成在特定命名空间或集群内的授权管理</p><h4 id="4-1、kubectl-create-rolebinding"><a href="#4-1、kubectl-create-rolebinding" class="headerlink" title="4.1、kubectl create rolebinding"></a>4.1、kubectl create rolebinding</h4><p>在特定 namespae 中创建 Role 或者 ClusterRole 的 RoleBinding 样例</p><p><strong>在 acme namespace 中授权用户 bob 具有 admin ClusterRole 的 RoleBinding</strong></p><div class="hljs code-wrapper"><pre><code class="hljs sh">kubectl create rolebinding bob-admin-binding --clusterrole=admin --user=bob --namespace=acme</code></pre></div><p><strong>在 acme namespace 中授权名称为 acme:myapp 的 service account 具有 view ClusterRole 的 RoleBinding</strong></p><div class="hljs code-wrapper"><pre><code class="hljs sh">kubectl create rolebinding myapp-view-binding --clusterrole=view --serviceaccount=acme:myapp --namespace=acme</code></pre></div><h4 id="4-2、kubectl-create-clusterrolebinding"><a href="#4-2、kubectl-create-clusterrolebinding" class="headerlink" title="4.2、kubectl create clusterrolebinding"></a>4.2、kubectl create clusterrolebinding</h4><p>在全部命名空间中创建 Role 或者 ClusterRole 的 ClusterRoleBinding 样例</p><p><strong>在整个集群内授权 “root” 用户具有 cluster-admin ClusterRole 的 ClusterRoleBinding</strong></p><div class="hljs code-wrapper"><pre><code class="hljs sh">kubectl create clusterrolebinding root-cluster-admin-binding --clusterrole=cluster-admin --user=root</code></pre></div><p><strong>在整个集群内授权 “kubelet” 用户具有 system:node ClusterRole 的 ClusterRoleBinding</strong></p><div class="hljs code-wrapper"><pre><code class="hljs sh">kubectl create clusterrolebinding kubelet-node-binding --clusterrole=system:node --user=kubelet</code></pre></div><p><strong>在 “acme” 命名空间中授权名称为 acme:myapp 的 service account 具有 view ClusterRole 的 ClusterRoleBinding</strong></p><div class="hljs code-wrapper"><pre><code class="hljs sh">kubectl create clusterrolebinding myapp-view-binding --clusterrole=view --serviceaccount=acme:myapp</code></pre></div><p>更详细使用请参考命令行帮助文档</p><h3 id="五、Service-Account-Permissions"><a href="#五、Service-Account-Permissions" class="headerlink" title="五、Service Account Permissions"></a>五、Service Account Permissions</h3><p>默认的 RBAC 权限策略仅向 control-plane 组件、nodes 和 controllers 进行授权，不包括 <code>kube-system</code> namespace 以外的 Service Account 进行授权(除了向已经被验证过的用户授予的 discovery 权限之外)</p><p>这允许你根据需要向特定的服务账户授予特定的权限；细粒度的权限角色绑定控制会更加安全，但是需要更大的精力来进行权限管理；更加宽松的权限角色绑定控制也许会给一些用户分配其不需要的权限，但是相对来说管理相对更加宽松</p><p>从最安全到最不安全的权限管理如下:</p><h4 id="5-1、为特定应用程序指定的服务账户授予特定的-Role-最佳实践"><a href="#5-1、为特定应用程序指定的服务账户授予特定的-Role-最佳实践" class="headerlink" title="5.1、为特定应用程序指定的服务账户授予特定的 Role(最佳实践)"></a>5.1、为特定应用程序指定的服务账户授予特定的 Role(最佳实践)</h4><p>**这种方式需要应用在 spec 中设置 serviceAccountName，同时这个 SserviceAccount 必须已经被创建(可以通过 API、manifest 文件或者 通过命令 <code>kubectl create serviceaccount</code> 等)**。例如在 “my-namespace” namespace 下授予 “my-sa” ServiceAccount view ClusterRole 如下:</p><div class="hljs code-wrapper"><pre><code class="hljs sh">kubectl create rolebinding my-sa-view \  --clusterrole=view \  --serviceaccount=my-namespace:my-sa \  --namespace=my-namespace</code></pre></div><h4 id="5-2、为特定应用程序默认的服务账户授予特定的-Role"><a href="#5-2、为特定应用程序默认的服务账户授予特定的-Role" class="headerlink" title="5.2、为特定应用程序默认的服务账户授予特定的 Role"></a>5.2、为特定应用程序默认的服务账户授予特定的 Role</h4><p><strong>如果应用程序在 spec 中没有设置 serviceAccountName，那么将会使用 “default” ServiceAccount。</strong></p><p><strong>注意: 如果对 default ServiceAccount 进行 RoleBinding(授权)，那么在当前命名空间内所有没有指定 serviceAccountName 的 pod 都将获得该权限。</strong> 例如在 “my-namespace” namespace 下授予 “default” ServiceAccount view ClusterRole 如下:</p><div class="hljs code-wrapper"><pre><code class="hljs sh">kubectl create rolebinding default-view \  --clusterrole=view \  --serviceaccount=my-namespace:default \  --namespace=my-namespace</code></pre></div><p>目前大多数 <a href="https://kubernetes.io/docs/concepts/cluster-administration/addons/">add-ons</a> 运行在 “kube-system” namespace 的 “default” ServiceAccount 下，如果想要 add-ons 使用超级用户的权限只需要对 “kube-system” namespace 下的 “default” ServiceAccount 授予超级用户权限即可，<strong>需要注意的是超级用户对 API secrets 具有读写权限，这将导致所有 add-ons 组件具有该权限</strong></p><div class="hljs code-wrapper"><pre><code class="hljs sh">kubectl create clusterrolebinding add-on-cluster-admin \  --clusterrole=cluster-admin \  --serviceaccount=kube-system:default</code></pre></div><h4 id="5-3、为特定命名空间的所有服务账户授权"><a href="#5-3、为特定命名空间的所有服务账户授权" class="headerlink" title="5.3、为特定命名空间的所有服务账户授权"></a>5.3、为特定命名空间的所有服务账户授权</h4><p>如果希望 namespace 中所有应用程序(无论属于哪个 ServiceAccount)都具有某一个 Role，则可以通过将该 Role 授予该 namespace 的 ServiceAccount 组来实现；例如授予 “my-namespace” namespace 下所有 ServiceAccount view ClusterRole 如下:</p><div class="hljs code-wrapper"><pre><code class="hljs sh">kubectl create rolebinding serviceaccounts-view \  --clusterrole=view \  --group=system:serviceaccounts:my-namespace \  --namespace=my-namespace</code></pre></div><h4 id="5-4、为集群范围内所有服务账户授权-不建议"><a href="#5-4、为集群范围内所有服务账户授权-不建议" class="headerlink" title="5.4、为集群范围内所有服务账户授权(不建议)"></a>5.4、为集群范围内所有服务账户授权(不建议)</h4><p>如果你懒得管理每个 namespace 的权限，那么可以将授权扩散到整个集群，将权限授予集群内每个 ServiceAccount；例如授予全部 namespace 中所有 ServiceAccount view ClusterRole:</p><div class="hljs code-wrapper"><pre><code class="hljs sh">kubectl create clusterrolebinding serviceaccounts-view \  --clusterrole=view \  --group=system:serviceaccounts</code></pre></div><h4 id="5-5、为集群范围内所有服务账户授予超级用户权限-no-zuo-no-die"><a href="#5-5、为集群范围内所有服务账户授予超级用户权限-no-zuo-no-die" class="headerlink" title="5.5、为集群范围内所有服务账户授予超级用户权限(no zuo no die)"></a>5.5、为集群范围内所有服务账户授予超级用户权限(no zuo no die)</h4><p>如果你根本不关心权限分配，那么可以向集群内所有 namespace 下所有 ServiceAccount 授予超级用户权限；<strong>注意: 这将允许具有读取权限的用户创建一个容器从而间接读取到超级用户凭据</strong></p><div class="hljs code-wrapper"><pre><code class="hljs sh">kubectl create clusterrolebinding serviceaccounts-cluster-admin \  --clusterrole=cluster-admin \  --group=system:serviceaccounts</code></pre></div><h3 id="六、Upgrading-from-1-5"><a href="#六、Upgrading-from-1-5" class="headerlink" title="六、Upgrading from 1.5"></a>六、Upgrading from 1.5</h3><p>在 Kubernetes 1.6 版本之前，许多部署使用了非常宽泛的 ABAC 授权策略，包括授予对所有服务帐户的完整API访问权限；默认的 RBAC 权限策略仅向 control-plane 组件、nodes 和 controllers 进行授权，不包括 <code>kube-system</code> namespace 以外的 Service Account 进行授权(除了向已经被验证过的用户授予的 discovery 权限之外)</p><p>这种方式虽然安全性更高，但是 RBAC 授权方式可能影响到已经存在的期望自动获得 API 权限的 workloads，以下有两种解决方案:</p><h4 id="6-1、Parallel-Authorizers"><a href="#6-1、Parallel-Authorizers" class="headerlink" title="6.1、Parallel Authorizers"></a>6.1、Parallel Authorizers</h4><p>并行授权策略允许同时运行 RBAC 和 ABAC，并且包含旧的 ABAC 授权策略</p><div class="hljs code-wrapper"><pre><code class="hljs sh">--authorization-mode=RBAC,ABAC --authorization-policy-file=mypolicy.jsonl</code></pre></div><p><strong>此时 RBAC 授权控制器将首先处理授权，如果请求被拒绝则转交给 ABAC 授权控制器处理；这种授权方式将会允许 RBAC 和 ABAC 同时处理授权请求，只要目标 Subjects 在 RBAC 或 ABAC 中任意一个授权器授权成功即可</strong></p><p>当日志级别设置为 2(–v=2) 或者更高时，可以在 API Server 日志中看到 RBAC 拒绝的日志(以 <code>RBAC DENY:</code> 开头)，你可以通过日志中该信息来确定哪些 Role 应该授予哪些 Subjects。一旦完成所有的授权处理，并且在日志中没有再出现 RBAC 授权拒绝的日志时，就可以删除掉 ABAC 授权</p><h4 id="6-2、Permissive-RBAC-Permissions"><a href="#6-2、Permissive-RBAC-Permissions" class="headerlink" title="6.2、Permissive RBAC Permissions"></a>6.2、Permissive RBAC Permissions</h4><p>您可以使用 RBAC RoleBinding 来复制一个允许的策略。</p><p><strong>注意: 以下策略允许所有服务帐户充当集群管理员。在容器中运行的任何应用程序都会自动接收服务帐户凭据，并可以针对 API 执行任何操作，包括查看和修改 secrets 权限；所以这种方法并不推荐。</strong></p><div class="hljs code-wrapper"><pre><code class="hljs sh">kubectl create clusterrolebinding permissive-binding \  --clusterrole=cluster-admin \  --user=admin \  --user=kubelet \  --group=system:serviceaccounts</code></pre></div>]]></content>
    
    
    <summary type="html">基于角色的访问控制使用 `rbac.authorization.k8s.io` API 组来实现权限控制，RBAC 允许管理员通过 Kubernetes API 动态的配置权限策略。**在 1.6 版本中 RBAC 还处于 Beat 阶段**，如果想要开启 RBAC 授权模式需要在 apiserver 组件中指定 `--authorization-mode=RBAC` 选项。</summary>
    
    
    
    <category term="Kubernetes" scheme="https://mritd.com/categories/kubernetes/"/>
    
    
    <category term="Linux" scheme="https://mritd.com/tags/linux/"/>
    
    <category term="Docker" scheme="https://mritd.com/tags/docker/"/>
    
    <category term="Kubernetes" scheme="https://mritd.com/tags/kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>How to build Kubernetes RPM</title>
    <link href="https://mritd.com/2017/07/12/how-to-build-kubernetes-rpm/"/>
    <id>https://mritd.com/2017/07/12/how-to-build-kubernetes-rpm/</id>
    <published>2017-07-12T14:52:38.000Z</published>
    <updated>2017-07-12T14:52:38.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>一直使用 Centos 运行 Kubernetes,有些时候基于二进制部署的情况下,手动复制二进制文件和创建 Systemd service 配置略显繁琐;最近找了一下 Kubernetes RPM 的 build 方式,以下记录一下 build 过程</p></blockquote><h3 id="一、RPM-build-方式选择"><a href="#一、RPM-build-方式选择" class="headerlink" title="一、RPM build 方式选择"></a>一、RPM build 方式选择</h3><p>目前我所知道的 build kubernetes RPM 的方式(测试过)总共 3 种,大致分为 2 类</p><ul><li>基于源码 build</li><li>基于已有 rpm 替换</li></ul><p>第一种方案的好处就是配置文件等能始终保持最新的,编译版本等不受限制;但是从源码 build 非常耗时,尤其是网络环境复杂的情况下,没有高配置国外服务器很难完成 build,而且要维护 build 所需 spec 文件等,自己维护这些未必能够尽善尽美;</p><p>第二种方式是创建速度快,build 方式简单可靠,但是由于是替换方式,所以 rpm 中的配置不一定能够即使更新,而且只能基于官方build 好以后的二进制文件进行替换,如果想要尝试 master 最新代码则无法实现</p><h3 id="二、基于源码-Build"><a href="#二、基于源码-Build" class="headerlink" title="二、基于源码 Build"></a>二、基于源码 Build</h3><p>对于 Centos RPM build 原理方式这里不再细说，基于源码 build 的关键就在于 spec 文件，我尝试过自己去写，后来对比一些开源项目的感觉 low 得很，所以以前一直采用一个国外哥们写的脚本 build(参见 <a href="https://github.com/mritd/kubernetes-rpm-builder">这里</a>)；这个脚本不太好的地方是作者已经停止了维护；经过不懈努力，找到了 Fedora 系统的 rpm 仓库，鼓捣了一阵摸清了套路；以下主要以 Fedora 仓库为例进行 build</p><p><strong>以下 Build 在一台 Do 8核心 16G VPS 上进行，由于众所周知的原因，国内 Build 很费劲，一般国外 VPS 都是按小时收费，有个 2 块钱就够了</strong></p><h4 id="2-1、安装-build-所需依赖"><a href="#2-1、安装-build-所需依赖" class="headerlink" title="2.1、安装 build 所需依赖"></a>2.1、安装 build 所需依赖</h4><p><strong>由于 spec 文件中定义了依赖于 golang 这个包，所以如果不装的话会报错；事实上如果使用刚刚安装的这个 golang 去 build 还是会挂掉，因为实际编译要求 golang &gt; 1.7，直接 yum 装的是 1.6，故下面又使用 gvm 装了一个 1.8 的 golang，上面的 golang 安装只是为了通过 spec 检查</strong></p><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment"># EPEL</span>yum install epel-release -y<span class="hljs-comment"># update 系统组件</span>yum update -y &amp;&amp; yum upgrade -y<span class="hljs-comment"># 安装基本的编译依赖</span>yum install golang go-md2man go-bindata gcc bison git rpm-build vim -y<span class="hljs-comment"># 安装 gvm(用于 golang 版本管理)</span>bash &lt; &lt;(curl -s -S -L https://raw.githubusercontent.com/moovweb/gvm/master/binscripts/gvm-installer)<span class="hljs-built_in">source</span> /root/.gvm/scripts/gvm<span class="hljs-comment"># 安装 1.8 之前需要先安装 1.4</span>gvm install go1.4 -Bgvm use go1.4<span class="hljs-comment"># 使用 golang 1.8 版本 build</span>gvm install go1.8gvm use go1.8</code></pre></div><h4 id="2-2、克隆-build-仓库"><a href="#2-2、克隆-build-仓库" class="headerlink" title="2.2、克隆 build 仓库"></a>2.2、克隆 build 仓库</h4><p><strong>Fedora 官方 Kubernetes 仓库地址在 <a href="https://src.fedoraproject.org/cgit/rpms/kubernetes.git/">这里</a>，如果有版本选择请自行区分</strong></p><div class="hljs code-wrapper"><pre><code class="hljs sh">git <span class="hljs-built_in">clone</span> https://src.fedoraproject.org/git/rpms/kubernetes.git</code></pre></div><h4 id="2-3、从-spec-获取所需文件"><a href="#2-3、从-spec-获取所需文件" class="headerlink" title="2.3、从 spec 获取所需文件"></a>2.3、从 spec 获取所需文件</h4><p>克隆好 build 仓库后首先查看 kubernetes.spec 文件，确定 build 所需文件，spec 文件如下</p><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment"># 省略...</span>%global provider                github%global provider_tld            com%global project                 kubernetes%global repo                    kubernetes<span class="hljs-comment"># https://github.com/kubernetes/kubernetes</span>%global provider_prefix         %&#123;provider&#125;.%&#123;provider_tld&#125;/%&#123;project&#125;/%&#123;repo&#125;%global import_path             k8s.io/kubernetes%global commit                  095136c3078ccf887b9034b7ce598a0a1faff769%global shortcommit              %(c=%&#123;commit&#125;; <span class="hljs-built_in">echo</span> <span class="hljs-variable">$&#123;c:0:7&#125;</span>)%global con_provider            github%global con_provider_tld        com%global con_project             kubernetes%global con_repo                contrib<span class="hljs-comment"># https://github.com/kubernetes/contrib</span>%global con_provider_prefix     %&#123;con_provider&#125;.%&#123;con_provider_tld&#125;/%&#123;con_project&#125;/%&#123;con_repo&#125;%global con_commit              0f5b210313371ff769da24d8264f5a7869c5a3f3%global con_shortcommit         %(c=%&#123;con_commit&#125;; <span class="hljs-built_in">echo</span> <span class="hljs-variable">$&#123;c:0:7&#125;</span>)%global kube_version            1.6.7%global kube_git_version        v%&#123;kube_version&#125;<span class="hljs-comment"># 省略...</span></code></pre></div><p><strong>从 spec 文件中可以看到 build 主要需要两个仓库的源码，一个是 kubernetes 主仓库，存放着主要的 build 源码；另一个是 contrib 仓库，存放着一些配置文件，如 systemd 配置等</strong></p><p><strong>接下来从 spec 文件的 source 段中可以解读到(source0、source1)最终所需的两个仓库压缩文件名为 <code>kubernetes-SHORTCOMMIT</code>、<code>contrib-SHORTCOMIT</code>，source 段如下</strong></p><div class="hljs code-wrapper"><pre><code class="hljs sh">Name:           kubernetesVersion:        %&#123;kube_version&#125;Release:        1%&#123;?dist&#125;Summary:        Container cluster managementLicense:        ASL 2.0URL:            https://%&#123;import_path&#125;ExclusiveArch:  x86_64 aarch64 ppc64le s390xSource0:        https://%&#123;provider_prefix&#125;/archive/%&#123;commit&#125;/%&#123;repo&#125;-%&#123;shortcommit&#125;.tar.gzSource1:        https://%&#123;con_provider_prefix&#125;/archive/%&#123;con_commit&#125;/%&#123;con_repo&#125;-%&#123;con_shortcommit&#125;.tar.gzSource3:        kubernetes-accounting.confSource4:        kubeadm.confSource33:       genmanpages.sh</code></pre></div><p><strong>我们准备 build 一个最新的 1.7.0 的 rpm，所以从 github 获取到 commitID 为 <code>d3ada0119e776222f11ec7945e6d860061339aad</code>，contrib 仓库同理，不过 contrib 一般直接取 master 即可 <code>7d344989fe6a3f11a6d84104b024a50960b021db</code>；接下来首要任务是替换 spec 中原有的 版本号和 commitID 如下</strong></p><div class="hljs code-wrapper"><pre><code class="hljs sh">%global kube_version            1.7.0%global con_commit              7d344989fe6a3f11a6d84104b024a50960b021db%global commit                  d3ada0119e776222f11ec7945e6d860061339aad</code></pre></div><h4 id="2-4、准备源码"><a href="#2-4、准备源码" class="headerlink" title="2.4、准备源码"></a>2.4、准备源码</h4><p>修改好文件以后，就可以下载源码文件了，源码下载不必去克隆 github 项目，直接从 spec 中给出的地址下载即可</p><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-built_in">cd</span> kuberneteswget https://github.com/kubernetes/kubernetes/archive/d3ada0119e776222f11ec7945e6d860061339aad/kubernetes-d3ada01.tar.gzwget https://github.com/kubernetes/contrib/archive/7d344989fe6a3f11a6d84104b024a50960b021db/contrib-7d34498.tar.gz</code></pre></div><h4 id="2-5、build-rpm"><a href="#2-5、build-rpm" class="headerlink" title="2.5、build rpm"></a>2.5、build rpm</h4><p>在正式开始 build 之前，还有一点需要注意的是 <strong>默认的 <code>kubernetes.spec</code> 文件中指定了该 rpm 依赖于 docker 这个包，在 CentOS 上可能我们会安装 docker-engine 或者 docker-ce，此时安装 kubernetes rpm 是无法安装的，因为他以来的包不存在，解决的办法就是编译之前删除 spec 文件中的 <code>Requires: docker</code> 即可</strong>，最后创建好 build 目录，并放置好源码文件开始 build 即可，当然 build 可以有不同选择</p><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment"># 由于我是 root 用户，所以目录位置在这</span><span class="hljs-comment"># 实际生产 强烈不推荐使用 root build(操作失误会损毁宿主机)</span><span class="hljs-comment"># 我的是一台临时 vps，所以无所谓了</span>mkdir -p /root/rpmbuild/SOURCES/mv ~/kubernetes/* /root/rpmbuild/SOURCES/<span class="hljs-built_in">cd</span> /root/rpmbuild/SOURCES/<span class="hljs-comment"># 执行 build</span>rpmbuild -ba kubernetes.spec</code></pre></div><p><strong>注意，由于我们选择的版本已经超出了仓库所支持的最大版本，所以有些 Patch 已经不再适用，如 spec 中的 <code>Patch12</code>、<code>Patch19</code> 会出错，所需要注释掉(%prep 段中也有一个)</strong></p><p><strong><code>rpmbuild 可选项有很多，常用的 3 个，可以根据自己实际需要进行 build:</code></strong></p><ul><li><code>-ba</code> : build 源码包+二进制包</li><li><code>-bb</code> : 只 build 二进制包</li><li><code>-bs</code> : 只 build 源码包</li></ul><p>最后 build 完成后如下</p><p><img src="https://cdn.oss.link/markdown/7tn2a.jpg" alt="rpms"></p>]]></content>
    
    
    <summary type="html">一直使用 Centos 运行 Kubernetes,有些时候基于二进制部署的情况下,手动复制二进制文件和创建 Systemd service 配置略显繁琐;最近找了一下 Kubernetes RPM 的 build 方式,以下记录一下 build 过程</summary>
    
    
    
    <category term="Kubernetes" scheme="https://mritd.com/categories/kubernetes/"/>
    
    
    <category term="Linux" scheme="https://mritd.com/tags/linux/"/>
    
    <category term="Docker" scheme="https://mritd.com/tags/docker/"/>
    
    <category term="Kubernetes" scheme="https://mritd.com/tags/kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>Kubernetes 使用 Ceph 存储</title>
    <link href="https://mritd.com/2017/06/03/use-ceph-storage-on-kubernetes/"/>
    <id>https://mritd.com/2017/06/03/use-ceph-storage-on-kubernetes/</id>
    <published>2017-06-03T04:38:55.000Z</published>
    <updated>2017-06-03T04:38:55.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>本文主要记录一下 Kubernetes 使用 Ceph 存储的相关配置过程，Kubernetes 集群环境采用的 kargo 部署方式，并且所有组件以容器化运行</p></blockquote><h3 id="一、基础环境准备"><a href="#一、基础环境准备" class="headerlink" title="一、基础环境准备"></a>一、基础环境准备</h3><p>Kubernetes 集群总共有 5 台，部署方式为 kargo 容器化部署，**采用 kargo 部署时确保配置中开启内核模块加载( <code>kubelet_load_modules: true</code> )**；Kubernetes 版本为 1.6.4，Ceph 采用最新的稳定版 Jewel</p><table><thead><tr><th>节点</th><th>IP</th><th>部署</th></tr></thead><tbody><tr><td>docker1</td><td>192.168.1.11</td><td>master、monitor、osd</td></tr><tr><td>docker2</td><td>192.168.1.12</td><td>master、monitor、osd</td></tr><tr><td>docker3</td><td>192.168.1.13</td><td>node、monitor、osd</td></tr><tr><td>docker4</td><td>192.168.1.14</td><td>node、osd</td></tr><tr><td>docker5</td><td>192.168.1.15</td><td>node、osd</td></tr></tbody></table><h3 id="二、部署-Ceph-集群"><a href="#二、部署-Ceph-集群" class="headerlink" title="二、部署 Ceph 集群"></a>二、部署 Ceph 集群</h3><p>具体安装请参考 <a href="https://mritd.me/2017/05/27/ceph-note-1/">Ceph 笔记(一)</a>、<a href="https://mritd.me/2017/05/30/ceph-note-2/">Ceph 笔记(二)</a>，以下直接上命令</p><h4 id="2-1、部署集群"><a href="#2-1、部署集群" class="headerlink" title="2.1、部署集群"></a>2.1、部署集群</h4><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment"># 创建集群配置目录</span>mkdir ceph-cluster &amp;&amp; <span class="hljs-built_in">cd</span> ceph-cluster<span class="hljs-comment"># 创建 monitor-node</span>ceph-deploy new docker1 docker2 docker3<span class="hljs-comment"># 追加 OSD 副本数量</span><span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;osd pool default size = 5&quot;</span> &gt;&gt; ceph.conf<span class="hljs-comment"># 安装 ceph</span>ceph-deploy install docker1 docker2 docker3 docker4 docker5<span class="hljs-comment"># init monitor node</span>ceph-deploy mon create-initial<span class="hljs-comment"># 初始化 ods</span>ceph-deploy osd prepare docker1:/dev/sda docker2:/dev/sda docker3:/dev/sda docker4:/dev/sda docker5:/dev/sda<span class="hljs-comment"># 激活 osd</span>ceph-deploy osd activate docker1:/dev/sda1:/dev/sda2 docker2:/dev/sda1:/dev/sda2 docker3:/dev/sda1:/dev/sda2 docker4:/dev/sda1:/dev/sda2 docker5:/dev/sda1:/dev/sda2<span class="hljs-comment"># 部署 ceph cli 工具和秘钥文件</span>ceph-deploy admin docker1 docker2 docker3 docker4 docker5<span class="hljs-comment"># 确保秘钥有读取权限</span>chmod +r /etc/ceph/ceph.client.admin.keyring<span class="hljs-comment"># 检测集群状态</span>ceph health</code></pre></div><h4 id="2-2、创建块设备"><a href="#2-2、创建块设备" class="headerlink" title="2.2、创建块设备"></a>2.2、创建块设备</h4><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment"># 创建存储池</span>rados mkpool data<span class="hljs-comment"># 创建 image</span>rbd create data --size 10240 -p data<span class="hljs-comment"># 关闭不支持特性</span>rbd feature <span class="hljs-built_in">disable</span> data exclusive-lock, object-map, fast-diff, deep-flatten -p data<span class="hljs-comment"># 映射(每个节点都要映射)</span>rbd map data --name client.admin -p data<span class="hljs-comment"># 格式化块设备(单节点即可)</span>mkfs.xfs /dev/rbd0</code></pre></div><h3 id="三、kubernetes-使用-Ceph"><a href="#三、kubernetes-使用-Ceph" class="headerlink" title="三、kubernetes 使用 Ceph"></a>三、kubernetes 使用 Ceph</h3><h4 id="3-1、PV-amp-PVC-方式"><a href="#3-1、PV-amp-PVC-方式" class="headerlink" title="3.1、PV &amp; PVC 方式"></a>3.1、PV &amp; PVC 方式</h4><p>传统的使用分布式存储的方案一般为 <code>PV &amp; PVC</code> 方式，也就是说管理员预先创建好相关 PV 和 PVC，然后对应的 deployment 或者 replication 挂载 PVC 来使用</p><p><strong>创建 Secret</strong></p><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment"># 获取管理 key 并进行 base64 编码</span>ceph auth get-key client.admin | base64<span class="hljs-comment"># 创建一个 secret 配置(key 为上条命令生成的)</span>cat &lt;&lt; <span class="hljs-string">EOF &gt;&gt; ceph-secret.yml</span><span class="hljs-string">apiVersion: v1</span><span class="hljs-string">kind: Secret</span><span class="hljs-string">metadata:</span><span class="hljs-string">  name: ceph-secret</span><span class="hljs-string">data:</span><span class="hljs-string">  key: QVFDaWtERlpzODcwQWhBQTdxMWRGODBWOFZxMWNGNnZtNmJHVGc9PQo=</span><span class="hljs-string">EOF</span>kubectl create -f ceph-secret.yml</code></pre></div><p><strong>创建 PV</strong></p><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment"># monitor 需要多个，pool 和 image 填写上面创建的</span>cat &lt;&lt; <span class="hljs-string">EOF &gt;&gt; test.pv.yml</span><span class="hljs-string">apiVersion: v1</span><span class="hljs-string">kind: PersistentVolume</span><span class="hljs-string">metadata:</span><span class="hljs-string">  name: test-pv</span><span class="hljs-string">spec:</span><span class="hljs-string">  capacity:</span><span class="hljs-string">    storage: 2Gi</span><span class="hljs-string">  accessModes:</span><span class="hljs-string">    - ReadWriteOnce </span><span class="hljs-string">  rbd:</span><span class="hljs-string">    monitors:</span><span class="hljs-string">      - 192.168.1.11:6789</span><span class="hljs-string">      - 192.168.1.12:6789</span><span class="hljs-string">      - 192.168.1.13:6789</span><span class="hljs-string">    pool: data</span><span class="hljs-string">    image: data</span><span class="hljs-string">    user: admin</span><span class="hljs-string">    secretRef:</span><span class="hljs-string">      name: ceph-secret</span><span class="hljs-string">    fsType: xfs</span><span class="hljs-string">    readOnly: false</span><span class="hljs-string">  persistentVolumeReclaimPolicy: Recycle</span><span class="hljs-string">EOF</span>kubectl create -f test.pv.yml</code></pre></div><p><strong>创建 PVC</strong></p><div class="hljs code-wrapper"><pre><code class="hljs sh">cat &lt;&lt; <span class="hljs-string">EOF &gt;&gt; test.pvc.yml</span><span class="hljs-string">kind: PersistentVolumeClaim</span><span class="hljs-string">apiVersion: v1</span><span class="hljs-string">metadata:</span><span class="hljs-string">  name: test-pvc</span><span class="hljs-string">spec:</span><span class="hljs-string">  accessModes:</span><span class="hljs-string">    - ReadWriteOnce</span><span class="hljs-string">  resources:</span><span class="hljs-string">    requests:</span><span class="hljs-string">      storage: 2Gi</span><span class="hljs-string">EOF</span>kubectl create -f test.pvc.yml</code></pre></div><p><strong>创建 Deployment并挂载</strong></p><div class="hljs code-wrapper"><pre><code class="hljs sh">cat &lt;&lt; <span class="hljs-string">EOF &gt;&gt; test.deploy.yml</span><span class="hljs-string">apiVersion: apps/v1beta1</span><span class="hljs-string">kind: Deployment</span><span class="hljs-string">metadata:</span><span class="hljs-string">  name: demo</span><span class="hljs-string">spec:</span><span class="hljs-string">  replicas: 3</span><span class="hljs-string">  template:</span><span class="hljs-string">    metadata:</span><span class="hljs-string">      labels:</span><span class="hljs-string">        app: demo</span><span class="hljs-string">    spec:</span><span class="hljs-string">      containers:</span><span class="hljs-string">      - name: demo</span><span class="hljs-string">        image: mritd/demo</span><span class="hljs-string">        ports:</span><span class="hljs-string">        - containerPort: 80</span><span class="hljs-string">        volumeMounts:</span><span class="hljs-string">          - mountPath: &quot;/data&quot;</span><span class="hljs-string">            name: data</span><span class="hljs-string">      volumes:</span><span class="hljs-string">        - name: data</span><span class="hljs-string">          persistentVolumeClaim:</span><span class="hljs-string">            claimName: test-pvc</span><span class="hljs-string">EOF</span>kubectl create -f test.deploy.yml</code></pre></div><h4 id="3-2、StoragaClass-方式"><a href="#3-2、StoragaClass-方式" class="headerlink" title="3.2、StoragaClass 方式"></a>3.2、StoragaClass 方式</h4><p>在 1.4 以后，kubernetes 提供了一种更加方便的动态创建 PV 的方式；也就是说使用 StoragaClass 时无需预先创建固定大小的 PV，等待使用者创建 PVC 来使用；而是直接创建 PVC 即可分配使用</p><p><strong>创建系统级 Secret</strong></p><p><strong>注意: 由于 StorageClass 要求 Ceph 的 Secret type 必须为 <code>kubernetes.io/rbd</code>，所以上一步创建的 <code>ceph-secret</code> 需要先被删除，然后使用如下命令重新创建；此时的 key 并没有经过 base64</strong></p><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment"># 这个 secret type 必须为 kubernetes.io/rbd，否则会造成 PVC 无法使用</span>kubectl create secret generic ceph-secret --<span class="hljs-built_in">type</span>=<span class="hljs-string">&quot;kubernetes.io/rbd&quot;</span> --from-literal=key=<span class="hljs-string">&#x27;AQCikDFZs870AhAA7q1dF80V8Vq1cF6vm6bGTg==&#x27;</span> --namespace=kube-systemkubectl create secret generic ceph-secret --<span class="hljs-built_in">type</span>=<span class="hljs-string">&quot;kubernetes.io/rbd&quot;</span> --from-literal=key=<span class="hljs-string">&#x27;AQCikDFZs870AhAA7q1dF80V8Vq1cF6vm6bGTg==&#x27;</span> --namespace=default</code></pre></div><p><strong>创建 StorageClass</strong></p><div class="hljs code-wrapper"><pre><code class="hljs sh">cat &lt;&lt; <span class="hljs-string">EOF &gt;&gt; test.storageclass.yml</span><span class="hljs-string">apiVersion: storage.k8s.io/v1</span><span class="hljs-string">kind: StorageClass</span><span class="hljs-string">metadata:</span><span class="hljs-string">  name: test-storageclass</span><span class="hljs-string">provisioner: kubernetes.io/rbd</span><span class="hljs-string">parameters:</span><span class="hljs-string">  monitors: 192.168.1.11:6789,192.168.1.12:6789,192.168.1.13:6789</span><span class="hljs-string">  # Ceph 客户端用户 ID(非 k8s 的)</span><span class="hljs-string">  adminId: admin</span><span class="hljs-string">  adminSecretName: ceph-secret</span><span class="hljs-string">  adminSecretNamespace: kube-system</span><span class="hljs-string">  pool: data</span><span class="hljs-string">  userId: admin</span><span class="hljs-string">  userSecretName: ceph-secret</span><span class="hljs-string">EOF</span>kubectl create -f test.storageclass.yml</code></pre></div><p><strong>关于上面的 adminId 等字段具体含义请参考这里 <a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/#ceph-rbd">Ceph RBD</a></strong></p><p><strong>创建 PVC</strong></p><div class="hljs code-wrapper"><pre><code class="hljs sh">cat &lt;&lt; <span class="hljs-string">EOF &gt;&gt; test.sc.pvc.yml</span><span class="hljs-string">kind: PersistentVolumeClaim</span><span class="hljs-string">apiVersion: v1</span><span class="hljs-string">metadata:</span><span class="hljs-string">  name: test-sc-pvc</span><span class="hljs-string">  annotations: </span><span class="hljs-string">    volume.beta.kubernetes.io/storage-class: test-storageclass</span><span class="hljs-string">spec:</span><span class="hljs-string">  accessModes:</span><span class="hljs-string">    - ReadWriteOnce </span><span class="hljs-string">  resources:</span><span class="hljs-string">    requests:</span><span class="hljs-string">      storage: 2Gi</span><span class="hljs-string">EOF</span>kubectl create -f test.sc.pvc.yml</code></pre></div><p><strong>创建 Deployment</strong></p><div class="hljs code-wrapper"><pre><code class="hljs sh">cat &lt;&lt; <span class="hljs-string">EOF &gt;&gt; test.sc.deploy.yml</span><span class="hljs-string">apiVersion: apps/v1beta1</span><span class="hljs-string">kind: Deployment</span><span class="hljs-string">metadata:</span><span class="hljs-string">  name: demo-sc</span><span class="hljs-string">spec:</span><span class="hljs-string">  replicas: 3</span><span class="hljs-string">  template:</span><span class="hljs-string">    metadata:</span><span class="hljs-string">      labels:</span><span class="hljs-string">        app: demo-sc</span><span class="hljs-string">    spec:</span><span class="hljs-string">      containers:</span><span class="hljs-string">      - name: demo-sc</span><span class="hljs-string">        image: mritd/demo</span><span class="hljs-string">        ports:</span><span class="hljs-string">        - containerPort: 80</span><span class="hljs-string">        volumeMounts:</span><span class="hljs-string">          - mountPath: &quot;/data&quot;</span><span class="hljs-string">            name: data</span><span class="hljs-string">      volumes:</span><span class="hljs-string">        - name: data</span><span class="hljs-string">          persistentVolumeClaim:</span><span class="hljs-string">            claimName: test-sc-pvc</span><span class="hljs-string">EOF</span>kubectl create -f test.sc.deploy.yml</code></pre></div><p>到此完成，检测是否成功最简单的方式就是看相关 pod 是否正常运行</p>]]></content>
    
    
    <summary type="html">本文主要记录一下 Kubernetes 使用 Ceph 存储的相关配置过程，Kubernetes 集群环境采用的 kargo 部署方式，并且所有组件以容器化运行</summary>
    
    
    
    <category term="Kubernetes" scheme="https://mritd.com/categories/kubernetes/"/>
    
    
    <category term="Linux" scheme="https://mritd.com/tags/linux/"/>
    
    <category term="Docker" scheme="https://mritd.com/tags/docker/"/>
    
    <category term="Kubernetes" scheme="https://mritd.com/tags/kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>Ceph 笔记(二)</title>
    <link href="https://mritd.com/2017/05/30/ceph-note-2/"/>
    <id>https://mritd.com/2017/05/30/ceph-note-2/</id>
    <published>2017-05-30T15:39:29.000Z</published>
    <updated>2017-05-30T15:39:29.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>本篇文章主要简述了 Ceph 的存储对象名词解释及其含义，以及对 Ceph 集群内 CRUSH bucket 调整、PG/PGP 参数调整等设置；同时参考了一些书籍资料简单的概述一下 Ceph 集群硬件要求等</p></blockquote><h3 id="一、Ceph-组件及定义"><a href="#一、Ceph-组件及定义" class="headerlink" title="一、Ceph 组件及定义"></a>一、Ceph 组件及定义</h3><h4 id="1-1、对象"><a href="#1-1、对象" class="headerlink" title="1.1、对象"></a>1.1、对象</h4><p>对象是 Ceph 中最小的存储单元，对象是一个数据和一个元数据绑定的整体；元数据中存放了具体数据的相关属性描述信息等；Ceph 为每个对象生成一个集群内唯一的对象标识符，以保证对象在集群内的唯一性；在传统文件系统的存储中，单个文件的大小是有一定限制的，而 Ceph 中对象随着其元数据区增大可以变得非常巨大</p><h4 id="1-2、CRUSH"><a href="#1-2、CRUSH" class="headerlink" title="1.2、CRUSH"></a>1.2、CRUSH</h4><p>在传统的文件存储系统中，数据的元数据占据着极其重要的位置，每次系统中新增数据时，元数据首先被更新，然后才是实际的数据被写入；在较小的存储系统中(GB/TB)，这种将元数据存储在某个固定的存储节点或者磁盘阵列中的做法还可以满足需求；当数据量增大到 PB/ZB 级别时，元数据查找性能将会成为一个很大的瓶颈；同时元数据的统一存放还可能造成单点故障，即当元数据丢失后，实际数据将无法被找回；与传统文件存储系统不同的是，<strong>Ceph 使用可扩展散列下的受控复制(Controlled Replication Under Scalable Hashing,CRUSH)算法来精确地计算数据应该被写入哪里/从哪里读取；CRUSH按需计算元数据，而不是存储元数据，从而解决了传统文件存储系统的瓶颈</strong></p><h4 id="1-3、CRUSH-查找"><a href="#1-3、CRUSH-查找" class="headerlink" title="1.3、CRUSH 查找"></a>1.3、CRUSH 查找</h4><p>在 Ceph 中，元数据的计算和负载是分布式的，并且只有在需要时才会执行；元数据的计算过程称之为 CRUSH 查找，不同于其他分布式文件系统，Ceph 的 CRUSH 查找是由客户端使用自己的资源来完成的，从而去除了中心查找带来的性能以及单点故障问题；CRUSH 查找时，客户端先通过 monitor 获取集群 map 副本，然后从 map 副本中获取集群配置信息；然后通过对象信息、池ID等生成对象；接着通过对象和 PG 数散列后得到 Ceph 池中最终存放该对象的 PG；最终在通过 CRUSH 算法确定该 PG 所需存储的 OSD 位置，<strong>一旦确定了 OSD 位置，那么客户端将直接与 OSD 通讯完成数据读取与写入，这直接去除了中间环节，保证了性能的极大提升</strong></p><h4 id="1-4、CRUSH-层级结构"><a href="#1-4、CRUSH-层级结构" class="headerlink" title="1.4、CRUSH 层级结构"></a>1.4、CRUSH 层级结构</h4><p>在 Ceph 中，CRUSH 是完全支持各种基础设施和用户自定义的；CRUSH 设备列表中预先定义了一系列的设备，包括磁盘、节点、机架、行、开关、电源电路、房间、数据中心等等；这些组件称之为故障区(CRUSH bucket)，用户可以通过自己的配置把不同的 OSD 分布在不同区域；<strong>此后 Ceph 存储数据时根据 CRUSH bucket 结构，将会保证每份数据都会在所定义的物理组件之间完全隔离；</strong>比如我们定义了多个机架上的不同 OSD，那么 Ceph 存储时就会智能的将数据副本分散到多个机架之上，防止某个机架上机器全部跪了以后数据全部丢失的情况</p><h4 id="1-5、恢复和再平衡"><a href="#1-5、恢复和再平衡" class="headerlink" title="1.5、恢复和再平衡"></a>1.5、恢复和再平衡</h4><p>当故障区内任何组件出现故障时，Ceph 都会将其标记为 down 和 out 状态；然后默认情况下 Ceph 会等待 300秒之后进行数据恢复和再平衡，这个值可以通过在配置文件中的 <code>mon osd down out interval</code> 参数来调整</p><h4 id="1-6、PG"><a href="#1-6、PG" class="headerlink" title="1.6、PG"></a>1.6、PG</h4><p>PG 是一组对象集合体，根据 Ceph 的复制级别，每个PG 中的数据会被复制到多个 OSD 上，以保证其高可用状态</p><h4 id="1-7、Ceph-池"><a href="#1-7、Ceph-池" class="headerlink" title="1.7、Ceph 池"></a>1.7、Ceph 池</h4><p>Ceph 池是一个存储对象的逻辑分区，每一个池中都包含若干个 PG，进而实现将一定对象映射到集群内不同 OSD 中，<strong>池可以以复制方式或者纠错码方式创建，但不可同时使用这两种方式</strong></p><h3 id="二、Ceph-组件调整及操作"><a href="#二、Ceph-组件调整及操作" class="headerlink" title="二、Ceph 组件调整及操作"></a>二、Ceph 组件调整及操作</h3><h4 id="2-1、池操作"><a href="#2-1、池操作" class="headerlink" title="2.1、池操作"></a>2.1、池操作</h4><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment"># 创建池</span>rados mkpool test-pool<span class="hljs-comment"># 列出池</span>rados lspools<span class="hljs-comment"># 复制池</span>rados cppool test-pool cp-pool<span class="hljs-comment"># 删除池</span>rados rmpool test-pool test-pool --yes-i-really-really-mean-it</code></pre></div><h4 id="2-2、对象操作"><a href="#2-2、对象操作" class="headerlink" title="2.2、对象操作"></a>2.2、对象操作</h4><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment"># 将对象加入到池内</span>rados put testfile anaconda-ks.cfg -p <span class="hljs-built_in">test</span><span class="hljs-comment"># 列出池内对象</span>rados ls -p <span class="hljs-built_in">test</span><span class="hljs-comment"># 检查对象信息</span>ceph osd map <span class="hljs-built_in">test</span> testfile<span class="hljs-comment"># 删除对象</span>rados rm testfile -p <span class="hljs-built_in">test</span></code></pre></div><h4 id="2-3、修改-PG-和-PGP"><a href="#2-3、修改-PG-和-PGP" class="headerlink" title="2.3、修改 PG 和 PGP"></a>2.3、修改 PG 和 PGP</h4><p>计算 PG 数为 Ceph 企业级存储必不可少的的一部分，其中集群内 PG 计算公式如下</p><div class="hljs code-wrapper"><pre><code class="hljs sh">PG 总数 = (OSD 数 * 100) / 最大副本数</code></pre></div><p>对于单个池来讲，我们还应该为池设定 PG 数，其中池的 PG 数计算公式如下</p><div class="hljs code-wrapper"><pre><code class="hljs sh">PG 总数 = (OSD 数 * 100) / 最大副本数 / 池数</code></pre></div><p>PGP 是为了实现定位而设计的 PG，PGP 的值应该与 PG 数量保持一致；<strong>当池的 pg_num 增加的时候，池内所有 PG 都会一分为二，但是他们仍然保持着以前 OSD 的映射关系；当增加 pgp_num 的时候，Ceph 集群才会将 PG 进行 OSD 迁移，然后开始再平衡过程</strong></p><p>获取现有 PG 和 PGP 值可以通过如下命令</p><div class="hljs code-wrapper"><pre><code class="hljs sh">ceph osd pool get <span class="hljs-built_in">test</span> pg_numceph osd pool get <span class="hljs-built_in">test</span> pgp_num</code></pre></div><p>当计算好 PG 和 PGP 以后可以通过以下命令设置</p><div class="hljs code-wrapper"><pre><code class="hljs sh">ceph osd pool <span class="hljs-built_in">set</span> <span class="hljs-built_in">test</span> pgp_num 32ceph osd pool <span class="hljs-built_in">set</span> <span class="hljs-built_in">test</span> pgp_num 32</code></pre></div><p>同样在创建 pool 的时候也可以同步指定</p><div class="hljs code-wrapper"><pre><code class="hljs sh">ceph osd pool create POOLNAME PG PGP</code></pre></div><h4 id="2-4、pool-副本数调整"><a href="#2-4、pool-副本数调整" class="headerlink" title="2.4、pool 副本数调整"></a>2.4、pool 副本数调整</h4><p>默认情况，当创建一个新的 pool 时，向 pool 内存储的数据只会有 2 个副本，查看 pool 副本数可以通过如下命令</p><div class="hljs code-wrapper"><pre><code class="hljs sh">ceph osd dump | grep pool</code></pre></div><p>当我们需要修改默认副本数以使其满足高可靠性需求时，可以通过如下命令完成</p><div class="hljs code-wrapper"><pre><code class="hljs sh">ceph osd pool <span class="hljs-built_in">set</span> POOLNAME size NUM</code></pre></div><h4 id="2-5、定制机群布局"><a href="#2-5、定制机群布局" class="headerlink" title="2.5、定制机群布局"></a>2.5、定制机群布局</h4><p>上面已经讲述了 CRUSH bucket 的概念，通过以下相关命令，我们可以定制自己的集群布局，以使 Ceph 完成数据的容灾处理</p><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment"># 查看现有集群布局</span>ceph osd tree<span class="hljs-comment"># 添加机架</span>ceph osd crush add-bucket rack01 rackceph osd crush add-bucket rack02 rackceph osd crush add-bucket rack03 rack<span class="hljs-comment"># 移动主机到不同的机架(dockerX 为我的主机名)</span>ceph osd crush move docker1 rack=rack01ceph osd crush move docker2 rack=rack02ceph osd crush move docker3 rack=rack03<span class="hljs-comment"># 移动每个机架到默认的根下</span>ceph osd crush move rack01 root=defaultceph osd crush move rack02 root=defaultceph osd crush move rack03 root=default</code></pre></div><p>最终集群整体布局如下</p><div class="hljs code-wrapper"><pre><code class="hljs sh">➜  ~ ceph osd treeID WEIGHT  TYPE NAME            UP/DOWN REWEIGHT PRIMARY-AFFINITY-1 0.01469 root default-5 0.00490     rack rack01-2 0.00490         host docker1 0 0.00490             osd.0         up  1.00000          1.00000-6 0.00490     rack rack02-3 0.00490         host docker2 1 0.00490             osd.1         up  1.00000          1.00000-7 0.00490     rack rack03-4 0.00490         host docker3 2 0.00490             osd.2         up  1.00000          1.00000</code></pre></div><h3 id="三、Ceph-硬件配置"><a href="#三、Ceph-硬件配置" class="headerlink" title="三、Ceph 硬件配置"></a>三、Ceph 硬件配置</h3><p>硬件规划一般是一个企业级存储的必要工作，以下简述了 Ceph 的一般硬件需求</p><h4 id="3-1、监控需求"><a href="#3-1、监控需求" class="headerlink" title="3.1、监控需求"></a>3.1、监控需求</h4><p>Ceph monitor 通过维护整个集群的 map 从而完成集群的健康处理；但是 monitor 并不参与实际的数据存储，所以实际上 monitor 节点 CPU 占用、内存占用都比较少；一般单核 CPU 加几个 G 的内存即可满足需求；虽然 monitor 节点不参与实际存储工作，但是 monitor 的网卡至少应该是冗余的，因为一旦网络出现故障则集群健康会难以保证</p><h4 id="3-2、OSD-需求"><a href="#3-2、OSD-需求" class="headerlink" title="3.2、OSD 需求"></a>3.2、OSD 需求</h4><p>OSD 作为 Ceph 集群的主要存储设施，其会占用一定的 CPU 和内存资源；一般推荐做法是每个节点的每块硬盘作为一个 OSD；同时 OSD 还需要写入日志，所以应当为 OSD 集成日志留有充足的空间；在出现故障时，OSD 需求的资源可能会更多，所以 OSD 节点根据实际情况(每个 OSD 会有一个线程)应该分配更多的 CPU 和内存；固态硬盘也会增加 OSD 存取速度和恢复速度</p><h4 id="3-3、MDS-需求"><a href="#3-3、MDS-需求" class="headerlink" title="3.3、MDS 需求"></a>3.3、MDS 需求</h4><p>MDS 服务专门为 CephFS 存储元数据，所以相对于 monitor 和 OSD 节点，这个 MDS 节点的 CPU 需求会大得多，同时内存占用也是海量的，所以 MDS 一般会使用一个强劲的物理机单独搭建</p>]]></content>
    
    
    <summary type="html">本篇文章主要简述了 Ceph 的存储对象名词解释及其含义，以及对 Ceph 集群内 CRUSH bucket 调整、PG/PGP 参数调整等设置；同时参考了一些书籍资料简单的概述一下 Ceph 集群硬件要求等</summary>
    
    
    
    <category term="Linux" scheme="https://mritd.com/categories/linux/"/>
    
    
    <category term="Linux" scheme="https://mritd.com/tags/linux/"/>
    
    <category term="Ceph" scheme="https://mritd.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title>Ceph 笔记(一)</title>
    <link href="https://mritd.com/2017/05/27/ceph-note-1/"/>
    <id>https://mritd.com/2017/05/27/ceph-note-1/</id>
    <published>2017-05-26T17:17:19.000Z</published>
    <updated>2017-05-26T17:17:19.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>Ceph 是一个符合POSIX、开源的分布式存储系统；其具备了极好的可靠性、统一性、鲁棒性；经过近几年的发展，Ceph 开辟了一个全新的数据存储途径。Ceph 具备了企业级存储的分布式、可大规模扩展、没有单点故障等特点，越来越受到人们青睐；以下记录了 Ceph 的相关学习笔记。</p></blockquote><h3 id="一、-Ceph-Quick-Start"><a href="#一、-Ceph-Quick-Start" class="headerlink" title="一、 Ceph Quick Start"></a>一、 Ceph Quick Start</h3><h4 id="1-1、安装前准备"><a href="#1-1、安装前准备" class="headerlink" title="1.1、安装前准备"></a>1.1、安装前准备</h4><blockquote><p>本文以 Centos 7 3.10 内核为基础环境，节点为 4 台 Vagrant 虚拟机；Ceph 版本为 Jewel.</p></blockquote><p>首先需要一台部署节点，这里使用的是宿主机；在部署节点上需要安装一些部署工具，如下</p><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment"># 安装 EPEL 源</span>yum install -y epel-release<span class="hljs-comment"># 添加 ceph 官方源</span>cat &lt;&lt; <span class="hljs-string">EOF &gt;&gt; /etc/yum.repos.d/ceph.repo</span><span class="hljs-string">[ceph-noarch]</span><span class="hljs-string">name=Ceph noarch packages</span><span class="hljs-string">baseurl=https://download.ceph.com/rpm-jewel/el7/noarch</span><span class="hljs-string">enabled=1</span><span class="hljs-string">gpgcheck=1</span><span class="hljs-string">type=rpm-md</span><span class="hljs-string">gpgkey=https://download.ceph.com/keys/release.asc</span><span class="hljs-string">EOF</span><span class="hljs-comment"># 安装部署工具</span>yum update -y &amp;&amp; yum install ceph-deploy -y</code></pre></div><p><strong>同时，ceph-deploy 工具需要使用 ssh 来自动化部署 Ceph 各个组件，因此需要保证部署节点能够免密码登录待部署节点；最后，待部署节点最好加入到部署节点的 hosts 中，方便使用域名(某些地方强制)连接管理</strong></p><h4 id="1-2、校对时钟"><a href="#1-2、校对时钟" class="headerlink" title="1.2、校对时钟"></a>1.2、校对时钟</h4><p>由于 Ceph 采用 Paxos 算法保证数据一致性，所以安装前需要先保证各个节点时钟同步</p><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment"># 安装 ntp 工具</span>yum install ntp ntpdate ntp-doc -y<span class="hljs-comment"># 校对系统时钟</span>ntpdate 0.cn.pool.ntp.org</code></pre></div><h4 id="1-3、创建集群配置"><a href="#1-3、创建集群配置" class="headerlink" title="1.3、创建集群配置"></a>1.3、创建集群配置</h4><p>ceph-deploy 工具部署集群前需要创建一些集群配置信息，其保存在 <code>ceph.conf</code> 文件中，这个文件未来将会被复制到每个节点的 <code>/etc/ceph/ceph.conf</code></p><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment"># 创建集群配置目录</span>mkdir ceph-cluster &amp;&amp; <span class="hljs-built_in">cd</span> ceph-cluster<span class="hljs-comment"># 创建 monitor-node</span>ceph-deploy new docker1<span class="hljs-comment"># 追加 OSD 副本数量(测试虚拟机总共有3台)</span><span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;osd pool default size = 3&quot;</span> &gt;&gt; ceph.conf</code></pre></div><h4 id="1-4、创建集群"><a href="#1-4、创建集群" class="headerlink" title="1.4、创建集群"></a>1.4、创建集群</h4><p>创建集群使用 ceph-deploy 工具即可</p><div class="hljs code-wrapper"><pre><code class="hljs apache"><span class="hljs-comment"># 安装 ceph</span><span class="hljs-attribute">ceph</span>-deploy install docker<span class="hljs-number">1</span> docker<span class="hljs-number">2</span> docker<span class="hljs-number">3</span><span class="hljs-comment"># 初始化 monitor node 和 秘钥文件</span><span class="hljs-attribute">ceph</span>-deploy mon create-initial<span class="hljs-comment"># 在两个 osd 节点创建一个目录作为 osd 存储</span><span class="hljs-attribute">mkdir</span> /data<span class="hljs-attribute">chown</span> -R ceph:ceph /data<span class="hljs-comment"># 初始化 osd</span><span class="hljs-attribute">ceph</span>-deploy osd prepare docker<span class="hljs-number">1</span>:/data docker<span class="hljs-number">2</span>:/data docker<span class="hljs-number">3</span>:/data<span class="hljs-comment"># 激活 osd</span><span class="hljs-attribute">ceph</span>-deploy osd activate docker<span class="hljs-number">1</span>:/data docker<span class="hljs-number">2</span>:/data docker<span class="hljs-number">3</span>:/data<span class="hljs-comment"># 部署 ceph cli 工具和秘钥文件</span><span class="hljs-attribute">ceph</span>-deploy admin docker<span class="hljs-number">1</span> docker<span class="hljs-number">2</span> docker<span class="hljs-number">3</span><span class="hljs-comment"># 确保秘钥有读取权限</span><span class="hljs-attribute">chmod</span> +r /etc/ceph/ceph.client.admin.keyring<span class="hljs-comment"># 检测集群状态</span><span class="hljs-attribute">ceph</span> health</code></pre></div><p>执行 <code>ceph health</code> 命令后应当返回 <code>HEALTH_OK</code>；如出现 <code>HEALTH_WARN clock skew detected on mon.docker2; Monitor clock skew detected</code>，说明时钟不同步，手动同步时钟稍等片刻后即可；其他错误可以通过如下命令重置集群重新部署</p><div class="hljs code-wrapper"><pre><code class="hljs sh">ceph-deploy purge &#123;ceph-node&#125; [&#123;ceph-node&#125;]ceph-deploy purgedata &#123;ceph-node&#125; [&#123;ceph-node&#125;]ceph-deploy forgetkeys</code></pre></div><p><strong>更多细节，如防火墙、SELinux配置等请参考 <a href="http://docs.ceph.com/docs/master/start/quick-start-preflight/#rhel-centos">官方文档</a></strong></p><h4 id="1-5、其他组件创建"><a href="#1-5、其他组件创建" class="headerlink" title="1.5、其他组件创建"></a>1.5、其他组件创建</h4><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment"># 创建 MDS</span>ceph-deploy mds create docker1<span class="hljs-comment"># 创建 RGW</span>ceph-deploy rgw create docker1<span class="hljs-comment"># 增加 monitor</span><span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;public network = 192.168.1.0/24&quot;</span> &gt;&gt; ceph.confceph-deploy --overwrite-conf mon create docker2 docker3<span class="hljs-comment"># 查看仲裁信息</span>ceph quorum_status --format json-pretty</code></pre></div><h3 id="二、Ceph-组件及测试"><a href="#二、Ceph-组件及测试" class="headerlink" title="二、Ceph 组件及测试"></a>二、Ceph 组件及测试</h3><h4 id="2-1、Ceph-架构图"><a href="#2-1、Ceph-架构图" class="headerlink" title="2.1、Ceph 架构图"></a>2.1、Ceph 架构图</h4><p>以下图片(摘自网络)展示了基本的 Ceph 架构</p><p><img src="https://cdn.oss.link/markdown/o8gct.jpg" alt="ceph 架构"></p><ul><li>OSD: Ceph 实际存储数据单元被称为 OSD，OSD 可以使用某个物理机的目录、磁盘设备，甚至是 RAID 阵列；</li><li>MON: 在 OSD 之上则分布着多个 MON(monitor)，Ceph 集群内组件的状态信息等被维护成一个个 map，而 MON 则负责维护集群所有组件 map 信息，各个集群内组件心跳请求 MON 以确保其 map 保持最新状态；当集群发生故障时，Ceph 将采用 Paxos 算法保证数据一致性，这其中仲裁等主要由 MON 完成，所以 MON 节点建议最少为 3 个，并且为奇数以防止脑裂情况的发生；</li><li>MDS: Ceph 本身使用对象形式存储数据，而对于外部文件系统访问支持则提供了上层的 CephFS 接口；CephFS 作为文件系统接口则需要一些元数据，这些原数据就存放在 MDS 中；目前 Ceph 只支持单个 MDS 工作，<strong>但是可以通过设置 MDS 副本，以保证 MDS 的可靠性</strong></li><li>RADOS: RADOS 全称 Reliable Autonomic Distributed Object Store，即可靠分布式对象存储；其作为在整个 Ceph 集群核心基础设施，向外部提供基本的数据操作</li><li>librados: 为了支持私有云等程序调用，Ceph 提供了 C 实现的 API 库 librados，librados 可以支持主流编程语言直接调用，沟通 RADOS 完成数据存取等操作</li><li>RBD: RDB 个人理解是一个命令行工具，一般位于宿主机上，通过该工具可以直接跟 librados 交互，实现创建存储对象，格式化 Ceph 块设备等操作</li><li>RADOS GW: 从名字可以看出来，这个组件是一个代理网关，通过 RADOS GW 可以将 RADOS 响应转化为 HTTP 响应，同样可以将外部 HTTP 请求转化为 RADOS 调用；RADOS GW 主要提供了三大功能: <strong>兼容 S3 接口、兼容 Swift 接口、提供管理 RestFul API</strong></li></ul><p>下图(摘自网络)从应用角度描述了 Ceph 架构</p><p><img src="https://cdn.oss.link/markdown/fh5z8.jpg" alt="APP Ceph 架构"></p><h4 id="2-2、对象存储测试"><a href="#2-2、对象存储测试" class="headerlink" title="2.2、对象存储测试"></a>2.2、对象存储测试</h4><p>此处直接上代码</p><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment"># 创建测试文件</span>dd <span class="hljs-keyword">if</span>=/dev/zero of=<span class="hljs-built_in">test</span> bs=1G count=1<span class="hljs-comment"># 创建对象存储池</span>rados mkpool data<span class="hljs-comment"># 放入对象</span>rados put test-file <span class="hljs-built_in">test</span> --pool=data<span class="hljs-comment"># 检查存储池</span>rados -p data ls<span class="hljs-comment"># 检查对象信息</span>ceph osd map data test-file<span class="hljs-comment"># 删除对象</span>rados -p data rm test-file<span class="hljs-comment"># 删除存储池(存储池写两遍并且加上确认)</span>rados rmpool data data --yes-i-really-really-mean-it</code></pre></div><h4 id="2-3、块设备测试"><a href="#2-3、块设备测试" class="headerlink" title="2.3、块设备测试"></a>2.3、块设备测试</h4><p><strong>官方文档中提示，使用 rdb 的客户端不建议与 OSD 等节点在同一台机器上</strong></p><blockquote><p>You may use a virtual machine for your ceph-client node, but do not execute the following procedures on the same physical node as your Ceph Storage Cluster nodes (unless you use a VM). See FAQ for details.</p></blockquote><p>这里从第四台虚拟机上执行操作，首先安装所需客户端工具</p><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment"># 部署节点上 ceph-cluster 目录下执行</span>ceph-deploy install docker4ceph-deploy admin docker4</code></pre></div><p>然后创建块设备</p><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment"># 块设备单位为 MB</span>rbd create data --size 10240<span class="hljs-comment"># 映射块设备</span>map foo --name client.admin</code></pre></div><p><strong>在上面的 map 映射操作时，可能出现如下错误</strong></p><div class="hljs code-wrapper"><pre><code class="hljs sh">RBD image feature <span class="hljs-built_in">set</span> mismatch. You can <span class="hljs-built_in">disable</span> features unsupported by the kernel with <span class="hljs-string">&quot;rbd feature disable&quot;</span></code></pre></div><p>查看系统日志可以看到如下输出</p><div class="hljs code-wrapper"><pre><code class="hljs sh">➜  ~ dmesg | tail[-1127592253.530346] rbd: image data: image uses unsupported features: 0x38[-1127590337.563180] libceph: mon0 192.168.1.11:6789 session established[-1127590337.563741] libceph: client4200 fsid dd9fdfee-438a-47aa-be21-114372bc1f44</code></pre></div><p><strong>问题原因: 在 Ceph 高版本进行 map image 时，默认 Ceph 在创建 image(上文 data)时会增加许多 features，这些 features 需要内核支持，在 Centos7 的内核上支持有限，所以需要手动关掉一些 features</strong></p><p>首先使用 <code>rbd info data</code> 命令列出创建的 image 的 features</p><div class="hljs code-wrapper"><pre><code class="hljs sh">rbd image <span class="hljs-string">&#x27;data&#x27;</span>:        size 10240 MB <span class="hljs-keyword">in</span> 2560 objects        order 22 (4096 kB objects)        block_name_prefix: rbd_data.37c6238e1f29        format: 2        features: layering, exclusive-lock, object-map, fast-diff, deep-flatten        flags:</code></pre></div><p>在 features 中我们可以看到默认开启了很多:</p><ul><li>layering: 支持分层</li><li>striping: 支持条带化 v2</li><li>exclusive-lock: 支持独占锁</li><li>object-map: 支持对象映射(依赖 exclusive-lock)</li><li>fast-diff: 快速计算差异(依赖 object-map)</li><li>deep-flatten: 支持快照扁平化操作</li><li>journaling: 支持记录 IO 操作(依赖独占锁）</li></ul><p><strong>而实际上 Centos 7 的 3.10 内核只支持 layering… 所以我们要手动关闭一些 features，然后重新 map；如果想要一劳永逸，可以在 ceph.conf 中加入 <code>rbd_default_features = 1</code> 来设置默认 features(数值仅是 layering 对应的 bit 码所对应的整数值)。</strong></p><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment"># 关闭不支持特性</span>rbd feature <span class="hljs-built_in">disable</span> data exclusive-lock, object-map, fast-diff, deep-flatten<span class="hljs-comment"># 重新映射</span>rbd map data --name client.admin<span class="hljs-comment"># 成功后返回设备位置</span>/dev/rbd0</code></pre></div><p><strong>最后我们便可以格式化正常挂载这个设备了</strong></p><div class="hljs code-wrapper"><pre><code class="hljs sh">➜  ~ mkfs.xfs /dev/rbd0meta-data=/dev/rbd0              isize=512    agcount=17, agsize=162816 blks         =                       sectsz=512   attr=2, projid32bit=1         =                       crc=1        finobt=0, sparse=0data     =                       bsize=4096   blocks=2621440, imaxpct=25         =                       sunit=1024   swidth=1024 blksnaming   =version 2              bsize=4096   ascii-ci=0 ftype=1<span class="hljs-built_in">log</span>      =internal <span class="hljs-built_in">log</span>           bsize=4096   blocks=2560, version=2         =                       sectsz=512   sunit=8 blks, lazy-count=1realtime =none                   extsz=4096   blocks=0, rtextents=0➜  ~ mkdir test1➜  ~ mount /dev/rbd0 test1<span class="hljs-comment"># 写入测试</span>➜  ~ dd <span class="hljs-keyword">if</span>=/dev/zero of=test1/test-file bs=1G count=1记录了1+0 的读入记录了1+0 的写出1073741824字节(1.1 GB)已复制，16.3689 秒，65.6 MB/秒➜  ~ ls test1test-file</code></pre></div><h4 id="2-4、CephFS-测试"><a href="#2-4、CephFS-测试" class="headerlink" title="2.4、CephFS 测试"></a>2.4、CephFS 测试</h4><p>在测试 CephFS 之前需要先创建两个存储池和一个 fs，创建存储池要指定 PG 数量</p><div class="hljs code-wrapper"><pre><code class="hljs sh">ceph osd pool create cephfs_data 32ceph osd pool create cephfs_metadata 32ceph fs new testfs cephfs_metadata cephfs_data</code></pre></div><p><strong>PG 概念:</strong></p><blockquote><p>当 Ceph 集群接收到存储请求时，Ceph 会将其分散到各个 PG 中，PG 是一组对象的逻辑集合；根据 Ceph 存储池的复制级别，每个 PG的数据会被复制并分发到集群的多个 OSD 上；一般来说增加 PG 数量能降低 OSD 负载，一般每个 OSD 大约分配 50 ~ 100 PG，<strong>关于 PG 数量一般遵循以下公式</strong></p></blockquote><ul><li>集群 PG 总数 = (OSD 总数 * 100) / 数据最大副本数</li><li>单个存储池 PG 数 = (OSD 总数 * 100) / 数据最大副本数 /存储池数</li></ul><p><strong>注意，PG 最终结果应当为最接近以上计算公式的 2 的 N 次幂(向上取值)；如我的虚拟机环境每个存储池 PG 数 = <code>3(OSD) * 100 / 3(副本) / 5(大约 5 个存储池) = 20</code>，向上取 2 的 N 次幂 为 32</strong></p><p>挂载 CephFS 一般有两种方式，一种是使用内核驱动挂载，一种是使用 <code>ceph-fuse</code> 用户空间挂载；内核方式挂载需要提取 Ceph 管理 key，如下</p><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment"># key 在 ceph.conf 中</span><span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;AQB37CZZblBkDRAAUrIrRGsHj/NqdKmVlMQ7ww==&quot;</span> &gt; ceph-key<span class="hljs-comment"># 创建目录挂载</span>mkdir test2mount -t ceph 192.168.1.11:6789:/ /root/test2 -o name=admin,secretfile=ceph-key<span class="hljs-comment"># 写入测试</span>➜  ~ dd <span class="hljs-keyword">if</span>=/dev/zero of=test2/testFs bs=1G count=1记录了1+0 的读入记录了1+0 的写出1073741824字节(1.1 GB)已复制，6.83251 秒，157 MB/秒</code></pre></div><p>使用 ceph-fuse 用户空间挂载方式比较简单，但需要先安装 <code>ceph-fuse</code> 软件包</p><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment"># 安装 ceph-fuse</span>yum install -y ceph-fuse<span class="hljs-comment"># 挂载</span>mkdir test3ceph-fuse -m 192.168.1.11:6789 test3<span class="hljs-comment"># 写入测试</span>➜  ~ dd <span class="hljs-keyword">if</span>=/dev/zero of=test3/testFs bs=1G count=1记录了1+0 的读入记录了1+0 的写出1073741824字节(1.1 GB)已复制，8.18417 秒，131 MB/秒</code></pre></div><h4 id="2-5、对象网关测试"><a href="#2-5、对象网关测试" class="headerlink" title="2.5、对象网关测试"></a>2.5、对象网关测试</h4><p>对象网关在 <strong>1.5、其他组件创建</strong> 部分已经做了创建(RGW)，此时直接访问 <code>http://ceph-node-ip:7480</code> 返回如下</p><div class="hljs code-wrapper"><pre><code class="hljs xml"><span class="hljs-tag">&lt;<span class="hljs-name">ListAllMyBucketsResult</span> <span class="hljs-attr">xmlns</span>=<span class="hljs-string">&quot;http://s3.amazonaws.com/doc/2006-03-01/&quot;</span>&gt;</span>    <span class="hljs-tag">&lt;<span class="hljs-name">Owner</span>&gt;</span>        <span class="hljs-tag">&lt;<span class="hljs-name">ID</span>&gt;</span>anonymous<span class="hljs-tag">&lt;/<span class="hljs-name">ID</span>&gt;</span>        <span class="hljs-tag">&lt;<span class="hljs-name">DisplayName</span>/&gt;</span>    <span class="hljs-tag">&lt;/<span class="hljs-name">Owner</span>&gt;</span>    <span class="hljs-tag">&lt;<span class="hljs-name">Buckets</span>/&gt;</span><span class="hljs-tag">&lt;/<span class="hljs-name">ListAllMyBucketsResult</span>&gt;</span></code></pre></div><p>这就说明网关已经 ok，由于手里没有能读写测试工具，这里暂不做过多说明</p><p><strong>本文主要参考 <a href="http://docs.ceph.com/docs/master/start/">Ceph 官方文档 Quick Start</a> 部分，如有其它未说明到的细节可从官方文档获取</strong></p>]]></content>
    
    
    <summary type="html">Ceph 是一个符合POSIX、开源的分布式存储系统；其具备了极好的可靠性、统一性、鲁棒性；经过近几年的发展，Ceph 开辟了一个全新的数据存储途径。Ceph 具备了企业级存储的分布式、可大规模扩展、没有单点故障等特点，越来越受到人们青睐；以下记录了 Ceph 的相关学习笔记。</summary>
    
    
    
    <category term="Linux" scheme="https://mritd.com/categories/linux/"/>
    
    
    <category term="Linux" scheme="https://mritd.com/tags/linux/"/>
    
    <category term="Ceph" scheme="https://mritd.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title>Docker 分配宿主机网段 IP</title>
    <link href="https://mritd.com/2017/05/12/docker-uses-the-host-network-segment-ip/"/>
    <id>https://mritd.com/2017/05/12/docker-uses-the-host-network-segment-ip/</id>
    <published>2017-05-12T14:42:00.000Z</published>
    <updated>2017-05-12T14:42:00.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>工作需要临时启动一个 gitlab,无奈 gitlab 需要 ssh 的 22 端口;而使用传统网桥方式映射端口则 clone 等都需要输入端口号,很麻烦;22 端口宿主机又有 sshd 监听;研究了下 docker 网络,记录一下如何分配宿主机网段 IP</p></blockquote><h3 id="创建-macvlan-网络"><a href="#创建-macvlan-网络" class="headerlink" title="创建 macvlan 网络"></a>创建 macvlan 网络</h3><p>关于 Docker 网络模式这里不再细说;由于默认的网桥方式无法满足需要,所以需要创建一个 macvlan 网络</p><div class="hljs code-wrapper"><pre><code class="hljs sh">docker network create -d macvlan  --subnet=172.16.0.0/19 --gateway=172.16.0.1 -o parent=eth0 gitlab-net</code></pre></div><ul><li><code>--subnet</code>: 指定网段(宿主机)</li><li><code>--gateway</code>: 指定网关(宿主机)</li><li><code>parent</code>: 注定父网卡(宿主机)</li></ul><p>创建以后可以使用 <code>docker network ls</code> 查看</p><div class="hljs code-wrapper"><pre><code class="hljs sh">➜  ~  docker network lsNETWORK ID          NAME                    DRIVER              SCOPEa4a2980c9165        agent_default           bridge              <span class="hljs-built_in">local</span>               a0f29102b413        bridge                  bridge              <span class="hljs-built_in">local</span>               2f46dc70b763        gitlab-net              macvlan             <span class="hljs-built_in">local</span>               51bd6222530f        host                    host                <span class="hljs-built_in">local</span>               7a14a09c3cfc        none                    null                <span class="hljs-built_in">local</span></code></pre></div><h3 id="创建使用容器"><a href="#创建使用容器" class="headerlink" title="创建使用容器"></a>创建使用容器</h3><p>接下来创建容器指定网络即可</p><div class="hljs code-wrapper"><pre><code class="hljs sh">docker run --net=gitlab-net --ip=172.16.0.170  -dt --name <span class="hljs-built_in">test</span> centos:7</code></pre></div><p><strong><code>--net</code> 指定使用的网络,<code>--ip</code> 用于指定网段内 IP</strong>;启动后只需要在容器内启动程序测试即可</p><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment"># 启动一个 nginx</span>yum install nginxnginx</code></pre></div><p>启动后在局域网内能直接通过 IP:80 访问,而且宿主机 80 不受影响</p><h3 id="docker-compose-测试"><a href="#docker-compose-测试" class="headerlink" title="docker-compose 测试"></a>docker-compose 测试</h3><p>docker-compose 示例如下</p><div class="hljs code-wrapper"><pre><code class="hljs sh">version: <span class="hljs-string">&#x27;2&#x27;</span>services:  centos:    image: centos:7    restart: always    <span class="hljs-built_in">command</span>: /bin/bash -c <span class="hljs-string">&quot;sleep 999999&quot;</span>    networks:      app_net:        ipv4_address: 10.10.1.34networks:  app_net:    driver: macvlan    driver_opts:      parent: enp3s0    ipam:      config:      - subnet: 10.10.1.0/24        gateway: 10.10.1.2<span class="hljs-comment">#        ip_range: 10.25.87.32/28</span></code></pre></div>]]></content>
    
    
    <summary type="html">工作需要临时启动一个 gitlab,无奈 gitlab 需要 ssh 的 22 端口;而使用传统网桥方式映射端口则 clone 等都需要输入端口号,很麻烦;22 端口宿主机又有 sshd 监听;研究了下 docker 网络,记录一下如何分配宿主机网段 IP</summary>
    
    
    
    <category term="Docker" scheme="https://mritd.com/categories/docker/"/>
    
    
    <category term="Linux" scheme="https://mritd.com/tags/linux/"/>
    
    <category term="Docker" scheme="https://mritd.com/tags/docker/"/>
    
  </entry>
  
  <entry>
    <title>Java 配合 mitmproxy HTTPS 抓包调试</title>
    <link href="https://mritd.com/2017/03/25/java-capturing-https-packets-use-mitmproxy/"/>
    <id>https://mritd.com/2017/03/25/java-capturing-https-packets-use-mitmproxy/</id>
    <published>2017-03-25T04:42:21.000Z</published>
    <updated>2017-03-25T04:42:21.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>今天对接接口，对方给的 Demo 和已有项目用的 HTTP 工具不是一个；后来出现人家的好使，我的死活不通的情况；无奈之下开始研究 Java 抓包，所以怕忘了记录一下……</p></blockquote><h3 id="一、mitmproxy-简介"><a href="#一、mitmproxy-简介" class="headerlink" title="一、mitmproxy 简介"></a>一、mitmproxy 简介</h3><p>mitmproxy 是一个命令行下的强大抓包工具，可以在命令行下抓取 HTTP(S) 数据包并加以分析；对于 HTTPS 抓包，首先要在本地添加 mitmproxy 的根证书，然后 mitmproxy 通过以下方式进行抓包：</p><p><img src="https://cdn.oss.link/markdown/x7lir.jpg" alt="mitmproxy1"></p><ul><li>1、客户端发起一个到 mitmproxy 的连接，并且发出HTTP CONNECT 请求</li><li>2、mitmproxy作出响应(200)，模拟已经建立了CONNECT通信管道</li><li>3、客户端确信它正在和远端服务器会话，然后启动SSL连接。在SSL连接中指明了它正在连接的主机名(SNI)</li><li>4、mitmproxy连接服务器，然后使用客户端发出的SNI指示的主机名建立SSL连接</li><li>5、服务器以匹配的SSL证书作出响应，这个SSL证书里包含生成的拦截证书所必须的通用名(CN)和服务器备用名(SAN)</li><li>6、mitmproxy生成拦截证书，然后继续进行与第３步暂停的客户端SSL握手</li><li>7、客户端通过已经建立的SSL连接发送请求，</li><li>8、mitmproxy通过第４步建立的SSL连接传递这个请求给服务器</li></ul><h3 id="二、抓包配置"><a href="#二、抓包配置" class="headerlink" title="二、抓包配置"></a>二、抓包配置</h3><h4 id="2-1、安装-mitmproxy"><a href="#2-1、安装-mitmproxy" class="headerlink" title="2.1、安装 mitmproxy"></a>2.1、安装 mitmproxy</h4><p>mitmproxy 是由  python 编写的，所以直接通过 pip 即可安装，mac 下也可使用 brew 工具安装</p><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment"># mac</span>brew install mitmproxy<span class="hljs-comment"># Linux</span>pip install mitmproxy<span class="hljs-comment"># CentOS 安装时可能会出现 &quot;致命错误：libxml/xmlversion.h：没有那个文件或目录&quot;</span><span class="hljs-comment"># 需要安装如下软件包即可解决</span>yum install libxml2 libxml2-devel libxslt libxslt-devel -y</code></pre></div><h4 id="2-2、HTTPS-证书配置"><a href="#2-2、HTTPS-证书配置" class="headerlink" title="2.2、HTTPS 证书配置"></a>2.2、HTTPS 证书配置</h4><p>首先由于 HTTPS 的安全性，直接抓包是什么也看不到的；所以需要先在本地配置 mitmproxy 的根证书，使其能够解密 HTTPS 流量完成一个中间人的角色；证书下载方式需要先在本地启动 mitmproxy，然后通过设置本地连接代理到 mitmproxy 端口，访问 <code>mitm.it</code> 即可，具体可查看 <a href="https://mitmproxy.org/doc/certinstall.html">官方文档</a></p><p><strong>首先启动 mitmproxy</strong></p><div class="hljs code-wrapper"><pre><code class="hljs sh">mitmproxy -p 4000 --no-mouse</code></pre></div><p><strong>浏览器通过设置代理访问 mitm.it</strong></p><p><img src="https://cdn.oss.link/markdown/unrnc.jpg" alt="access"></p><p>选择对应平台并将其证书加入到系统信任根证书列表即可；对于 Java 程序来说可能有时候并不会生效，所以必须 <strong>修改 keystore</strong>，修改如下</p><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment"># Linux 一般在 JAVA_HOME/jre/lib/security/cacerts 下</span><span class="hljs-comment"># Mac 在 /Library/Java/JavaVirtualMachines/JAVA_HOME/Contents/Home/jre/lib/security/cacerts</span>sudo keytool -importcert -<span class="hljs-built_in">alias</span> mitmproxy -keystore /Library/Java/JavaVirtualMachines/jdk1.8.0_77.jdk/Contents/Home/jre/lib/security/cacerts -storepass changeit -trustcacerts -file ~/.mitmproxy/mitmproxy-ca-cert.pem</code></pre></div><h4 id="2-4、Java-抓包调试"><a href="#2-4、Java-抓包调试" class="headerlink" title="2.4、Java 抓包调试"></a>2.4、Java 抓包调试</h4><p>JVM 本身在启动时就可以设置代理参数，也可以通过代码层设置；以下为代码层设置代理方式</p><div class="hljs code-wrapper"><pre><code class="hljs sh">public void <span class="hljs-function"><span class="hljs-title">beforeTest</span></span>()&#123;    logger.info(<span class="hljs-string">&quot;设置抓包代理......&quot;</span>);    System.setProperty(<span class="hljs-string">&quot;https.proxyHost&quot;</span>, <span class="hljs-string">&quot;127.0.0.1&quot;</span>);    System.setProperty(<span class="hljs-string">&quot;https.proxyPort&quot;</span>, <span class="hljs-string">&quot;4000&quot;</span>);&#125;</code></pre></div><p><strong>然后保证在发送 HTTPS 请求之前此代码执行即可，以下为抓包示例</strong></p><p><img src="https://cdn.oss.link/markdown/kuzhd.jpg" alt="zhuabao"></p><p>通过方向键+回车即可选择某个请求查看报文信息</p><p><img src="https://cdn.oss.link/markdown/vfifu.jpg" alt="detail"></p><h3 id="三、Java-其他代理设置"><a href="#三、Java-其他代理设置" class="headerlink" title="三、Java 其他代理设置"></a>三、Java 其他代理设置</h3><p>Java 代理一般可以通过 2 种方式设置，一种是通过代码层，如下</p><div class="hljs code-wrapper"><pre><code class="hljs sh">// HTTP 代理，只能代理 HTTP 请求System.setProperty(<span class="hljs-string">&quot;http.proxyHost&quot;</span>, <span class="hljs-string">&quot;127.0.0.1&quot;</span>);System.setProperty(<span class="hljs-string">&quot;http.proxyPort&quot;</span>, <span class="hljs-string">&quot;9876&quot;</span>); // HTTPS 代理，只能代理 HTTPS 请求System.setProperty(<span class="hljs-string">&quot;https.proxyHost&quot;</span>, <span class="hljs-string">&quot;127.0.0.1&quot;</span>);System.setProperty(<span class="hljs-string">&quot;https.proxyPort&quot;</span>, <span class="hljs-string">&quot;9876&quot;</span>);// 同时支持代理 HTTP/HTTPS 请求System.setProperty(<span class="hljs-string">&quot;proxyHost&quot;</span>, <span class="hljs-string">&quot;127.0.0.1&quot;</span>);System.setProperty(<span class="hljs-string">&quot;proxyPort&quot;</span>, <span class="hljs-string">&quot;9876&quot;</span>); // SOCKS 代理，支持 HTTP 和 HTTPS 请求// 注意：如果设置了 SOCKS 代理就不要设 HTTP/HTTPS 代理System.setProperty(<span class="hljs-string">&quot;socksProxyHost&quot;</span>, <span class="hljs-string">&quot;127.0.0.1&quot;</span>);System.setProperty(<span class="hljs-string">&quot;socksProxyPort&quot;</span>, <span class="hljs-string">&quot;1080&quot;</span>);</code></pre></div><p>另一种还可以通过 JVM 启动参数设置</p><div class="hljs code-wrapper"><pre><code class="hljs sh">-DproxyHost=127.0.0.1 -DproxyPort=9876</code></pre></div><p>本文参考：</p><ul><li><a href="http://www.aneasystone.com/archives/2015/12/java-and-http-using-proxy.html">Java 和 HTTP 的那些事</a></li><li><a href="http://blog.csdn.net/qq_30513483/article/details/53258637">一步一步教你https抓包</a></li></ul>]]></content>
    
    
    <summary type="html">今天对接接口，对方给的 Demo 和已有项目用的 HTTP 工具不是一个；后来出现人家的好使，我的死活不通的情况；无奈之下开始研究 Java 抓包，所以怕忘了记录一下......</summary>
    
    
    
    <category term="Linux" scheme="https://mritd.com/categories/linux/"/>
    
    <category term="Java" scheme="https://mritd.com/categories/linux/java/"/>
    
    
    <category term="Java" scheme="https://mritd.com/tags/java/"/>
    
  </entry>
  
  <entry>
    <title>vim E492 Not an editor command Plugin xxxx</title>
    <link href="https://mritd.com/2017/03/21/vim-e492-not-an-editor-command-plugin-xxxx/"/>
    <id>https://mritd.com/2017/03/21/vim-e492-not-an-editor-command-plugin-xxxx/</id>
    <published>2017-03-21T12:26:37.000Z</published>
    <updated>2017-03-21T12:26:37.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>最近自用的 vim 装了不少插件，但是发现 <code>kubectl edit</code> 或者 <code>git merge</code> 时，调用 vim 总是会弹出各种错误，记录一下解决方法</p></blockquote><p><strong>出现这个错误一开始以为是 vim 没走 <code>.vimrc</code> 配置；后来翻了一堆资料，发现 <code>kubectl edit</code> 或者 <code>git merge</code> 后并非直接调用 vim，而是调用的 <code>/usr/bin/view</code>，那么看一下这个文件</strong></p><p><img src="https://cdn.oss.link/markdown/9c646.png" alt="view"></p><p><strong>这东西就是链接到了 vi，只要把它链接到 vim 就完了</strong></p><p><img src="https://cdn.oss.link/markdown/f0c4e.png" alt="relink view"></p>]]></content>
    
    
    <summary type="html">vim E492 Not an editor command Plugin xxxx</summary>
    
    
    
    <category term="Linux" scheme="https://mritd.com/categories/linux/"/>
    
    
    <category term="Linux" scheme="https://mritd.com/tags/linux/"/>
    
  </entry>
  
  <entry>
    <title>Btrfs 笔记</title>
    <link href="https://mritd.com/2017/03/20/btrfs-note/"/>
    <id>https://mritd.com/2017/03/20/btrfs-note/</id>
    <published>2017-03-20T13:27:19.000Z</published>
    <updated>2017-03-20T13:27:19.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>btrfs 是 Oracle 07 年基于 GPL 协议开源的 Linux 文件系统，其目的是替换传统的 Ext3、Ext4 系列文件系统；Ext 系列文件系统存在着诸多问题，比如反删除能力有限等；而 btrfs 在解决问题同时提供了更加强大的高级特性</p></blockquote><h3 id="一、Btrfs-特性"><a href="#一、Btrfs-特性" class="headerlink" title="一、Btrfs 特性"></a>一、Btrfs 特性</h3><p>btrfs 在文件系统级别支持写时复制(cow)机制，并且支持快照(增量快照)、支持对单个文件快照；同时支持单个超大文件、文件检查、内建 RAID；支持 B 树子卷(组合多个物理卷，多卷支持)等，具体如下</p><p><strong>btrfs 核心特性：</strong></p><ul><li>多物理卷支持：btrfs 可有多个物理卷组成(类似 LVM)；支持 RAID 以及联机 添加、删除、修改</li><li>写时复制更新机制(cow)：复制、更新、替换指针，而非传统意义上的覆盖</li><li>支持数据及元数据校验码：checksum 机制</li><li>支持创建子卷：sub_volume 机制，同时可多层创建</li><li>支持快照：基于 cow 实现快照，并且相对于 LVM 可以实快照的快照(增量快照)</li><li>支持透明压缩：后台自动压缩文件(消耗一定 CPU)，对前端程序透明</li></ul><h3 id="二、btrfs-常用命令"><a href="#二、btrfs-常用命令" class="headerlink" title="二、btrfs 常用命令"></a>二、btrfs 常用命令</h3><h4 id="2-1、创建文件系统"><a href="#2-1、创建文件系统" class="headerlink" title="2.1、创建文件系统"></a>2.1、创建文件系统</h4><p>同传统的 ext 系列文件系统一样，btrfs 文件系统格式化同样采用 <code>mkfs</code> 系列命令 <code>mkfs.btrfs</code>，其常用选项如下：</p><ul><li><code>-L</code> 指定卷标</li><li><code>-m</code> 指明元数据存放机制(RAID)</li><li><code>-d</code> 指明数据存放机制(RAID)</li><li><code>-O</code> 格式化时指定文件系统开启那些特性(不一定所有内核支持)，如果需要查看支持那些特性可使用 <code>mkfs.btrfs -O list-all</code></li></ul><h4 id="2-2、挂载-btrfs"><a href="#2-2、挂载-btrfs" class="headerlink" title="2.2、挂载 btrfs"></a>2.2、挂载 btrfs</h4><p>同 ext 系列一样，仍然使用 <code>mount</code> 命令，基本挂载如下：</p><div class="hljs code-wrapper"><pre><code class="hljs sh">mount -t brtfs DEVICE MOUNT_POINT</code></pre></div><p>在挂载时也可以直接开启文件系统一些特性，如<strong>透明压缩</strong></p><div class="hljs code-wrapper"><pre><code class="hljs sh">mount -t btrfs -o compress=&#123;lzo|zlib&#125; DEVICE MOUNT_POINT</code></pre></div><p>同时 btrfs 支持子卷，也可以单独挂载子卷</p><div class="hljs code-wrapper"><pre><code class="hljs sh">mount -t btrfs -o subvol=SUBVOL_NAME DEVICE</code></pre></div><h4 id="2-3、btrfs-相关命令"><a href="#2-3、btrfs-相关命令" class="headerlink" title="2.3、btrfs 相关命令"></a>2.3、btrfs 相关命令</h4><p>管理 btrfs 使用 <code>btrfs</code> 命令，该命令包含诸多子命令已完成不同的功能管理，常用命令如下</p><ul><li><strong>btrfs 文件系统属性查看：</strong> <code>btrfs filesystem show</code></li><li><strong>调整文件系统大小：</strong> <code>btrfs filesystem resize +10g MOUNT_POINT</code></li><li><strong>添加硬件设备：</strong> <code>btrfs filesystem add DEVICE MOUNT_POINT</code></li><li><strong>均衡文件负载：</strong> <code>btrfs blance status|start|pause|resume|cancel MOUNT_POINT</code></li><li><strong>移除物理卷(联机、自动移动)：</strong> <code>btrfs device delete DEVICE MOUNT_POINT</code></li><li><strong>动态调整数据存放机制：</strong> <code>btrfs balance start -dconvert=RAID MOUNT_POINT</code></li><li><strong>动态调整元数据存放机制：</strong> <code>btrfs balance start -mconvert=RAID MOUNT_POINT</code></li><li><strong>动态调整文件系统数据数据存放机制：</strong> <code>btrfs balance start -sconvert=RAID MOUNT_POINT</code></li><li><strong>创建子卷：</strong> <code>btrfs subvolume create MOUNT_POINT/DIR</code></li><li><strong>列出所有子卷：</strong> <code>btrfs subvolume list MOUNT_POINT</code></li><li><strong>显示子卷详细信息：</strong> <code>btrfs subvolume show MOUNT_POINT</code></li><li><strong>删除子卷：</strong> <code>btrfs subvolume delete MOUNT_POIN/DIR</code></li><li><strong>创建子卷快照(子卷快照必须存放与当前子卷的同一父卷中)：</strong> <code>btrfs subvolume snapshot SUBVOL PARVOL</code></li><li><strong>删除快照同删除子卷一样：</strong> <code>btrfs subvolume delete MOUNT_POIN/DIR</code></li></ul>]]></content>
    
    
    <summary type="html">Btrfs 笔记</summary>
    
    
    
    <category term="Linux" scheme="https://mritd.com/categories/linux/"/>
    
    
    <category term="Linux" scheme="https://mritd.com/tags/linux/"/>
    
  </entry>
  
  <entry>
    <title>CentOS 7 配置 VNC Server</title>
    <link href="https://mritd.com/2017/03/18/set-up-vnc-server-on-centos-7/"/>
    <id>https://mritd.com/2017/03/18/set-up-vnc-server-on-centos-7/</id>
    <published>2017-03-18T13:53:07.000Z</published>
    <updated>2017-03-18T13:53:07.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>最近决定把小主机扔到客厅跟路由器放在一起(远程开机 666），因为本来就跑的是 Linux，平时图形化需求也不多；但是为了保险起见准备搞一个 VNC，以便必要时图形化上去，比如强制删除一些 Virtual Box 虚拟机等，记录一下安装过程</p></blockquote><h4 id="安装-VNC-Server"><a href="#安装-VNC-Server" class="headerlink" title="安装 VNC Server"></a>安装 VNC Server</h4><p>VNC Server 软件有很多，这里使用 <code>tigervnc-server</code></p><div class="hljs code-wrapper"><pre><code class="hljs sh">yum install epel-release -yyum makecacheyum install tigervnc-server -y</code></pre></div><h4 id="开机自启动"><a href="#开机自启动" class="headerlink" title="开机自启动"></a>开机自启动</h4><p>这地方踩了很多坑，网上不少帖子都是写的先测试 VNC Server，执行 <code>vncserver</code> 命令，然后云云；查设置开机自启动也是五花八门，大部分人的路子就是自己写一个 init 脚本，让系统开机时候执行它…… 从职业踩坑经验来看，这东西绝对有更有逼格的 Systemd 的打开方式；果不其然翻了一下 rpm 包 发现了一个 Systemd 模板 Service</p><div class="hljs code-wrapper"><pre><code class="hljs sh">[root@mritd ~]<span class="hljs-comment"># rpm -ql  tigervnc-server</span>/etc/sysconfig/vncservers/usr/bin/vncserver/usr/bin/x0vncserver/usr/lib/systemd/system/vncserver@.service/usr/share/man/man1/vncserver.1.gz/usr/share/man/man1/x0vncserver.1.gz</code></pre></div><p>由于好奇心作祟，先 <code>systemctl enable vncserver@:1.service</code> 了一下，后来发现起不来，所以 vim 看了一下原模板 Service，里面想写描述了如何设置开机启动</p><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment"># Quick HowTo:</span><span class="hljs-comment"># 1. Copy this file to /etc/systemd/system/vncserver@.service</span><span class="hljs-comment"># 2. Edit /etc/systemd/system/vncserver@.service, replacing &lt;USER&gt;</span><span class="hljs-comment">#    with the actual user name. Leave the remaining lines of the file unmodified</span><span class="hljs-comment">#    (ExecStart=/usr/sbin/runuser -l &lt;USER&gt; -c &quot;/usr/bin/vncserver %i&quot;</span><span class="hljs-comment">#     PIDFile=/home/&lt;USER&gt;/.vnc/%H%i.pid)</span><span class="hljs-comment"># 3. Run `systemctl daemon-reload`</span><span class="hljs-comment"># 4. Run `systemctl enable vncserver@:&lt;display&gt;.service`</span></code></pre></div><p>也就是说把这个模板文件 cp 到 <code>/etc/systemd/system/vncserver@.service</code> 然后替换用户名，执行两条命令，最后执行 <code>vncserver</code> 用于初始化密码和配置文件就行了</p><p>重装了一天系统，有点烦躁，听首歌安静一下…</p><div style="position:relative;height:0;padding-bottom:56.25%"><iframe src="https://www.youtube.com/embed/AsC0CN2eGkY?rel=0?ecver=2" width="640" height="360" frameborder="0" style="position:absolute;width:100%;height:100%;left:0" allowfullscreen></iframe></div>]]></content>
    
    
    <summary type="html">最近决定把小主机扔到客厅跟路由器放在一起(远程开机 666），因为本来就跑的是 Linux，平时图形化需求也不多；但是为了保险起见准备搞一个 VNC，以便必要时图形化上去，比如强制删除一些 Virtual Box 虚拟机等，记录一下安装过程</summary>
    
    
    
    <category term="Linux" scheme="https://mritd.com/categories/linux/"/>
    
    
    <category term="Linux" scheme="https://mritd.com/tags/linux/"/>
    
  </entry>
  
  <entry>
    <title>kargo 集群扩展及细粒度配置</title>
    <link href="https://mritd.com/2017/03/10/kargo-cluster-expansion-and-fine-grained-configuration/"/>
    <id>https://mritd.com/2017/03/10/kargo-cluster-expansion-and-fine-grained-configuration/</id>
    <published>2017-03-10T15:57:12.000Z</published>
    <updated>2017-03-10T15:57:12.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>上一篇写了一下一下使用 kargo 快速部署 Kubernetes 高可用集群，但是一些细节部分不算完善，这里准备补一下，详细说明一下一些问题；比如后期如何扩展、一些配置如何自定义等</p></blockquote><h3 id="一、集群扩展"><a href="#一、集群扩展" class="headerlink" title="一、集群扩展"></a>一、集群扩展</h3><p>如果已经有了一个 kargo 搭建的集群，那么扩展其极其容易；只需要修改集群 <code>inventory</code> 配置，加入新节点重新运行命令价格参数即可，如下新增一个 node6 节点</p><div class="hljs code-wrapper"><pre><code class="hljs sh">vim inventory/inventory.cfg<span class="hljs-comment"># 在 Kubernetes node 组中加入新的 node6 节点</span>[all]node1    ansible_host=192.168.1.11 ip=192.168.1.11node2    ansible_host=192.168.1.12 ip=192.168.1.12node3    ansible_host=192.168.1.13 ip=192.168.1.13node4    ansible_host=192.168.1.14 ip=192.168.1.14node5    ansible_host=192.168.1.15 ip=192.168.1.15node6    ansible_host=192.168.1.16 ip=192.168.1.16[kube-master]node1node2node3node5[kube-node]node1node2node3node4node5node6[etcd]node1node2node3[k8s-cluster:children]kube-nodekube-master[calico-rr]</code></pre></div><p><strong>然后重新运行集群命令，注意增加 <code>--limit</code> 参数</strong></p><div class="hljs code-wrapper"><pre><code class="hljs sh">ansible-playbook -i inventory/inventory.cfg cluster.yml -b -v --private-key=~/.ssh/id_rsa --<span class="hljs-built_in">limit</span> node6</code></pre></div><p><strong>稍等片刻 node6 节点便加入现有集群，如果有多个节点加入，只需要以逗号分隔即可，如 <code>--limit node5,node6</code>；在此过程中只会操作新增的 node 节点，不会影响现有集群，可以实现动态集群扩容(master 也可以扩展)</strong></p><h3 id="二、kargo-细粒度控制"><a href="#二、kargo-细粒度控制" class="headerlink" title="二、kargo 细粒度控制"></a>二、kargo 细粒度控制</h3><p>对于 kargo 高度自动化的工具，可能有些东西我们已经预先处理好了，<strong>比如事先已经安装了 docker，而且 docker 配置了一些参数(日志驱动、存储驱动等)；这时候我们可能并不希望 kargo 再去处理，因为 kargo 会进行覆盖，可能导致一些问题</strong></p><p><strong>kargo 是基于 ansible 的，实际上也就是 ansible，只不过它帮我们写好了配置文件而已；按照 ansible 的规则，Play Book 首先执行 roles 目录下的 roles，在这些 roles 中定义了如何配置集群、如何初始化网络、怎么配置 docker 等等，所以只要我们去更改这些 roles 规则就可以实现一些功能的定制，roles 目录位置如下</strong></p><p><img src="https://cdn.oss.link/markdown/jmy7o.jpg" alt="roles"></p><p>如果需要更改某些默认配置，那么只需要更改对应目录下的 role 即可，<strong>每个 role 子目录都是一个组件的配置过程(动作)，动作实际上就是不同的 task，所有的 task 定义在 <code>tasks/main.yml</code> 中，如果我们注释(删掉)了相关 task，那么也就关闭了 kargo 对应的处理；如下禁用了 kargo 安装 docker，但是允许 kargo 覆盖 docker service 文件</strong></p><p><img src="https://cdn.oss.link/markdown/vv2px.jpg" alt="docker task"></p><p><strong>禁用掉 docker 仓库以及 docker 的安装动作</strong></p><div class="hljs code-wrapper"><pre><code class="hljs sh">vim roles/docker/tasks/main.yml---- name: gather os specific variables  include_vars: <span class="hljs-string">&quot;&#123; &#123; item &#125; &#125;&quot;</span>  with_first_found:    - files:      - <span class="hljs-string">&quot;&#123; &#123; ansible_distribution|lower &#125; &#125;-&#123; &#123; ansible_distribution_version|lower|replace(&#x27;/&#x27;, &#x27;_&#x27;) &#125; &#125;.yml&quot;</span>      - <span class="hljs-string">&quot;&#123; &#123; ansible_distribution|lower &#125; &#125;-&#123; &#123; ansible_distribution_release &#125; &#125;.yml&quot;</span>      - <span class="hljs-string">&quot;&#123; &#123; ansible_distribution|lower &#125; &#125;-&#123; &#123; ansible_distribution_major_version|lower|replace(&#x27;/&#x27;, &#x27;_&#x27;) &#125; &#125;.yml&quot;</span>      - <span class="hljs-string">&quot;&#123; &#123; ansible_distribution|lower &#125; &#125;.yml&quot;</span>      - <span class="hljs-string">&quot;&#123; &#123; ansible_os_family|lower &#125; &#125;.yml&quot;</span>      - defaults.yml      paths:      - ../vars      skip: <span class="hljs-literal">true</span>  tags: facts- include: set_facts_dns.yml  when: dns_mode != <span class="hljs-string">&#x27;none&#x27;</span> and resolvconf_mode == <span class="hljs-string">&#x27;docker_dns&#x27;</span>  tags: facts- name: check <span class="hljs-keyword">for</span> minimum kernel version  fail:    msg: &gt;          docker requires a minimum kernel version of          &#123; &#123; docker_kernel_min_version &#125; &#125; on          &#123; &#123; ansible_distribution &#125; &#125;-&#123; &#123; ansible_distribution_version &#125; &#125;  when: (not ansible_os_family <span class="hljs-keyword">in</span> [<span class="hljs-string">&quot;CoreOS&quot;</span>, <span class="hljs-string">&quot;Container Linux by CoreOS&quot;</span>]) and (ansible_kernel|version_compare(docker_kernel_min_version, <span class="hljs-string">&quot;&lt;&quot;</span>))  tags: facts<span class="hljs-comment"># 禁用 docker 仓库处理，因为默认 kargo 会写入国外 docker 源，我已经自己设置了清华大学的镜像源</span><span class="hljs-comment">#- name: ensure docker repository public key is installed</span><span class="hljs-comment">#  action: &quot;&#123; &#123; docker_repo_key_info.pkg_key &#125; &#125;&quot;</span><span class="hljs-comment">#  args:</span><span class="hljs-comment">#    id: &quot;&#123; &#123;item&#125; &#125;&quot;</span><span class="hljs-comment">#    keyserver: &quot;&#123; &#123;docker_repo_key_info.keyserver&#125; &#125;&quot;</span><span class="hljs-comment">#    state: present</span><span class="hljs-comment">#  register: keyserver_task_result</span><span class="hljs-comment">#  until: keyserver_task_result|success</span><span class="hljs-comment">#  retries: 4</span><span class="hljs-comment">#  delay: &quot;&#123; &#123; retry_stagger | random + 3 &#125; &#125;&quot;</span><span class="hljs-comment">#  with_items: &quot;&#123; &#123; docker_repo_key_info.repo_keys &#125; &#125;&quot;</span><span class="hljs-comment">#  when: not ansible_os_family in [&quot;CoreOS&quot;, &quot;Container Linux by CoreOS&quot;]</span><span class="hljs-comment">#</span><span class="hljs-comment">#- name: ensure docker repository is enabled</span><span class="hljs-comment">#  action: &quot;&#123; &#123; docker_repo_info.pkg_repo &#125; &#125;&quot;</span><span class="hljs-comment">#  args:</span><span class="hljs-comment">#    repo: &quot;&#123; &#123;item&#125; &#125;&quot;</span><span class="hljs-comment">#    state: present</span><span class="hljs-comment">#  with_items: &quot;&#123; &#123; docker_repo_info.repos &#125; &#125;&quot;</span><span class="hljs-comment">#  when: (not ansible_os_family in [&quot;CoreOS&quot;, &quot;Container Linux by CoreOS&quot;]) and (docker_repo_info.repos|length &gt; 0)</span><span class="hljs-comment">#</span><span class="hljs-comment">#- name: Configure docker repository on RedHat/CentOS</span><span class="hljs-comment">#  template:</span><span class="hljs-comment">#    src: &quot;rh_docker.repo.j2&quot;</span><span class="hljs-comment">#    dest: &quot;/etc/yum.repos.d/docker.repo&quot;</span><span class="hljs-comment">#  when: ansible_distribution in [&quot;CentOS&quot;,&quot;RedHat&quot;]</span><span class="hljs-comment">#</span><span class="hljs-comment"># 这步 kargo 会重新安装 docker，已经装好了，所以不需要再覆盖安装</span><span class="hljs-comment">#- name: ensure docker packages are installed</span><span class="hljs-comment">#  action: &quot;&#123; &#123; docker_package_info.pkg_mgr &#125; &#125;&quot;</span><span class="hljs-comment">#  args:</span><span class="hljs-comment">#    pkg: &quot;&#123; &#123;item.name&#125; &#125;&quot;</span><span class="hljs-comment">#    force: &quot;&#123; &#123;item.force|default(omit)&#125; &#125;&quot;</span><span class="hljs-comment">#    state: present</span><span class="hljs-comment">#  register: docker_task_result</span><span class="hljs-comment">#  until: docker_task_result|success</span><span class="hljs-comment">#  retries: 4</span><span class="hljs-comment">#  delay: &quot;&#123; &#123; retry_stagger | random + 3 &#125; &#125;&quot;</span><span class="hljs-comment">#  with_items: &quot;&#123; &#123; docker_package_info.pkgs &#125; &#125;&quot;</span><span class="hljs-comment">#  notify: restart docker</span><span class="hljs-comment">#  when: (not ansible_os_family in [&quot;CoreOS&quot;, &quot;Container Linux by CoreOS&quot;]) and (docker_package_info.pkgs|length &gt; 0)</span><span class="hljs-comment">#</span><span class="hljs-comment"># 对于 docker 版本的检查个人感觉还是有点必要的</span>- name: check minimum docker version <span class="hljs-keyword">for</span> docker_dns mode. You need at least docker version &gt;= 1.12 <span class="hljs-keyword">for</span> resolvconf_mode=docker_dns  <span class="hljs-built_in">command</span>: <span class="hljs-string">&quot;docker version -f &#x27;&#123; &#123; &#x27;&#123; &#123;&#x27; &#125; &#125;.Client.Version&#123; &#123; &#x27;&#125; &#125;&#x27; &#125; &#125;&#x27;&quot;</span>  register: docker_version  failed_when: docker_version.stdout|version_compare(<span class="hljs-string">&#x27;1.12&#x27;</span>, <span class="hljs-string">&#x27;&lt;&#x27;</span>)  changed_when: <span class="hljs-literal">false</span>  when: dns_mode != <span class="hljs-string">&#x27;none&#x27;</span> and resolvconf_mode == <span class="hljs-string">&#x27;docker_dns&#x27;</span><span class="hljs-comment"># kargo 对 docker service 的配置会在此写入，我感觉还不错，所以留着了；但是注意的是它会把原来的覆盖掉</span>- name: Set docker systemd config  include: systemd.yml- name: ensure docker service is started and enabled  service:    name: <span class="hljs-string">&quot;&#123; &#123; item &#125; &#125;&quot;</span>    enabled: yes    state: started  with_items:    - docker</code></pre></div><p><strong>kargo 在进行各种任务(task)时可能会释放一些配置文件，比如 docker service 配置文件、kubernetes 配置文件等；这些文件一般位于 <code>roles/组件/templates</code> 目录，比如 docker 的 service 配置位于如下位置；我们可以更改，甚至直接换一个，把里面写死变成我们自己的</strong></p><p><img src="https://cdn.oss.link/markdown/f1f9g.jpg" alt="docker service template"></p><h3 id="三、其他相关"><a href="#三、其他相关" class="headerlink" title="三、其他相关"></a>三、其他相关</h3><p><strong>以上只是介绍了自定义配置的大体思路，更深度的处理需要去玩转  ansible，如果玩明白了 ansible 那么基本上这个 kargo 就可以随便搞了；要写的差不多也就这么多了，感觉这东西比 kubeadm 要好的多，所有操作都是可视化的，没有莫名其妙的问题；其他的可以参考 <a href="http://ansible-tran.readthedocs.io/en/latest/">ansible 中文文档</a>、<a href="https://github.com/kubernetes-incubator/kargo/blob/master/README.md">kargo 官方文档</a></strong></p>]]></content>
    
    
    <summary type="html">上一篇写了一下一下使用 kargo 快速部署 Kubernetes 高可用集群，但是一些细节部分不算完善，这里准备补一下，详细说明一下一些问题；比如后期如何扩展、一些配置如何自定义等</summary>
    
    
    
    <category term="Kubernetes" scheme="https://mritd.com/categories/kubernetes/"/>
    
    
    <category term="Linux" scheme="https://mritd.com/tags/linux/"/>
    
    <category term="Docker" scheme="https://mritd.com/tags/docker/"/>
    
    <category term="Kubernetes" scheme="https://mritd.com/tags/kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>Kubernetes Nginx Ingress 教程</title>
    <link href="https://mritd.com/2017/03/04/how-to-use-nginx-ingress/"/>
    <id>https://mritd.com/2017/03/04/how-to-use-nginx-ingress/</id>
    <published>2017-03-04T15:16:36.000Z</published>
    <updated>2017-03-04T15:16:36.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>最近发现好多人问 Ingress，同时一直也没去用 Nginx 的 Ingress，索性鼓捣了一把，发现跟原来确实有了点变化，在这里写篇文章记录一下</p></blockquote><h3 id="一、Ingress-介绍"><a href="#一、Ingress-介绍" class="headerlink" title="一、Ingress 介绍"></a>一、Ingress 介绍</h3><p>Kubernetes 暴露服务的方式目前只有三种：LoadBlancer Service、NodePort Service、Ingress；前两种估计都应该很熟悉，具体的可以参考下 <a href="https://mritd.me/2016/12/06/try-traefik-on-kubernetes/">这篇文章</a>；下面详细的唠一下这个 Ingress</p><h4 id="1-1、Ingress-是个什么玩意"><a href="#1-1、Ingress-是个什么玩意" class="headerlink" title="1.1、Ingress 是个什么玩意"></a>1.1、Ingress 是个什么玩意</h4><p>可能从大致印象上 Ingress 就是能利用 Nginx、Haproxy 啥的负载均衡器暴露集群内服务的工具；那么问题来了，集群内服务想要暴露出去面临着几个问题：</p><h4 id="1-2、Pod-漂移问题"><a href="#1-2、Pod-漂移问题" class="headerlink" title="1.2、Pod 漂移问题"></a>1.2、Pod 漂移问题</h4><p>众所周知 Kubernetes 具有强大的副本控制能力，能保证在任意副本(Pod)挂掉时自动从其他机器启动一个新的，还可以动态扩容等，总之一句话，这个 Pod 可能在任何时刻出现在任何节点上，也可能在任何时刻死在任何节点上；那么自然随着 Pod 的创建和销毁，Pod IP 肯定会动态变化；<strong>那么如何把这个动态的 Pod IP 暴露出去？这里借助于 Kubernetes 的 Service 机制，Service 可以以标签的形式选定一组带有指定标签的 Pod，并监控和自动负载他们的 Pod IP，那么我们向外暴露只暴露 Service IP 就行了</strong>；这就是 NodePort 模式：即在每个节点上开起一个端口，然后转发到内部 Pod IP 上，如下图所示</p><p><img src="https://cdn.oss.link/markdown/5a1i4.jpg" alt="NodePort"></p><h4 id="1-3、端口管理问题"><a href="#1-3、端口管理问题" class="headerlink" title="1.3、端口管理问题"></a>1.3、端口管理问题</h4><p>采用 NodePort 方式暴露服务面临一个坑爹的问题是，服务一旦多起来，NodePort 在每个节点上开启的端口会及其庞大，而且难以维护；这时候引出的思考问题是 <strong>“能不能使用 Nginx 啥的只监听一个端口，比如 80，然后按照域名向后转发？”</strong> 这思路很好，简单的实现就是使用 DaemonSet 在每个 node 上监听 80，然后写好规则，<strong>因为 Nginx 外面绑定了宿主机 80 端口(就像 NodePort)，本身又在集群内，那么向后直接转发到相应 Service IP 就行了</strong>，如下图所示</p><p><img src="https://cdn.oss.link/markdown/rrcuu.jpg" alt="use nginx proxy"></p><h4 id="1-4、域名分配及动态更新问题"><a href="#1-4、域名分配及动态更新问题" class="headerlink" title="1.4、域名分配及动态更新问题"></a>1.4、域名分配及动态更新问题</h4><p>从上面的思路，采用 Nginx 似乎已经解决了问题，但是其实这里面有一个很大缺陷：<strong>每次有新服务加入怎么改 Nginx 配置？总不能手动改或者来个 Rolling Update 前端 Nginx Pod 吧？</strong>这时候 “伟大而又正直勇敢的” Ingress 登场，<strong>如果不算上面的 Nginx，Ingress 只有两大组件：Ingress Controller 和 Ingress</strong></p><p>Ingress 这个玩意，简单的理解就是 <strong>你原来要改 Nginx 配置，然后配置各种域名对应哪个 Service，现在把这个动作抽象出来，变成一个 Ingress 对象，你可以用 yml 创建，每次不要去改 Nginx 了，直接改 yml 然后创建/更新就行了</strong>；那么问题来了：”Nginx 咋整？”</p><p>Ingress Controller 这东西就是解决 “Nginx 咋整” 的；<strong>Ingress Controoler 通过与 Kubernetes API 交互，动态的去感知集群中 Ingress 规则变化，然后读取他，按照他自己模板生成一段 Nginx 配置，再写到 Nginx Pod 里，最后 reload 一下</strong>，工作流程如下图</p><p><img src="https://cdn.oss.link/markdown/e5fcy.jpg" alt="Ingress"></p><p><strong>当然在实际应用中，最新版本 Kubernetes 已经将 Nginx 与 Ingress Controller 合并为一个组件，所以 Nginx 无需单独部署，只需要部署 Ingress Controller 即可</strong></p><h3 id="二、怼一个-Nginx-Ingress"><a href="#二、怼一个-Nginx-Ingress" class="headerlink" title="二、怼一个 Nginx Ingress"></a>二、怼一个 Nginx Ingress</h3><p>上面啰嗦了那么多，只是为了讲明白 Ingress 的各种理论概念，下面实际部署很简单</p><h4 id="2-1、部署默认后端"><a href="#2-1、部署默认后端" class="headerlink" title="2.1、部署默认后端"></a>2.1、部署默认后端</h4><p>我们知道 <strong>前端的 Nginx 最终要负载到后端 service 上，那么如果访问不存在的域名咋整？</strong>官方给出的建议是部署一个 <strong>默认后端</strong>，对于未知请求全部负载到这个默认后端上；这个后端啥也不干，就是返回 404，部署如下</p><div class="hljs code-wrapper"><pre><code class="hljs sh">➜  ~ kubectl create -f default-backend.yamldeployment <span class="hljs-string">&quot;default-http-backend&quot;</span> createdservice <span class="hljs-string">&quot;default-http-backend&quot;</span> created</code></pre></div><p>这个 <code>default-backend.yaml</code> 文件可以在 <a href="https://github.com/kubernetes/ingress/blob/master/examples/deployment/nginx/default-backend.yaml">官方 Ingress 仓库</a> 找到，由于篇幅限制这里不贴了，仓库位置如下</p><p><img src="https://cdn.oss.link/markdown/1ct6w.jpg" alt="default-backend"></p><h4 id="2-2、部署-Ingress-Controller"><a href="#2-2、部署-Ingress-Controller" class="headerlink" title="2.2、部署 Ingress Controller"></a>2.2、部署 Ingress Controller</h4><p>部署完了后端就得把最重要的组件 Nginx+Ingres Controller(官方统一称为 Ingress Controller) 部署上</p><div class="hljs code-wrapper"><pre><code class="hljs sh">➜  ~ kubectl create -f nginx-ingress-controller.yamldaemonset <span class="hljs-string">&quot;nginx-ingress-lb&quot;</span> created</code></pre></div><p>**注意：官方的 Ingress Controller 有个坑，至少我看了 DaemonSet 方式部署的有这个问题：没有绑定到宿主机 80 端口，也就是说前端 Nginx 没有监听宿主机 80 端口(这还玩个卵啊)；所以需要把配置搞下来自己加一下 <code>hostNetwork</code>**，截图如下</p><p><img src="https://cdn.oss.link/markdown/n1fsc.jpg" alt="add hostNetwork"></p><p>同样配置文件自己找一下，地址 <a href="https://github.com/kubernetes/ingress/blob/master/examples/daemonset/nginx/nginx-ingress-daemonset.yaml">点这里</a>，仓库截图如下</p><p><img src="https://cdn.oss.link/markdown/jirhn.jpg" alt="Ingress Controller"></p><p><strong>当然它支持以 deamonset 的方式部署，这里用的就是(个人喜欢而已)，所以你发现我上面截图是 deployment，但是链接给的却是 daemonset，因为我截图截错了…..</strong></p><h4 id="2-3、部署-Ingress"><a href="#2-3、部署-Ingress" class="headerlink" title="2.3、部署 Ingress"></a>2.3、部署 Ingress</h4><p><strong>这个可就厉害了，这个部署完就能装逼了</strong></p><p><img src="https://cdn.oss.link/markdown/v450z.jpg" alt="daitouzhaungbi"><br><img src="https://cdn.oss.link/markdown/b1kz2.jpg" alt="zhanxianjishu"></p><p><strong>咳咳，回到正题，从上面可以知道 Ingress 就是个规则，指定哪个域名转发到哪个 Service，所以说首先我们得有个 Service，当然 Service 去哪找这里就不管了；这里默认为已经有了两个可用的 Service，以下以 Dashboard 和 kibana 为例</strong></p><p><strong>先写一个 Ingress 文件，语法格式啥的请参考 <a href="https://kubernetes.io/docs/user-guide/ingress">官方文档</a>，由于我的 Dashboard 和 Kibana 都在 kube-system 这个命名空间，所以要指定 namespace</strong>，写之前 Service 分布如下</p><p><img src="https://cdn.oss.link/markdown/vtg8f.jpg" alt="All Service"></p><div class="hljs code-wrapper"><pre><code class="hljs sh">vim dashboard-kibana-ingress.ymlapiVersion: extensions/v1beta1kind: Ingressmetadata:  name: dashboard-kibana-ingress  namespace: kube-systemspec:  rules:  - host: dashboard.mritd.me    http:      paths:      - backend:          serviceName: kubernetes-dashboard          servicePort: 80  - host: kibana.mritd.me    http:      paths:      - backend:          serviceName: kibana-logging          servicePort: 5601</code></pre></div><p><strong>装逼成功截图如下</strong></p><p><img src="https://cdn.oss.link/markdown/pyhdy.jpg" alt="Dashboard"></p><p><img src="https://cdn.oss.link/markdown/p3qli.jpg" alt="Kibana"></p><h3 id="三、部署-Ingress-TLS"><a href="#三、部署-Ingress-TLS" class="headerlink" title="三、部署 Ingress TLS"></a>三、部署 Ingress TLS</h3><p>上面已经搞定了 Ingress，下面就顺便把 TLS 怼上；官方给出的样例很简单，大致步骤就两步：<strong>创建一个含有证书的 secret、在 Ingress 开启证书</strong>；但是我不得不喷一下，文档就提那么一嘴，大坑一堆，比如多域名配置，还有下面这文档特么的是逗我玩呢？</p><p><img src="https://cdn.oss.link/markdown/t3n1j.jpg" alt="douniwan"></p><h4 id="3-1、创建证书"><a href="#3-1、创建证书" class="headerlink" title="3.1、创建证书"></a>3.1、创建证书</h4><p>首先第一步当然要有个证书，由于我这个 Ingress 有两个服务域名，所以证书要支持两个域名；生成证书命令如下：</p><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment"># 生成 CA 自签证书</span>mkdir cert &amp;&amp; <span class="hljs-built_in">cd</span> certopenssl genrsa -out ca-key.pem 2048openssl req -x509 -new -nodes -key ca-key.pem -days 10000 -out ca.pem -subj <span class="hljs-string">&quot;/CN=kube-ca&quot;</span><span class="hljs-comment"># 编辑 openssl 配置</span>cp /etc/pki/tls/openssl.cnf .vim openssl.cnf<span class="hljs-comment"># 主要修改如下</span>[req]req_extensions = v3_req <span class="hljs-comment"># 这行默认注释关着的 把注释删掉</span><span class="hljs-comment"># 下面配置是新增的</span>[ v3_req ]basicConstraints = CA:FALSEkeyUsage = nonRepudiation, digitalSignature, keyEnciphermentsubjectAltName = @alt_names[alt_names]DNS.1 = dashboard.mritd.meDNS.2 = kibana.mritd.me<span class="hljs-comment"># 生成证书</span>openssl genrsa -out ingress-key.pem 2048openssl req -new -key ingress-key.pem -out ingress.csr -subj <span class="hljs-string">&quot;/CN=kube-ingress&quot;</span> -config openssl.cnfopenssl x509 -req -<span class="hljs-keyword">in</span> ingress.csr -CA ca.pem -CAkey ca-key.pem -CAcreateserial -out ingress.pem -days 365 -extensions v3_req -extfile openssl.cnf</code></pre></div><h4 id="3-2、创建-secret"><a href="#3-2、创建-secret" class="headerlink" title="3.2、创建 secret"></a>3.2、创建 secret</h4><p>创建好证书以后，需要将证书内容放到 secret 中，secret 中全部内容需要 base64 编码，然后注意去掉换行符(变成一行)；以下是我的 secret 样例(上一步中 ingress.pem 是证书，ingress-key.pem 是证书的 key)</p><div class="hljs code-wrapper"><pre><code class="hljs sh">vim ingress-secret.ymlapiVersion: v1data:  tls.crt: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUM5akNDQWQ2Z0F3SUJBZ0lKQU5TR2dNNnYvSVd5TUEwR0NTcUdTSWIzRFFFQkJRVUFNQkl4RURBT0JnTlYKQkFNTUIydDFZbVV0WTJFd0hoY05NVGN3TXpBME1USTBPRFF5V2hjTk1UZ3dNekEwTVRJME9EUXlXakFYTVJVdwpFd1lEVlFRRERBeHJkV0psTFdsdVozSmxjM013Z2dFaU1BMEdDU3FHU0liM0RRRUJBUVVBQTRJQkR3QXdnZ0VLCkFvSUJBUUM2dkNZRFhGSFpQOHI5Zk5jZXlkV015VVlELzAwQ2xnS0M2WjNpYWZ0QlRDK005TmcrQzloUjhJUE4KWW00cjZOMkw1MmNkcmZvQnBHZXovQVRIT0NJYUhJdlp1K1ZaTzNMZjcxZEVLR09nV21LMTliSVAzaGpSeDZhWQpIeGhEVWNab3ZzYWY1UWJHRnUydEF4L2doMTFMdXpTZWJkT0Y1dUMrWHBhTGVzWWdQUjhFS0cxS0VoRXBLMDFGCmc4MjhUU1g2TXVnVVZmWHZ1OUJRUXExVWw0Q2VMOXhQdVB5T3lMSktzbzNGOEFNUHFlaS9USWpsQVFSdmRLeFYKVUMzMnBtTHRlUFVBb2thNDRPdElmR3BIOTZybmFsMW0rMXp6YkdTemRFSEFaL2k1ZEZDNXJOaUthRmJnL2NBRwppalhlQ01xeGpzT3JLMEM4MDg4a0tjenJZK0JmQWdNQkFBR2pTakJJTUM0R0ExVWRFUVFuTUNXQ0VtUmhjMmhpCmIyRnlaQzV0Y21sMFpDNXRaWUlQYTJsaVlXNWhMbTF5YVhSa0xtMWxNQWtHQTFVZEV3UUNNQUF3Q3dZRFZSMFAKQkFRREFnWGdNQTBHQ1NxR1NJYjNEUUVCQlFVQUE0SUJBUUNFN1ByRzh6MytyaGJESC8yNGJOeW5OUUNyYVM4NwphODJUUDNxMmsxUUJ1T0doS1pwR1N3SVRhWjNUY0pKMkQ2ZlRxbWJDUzlVeDF2ckYxMWhGTWg4MU9GMkF2MU4vCm5hSU12YlY5cVhYNG16eGNROHNjakVHZ285bnlDSVpuTFM5K2NXejhrOWQ1UHVaejE1TXg4T3g3OWJWVFpkZ0sKaEhCMGJ5UGgvdG9hMkNidnBmWUR4djRBdHlrSVRhSlFzekhnWHZnNXdwSjlySzlxZHd1RHA5T3JTNk03dmNOaQpseWxDTk52T3dNQ0h3emlyc01nQ1FRcVRVamtuNllLWmVsZVY0Mk1yazREVTlVWFFjZ2dEb1FKZEM0aWNwN0sxCkRPTDJURjFVUGN0ODFpNWt4NGYwcUw1aE1sNGhtK1BZRyt2MGIrMjZjOVlud3ROd24xdmMyZVZHCi0tLS0tRU5EIENFUlRJRklDQVRFLS0tLS0K  tls.key: LS0tLS1CRUdJTiBSU0EgUFJJVkFURSBLRVktLS0tLQpNSUlFb3dJQkFBS0NBUUVBdXJ3bUExeFIyVC9LL1h6WEhzblZqTWxHQS85TkFwWUNndW1kNG1uN1FVd3ZqUFRZClBndllVZkNEeldKdUsramRpK2RuSGEzNkFhUm5zL3dFeHpnaUdoeUwyYnZsV1R0eTMrOVhSQ2hqb0ZwaXRmV3kKRDk0WTBjZW1tQjhZUTFIR2FMN0duK1VHeGhidHJRTWY0SWRkUzdzMG5tM1RoZWJndmw2V2kzckdJRDBmQkNodApTaElSS1N0TlJZUE52RTBsK2pMb0ZGWDE3N3ZRVUVLdFZKZUFuaS9jVDdqOGpzaXlTcktOeGZBREQ2bm92MHlJCjVRRUViM1NzVlZBdDlxWmk3WGoxQUtKR3VPRHJTSHhxUi9lcTUycGRadnRjODJ4a3MzUkJ3R2Y0dVhSUXVhelkKaW1oVzRQM0FCb28xM2dqS3NZN0RxeXRBdk5QUEpDbk02MlBnWHdJREFRQUJBb0lCQUJtRmIzaVVISWVocFYraAp1VkQyNnQzVUFHSzVlTS82cXBzenpLVk9NTTNLMk5EZUFkUHhFSDZhYlprYmM4MUNoVTBDc21BbkQvMDdlQVRzClU4YmFrQ2FiY2kydTlYaU5uSFNvcEhlblFYNS8rKys4aGJxUGN6cndtMzg4K0xieXJUaFJvcG5sMWxncWVBOW0KVnV2NzlDOU9oYkdGZHh4YzRxaUNDdmRETDJMbVc2bWhpcFRKQnF3bUZsNUhqeVphdGcyMVJ4WUtKZ003S1p6TAplYWU0bTJDR3R0bmNyUktodklaQWxKVmpyRWoxbmVNa3RHODFTT3QyN0FjeDRlSnozbmcwbjlYSmdMMHcwU05ZCmlwd3I5Uk5PaDkxSGFsQ3JlWVB3bDRwajIva0JIdnozMk9Qb2FOSDRQa2JaeTEzcks1bnFrMHBXdUthOEcyY00KLzY4cnQrRUNnWUVBN1NEeHRzRFFBK2JESGdUbi9iOGJZQ3VhQ2N4TDlObHIxd2tuTG56VVRzRnNkTDByUm1uZAp5bWQ4aU95ME04aUVBL0xKb3dPUGRRY240WFdWdS9XbWV5MzFVR2NIeHYvWlVSUlJuNzgvNmdjZUJSNzZJL2FzClIrNVQ1TEMyRmducVd2MzMvdG0rS0gwc0J4dEM3U2tSK3Y2UndVQk1jYnM3c0dUQlR4NVV2TkVDZ1lFQXlaaUcKbDBKY0dzWHhqd1JPQ0FLZytEMlJWQ3RBVmRHbjVMTmVwZUQ4bFNZZ3krZGxQaCt4VnRiY2JCV0E3WWJ4a1BwSAorZHg2Z0p3UWp1aGN3U25uOU9TcXRrZW04ZmhEZUZ2MkNDbXl4ZlMrc1VtMkxqVzM1NE1EK0FjcWtwc0xMTC9GCkIvK1JmcmhqZW5lRi9BaERLalowczJTNW9BR0xRVFk4aXBtM1ZpOENnWUJrZGVHUnNFd3dhdkpjNUcwNHBsODkKdGhzemJYYjhpNlJSWE5KWnNvN3JzcXgxSkxPUnlFWXJldjVhc0JXRUhyNDNRZ1BFNlR3OHMwUmxFMERWZWJRSApXYWdsWVJEOWNPVXJvWFVYUFpvaFZ0U1VETlNpcWQzQk42b1pKL2hzaTlUYXFlQUgrMDNCcjQ0WWtLY2cvSlplCmhMMVJaeUU3eWJ2MjlpaWprVkVMRVFLQmdRQ2ZQRUVqZlNFdmJLYnZKcUZVSm05clpZWkRpNTVYcXpFSXJyM1cKSEs2bVNPV2k2ZlhJYWxRem1hZW1JQjRrZ0hDUzZYNnMyQUJUVWZLcVR0UGxKK3EyUDJDd2RreGgySTNDcGpEaQpKYjIyS3luczg2SlpRY2t2cndjVmhPT1Z4YTIvL1FIdTNXblpSR0FmUGdXeEcvMmhmRDRWN1R2S0xTNEhwb1dQCm5QZDV0UUtCZ0QvNHZENmsyOGxaNDNmUWpPalhkV0ZTNzdyVFZwcXBXMlFoTDdHY0FuSXk5SDEvUWRaOXYxdVEKNFBSanJseEowdzhUYndCeEp3QUtnSzZmRDBXWmZzTlRLSG01V29kZUNPWi85WW13cmpPSkxEaUU3eFFNWFBzNQorMnpVeUFWVjlCaDI4cThSdnMweHplclQ1clRNQ1NGK0Q5NHVJUmkvL3ZUMGt4d05XdFZxCi0tLS0tRU5EIFJTQSBQUklWQVRFIEtFWS0tLS0tCg==kind: Secretmetadata:  name: ingress-secret  namespace: kube-system<span class="hljs-built_in">type</span>: Opaque</code></pre></div><p><strong>创建完成后 create 一下就可</strong></p><div class="hljs code-wrapper"><pre><code class="hljs sh">➜  ~ kubectl create -f ingress-secret.ymlsecret <span class="hljs-string">&quot;ingress-secret&quot;</span> created</code></pre></div><p><strong>其实这个配置比如证书转码啥的没必要手动去做，可以直接使用下面的命令创建，这里写这么多只是为了把步骤写清晰</strong></p><div class="hljs code-wrapper"><pre><code class="hljs sh">kubectl create secret tls ingress-secret --key cert/ingress-key.pem --cert cert/ingress.pem</code></pre></div><h4 id="3-3、重新部署-Ingress"><a href="#3-3、重新部署-Ingress" class="headerlink" title="3.3、重新部署 Ingress"></a>3.3、重新部署 Ingress</h4><p>生成完成后需要在 Ingress 中开启 TLS，Ingress 修改后如下</p><div class="hljs code-wrapper"><pre><code class="hljs sh">apiVersion: extensions/v1beta1kind: Ingressmetadata:  name: dashboard-kibana-ingress  namespace: kube-systemspec:  tls:  - hosts:    - dashboard.mritd.me    - kibana.mritd.me    secretName: ingress-secret  rules:  - host: dashboard.mritd.me    http:      paths:      - backend:          serviceName: kubernetes-dashboard          servicePort: 80  - host: kibana.mritd.me    http:      paths:      - backend:          serviceName: kibana-logging          servicePort: 5601</code></pre></div><p><strong>注意：一个 Ingress 只能使用一个 secret(secretName 段只能有一个)，也就是说只能用一个证书，更直白的说就是如果你在一个 Ingress 中配置了多个域名，那么使用 TLS 的话必须保证证书支持该 Ingress 下所有域名；并且这个 <code>secretName</code> 一定要放在上面域名列表最后位置，否则会报错 <code>did not find expected key</code> 无法创建；同时上面的 <code>hosts</code> 段下域名必须跟下面的 <code>rules</code> 中完全匹配</strong></p><p><strong>更需要注意一点：之所以这里单独开一段就是因为有大坑；Kubernetes Ingress 默认情况下，当你不配置证书时，会默认给你一个 TLS 证书的，也就是说你 Ingress 中配置错了，比如写了2个 <code>secretName</code>、或者 <code>hosts</code> 段中缺了某个域名，那么对于写了多个 <code>secretName</code> 的情况，所有域名全会走默认证书；对于 <code>hosts</code> 缺了某个域名的情况，缺失的域名将会走默认证书，部署时一定要验证一下证书，不能 “有了就行”；更新 Ingress 证书可能需要等一段时间才会生效</strong></p><p>最后重新部署一下即可</p><div class="hljs code-wrapper"><pre><code class="hljs sh">➜  ~ kubectl delete -f dashboard-kibana-ingress.ymlingress <span class="hljs-string">&quot;dashboard-kibana-ingress&quot;</span> deleted➜  ~ kubectl create -f dashboard-kibana-ingress.ymlingress <span class="hljs-string">&quot;dashboard-kibana-ingress&quot;</span> created</code></pre></div><p><strong>注意：部署 TLS 后 80 端口会自动重定向到 443</strong>，最终访问截图如下</p><p><img src="https://cdn.oss.link/markdown/6o0pj.jpg" alt="Ingress TLS"></p><p><img src="https://cdn.oss.link/markdown/2ch1k.jpg" alt="Ingress TLS Certificate"></p><p><strong>历时 5 个小时鼓捣，到此结束</strong></p>]]></content>
    
    
    <summary type="html">最近发现好多人问 Ingress，同时一直也没去用 Nginx 的 Ingress，索性鼓捣了一把，发现跟原来确实有了点变化，在这里写篇文章记录一下</summary>
    
    
    
    <category term="Kubernetes" scheme="https://mritd.com/categories/kubernetes/"/>
    
    
    <category term="Linux" scheme="https://mritd.com/tags/linux/"/>
    
    <category term="Docker" scheme="https://mritd.com/tags/docker/"/>
    
    <category term="Kubernetes" scheme="https://mritd.com/tags/kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>快速部署 kubernetes 高可用集群</title>
    <link href="https://mritd.com/2017/03/03/set-up-kubernetes-ha-cluster-by-kargo/"/>
    <id>https://mritd.com/2017/03/03/set-up-kubernetes-ha-cluster-by-kargo/</id>
    <published>2017-03-03T15:16:04.000Z</published>
    <updated>2017-03-03T15:16:04.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>鼓捣 kubernetes 好长一段时间了，kubernetes 1.5 后新增了 kubeadm 工具用于快速部署 kubernetes 集群，不过该工具尚未稳定，无法自动部署高可用集群，而且还存在一些 BUG，所以生产环境还无法使用；本文基于 kargo 工具实现一键部署 kubernetes 容器化(可选) 高可用集群</p></blockquote><h3 id="一、基本环境准备"><a href="#一、基本环境准备" class="headerlink" title="一、基本环境准备"></a>一、基本环境准备</h3><p>本文基本环境如下:</p><p><strong>五台虚拟机，基于 vagrant 启动(穷)，vagrant 配置文件参考 <a href="https://github.com/mritd/config/tree/master/vagrant">这里</a></strong></p><table><thead><tr><th>主机地址</th><th>节点角色</th></tr></thead><tbody><tr><td>192.168.1.11</td><td>master</td></tr><tr><td>192.168.1.12</td><td>master</td></tr><tr><td>192.168.1.13</td><td>node</td></tr><tr><td>192.168.1.14</td><td>node</td></tr><tr><td>192.168.1.15</td><td>node</td></tr></tbody></table><p>同时保证部署机器对集群内节点拥有 root 免密登录权限，<strong>由于墙的原因，部署所需镜像已经全部打包到百度云，点击 <a href="https://pan.baidu.com/s/1jHMvMn0">这里</a> 下载，然后进行 load 即可；注意: 直接使用我的 <code>vagrant</code> 文件时，请删除我在 <code>init.sh</code> 脚本里对 docker 设置的本地代理，直接使用可能导致 docker 无法 pull 任何镜像；vagrant 可能需要执行 <code>vagrant plugin install vagrant-hosts</code> 安装插件以支持自动设置 host；如果自己采用其他虚拟机请保证单台虚拟机最低 1.5G 内存，否则会导致安装失败，别问我怎么知道的</strong></p><p><strong>最新更新：经过测试，请使用 pip 安装 ansible，保证 <code>ansible &gt;= 2.2.1</code> &amp;&amp; <code>jinja2 &gt;= 2.8,&lt; 2.9</code>；否则可能出现安装时校验失败问题</strong></p><h3 id="二、搭建集群"><a href="#二、搭建集群" class="headerlink" title="二、搭建集群"></a>二、搭建集群</h3><h4 id="2-1、获取源码"><a href="#2-1、获取源码" class="headerlink" title="2.1、获取源码"></a>2.1、获取源码</h4><p><strong>kargo 是基于 ansible 的 Playbooks 的，其官方推荐的 kargo-cli 目前只适用于各种云平台部署安装，所以我们需要手动使用 Playbooks 部署，当然第一步先把源码搞下来</strong></p><div class="hljs code-wrapper"><pre><code class="hljs sh">git <span class="hljs-built_in">clone</span> https://github.com/kubernetes-incubator/kargo.git</code></pre></div><h4 id="2-2、安装-ansible"><a href="#2-2、安装-ansible" class="headerlink" title="2.2、安装 ansible"></a>2.2、安装 ansible</h4><p>既然 kargo 是基于 ansible 的(实际上就是 Playbooks)，那么自然要先安装 ansible，同时下面配置生成会用到 python3，所以也一并安装</p><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment"># 安装 python 及 epel</span>yum install -y epel-release python-pip python34 python34-pip<span class="hljs-comment"># 安装 ansible(必须先安装 epel 源再安装 ansible)</span>yum install -y ansible</code></pre></div><h4 id="2-3、编辑配置文件"><a href="#2-3、编辑配置文件" class="headerlink" title="2.3、编辑配置文件"></a>2.3、编辑配置文件</h4><p><strong>注意：以下配置段中，所有双大括号 <code>&#123; &#123;</code> 、<code>&#125; &#125;</code>，中间全部加了空格，因为双大括号会跟主题模板引擎产生冲突，默认应该是没有的，请自行 vim 替换</strong></p><p>首先根据自己需要更改 kargo 的配置，配置文件位于 <code>inventory/group_vars/k8s-cluster.yml</code>，<strong>最新稳定版本版本(2.1.0) 配置文件还未更名，全部在 <code>inventory/group_vars/all.yml</code> 中，这里采用最新版本的原因是…借着写博客我也看看更新了啥(偷笑…)</strong></p><div class="hljs code-wrapper"><pre><code class="hljs sh">vim inventory/group_vars/k8s-cluster.yml<span class="hljs-comment"># 配置文件如下</span><span class="hljs-comment"># 启动集群的基础系统</span>bootstrap_os: centos<span class="hljs-comment"># etcd 数据存放位置</span>etcd_data_dir: /var/lib/etcd<span class="hljs-comment"># 二进制文件将要安装的位置</span>bin_dir: /usr/<span class="hljs-built_in">local</span>/bin<span class="hljs-comment"># Kubernetes 配置文件存放目录以及命名空间</span>kube_config_dir: /etc/kuberneteskube_script_dir: <span class="hljs-string">&quot;&#123; &#123; bin_dir &#125; &#125;/kubernetes-scripts&quot;</span>kube_manifest_dir: <span class="hljs-string">&quot;&#123; &#123; kube_config_dir &#125; &#125;/manifests&quot;</span>system_namespace: kube-system<span class="hljs-comment"># 日志存放位置</span>kube_log_dir: <span class="hljs-string">&quot;/var/log/kubernetes&quot;</span><span class="hljs-comment"># kubernetes 证书存放位置</span>kube_cert_dir: <span class="hljs-string">&quot;&#123; &#123; kube_config_dir &#125; &#125;/ssl&quot;</span><span class="hljs-comment"># token存放位置</span>kube_token_dir: <span class="hljs-string">&quot;&#123; &#123; kube_config_dir &#125; &#125;/tokens&quot;</span><span class="hljs-comment"># basic auth 认证文件存放位置</span>kube_users_dir: <span class="hljs-string">&quot;&#123; &#123; kube_config_dir &#125; &#125;/users&quot;</span><span class="hljs-comment"># 关闭匿名授权</span>kube_api_anonymous_auth: <span class="hljs-literal">false</span><span class="hljs-comment">## 使用的 kubernetes 版本</span>kube_version: v1.5.3<span class="hljs-comment"># 安装过程中缓存文件下载位置(最少 1G)</span>local_release_dir: <span class="hljs-string">&quot;/tmp/releases&quot;</span><span class="hljs-comment"># 重试次数，比如下载失败等情况</span>retry_stagger: 5<span class="hljs-comment"># 证书组</span>kube_cert_group: kube-cert<span class="hljs-comment"># 集群日志等级</span>kube_log_level: 2<span class="hljs-comment"># HTTP 下 api server 密码及用户</span>kube_api_pwd: <span class="hljs-string">&quot;changeme&quot;</span>kube_users:  kube:    pass: <span class="hljs-string">&quot;&#123; &#123;kube_api_pwd&#125; &#125;&quot;</span>    role: admin  root:    pass: <span class="hljs-string">&quot;&#123; &#123;kube_api_pwd&#125; &#125;&quot;</span>    role: admin<span class="hljs-comment"># 网络 CNI 组件 (calico, weave or flannel)</span>kube_network_plugin: calico<span class="hljs-comment"># 服务地址分配</span>kube_service_addresses: 10.233.0.0/18<span class="hljs-comment"># pod 地址分配</span>kube_pods_subnet: 10.233.64.0/18<span class="hljs-comment"># 网络节点大小分配</span>kube_network_node_prefix: 24<span class="hljs-comment"># api server 监听地址及端口</span>kube_apiserver_ip: <span class="hljs-string">&quot;&#123; &#123; kube_service_addresses|ipaddr(&#x27;net&#x27;)|ipaddr(1)|ipaddr(&#x27;address&#x27;) &#125; &#125;&quot;</span>kube_apiserver_port: 6443 <span class="hljs-comment"># (https)</span>kube_apiserver_insecure_port: 8080 <span class="hljs-comment"># (http)</span><span class="hljs-comment"># 默认 dns 后缀</span>cluster_name: cluster.local<span class="hljs-comment"># Subdomains of DNS domain to be resolved via /etc/resolv.conf for hostnet pods</span>ndots: 2<span class="hljs-comment"># DNS 组件 dnsmasq_kubedns/kubedns</span>dns_mode: dnsmasq_kubedns<span class="hljs-comment"># Can be docker_dns, host_resolvconf or none</span>resolvconf_mode: docker_dns<span class="hljs-comment"># 部署 netchecker 来检测 DNS 和 HTTP 状态</span>deploy_netchecker: <span class="hljs-literal">false</span><span class="hljs-comment"># skydns service IP 配置</span>skydns_server: <span class="hljs-string">&quot;&#123; &#123; kube_service_addresses|ipaddr(&#x27;net&#x27;)|ipaddr(3)|ipaddr(&#x27;address&#x27;) &#125; &#125;&quot;</span>dns_server: <span class="hljs-string">&quot;&#123; &#123; kube_service_addresses|ipaddr(&#x27;net&#x27;)|ipaddr(2)|ipaddr(&#x27;address&#x27;) &#125; &#125;&quot;</span>dns_domain: <span class="hljs-string">&quot;&#123; &#123; cluster_name &#125; &#125;&quot;</span><span class="hljs-comment"># docker 存储目录</span>docker_daemon_graph: <span class="hljs-string">&quot;/var/lib/docker&quot;</span><span class="hljs-comment"># docker 的额外配置参数，默认会在 /etc/systemd/system/docker.service.d/ 创建相关配置，如果节点已经安装了 docker，并且做了自己的配置，比如启用了 devicemapper ，那么要更改这里，并把自己的 devicemapper 参数加到这里，因为 kargo 会复写 systemd service 文件，会导致自己在 service 中配置的参数被清空，最后 docker 将无法启动</span><span class="hljs-comment">## A string of extra options to pass to the docker daemon.</span><span class="hljs-comment">## This string should be exactly as you wish it to appear.</span><span class="hljs-comment">## An obvious use case is allowing insecure-registry access</span><span class="hljs-comment">## to self hosted registries like so:</span>docker_options: <span class="hljs-string">&quot;--insecure-registry=&#123; &#123; kube_service_addresses &#125; &#125; --graph=&#123; &#123; docker_daemon_graph &#125; &#125; --iptables=false&quot;</span>docker_bin_dir: <span class="hljs-string">&quot;/usr/bin&quot;</span><span class="hljs-comment"># 组件部署方式</span><span class="hljs-comment"># Settings for containerized control plane (etcd/kubelet/secrets)</span>etcd_deployment_type: dockerkubelet_deployment_type: dockercert_management: scriptvault_deployment_type: docker<span class="hljs-comment"># K8s image pull policy (imagePullPolicy)</span>k8s_image_pull_policy: IfNotPresent<span class="hljs-comment"># Monitoring apps for k8s</span>efk_enabled: <span class="hljs-literal">false</span></code></pre></div><h4 id="2-4、生成集群配置"><a href="#2-4、生成集群配置" class="headerlink" title="2.4、生成集群配置"></a>2.4、生成集群配置</h4><p>配置完基本集群参数后，还需要生成一个集群配置文件，用于指定需要在哪几台服务器安装，和指定 master、node 节点分布，以及 etcd 集群等安装在那几台机器上</p><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment"># 定义集群 IP</span>IPS=(192.168.1.11 192.168.1.12 192.168.1.13 192.168.1.14 192.168.1.15)<span class="hljs-comment"># 生成配置</span><span class="hljs-built_in">cd</span> kargoCONFIG_FILE=inventory/inventory.cfg python3 contrib/inventory_builder/inventory.py <span class="hljs-variable">$&#123;IPS[@]&#125;</span></code></pre></div><p>生成的配置如下，已经很简单了，怎么改动相信猜也能猜到</p><div class="hljs code-wrapper"><pre><code class="hljs sh">vim inventory/inventory.cfg[all]node1    ansible_host=192.168.1.11 ip=192.168.1.11node2    ansible_host=192.168.1.12 ip=192.168.1.12node3    ansible_host=192.168.1.13 ip=192.168.1.13node4    ansible_host=192.168.1.14 ip=192.168.1.14node5    ansible_host=192.168.1.15 ip=192.168.1.15[kube-master]node1node2[kube-node]node1node2node3node4node5[etcd]node1node2node3[k8s-cluster:children]kube-nodekube-master[calico-rr]</code></pre></div><h4 id="2-5、一键部署"><a href="#2-5、一键部署" class="headerlink" title="2.5、一键部署"></a>2.5、一键部署</h4><p>首先启动 vagrant 虚拟机，<strong>不过注意的是本文提供的 vagrant 文件默认安装了 docker，并配置了 devicemapper 和docker 代理，所以使用时上面的 docker 参数需要替换成自己的，因为默认 kargo 会覆盖 docker 的 service 文件；会导致我已经配置完的 docker devicemapper 参数失效，所以要把自己配置的参数加到配置文件中，如下</strong></p><div class="hljs code-wrapper"><pre><code class="hljs sh">docker_options: <span class="hljs-string">&quot;--insecure-registry=&#123; &#123; kube_service_addresses &#125; &#125; --graph=&#123; &#123; docker_daemon_graph &#125; &#125; --iptables=false --storage-driver=devicemapper --storage-opt=dm.thinpooldev=/dev/mapper/docker-thinpool --storage-opt dm.use_deferred_removal=true --storage-opt=dm.use_deferred_deletion=true&quot;</span></code></pre></div><p><strong>这个 vagrant 配置文件自动设置了主机名、host、ssh 密钥，实际生产环境仍需自己处理</strong></p><p><strong>每个虚拟机需要自己登陆并生成 ssh-key(ssh-keygen)，因为 ansible 会用到</strong></p><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment"># 启动虚拟机</span><span class="hljs-built_in">cd</span> vagrant &amp;&amp; vagrant up<span class="hljs-comment"># 走你(没梯子先 load 好镜像)</span><span class="hljs-built_in">cd</span> kargo <span class="hljs-comment"># 私钥指定的是每个虚拟机 ssh 目录下的私钥</span>ansible-playbook -i inventory/inventory.cfg cluster.yml -b -v --private-key=~/.ssh/id_rsa</code></pre></div><p>部署成功后截图如下</p><p><img src="https://cdn.oss.link/markdown/ksgtg.jpg" alt="deploy success"></p><p>相关 pod 启动情况如下 </p><p><img src="https://cdn.oss.link/markdown/c7zaz.jpg" alt="deploy pods"></p><p><strong>最后附上我部署是的 kargo 配置</strong></p><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment"># 启动集群的基础系统</span>bootstrap_os: centos<span class="hljs-comment"># etcd 数据存放位置</span>etcd_data_dir: /var/lib/etcd<span class="hljs-comment"># 二进制文件将要安装的位置</span>bin_dir: /usr/<span class="hljs-built_in">local</span>/bin<span class="hljs-comment"># Kubernetes 配置文件存放目录以及命名空间</span>kube_config_dir: /etc/kuberneteskube_script_dir: <span class="hljs-string">&quot;&#123; &#123; bin_dir &#125; &#125;/kubernetes-scripts&quot;</span>kube_manifest_dir: <span class="hljs-string">&quot;&#123; &#123; kube_config_dir &#125; &#125;/manifests&quot;</span>system_namespace: kube-system<span class="hljs-comment"># 日志存放位置</span>kube_log_dir: <span class="hljs-string">&quot;/var/log/kubernetes&quot;</span><span class="hljs-comment"># kubernetes 证书存放位置</span>kube_cert_dir: <span class="hljs-string">&quot;&#123; &#123; kube_config_dir &#125; &#125;/ssl&quot;</span><span class="hljs-comment"># token存放位置</span>kube_token_dir: <span class="hljs-string">&quot;&#123; &#123; kube_config_dir &#125; &#125;/tokens&quot;</span><span class="hljs-comment"># basic auth 认证文件存放位置</span>kube_users_dir: <span class="hljs-string">&quot;&#123; &#123; kube_config_dir &#125; &#125;/users&quot;</span><span class="hljs-comment"># 关闭匿名授权</span>kube_api_anonymous_auth: <span class="hljs-literal">false</span><span class="hljs-comment">## 使用的 kubernetes 版本</span>kube_version: v1.5.3<span class="hljs-comment"># 安装过程中缓存文件下载位置(最少 1G)</span>local_release_dir: <span class="hljs-string">&quot;/tmp/releases&quot;</span><span class="hljs-comment"># 重试次数，比如下载失败等情况</span>retry_stagger: 5<span class="hljs-comment"># 证书组</span>kube_cert_group: kube-cert<span class="hljs-comment"># 集群日志等级</span>kube_log_level: 2<span class="hljs-comment"># HTTP 下 api server 密码及用户</span>kube_api_pwd: <span class="hljs-string">&quot;test123&quot;</span>kube_users:  kube:    pass: <span class="hljs-string">&quot;&#123; &#123;kube_api_pwd&#125; &#125;&quot;</span>    role: admin  root:    pass: <span class="hljs-string">&quot;&#123; &#123;kube_api_pwd&#125; &#125;&quot;</span>    role: admin<span class="hljs-comment"># 网络 CNI 组件 (calico, weave or flannel)</span>kube_network_plugin: calico<span class="hljs-comment"># 服务地址分配</span>kube_service_addresses: 10.233.0.0/18<span class="hljs-comment"># pod 地址分配</span>kube_pods_subnet: 10.233.64.0/18<span class="hljs-comment"># 网络节点大小分配</span>kube_network_node_prefix: 24<span class="hljs-comment"># api server 监听地址及端口</span>kube_apiserver_ip: <span class="hljs-string">&quot;&#123; &#123; kube_service_addresses|ipaddr(&#x27;net&#x27;)|ipaddr(1)|ipaddr(&#x27;address&#x27;) &#125; &#125;&quot;</span>kube_apiserver_port: 6443 <span class="hljs-comment"># (https)</span>kube_apiserver_insecure_port: 8080 <span class="hljs-comment"># (http)</span><span class="hljs-comment"># 默认 dns 后缀</span>cluster_name: cluster.local<span class="hljs-comment"># Subdomains of DNS domain to be resolved via /etc/resolv.conf for hostnet pods</span>ndots: 2<span class="hljs-comment"># DNS 组件 dnsmasq_kubedns/kubedns</span>dns_mode: dnsmasq_kubedns<span class="hljs-comment"># Can be docker_dns, host_resolvconf or none</span>resolvconf_mode: docker_dns<span class="hljs-comment"># 部署 netchecker 来检测 DNS 和 HTTP 状态</span>deploy_netchecker: <span class="hljs-literal">true</span> <span class="hljs-comment"># skydns service IP 配置</span>skydns_server: <span class="hljs-string">&quot;&#123; &#123; kube_service_addresses|ipaddr(&#x27;net&#x27;)|ipaddr(3)|ipaddr(&#x27;address&#x27;) &#125; &#125;&quot;</span>dns_server: <span class="hljs-string">&quot;&#123; &#123; kube_service_addresses|ipaddr(&#x27;net&#x27;)|ipaddr(2)|ipaddr(&#x27;address&#x27;) &#125; &#125;&quot;</span>dns_domain: <span class="hljs-string">&quot;&#123; &#123; cluster_name &#125; &#125;&quot;</span><span class="hljs-comment"># docker 存储目录</span>docker_daemon_graph: <span class="hljs-string">&quot;/var/lib/docker&quot;</span><span class="hljs-comment"># docker 的额外配置参数，默认会在 /etc/systemd/system/docker.service.d/ 创建相关配置，如果节点已经安装了 docker，并且做了自己的配置，比如启用的 device mapper ，那么要删除/更改这里，防止冲突导致 docker 无法启动</span><span class="hljs-comment">## A string of extra options to pass to the docker daemon.</span><span class="hljs-comment">## This string should be exactly as you wish it to appear.</span><span class="hljs-comment">## An obvious use case is allowing insecure-registry access</span><span class="hljs-comment">## to self hosted registries like so:</span>docker_options: <span class="hljs-string">&quot;--insecure-registry=&#123; &#123; kube_service_addresses &#125; &#125; --graph=&#123; &#123; docker_daemon_graph &#125; &#125; --iptables=false --storage-driver=devicemapper --storage-opt=dm.thinpooldev=/dev/mapper/docker-thinpool --storage-opt dm.use_deferred_removal=true --storage-opt=dm.use_deferred_deletion=true&quot;</span>docker_bin_dir: <span class="hljs-string">&quot;/usr/bin&quot;</span><span class="hljs-comment"># 组件部署方式</span><span class="hljs-comment"># Settings for containerized control plane (etcd/kubelet/secrets)</span>etcd_deployment_type: dockerkubelet_deployment_type: dockercert_management: scriptvault_deployment_type: docker<span class="hljs-comment"># K8s image pull policy (imagePullPolicy)</span>k8s_image_pull_policy: IfNotPresent<span class="hljs-comment"># Monitoring apps for k8s</span>efk_enabled: <span class="hljs-literal">true</span></code></pre></div>]]></content>
    
    
    <summary type="html">鼓捣 kubernetes 好长一段时间了，kubernetes 1.5 后新增了 kubeadm 工具用于快速部署 kubernetes 集群，不过该工具尚未稳定，无法自动部署高可用集群，而且还存在一些 BUG，所以生产环境还无法使用；本文基于 kargo 工具实现一键部署 kubernetes 容器化(可选) 高可用集群</summary>
    
    
    
    <category term="Kubernetes" scheme="https://mritd.com/categories/kubernetes/"/>
    
    
    <category term="Linux" scheme="https://mritd.com/tags/linux/"/>
    
    <category term="Docker" scheme="https://mritd.com/tags/docker/"/>
    
    <category term="Kubernetes" scheme="https://mritd.com/tags/kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>Vagrant 使用</title>
    <link href="https://mritd.com/2017/03/01/how-to-use-vagrant/"/>
    <id>https://mritd.com/2017/03/01/how-to-use-vagrant/</id>
    <published>2017-03-01T14:07:59.000Z</published>
    <updated>2017-03-01T14:07:59.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>Vagrant 是一个开源的 基于 ruby 的开源虚拟机管理工具；最近在鼓捣 kubernetes ，常常需要做集群部署测试，由于比较穷 😂😂😂；所以日常测试全部是自己开虚拟机；每次使用 VirtualBox开5个虚拟机很烦，而且为了保证环境干净不受其他因素影响，所以每次测试都是新开…..每次都会有种 WTF 的感觉，所以研究了一下 Vagrant 这个工具，发现很好用，一下记录一下简单的使用</p></blockquote><h3 id="一、Vagrant-介绍"><a href="#一、Vagrant-介绍" class="headerlink" title="一、Vagrant 介绍"></a>一、Vagrant 介绍</h3><p>上面已经简单的说了一下 Vagrant，Vagrant 定位为一个虚拟机管理工具；它能够以脚本化的方式启动、停止、和和删除虚拟机，当然这些手动也没费劲；更重要的是它能够自己定义网络分配、初始化执行的脚本、添加硬盘等各种复杂的动作；最重要的是 Vagrant 提供了类似于 docker image 的 box；Vagrant Box 就是一个完整的虚拟机分发包，可以自己制作也可以从网络下载；并且 Vagrant 开源特性使得各路大神开发了很多 Vagrant 插件方便我们使用，基于以上这些特点，我们可以实现:</p><ul><li>一个脚本定义好虚拟机的数量</li><li>一个脚本定义好虚拟机初始化工作，比如装 docker</li><li>一个脚本完成多台虚拟机网络配置</li><li>一条命令启动、停止、删除多个虚拟机</li><li>更多玩法自行摸索…..</li></ul><h3 id="二、Vagrant-使用"><a href="#二、Vagrant-使用" class="headerlink" title="二、Vagrant 使用"></a>二、Vagrant 使用</h3><h4 id="2-1、Vagrant-安装"><a href="#2-1、Vagrant-安装" class="headerlink" title="2.1、Vagrant 安装"></a>2.1、Vagrant 安装</h4><p>Vagrant 安装极其简单，目前官方已经打包好了各个平台的安装包文件，地址访问 <a href="https://www.vagrantup.com/downloads.html">Vagrant 官方下载地址</a>；截图如下</p><p><img src="https://cdn.oss.link/markdown/m46fa.jpg" alt="vagrant download"></p><p>以下为 CentOS 上的安装命令</p><div class="hljs code-wrapper"><pre><code class="hljs sh">wget https://releases.hashicorp.com/vagrant/1.9.2/vagrant_1.9.2_x86_64.rpmrpm -ivh vagrant_1.9.2_x86_64.rpm</code></pre></div><h4 id="2-2、Vagrant-Box-下载"><a href="#2-2、Vagrant-Box-下载" class="headerlink" title="2.2、Vagrant Box 下载"></a>2.2、Vagrant Box 下载</h4><p>装虚拟机大家都不陌生，首先应该搞个系统镜像；同样 Vagrant 也需要先搞一个 Vagrant Box，Vagrant Box 是一个已经预装好操作系统的虚拟机打包文件；根据不同系统可以选择不同的 Vagrant Box，官方维护了一个 Vagrant Box 仓库，地址 <a href="https://atlas.hashicorp.com/boxes/search">点这里</a>，截图如下</p><p><img src="https://cdn.oss.link/markdown/2duz7.jpg" alt="vagrant boxes"></p><p>点击对应的系统后可以看到如下界面</p><p><img src="https://cdn.oss.link/markdown/kiohr.jpg" alt="box detail"></p><p>该页面罗列出了使用不同虚拟机时应当使用扥添加明令；当然执行这些命令后 vagrant 将会从网络下载这个 box 文件并添加到本地 box 仓库；<strong>不过众所周知的原因，这个下载速度会让你怀疑人生，所有简单的办法是执行以下这条命令，然后会显示 box 的实际下载地址；拿到地址以后用梯子先把文件下载下来，然后使用 vagrant 导入也可以(centos7 本地已经有了一下以 ubuntu 为例)</strong></p><p><img src="https://cdn.oss.link/markdown/p36th.jpg" alt="box download url"><br>下载后使用 <code>vagrant box add xxxx.box</code> 即可将 box 导入到本地仓库</p><h4 id="2-3、启动一个虚拟机"><a href="#2-3、启动一个虚拟机" class="headerlink" title="2.3、启动一个虚拟机"></a>2.3、启动一个虚拟机</h4><p>万事俱备只差东风，在上一步执行 <code>vagrant init ubuntu/trusty64; vagrant up --provider virtualbox</code> 命令获取 box 下载地址时，已经在当前目录下生成了一个 Vagrantfile 文件，这个文件其实就是虚拟机配置文件，具体下面再说；box 导入以后先启动一下再说，执行 <code>vagrnat up</code> 即可</p><p>其他几个常用命令如下</p><ul><li><code>vagrant box [list|add|remove]</code> 查看添加删除 box 等</li><li><code>vagrant up</code> 启动虚拟机</li><li><code>vagrant halt</code> 关闭虚拟机</li><li><code>vagrant init</code> 初始化一个指定系统的 Vagrantfile 文件</li><li><code>vagrant destroy</code> 删除虚拟机</li><li><code>vagrant ssh</code> ssh 到虚拟机里</li></ul><p><strong>特别说明一下 ssh 这个命令，一般默认的规范是 <code>vagrant ssh VM_NAME</code> 后，会以 vagrant 用户身份登录到目标虚拟机，如果当前目录的 Vagrantfile 中只有一个虚拟机那么无需指定虚拟机名称(init 后默认就是)；虚拟机内(box 封装时)vagrant这个用户拥有全局免密码 sudo 权限；root 用户一般密码为 vagrant</strong></p><h3 id="三、Vagrantfile"><a href="#三、Vagrantfile" class="headerlink" title="三、Vagrantfile"></a>三、Vagrantfile</h3><blockquote><p>我发现基本国内所有的 Vagrant 教程都是简单的提了一嘴那几个常用命令；包括我上面也写了点，估计可能到这已经被喷了(“妈的那几个命令老子 help 一下就出来了，一看一猜就知道啥意思 用得着你讲？”)；个人觉得 Vagrant 最复杂的是这个配置文件，以下直接上一个目前仓库里的做示例，仓库地址 <a href="https://github.com/mritd/config/tree/master/vagrant">戳这里</a></p></blockquote><p><strong>直接贴 Vagrantfile，以下配置在进行 <code>vagrant up</code> 之前可能需要使用 <code>vagrant plugin install vagrant-host</code> 插件，以支持自动在各节点之间添加 host</strong></p><div class="hljs code-wrapper"><pre><code class="hljs sh">Vagrant.configure(<span class="hljs-string">&quot;2&quot;</span>) <span class="hljs-keyword">do</span> |config|    <span class="hljs-comment"># 定义虚拟机数量</span>    vms = Array(1..5)    <span class="hljs-comment"># 数据盘存放目录</span>    <span class="hljs-variable">$data_base_dir</span> = <span class="hljs-string">&quot;/data/vm/disk&quot;</span>    vms.each <span class="hljs-keyword">do</span> |i|        config.vm.define <span class="hljs-string">&quot;docker#&#123;i&#125;&quot;</span> <span class="hljs-keyword">do</span> |docker|            <span class="hljs-comment"># 设置虚拟机的Box</span>            docker.vm.box = <span class="hljs-string">&quot;centos/7&quot;</span>            <span class="hljs-comment"># 不检查 box 更新</span>            docker.vm.box_check_update = <span class="hljs-literal">false</span>             <span class="hljs-comment"># 设置虚拟机的主机名</span>            docker.vm.hostname=<span class="hljs-string">&quot;docker#&#123;i&#125;.node&quot;</span>            <span class="hljs-comment"># 设置虚拟机的IP (wlp2s0 为桥接本机的网卡)</span>            docker.vm.network <span class="hljs-string">&quot;public_network&quot;</span>, ip: <span class="hljs-string">&quot;192.168.1.1#&#123;i&#125;&quot;</span>, bridge: <span class="hljs-string">&quot;wlp2s0&quot;</span>            <span class="hljs-comment"># 设置主机与虚拟机的共享目录</span>            <span class="hljs-comment">#docker.vm.synced_folder &quot;~/Desktop/share&quot;, &quot;/home/vagrant/share&quot;</span>            <span class="hljs-comment"># VirtaulBox相关配置</span>            docker.vm.provider <span class="hljs-string">&quot;virtualbox&quot;</span> <span class="hljs-keyword">do</span> |v|                <span class="hljs-comment"># 设置虚拟机的名称</span>                v.name = <span class="hljs-string">&quot;docker#&#123;i&#125;&quot;</span>                <span class="hljs-comment"># 设置虚拟机的内存大小  </span>                v.memory = 1536                 <span class="hljs-comment"># 设置虚拟机的CPU个数</span>                v.cpus = 1                <span class="hljs-comment"># 增加磁盘</span>                docker_disk = <span class="hljs-string">&quot;#<span class="hljs-variable">$data_base_dir</span>/docker-disk#&#123;i&#125;.vdi&quot;</span>                data_disk = <span class="hljs-string">&quot;#<span class="hljs-variable">$data_base_dir</span>/data-disk#&#123;i&#125;.vdi&quot;</span>                <span class="hljs-comment"># 判断虚拟机启动后</span>                <span class="hljs-keyword">if</span> ARGV[0] == <span class="hljs-string">&quot;up&quot;</span>                    <span class="hljs-comment"># 如果两个文件都不存在 则创建 SATA 控制器(这里调用的是 Virtual Box 的命令)</span>                    <span class="hljs-keyword">if</span> ! File.exist?(docker_disk) &amp;&amp; ! File.exist?(data_disk)                        v.customize [                            <span class="hljs-string">&#x27;storagectl&#x27;</span>, :id,                            <span class="hljs-string">&#x27;--name&#x27;</span>, <span class="hljs-string">&#x27;SATA Controller&#x27;</span>,                            <span class="hljs-string">&#x27;--add&#x27;</span>, <span class="hljs-string">&#x27;sata&#x27;</span>,                            <span class="hljs-string">&#x27;--portcount&#x27;</span>, <span class="hljs-string">&#x27;5&#x27;</span>,                            <span class="hljs-string">&#x27;--controller&#x27;</span>, <span class="hljs-string">&#x27;IntelAhci&#x27;</span>,                            <span class="hljs-string">&#x27;--bootable&#x27;</span>, <span class="hljs-string">&#x27;on&#x27;</span>                        ]                    end                    <span class="hljs-comment"># 创建磁盘文件</span>                    <span class="hljs-keyword">if</span> ! File.exist?(docker_disk)                        v.customize [                            <span class="hljs-string">&#x27;createhd&#x27;</span>,                             <span class="hljs-string">&#x27;--filename&#x27;</span>, docker_disk,                             <span class="hljs-string">&#x27;--format&#x27;</span>, <span class="hljs-string">&#x27;VDI&#x27;</span>,                             <span class="hljs-string">&#x27;--size&#x27;</span>, 10 * 1024 <span class="hljs-comment"># 10 GB</span>                        ]                     end                    <span class="hljs-keyword">if</span> ! File.exist?(data_disk)                        v.customize [                            <span class="hljs-string">&#x27;createhd&#x27;</span>,                             <span class="hljs-string">&#x27;--filename&#x27;</span>, data_disk,                             <span class="hljs-string">&#x27;--format&#x27;</span>, <span class="hljs-string">&#x27;VDI&#x27;</span>,                             <span class="hljs-string">&#x27;--size&#x27;</span>, 10 * 1024 <span class="hljs-comment"># 10 GB</span>                        ]                     end                    <span class="hljs-comment"># 连接到 SATA 控制器</span>                    v.customize [                        <span class="hljs-string">&#x27;storageattach&#x27;</span>, :id,                         <span class="hljs-string">&#x27;--storagectl&#x27;</span>, <span class="hljs-string">&#x27;SATA Controller&#x27;</span>,                         <span class="hljs-string">&#x27;--port&#x27;</span>, 1, <span class="hljs-string">&#x27;--device&#x27;</span>, 0,                         <span class="hljs-string">&#x27;--type&#x27;</span>, <span class="hljs-string">&#x27;hdd&#x27;</span>, <span class="hljs-string">&#x27;--medium&#x27;</span>,                         docker_disk                    ]                    v.customize [                        <span class="hljs-string">&#x27;storageattach&#x27;</span>, :id,                         <span class="hljs-string">&#x27;--storagectl&#x27;</span>, <span class="hljs-string">&#x27;SATA Controller&#x27;</span>,                         <span class="hljs-string">&#x27;--port&#x27;</span>, 2, <span class="hljs-string">&#x27;--device&#x27;</span>, 0,                         <span class="hljs-string">&#x27;--type&#x27;</span>, <span class="hljs-string">&#x27;hdd&#x27;</span>, <span class="hljs-string">&#x27;--medium&#x27;</span>,                         data_disk                    ]                end            end            <span class="hljs-comment"># 增加各节点 host 配置</span>            config.vm.provision :hosts <span class="hljs-keyword">do</span> |provisioner|                vms.each <span class="hljs-keyword">do</span> |x|                    provisioner.add_host <span class="hljs-string">&quot;192.168.1.1#&#123;x&#125;&quot;</span>, [<span class="hljs-string">&quot;docker#&#123;x&#125;.node&quot;</span>]                end            end            <span class="hljs-comment"># 自定义执行脚本</span>            docker.vm.provision <span class="hljs-string">&quot;shell&quot;</span>, path: <span class="hljs-string">&quot;init.sh&quot;</span>            <span class="hljs-comment"># 每次开机后重启 network 和 ssh，解决公网网卡不启动问题 </span>            docker.vm.provision <span class="hljs-string">&quot;shell&quot;</span>, run: <span class="hljs-string">&quot;always&quot;</span>, inline: &lt;&lt;-<span class="hljs-string">SHELL</span><span class="hljs-string">                systemctl restart network</span><span class="hljs-string">                systemctl restart sshd</span><span class="hljs-string">                echo -e &quot;\033[32mvirtual machine docker#&#123;i&#125; init success!\033[0m&quot;</span><span class="hljs-string">            SHELL</span>        end    endend</code></pre></div><p>以上基本都加了注释，所以大致应该很清晰，至于第一行那个 <code>Vagrant.configure(&quot;2&quot;)</code> 代表调用第二版 API，不能改动，其他的可参考注释同时综合仓库中的其他配置文件即可</p><p><strong>Vagrantfile 实质上就是一个 ruby 文件，可以自己在里面定义变量等，可以在里面按照 ruby 的语法进行各种复杂的操作；具体 ruby 语法可以参考相关文档学习一下</strong></p>]]></content>
    
    
    <summary type="html">Vagrant 是一个开源的 基于 ruby 的开源虚拟机管理工具；最近在鼓捣 kubernetes ，常常需要做集群部署测试，由于比较穷 😂😂😂；所以日常测试全部是自己开虚拟机；每次使用 VirtualBox开5个虚拟机很烦，而且为了保证环境干净不受其他因素影响，所以每次测试都是新开.....每次都会有种 WTF 的感觉，所以研究了一下 Vagrant 这个工具，发现很好用，一下记录一下简单的使用</summary>
    
    
    
    <category term="Linux" scheme="https://mritd.com/categories/linux/"/>
    
    
    <category term="Linux" scheme="https://mritd.com/tags/linux/"/>
    
  </entry>
  
  <entry>
    <title>如何下载 Kubernetes 镜像和 rpm</title>
    <link href="https://mritd.com/2017/02/27/how-to-download-kubernetes-images-and-rpm/"/>
    <id>https://mritd.com/2017/02/27/how-to-download-kubernetes-images-and-rpm/</id>
    <published>2017-02-27T14:02:48.000Z</published>
    <updated>2017-02-27T14:02:48.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>随着 kubernetes 容器化部署逐渐推进，gcr.io 镜像、kubernetes rpm 下载由于 “伟大的” 墙的原因成为阻碍玩 kubernetes 第一道屏障，以下记录了个人维护的 yum 仓库和 gcr.io 反代仓库使用</p></blockquote><h3 id="一、yum-源"><a href="#一、yum-源" class="headerlink" title="一、yum 源"></a>一、yum 源</h3><p>目前个人维护了一个 kubernetes 的 yum 源，目前 yum 源包含 rpm 如下</p><table><thead><tr><th>rpm 包</th><th>版本</th></tr></thead><tbody><tr><td>etcd</td><td>3.1.0-1.x86_64</td></tr><tr><td>flannel</td><td>0.7.0-1.x86_64</td></tr><tr><td>kubernetes</td><td>1.5.3-1.x86_64</td></tr><tr><td>kubeadm</td><td>1.6.0-0.alpha.0.2074.a092d8e0f95f52.x86_64</td></tr><tr><td>kubectl</td><td>1.5.3-0.x86_64</td></tr><tr><td>kubelet</td><td>1.5.3-0.x86_64</td></tr><tr><td>kubernetes-cni</td><td>0.3.0.1-0.07a8a2.x86_64</td></tr></tbody></table><p>使用方法如下</p><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment"># 添加 yum 源</span>tee /etc/yum.repos.d/mritd.repo &lt;&lt; <span class="hljs-string">EOF</span><span class="hljs-string">[mritd]</span><span class="hljs-string">name=Mritd Repository</span><span class="hljs-string">baseurl=https://yum.mritd.me/centos/7/x86_64</span><span class="hljs-string">enabled=1</span><span class="hljs-string">gpgcheck=1</span><span class="hljs-string">gpgkey=https://cdn.oss.link/keys/rpm.public.key</span><span class="hljs-string">EOF</span><span class="hljs-comment"># 刷新cache</span>yum makecache<span class="hljs-comment"># 安装 yum-utils</span>yum install -y yum-utils socat <span class="hljs-comment"># 下载 rpm 到本地</span>yumdownloader kubelet kubectl kubernetes-cni kubeadm<span class="hljs-comment"># 安装 rpm</span>rpm -ivh kube*.rpm</code></pre></div><p><strong>所有关于 yum 源地址变更等都将在 <a href="https://yum.mritd.me/">https://yum.mritd.me</a> 页面公告，如出现不能使用请访问此页面查看相关原因</strong>；<strong>如果实在下载过慢可以将 <code>yum.mritd.me</code> 替换成 <code>yumrepo.b0.upaiyun.com</code>，此域名 yum 源在 CDN 上，由于流量有限，请使用 yumdownloader 工具下载到本地分发安装，谢谢</strong></p><h3 id="二、kubernetes-镜像"><a href="#二、kubernetes-镜像" class="headerlink" title="二、kubernetes 镜像"></a>二、kubernetes 镜像</h3><p>关于 kubernetes 镜像下载，一般有三种方式：</p><ul><li>直接从国外服务器 pull 然后 save 出来传到本地</li><li>通过第三方仓库做中转，如 Docker hub</li><li>在本地/国外能访问的服务器通过官方 registry 加代理反代 gcr.io</li></ul><p><strong>个人在国外服务器上维护了一个 gcr.io 的反代仓库，使用方式如下</strong></p><div class="hljs code-wrapper"><pre><code class="hljs sh">docker pull gcr.mritd.me/google_containers/kube-apiserver-amd64:v1.5.3</code></pre></div><p>如果对于 gcr.mritd.me 访问过慢可参考 <a href="https://mritd.me/2017/02/09/gcr.io-registy-proxy/">gcr.io 仓库代理</a> 使用带有梯子的本地私服，如果使用 Docker Hub 等中转可参考 <a href="https://mritd.me/2016/10/29/set-up-kubernetes-cluster-by-kubeadm/#22%E9%95%9C%E5%83%8F%E4%BB%8E%E5%93%AA%E6%9D%A5">kubeadm 搭建 kubernetes 集群</a></p>]]></content>
    
    
    <summary type="html">随着 kubernetes 容器化部署逐渐推进，gcr.io 镜像、kubernetes rpm 下载由于 &quot;伟大的&quot; 墙的原因成为阻碍玩 kubernetes 第一道屏障，以下记录了个人维护的 yum 仓库和 gcr.io 反代仓库使用</summary>
    
    
    
    <category term="Kubernetes" scheme="https://mritd.com/categories/kubernetes/"/>
    
    
    <category term="Linux" scheme="https://mritd.com/tags/linux/"/>
    
    <category term="Docker" scheme="https://mritd.com/tags/docker/"/>
    
    <category term="Kubernetes" scheme="https://mritd.com/tags/kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>Jekyll + Travis CI 自动化部署博客</title>
    <link href="https://mritd.com/2017/02/25/jekyll-blog-+-travis-ci-auto-deploy/"/>
    <id>https://mritd.com/2017/02/25/jekyll-blog-+-travis-ci-auto-deploy/</id>
    <published>2017-02-25T04:22:49.000Z</published>
    <updated>2017-02-25T04:22:49.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>由于 Github 访问过慢，所以博客一直放在自己的服务器上托管；博客采用了 Jekyll 生成静态展点，最近鼓捣了一下完成了 Travis CI 自动化部署，顺便在此记录下</p></blockquote><h3 id="一、原部署方式"><a href="#一、原部署方式" class="headerlink" title="一、原部署方式"></a>一、原部署方式</h3><h4 id="1-1、原部署流程"><a href="#1-1、原部署流程" class="headerlink" title="1.1、原部署流程"></a>1.1、原部署流程</h4><p>由于博客访问量不大，同时 jekyll 启动后会开启 http 服务器，还能自动监听文件变化实现刷新；所以以前的方式就是打了一个 docker 镜像，然后镜像每隔 15 分钟拉取 Github 仓库，实现定时更新，基本流程如下</p><ul><li>本地写博客 Markdown 文件 commit 到 Github</li><li>服务器上将 jekyll 打成 docker 镜像启动</li><li>服务器镜像内使用 crond 每隔 15 分钟拉取最新代码</li><li>jekyll 获得最新代码后自动刷新</li></ul><p>整体 “架构” 如下</p><p><img src="https://cdn.oss.link/markdown/44qmr.jpg" alt="老架构"></p><h4 id="1-2、存在问题"><a href="#1-2、存在问题" class="headerlink" title="1.2、存在问题"></a>1.2、存在问题</h4><p>按照以前的方式其实存在一个很大问题就是部署不及时，每次写完文章实际上都是自己 ssh 到服务器手动 pull 一下，感觉很繁琐；另一个大问题(这也不能算 bug) jekyll 的 rss 插件默认生成的 rss 引用地址为 <code>jekyll server -H x.x.x.x</code> 的监听地址，而容器化启动 jekyll 监听地址必然是 <code>0.0.0.0</code>；后果就是 feed.xml 无法访问，如下所示(这里监听的是 localhost)</p><p><img src="https://cdn.oss.link/markdown/fq9im.jpg" alt="feed error"></p><h4 id="1-3、新部署思路"><a href="#1-3、新部署思路" class="headerlink" title="1.3、新部署思路"></a>1.3、新部署思路</h4><p>从问题角度上来说，每次手动 pull 虽然有点烦，但是并不是大问题；而 rss 订阅由于网友反馈，再加个人强迫症感觉确实是个大毛病；随着试验发现，<strong>在进行 <code>jekyll build</code> 时生成的 feed.xml 中的引用地址会正确读取 _config.yml 中的网址</strong>；而 <code>jekyll server</code> 命令实际上也是先 build，然后生成静态文件到 <code>_site</code> 目录，最后搞个 http 服务器发布出去</p><p>基于以上试验可以得到一个简单的解决方案：不使用 <code>jekyll server</code> 启动，先 build 生成正确的 feed.xml 等静态文件，然后自己搞个 nginx 把它发布出去</p><h3 id="二、Travis-CI-自动化部署"><a href="#二、Travis-CI-自动化部署" class="headerlink" title="二、Travis CI 自动化部署"></a>二、Travis CI 自动化部署</h3><h4 id="2-1、任务拆分"><a href="#2-1、任务拆分" class="headerlink" title="2.1、任务拆分"></a>2.1、任务拆分</h4><p>从上面的结论上基本要实现自动化部署需要以下步骤：</p><ul><li>Github commit 后要能自动 build</li><li>生成的 _site 目录文件能实时更新到容器</li></ul><h4 id="2-2、Travis-CI-配置"><a href="#2-2、Travis-CI-配置" class="headerlink" title="2.2、Travis CI 配置"></a>2.2、Travis CI 配置</h4><h5 id="2-2-1、基本配置"><a href="#2-2-1、基本配置" class="headerlink" title="2.2.1、基本配置"></a>2.2.1、基本配置</h5><p>对于自动 build，好在 Travis CI 对于开源项目完全免费，并且能自动感知到 Github 的 commit；所以自动 build 生成 静态文件这个过程就由 Travis CI 完成，以下为配置过程</p><p>首先注册好 Travis CI 账号，然后点击最左侧 <code>+</code> 按钮添加项目</p><p><img src="https://cdn.oss.link/markdown/7axvx.jpg" alt="add repo"></p><p>在想要使用 Travis CI 的项目上开启 build</p><p><img src="https://cdn.oss.link/markdown/ouod9.jpg" alt="open"></p><p>点击设置按钮设置一下项目</p><p><img src="https://cdn.oss.link/markdown/p1cad.jpg" alt="set options"></p><h5 id="2-2-2、-travis-yml-配置"><a href="#2-2-2、-travis-yml-配置" class="headerlink" title="2.2.2、.travis.yml 配置"></a>2.2.2、.travis.yml 配置</h5><p>当项目内存在 <code>.travis.yml</code> 文件时，Travis CI 会按照其定义完成自动 build 过程，所以开启了上述配置以后还要在项目下创建 <code>.travis.yml</code> 配置文件，配置文件定义如下</p><div class="hljs code-wrapper"><pre><code class="hljs sh">language: rubyrvm:  - 2.3.3before_install:  - openssl aes-256-cbc -K <span class="hljs-variable">$encrypted_ecabfac08d8e_key</span> -iv <span class="hljs-variable">$encrypted_ecabfac08d8e_iv</span>    -<span class="hljs-keyword">in</span> id_rsa.enc -out ~/.ssh/id_rsa -d  - chmod 600 ~/.ssh/id_rsa  - <span class="hljs-built_in">echo</span> -e <span class="hljs-string">&quot;Host mritd.me\n\tStrictHostKeyChecking no\n&quot;</span> &gt;&gt; ~/.ssh/configscript:  - bundle install  - bundle <span class="hljs-built_in">exec</span> jekyll buildafter_success:  - git <span class="hljs-built_in">clone</span> https://github.com/mritd/mritd.me.git  - <span class="hljs-built_in">cd</span> mritd.me &amp;&amp; rm -rf * &amp;&amp; cp -r ../_site/* .  - git config user.name <span class="hljs-string">&quot;mritd&quot;</span>  - git config user.email <span class="hljs-string">&quot;mritd1234@gmail.com&quot;</span>  - git add --all .  - git commit -m <span class="hljs-string">&quot;Travis CI Auto Builder&quot;</span>  - git push --force https://<span class="hljs-variable">$DEPLOY_TOKEN</span>@github.com/mritd/mritd.me.git master  - ssh root@mritd.me <span class="hljs-string">&quot;docker restart mritd_jekyll_1&quot;</span>branches:  only:  - masterenv:  global:  - NOKOGIRI_USE_SYSTEM_LIBRARIES=<span class="hljs-literal">true</span></code></pre></div><p>其中 <code>language</code>声明使用的语言，<code>rvm</code> 是 ruby 的管理工具，并定义了 ruby 版本；<code>before_install</code> 定义了执行前的预处理动作，上面增加了一个密钥用于远程登录服务器；<code>script</code> 段真正定义了编译过程的命令，<code>after_success</code> 定义了如何 build 后如何处理发布物，<code>branches</code> 指定有哪些分支变动后触发 CI build，env 是一些环境变量，上面添加了一个变量(根据 jekyll 官方文档)用于加速 jekyll 编译(有些配置不理解往下看)</p><p><strong>具体 <code>.travis.yml</code> 配置请参考 <a href="https://docs.travis-ci.com/">官方文档</a></strong></p><h4 id="2-3、静态文件的自动更新"><a href="#2-3、静态文件的自动更新" class="headerlink" title="2.3、静态文件的自动更新"></a>2.3、静态文件的自动更新</h4><p>Travis CI 在完成 build 后会在 <code>_site</code> 目录生成博客的静态文件，而如如何将这些静态文件发送到服务器完成更新是个待解决的问题</p><h5 id="2-3-1、解决思路"><a href="#2-3-1、解决思路" class="headerlink" title="2.3.1、解决思路"></a>2.3.1、解决思路</h5><p>将 Travis CI 生成的静态文件推送到 Github，博客仍然 docker 化部署，采用 <code>nginx</code> + <code>静态文件</code> 方式；每次容器启动后都要从 Github pull 最新的静态文件，流程如下</p><ul><li>本地提交博客 Markdown 文件 到 Github</li><li>Github 触发 Travis CI 执行自动编译</li><li>Travis CI 编译后 push 静态文件到 Github</li><li>Travis CI 通知服务器重启博客容器</li><li>容器重启拉取最新静态文件完成更新</li></ul><p>流程图如下</p><p><img src="https://cdn.oss.link/markdown/8tro9.jpg" alt="new auto deploy"></p><h5 id="2-3-2、实现方法"><a href="#2-3-2、实现方法" class="headerlink" title="2.3.2、实现方法"></a>2.3.2、实现方法</h5><p>其实主要问题是从上面第三步开始：</p><p>Travis CI push 静态文件到 Github 通过 Github 的 token<br>实现授权，代码如下</p><div class="hljs code-wrapper"><pre><code class="hljs sh">after_success:  - git <span class="hljs-built_in">clone</span> https://github.com/mritd/mritd.me.git  - <span class="hljs-built_in">cd</span> mritd.me &amp;&amp; rm -rf * &amp;&amp; cp -r ../_site/* .  - git config user.name <span class="hljs-string">&quot;mritd&quot;</span>  - git config user.email <span class="hljs-string">&quot;mritd1234@gmail.com&quot;</span>  - git add --all .  - git commit -m <span class="hljs-string">&quot;Travis CI Auto Builder&quot;</span>  - git push --force https://<span class="hljs-variable">$DEPLOY_TOKEN</span>@github.com/mritd/mritd.me.git master</code></pre></div><p><code>$DEPLOY_TOKEN</code> 是从 Github 授权得到的，然后给于相应权限即可</p><p><img src="https://cdn.oss.link/markdown/pco7k.jpg" alt="Github token"></p><p><strong>关于代码中 <code>$DEPLOY_TOKEN</code> 这种重要的密码类变量，Travis CI 在每个项目下提供了设置环境变量功能，如下图</strong></p><p><img src="https://cdn.oss.link/markdown/7zmj2.jpg" alt="setting"></p><p><img src="https://cdn.oss.link/markdown/0b91x.jpg" alt="add env"></p><p><strong>设置后可在 <code>.travis.yml</code> 中直接引用，不过注意一定要关闭 <code>Display value in build log</code> 功能，否则 CI log 中会显示 <code>export XXXX=XXXX</code> 这种 log 从而暴露重要密码(公有项目的 log 别人可以查看的)；如果开启了那么尽快找到相应 build 并删除 log 日志，如下</strong></p><p><img src="https://cdn.oss.link/markdown/kal69.jpg" alt="delete log"></p><p>在成功 push 了静态文件以后，就要实现服务器的自动更新，自动更新很简单，只需要写个脚本让容器启动后自动 pull 即可，这里不再阐述；下面说一下怎么通知服务器重启容器，这里的思路很简单，<strong>让 CI ssh 上去执行一些 docker 命令即可</strong>；但是有个很大问题是 <strong>SSH 密码怎么整？</strong></p><p><strong>Travis CI 提供了存放加密文件的方式，文档见 <a href="https://docs.travis-ci.com/user/encrypting-files/">这里</a></strong></p><p>简单的说就是将你的 ssh 私钥加密以后扔进去即可，按照稳当的步骤很简单：</p><ul><li>先安装 ruby 环境，然后用 gem 装 travis (gem install travis)</li><li>然后登陆 travis (travis login)</li><li>登陆后加密文件 (travis encrypt-file xxxx)，注意要在 <code>.travis.yml</code> 同级目录下执行，待加密文件可在任意位置</li><li>travis 会在 <code>.travis.yml</code> 写入 <code>before_install</code> 段解密还原回该文件，如果是 ssh 密钥的话参考上面的配置再改一下权限即可</li></ul><p>最后一个小问题是可能会有主机信任问题，因为 CI 服务器第一次连接我的服务器会出现 ssh 主机验证，官方给出的做法是添加 addons 配置</p><div class="hljs code-wrapper"><pre><code class="hljs sh">addons:  ssh_known_hosts: mritd.me</code></pre></div><h3 id="三、一些想法"><a href="#三、一些想法" class="headerlink" title="三、一些想法"></a>三、一些想法</h3><p>这只是一个小博客，所以服务中断以下无所谓；如果大型部署肯定不会直接重启容器，至少应该是 k8s 滚动升级等措施；与服务器通讯也不应该采用 ssh 方式，虽然 Travis CI 做了加密，但是总感觉不那么稳妥，最好应该写个程序开放一个 REST 接口，并做好授权，必要的话需要开启一次性认证令牌那种方式，其他的后续接着优化 ( :</p>]]></content>
    
    
    <summary type="html">由于 Github 访问过慢，所以博客一直放在自己的服务器上托管；博客采用了 Jekyll 生成静态展点，最近鼓捣了一下完成了 Travis CI 自动化部署，顺便在此记录下</summary>
    
    
    
    <category term="Linux" scheme="https://mritd.com/categories/linux/"/>
    
    
    <category term="Linux" scheme="https://mritd.com/tags/linux/"/>
    
  </entry>
  
  <entry>
    <title>Java CAS 理解</title>
    <link href="https://mritd.com/2017/02/06/java-cas/"/>
    <id>https://mritd.com/2017/02/06/java-cas/</id>
    <published>2017-02-06T00:35:19.000Z</published>
    <updated>2017-02-06T00:35:19.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>CAS(Compare and Swap) 是利用底层硬件平台特性，实现原子性操作的算法，Java 1.5 以后 JUC(java.util.concurrent) 实现主要以此为基础；找了不少资料以下记录一下个人对于 CAS 的理解(部分资料 copy 的)</p></blockquote><h3 id="一、CAS-简述"><a href="#一、CAS-简述" class="headerlink" title="一、CAS 简述"></a>一、CAS 简述</h3><p>从最基础的 Java 中的 <code>i++</code> 操作来说，<code>i++</code> 并非原子操作，实质上相当于先读取 i 值，然后在内存中创建缓存变量保存 ++ 后结果，最后写会变量 i；而在这期间 i 变量都可能被其他线程读或写，从而造成线程安全性问题</p><p>CAS 算法大致原理是：在对变量进行计算之前(如 ++ 操作)，首先读取原变量值，称为 <code>旧的预期值 A</code>，然后在更新之前再获取当前内存中的值，称为 <code>当前内存值 V</code>，如果 <code>A==V</code> 则说明变量从未被其他线程修改过，此时将会写入新值 B，如果 <code>A!=V</code> 则说明变量已经被其他线程修改过，当前线程应当什么也不做；</p><h3 id="二、-CAS-原理"><a href="#二、-CAS-原理" class="headerlink" title="二、 CAS 原理"></a>二、 CAS 原理</h3><h4 id="2-1、openjdk-中-CAS-实现"><a href="#2-1、openjdk-中-CAS-实现" class="headerlink" title="2.1、openjdk 中 CAS 实现"></a>2.1、openjdk 中 CAS 实现</h4><p>翻了一下 AtomicInteger 的源码，发现其实质上都会调用到 Unsafe 类中的方法，而 Unsafe 中大部分方法是 native 的，也就是说实质使用 JNI 上调用了 C 来沟通底层硬件完成 CAS；具体调用源码(openjdk)为 <code>unsafe.cpp</code>、<code>atomic.cpp</code>、<code>atomicwindowsx86.inline.hpp</code>；以下是一部分代码片段(不懂 C…)</p><div class="hljs code-wrapper"><pre><code class="hljs c"><span class="hljs-comment">// Adding a lock prefix to an instruction on MP machine</span><span class="hljs-comment">// VC++ doesn&#x27;t like the lock prefix to be on a single line</span><span class="hljs-comment">// so we can&#x27;t insert a label after the lock prefix.</span><span class="hljs-comment">// By emitting a lock prefix, we can define a label after it.</span><span class="hljs-meta">#<span class="hljs-meta-keyword">define</span> LOCK_IF_MP(mp) __asm cmp mp, 0  \</span>                       __asm je L0      \                       __asm _emit <span class="hljs-number">0xF0</span> \                       __asm L0:<span class="hljs-function"><span class="hljs-keyword">inline</span> jlong    <span class="hljs-title">Atomic::cmpxchg</span>    <span class="hljs-params">(jlong    exchange_value, <span class="hljs-keyword">volatile</span> jlong*    dest, jlong    compare_value)</span> </span>&#123;  <span class="hljs-keyword">return</span> (*os::atomic_cmpxchg_long_func)(exchange_value, dest, compare_value);&#125;</code></pre></div><blockquote><p>下面开始抄的 （：…. 看 C 完全懵逼</p></blockquote><p>如上面源代码所示，程序会根据当前处理器的类型来决定是否为 cmpxchg 指令添加lock前缀；如果程序跑在多核处理器上，就为 cmpxchg 指令加上 lock 前缀 (lock cmpxchg)；反之，如果程序跑在单核处理器上，就省略 lock 前缀 (单处理器自身会维护单处理器内的顺序一致性，不需要 lock 前缀提供的内存屏障效果)</p><h4 id="2-2、intel-lock-前缀说明"><a href="#2-2、intel-lock-前缀说明" class="headerlink" title="2.2、intel lock 前缀说明"></a>2.2、intel lock 前缀说明</h4><p>intel 的手册对lock前缀的说明如下:</p><ul><li>确保对内存的读-改-写操作原子执行。在Pentium及Pentium之前的处理器中，带有lock前缀的指令在执行期间会锁住总线，使得其他处理器暂时无法通过总线访问内存。很显然，这会带来昂贵的开销。从Pentium 4，Intel Xeon及P6处理器开始，intel在原有总线锁的基础上做了一个很有意义的优化：如果要访问的内存区域（area of memory）在lock前缀指令执行期间已经在处理器内部的缓存中被锁定（即包含该内存区域的缓存行当前处于独占或以修改状态），并且该内存区域被完全包含在单个缓存行（cache line）中，那么处理器将直接执行该指令。由于在指令执行期间该缓存行会一直被锁定，其它处理器无法读/写该指令要访问的内存区域，因此能保证指令执行的原子性。这个操作过程叫做缓存锁定（cache locking），缓存锁定将大大降低lock前缀指令的执行开销，但是当多处理器之间的竞争程度很高或者指令访问的内存地址未对齐时，仍然会锁住总线。</li><li>禁止该指令与之前和之后的读和写指令重排序。</li><li>把写缓冲区中的所有数据刷新到内存中</li></ul><h4 id="2-3、cpu-锁"><a href="#2-3、cpu-锁" class="headerlink" title="2.3、cpu 锁"></a>2.3、cpu 锁</h4><p>关于CPU的锁有如下3种:</p><ul><li>处理器自动保证基本内存操作的原子性</li></ul><p>首先处理器会自动保证基本的内存操作的原子性。处理器保证从系统内存当中读取或者写入一个字节是原子的，意思是当一个处理器读取一个字节时，其他处理器不能访问这个字节的内存地址。奔腾6和最新的处理器能自动保证单处理器对同一个缓存行里进行16/32/64位的操作是原子的，但是复杂的内存操作处理器不能自动保证其原子性，比如跨总线宽度，跨多个缓存行，跨页表的访问。但是处理器提供总线锁定和缓存锁定两个机制来保证复杂内存操作的原子性。</p><ul><li>使用总线锁保证原子性</li></ul><p>第一个机制是通过总线锁保证原子性。如果多个处理器同时对共享变量进行读改写（i++就是经典的读改写操作）操作，那么共享变量就会被多个处理器同时进行操作，这样读改写操作就不是原子的，操作完之后共享变量的值会和期望的不一致，举个例子：如果i=1,我们进行两次i++操作，我们期望的结果是3，但是有可能结果是2；</p><p>原因是有可能多个处理器同时从各自的缓存中读取变量i，分别进行加一操作，然后分别写入系统内存当中。那么想要保证读改写共享变量的操作是原子的，就必须保证CPU1读改写共享变量的时候，CPU2不能操作缓存了该共享变量内存地址的缓存。</p><p>处理器使用总线锁就是来解决这个问题的。所谓总线锁就是使用处理器提供的一个LOCK＃信号，当一个处理器在总线上输出此信号时，其他处理器的请求将被阻塞住,那么该处理器可以独占使用共享内存。</p><ul><li>使用缓存锁保证原子性</li></ul><p>第二个机制是通过缓存锁定保证原子性。在同一时刻我们只需保证对某个内存地址的操作是原子性即可，但总线锁定把CPU和内存之间通信锁住了，这使得锁定期间，其他处理器不能操作其他内存地址的数据，所以总线锁定的开销比较大，最近的处理器在某些场合下使用缓存锁定代替总线锁定来进行优化。</p><p>频繁使用的内存会缓存在处理器的L1，L2和L3高速缓存里，那么原子操作就可以直接在处理器内部缓存中进行，并不需要声明总线锁，在奔腾6和最近的处理器中可以使用“缓存锁定”的方式来实现复杂的原子性。所谓“缓存锁定”就是如果缓存在处理器缓存行中内存区域在LOCK操作期间被锁定，当它执行锁操作回写内存时，处理器不在总线上声言LOCK＃信号，而是修改内部的内存地址，并允许它的缓存一致性机制来保证操作的原子性，因为缓存一致性机制会阻止同时修改被两个以上处理器缓存的内存区域数据，当其他处理器回写已被锁定的缓存行的数据时会起缓存行无效，在例1中，当CPU1修改缓存行中的i时使用缓存锁定，那么CPU2就不能同时缓存了i的缓存行。</p><p>但是有两种情况下处理器不会使用缓存锁定。第一种情况是：当操作的数据不能被缓存在处理器内部，或操作的数据跨多个缓存行（cache line），则处理器会调用总线锁定。第二种情况是：有些处理器不支持缓存锁定。对于Inter486和奔腾处理器,就算锁定的内存区域在处理器的缓存行中也会调用总线锁定。</p><p>以上两个机制我们可以通过Inter处理器提供了很多LOCK前缀的指令来实现。比如位测试和修改指令BTS，BTR，BTC，交换指令XADD，CMPXCHG和其他一些操作数和逻辑指令，比如ADD（加），OR（或）等，被这些指令操作的内存区域就会加锁，导致其他处理器不能同时访问它。</p><h3 id="三、CAS-缺点"><a href="#三、CAS-缺点" class="headerlink" title="三、CAS 缺点"></a>三、CAS 缺点</h3><h4 id="3-1、ABA-问题"><a href="#3-1、ABA-问题" class="headerlink" title="3.1、ABA 问题"></a>3.1、ABA 问题</h4><p>由于 CAS 设计机制就是获取某两个时刻(初始预期值和当前内存值)变量值，并进行比较更新，所以说如果在获取初始预期值和当前内存值这段时间间隔内，变量值由 A 变为 B 再变为 A，那么对于 CAS 来说是不可感知的，但实际上变量已经发生了变化；解决办法是在每次获取时加版本号，并且每次更新对版本号 +1，这样当发生 ABA 问题时通过版本号可以得知变量被改动过</p><p>JDK 1.5 以后的 AtomicStampedReference 类就提供了此种能力，其中的 compareAndSet 方法就是 <strong>首先检查当前引用是否等于预期引用，并且当前标志是否等于预期标志，如果全部相等，则以原子方式将该引用和该标志的值设置为给定的更新值。</strong></p><h4 id="3-2、循环时间长开销大"><a href="#3-2、循环时间长开销大" class="headerlink" title="3.2、循环时间长开销大"></a>3.2、循环时间长开销大</h4><p>所谓循环时间长开销大问题就是当 CAS 判定变量被修改了以后则放弃本次修改，<strong>但往往为了保证数据正确性该计算会以循环的方式再次发起 CAS，如果多次 CAS 判定失败，则会产生大量的时间消耗和性能浪费</strong>；如果JVM能支持处理器提供的pause指令那么效率会有一定的提升，pause指令有两个作用，第一它可以延迟流水线执行指令（de-pipeline）,使CPU不会消耗过多的执行资源，延迟的时间取决于具体实现的版本，在一些处理器上延迟时间是零。第二它可以避免在退出循环的时候因内存顺序冲突（memory order violation）而引起CPU流水线被清空（CPU pipeline flush），从而提高CPU的执行效率。</p><h4 id="3-3、只能保证一个共享变量的原子操作"><a href="#3-3、只能保证一个共享变量的原子操作" class="headerlink" title="3.3、只能保证一个共享变量的原子操作"></a>3.3、只能保证一个共享变量的原子操作</h4><p>CAS 只对单个共享变量有效，当操作涉及跨多个共享变量时 CAS 无效；<strong>从 JDK 1.5开始提供了 AtomicReference 类来保证引用对象之间的原子性，你可以把多个变量放在一个对象里来进行 CAS 操作</strong></p><h3 id="四、JUC-实现"><a href="#四、JUC-实现" class="headerlink" title="四、JUC 实现"></a>四、JUC 实现</h3><p>由于 Java 的 CAS 同时具有 volatile 读和volatile写的内存语义，因此 Java 线程之间的通信现在有了下面四种方式：</p><ul><li>A线程写 volatile 变量，随后B线程读这个 volatile 变量</li><li>A线程写 volatile 变量，随后B线程用 CAS 更新这个 volatile 变量</li><li>A线程用 CAS 更新一个 volatile 变量，随后B线程用 CAS 更新这个 volatile 变量</li><li>A线程用 CAS 更新一个 volatile 变量，随后B线程读这个 volatile 变量</li></ul><p>Java 的 CAS 会使用现代处理器上提供的高效机器级别原子指令，这些原子指令以原子方式对内存执行读-改-写操作，这是在多处理器中实现同步的关键（从本质上来说，能够支持原子性读-改-写指令的计算机器，是顺序计算图灵机的异步等价机器，因此任何现代的多处理器都会去支持某种能对内存执行原子性读-改-写操作的原子指令）。同时，volatile 变量的读/写和 CAS 可以实现线程之间的通信。把这些特性整合在一起，就形成了整个 concurrent 包得以实现的基石。如果我们仔细分析 concurrent 包的源代码实现，会发现一个通用化的实现模式：</p><ul><li>首先，声明共享变量为 volatile；</li><li>然后，使用 CAS 的原子条件更新来实现线程之间的同步；</li><li>同时，配合以 volatile 的读/写和 CAS 所具有的 volatile 读和写的内存语义来实现线程之间的通信。</li></ul><p>AQS，非阻塞数据结构和原子变量类 ( <code>java.util.concurrent.atomic</code> 包中的类)，这些 concurrent 包中的基础类都是使用这种模式来实现的，而 concurrent 包中的高层类又是依赖于这些基础类来实现的。从整体来看，concurrent 包的实现示意图如下：</p><p><img src="https://cdn.oss.link/markdown/l72qr.jpg" alt="concurrent"></p><h3 id="五、JDK9-改变"><a href="#五、JDK9-改变" class="headerlink" title="五、JDK9 改变"></a>五、JDK9 改变</h3><p>随着 JDK9 即将发布，CAS 相关主要类 <code>Unsafe</code> 有些变动，以下变动主要由 <code>Mikael Vidstedt</code> 提交，更新内容如下</p><ul><li>避免代码重复，<code>sun.misc.Unsafe</code> 将全部实现委托给 <code>jdk.internal.misc.Unsafe</code>，这意味着java虚拟机(特别是 <code>unsafe.cpp</code> ）不再需要关心s.m.Unsafe的实现。</li><li><code>s.m.Unsafe</code> 的委托方法通常会被内联，但是为了避免性能下降的风险，仍然添加了 <code>@ForceInline</code> 注解</li><li>更新文档，指明用户应该确保Unsafe类的参数正确</li><li>参数检查从Unsage.cpp移入java，简化本地代码以及允许JIT进一步优化</li><li>放松了特定参数的检查，比方说最近引入的 <code>U.copySwapMemory</code> 没有检查空指针。具体原因可以参考 <code>j.i.m.U.checkPointer</code> 的文档。除了 <code>U.copySwapMemory</code>，现在Unsafe类方法也都没有对参数执行NULL检查</li><li>在 <code>U.copySwapMemory</code> 类的基础上，对 <code>j.i.m.U.copyMemory</code> 增加了一个测试案例。请随时提醒我合并过来（本该如此）</li></ul><p>在 Mikael Vidstedt 看来，Usage 类的清理算是 “相当激进” 了，值得注意的地方有：</p><ul><li>Unsafe_方法以及 <code>unsafe.cpp</code> 中的其他本地方法被申明为静态方法</li><li>新增 <code>unsafe.hpp</code> 代码文件，文件中移入VM其他组件的一些方法。移除部分 <code>extern</code> 函数声明（不要过度使用extern）</li><li>对于不怎么用到的UNSAFE_LEAF，移除警告性质的注释（没有必要，只是个VM_LEAF）</li><li>一些简单的leaf方法使用UNSAFE_LEAF</li><li>UNSAFE_ENTRY/UNSAFE_END代码块新增大括号，帮助自动缩进</li><li>移除未使用的Unsafe_&lt;…&gt;##140形式的函数和宏</li><li>更新宏参数，与unsafe.cpp的宏定义保持一致</li><li>更换带断言的参数检查，正如前面提及，这些检查移入了 <code>j.i.m.Unsafe</code>，移除所有 <code>s.m.Unsafe</code> 相关的代码</li></ul><p>本文参考自 <a href="http://zl198751.iteye.com/blog/1848575">JAVA CAS原理深度分析</a>、<a href="http://zkread.com/article/780437.html">Java 9发布在即，Oracle OpenJDK着手优化Unsafe类</a></p>]]></content>
    
    
    <summary type="html">CAS(Compare and Swap) 是利用底层硬件平台特性，实现原子性操作的算法，Java 1.5 以后 JUC(java.util.concurrent) 实现主要以此为基础；找了不少资料以下记录一下个人对于 CAS 的理解(部分资料 copy 的)</summary>
    
    
    
    <category term="Java" scheme="https://mritd.com/categories/java/"/>
    
    
    <category term="Java" scheme="https://mritd.com/tags/java/"/>
    
  </entry>
  
  <entry>
    <title>Logical Volume Manager 笔记</title>
    <link href="https://mritd.com/2017/01/27/logical-volume-manager-note/"/>
    <id>https://mritd.com/2017/01/27/logical-volume-manager-note/</id>
    <published>2017-01-27T02:05:26.000Z</published>
    <updated>2017-01-27T02:05:26.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>Logical Volume Manager 简称 LVM，LVM 是一种可用在Linux内核的逻辑分卷管理器；可用于管理磁盘驱动器或其他类似的大容量存储设备；LVM 依赖于 内核 device mapper 机制，可以实现动态伸缩逻辑卷大小，而屏蔽底层硬件变化，为后期磁盘扩展提供便利</p></blockquote><h3 id="一、LVM-简介"><a href="#一、LVM-简介" class="headerlink" title="一、LVM 简介"></a>一、LVM 简介</h3><p>LVM 技术依据底层内核的 device mapper 机制，聚合底层硬件存储设备存储空间，在上层抽象出可扩展的逻辑分区；LVM 主要术语(磁盘术语)如下</p><ul><li><code>PV</code> : 底层物理存储设备，如 <code>/dev/sda</code></li><li><code>VG</code> : 卷组，意将多个 <code>PV</code> 组合后的抽象存储介质统称</li><li><code>PE</code> : <code>VG</code> 将 <code>PV</code> 聚合后，需向上层提供存储能力，而 <code>PE</code> 即为 <code>VG</code> 中最小的存储单位，<strong>一般默认为 4MB</strong></li><li><code>LV</code> : 在 <code>VG</code> 之上划分一定量的存储空间，形成逻辑分区，即为 <code>LV</code></li><li><code>LE</code> : <code>LV</code> 从 <code>VG</code> 之上划分，即 <code>LV</code> 实质上有 <code>VG</code> 上最小存储单位 <code>PE</code> 构成；但是 <code>PE</code> 构成 <code>LV</code> 之后，又称作 <code>LE</code></li></ul><p><strong>以上各术语(定义)之间关系如下图所示</strong></p><p><img src="https://cdn.oss.link/markdown/mbzrv.jpg" alt="PV_VG_LV"></p><h3 id="二、LVM-相关命令"><a href="#二、LVM-相关命令" class="headerlink" title="二、LVM 相关命令"></a>二、LVM 相关命令</h3><h4 id="2-1、PV-管理工具"><a href="#2-1、PV-管理工具" class="headerlink" title="2.1、PV 管理工具"></a>2.1、PV 管理工具</h4><ul><li>pvs : 简要显示 pv 信息</li><li>pvdisplay : 详细显示 pv 信息</li><li>pvcreate : 创建 pv</li><li>pvmove : 移动 pv (再删除 vg 前必须移动有数据的 pv)</li><li>pvremove : 删除 pv</li></ul><h4 id="2-2、VG-管理命令"><a href="#2-2、VG-管理命令" class="headerlink" title="2.2、VG 管理命令"></a>2.2、VG 管理命令</h4><ul><li>vgs : 简要显示 vg 信息</li><li>vgdisplay : 详细显示 vg 信息</li><li>vgcreate : 创建 vg</li><li>vgrename : 重命名 vg</li><li>vgremove : 删除 vg</li><li>vgscan : 扫描 vg</li><li>vgextend : 扩展 vg</li><li>vgsplit : 切割 vg (将 vg 中 pv 移动到已存在 vg 中)</li><li>vgreduce : 缩减 vg (删除 vg 内指定 pv)</li></ul><h4 id="2-3、LV-管理命令"><a href="#2-3、LV-管理命令" class="headerlink" title="2.3、LV 管理命令"></a>2.3、LV 管理命令</h4><ul><li>lvs : 简要显示 lv 信息</li><li>lvdisplay : 详细显示 lv 信息</li><li>lvcreate : 创建逻辑卷</li><li>lvextend : 扩展逻辑卷</li><li>lvreduce : 缩减逻辑分区</li><li>lvremove : 删除逻辑卷</li></ul><h5 id="2-3-1、lvcreate"><a href="#2-3-1、lvcreate" class="headerlink" title="2.3.1、lvcreate"></a>2.3.1、lvcreate</h5><p>lvcreate 用于从 vg 上创建 lv 逻辑分区，基本命令格式如下 :</p><div class="hljs code-wrapper"><pre><code class="hljs sh">lvcreate -L <span class="hljs-comment">#[mMgGtT] -n NAME VolumeGroup</span></code></pre></div><p>其常用选项如下 : </p><ul><li>-L | –size : 指定要创建的 lv 大小，采用 <code>+5G</code> 这种方式</li><li>-l | –extents : 指定大小范围，类似于 fdisk 分区时选择盘区范围</li><li>-s | –snapshot : 创建一个快照卷，创建快照卷时后面必须跟快照卷名称</li><li>p : 权限(r、rw)</li></ul><h5 id="2-3-2、lvextend"><a href="#2-3-2、lvextend" class="headerlink" title="2.3.2、lvextend"></a>2.3.2、lvextend</h5><p>lvextend 支持在线扩展扩展逻辑分区大小，命令格式如下 : </p><div class="hljs code-wrapper"><pre><code class="hljs sh">lvextend -L [+]<span class="hljs-comment">#[mMgGtT] /dev/VG_NAME/LV_NAME</span></code></pre></div><p><strong>注意 -L 选项后 + 可以省略，但是省略后代表总容量，如当前 LV 2G，需扩展 3G，则不写 + 需要输入 5G</strong></p><p><strong>lv 扩展后并不能立即体现在 df 等命令上，因为虽然逻辑卷已经扩展，但是文件系统尚未扩展，对于 ext 系列的文件系统可采用 <code>resize2fs /dev/VG_NAME/LV_NAME</code> 的方式刷新文件系统，其他分区格式需采用其他工具</strong></p><h5 id="2-3-3、lvreduce"><a href="#2-3-3、lvreduce" class="headerlink" title="2.3.3、lvreduce"></a>2.3.3、lvreduce</h5><p><strong>在缩减 lv 时，必须先卸载文件系统，然后缩减文件系统，并且缩减文件系统后要强制做文件系统检测，以防止发生损坏，操作命令及顺序如下 :</strong></p><ul><li>umount /dev/VG_NAME/LV_NAME</li><li>e2fsck -f /dev/VG_NAME/LV_NAME</li><li>resize2fs /dev/VG_NAME/LV_NAME #[mMgGtT]</li><li>lvreduce [-]#[mMgGtT] /dev/VG_NAME/LV_NAME</li><li>mount /dev/VG_NAME/LV_NAME DIR</li></ul>]]></content>
    
    
    <summary type="html">Logical Volume Manager 简称 LVM，LVM 是一种可用在Linux内核的逻辑分卷管理器；可用于管理磁盘驱动器或其他类似的大容量存储设备；LVM 依赖于 内核 device mapper 机制，可以实现动态伸缩逻辑卷大小，而屏蔽底层硬件变化，为后期磁盘扩展提供便利</summary>
    
    
    
    <category term="Linux" scheme="https://mritd.com/categories/linux/"/>
    
    
    <category term="Linux" scheme="https://mritd.com/tags/linux/"/>
    
  </entry>
  
  <entry>
    <title>利用 frp 进行内网穿透</title>
    <link href="https://mritd.com/2017/01/21/use-frp-for-internal-network-wear/"/>
    <id>https://mritd.com/2017/01/21/use-frp-for-internal-network-wear/</id>
    <published>2017-01-21T06:45:42.000Z</published>
    <updated>2017-01-21T06:45:42.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>最近新入了一台小主机，家里还有个树莓派，索性想通过小主机跑点东西，然后通过外网访问家里的设备；不过鉴于大天朝内网环境，没有公网 IP 并且多层路由的情况下只能选择使用内网穿透方案，以下记录了一下使用 frp 进行内网穿透的过程</p></blockquote><h3 id="一、内网穿透原理"><a href="#一、内网穿透原理" class="headerlink" title="一、内网穿透原理"></a>一、内网穿透原理</h3><p>简单地说，内网穿透依赖于 NAT 原理，根据 NAT 设备不同大致可分为以下 4 大类(前3种NAT类型可统称为cone类型)：</p><ul><li>全克隆(Full Cone)：NAT 把所有来自相同内部 IP 地址和端口的请求映射到相同的外部 IP 地址和端口上，任何一个外部主机均可通过该映射反向发送 IP 包到该内部主机</li><li>限制性克隆(Restricted Cone)：NAT 把所有来自相同内部 IP 地址和端口的请求映射到相同的外部 IP 地址和端口；但是，只有当内部主机先给 IP 地址为 X 的外部主机发送 IP 包时，该外部主机才能向该内部主机发送 IP 包</li><li>端口限制性克隆(Port Restricted Cone)：端口限制性克隆与限制性克隆类似，只是多了端口号的限制，即只有内部主机先向 IP 地址为 X，端口号为 P 的外部主机发送1个 IP 包,该外部主机才能够把源端口号为 P 的 IP 包发送给该内部主机</li><li>对称式NAT(Symmetric NAT)：这种类型的 NAT 与上述3种类型的不同，在于当同一内部主机使用相同的端口与不同地址的外部主机进行通信时， NAT 对该内部主机的映射会有所不同；对称式 NAT 不保证所有会话中的私有地址和公开 IP 之间绑定的一致性；相反，它为每个新的会话分配一个新的端口号；导致此种 NAT 根本没法穿透</li></ul><p>内网穿透的作用就是利用以上规则，创建一条从外部服务器到内部设备的 “隧道”，具体的 NAT 原理等可参考 <a href="http://www.cnblogs.com/eyye/archive/2012/10/23/2734807.html">内网打洞</a>、<a href="http://blog.csdn.net/hzhsan/article/details/45038265">网络地址转换NAT原理</a></p><h3 id="二、环境准备"><a href="#二、环境准备" class="headerlink" title="二、环境准备"></a>二、环境准备</h3><p>实际上根据以上 NAT 规则，基本上大部分家用设备和运营商上级路由等都在前三种规则之中，所以只需要借助成熟的内网穿透工具即可，以下为本次穿透环境</p><ul><li>最新版本 frp</li><li>一台公网 VPS 服务器</li><li>内网一台服务器，最好 Linux 系统</li></ul><h3 id="三、服务端搭建"><a href="#三、服务端搭建" class="headerlink" title="三、服务端搭建"></a>三、服务端搭建</h3><p>服务器作为公网访问唯一的固定地址，即作为 server 端；内网客户端作为 client 端，会主动向 server 端创建连接，此时再从 server 端反向发送数据即可实现内网穿透</p><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment"># 下载 frp 并解压</span>wget https://github.com/fatedier/frp/releases/download/v0.9.3/frp_0.9.3_linux_amd64.tar.gztar -zxvf frp_0.9.3_linux_amd64.tar.gz<span class="hljs-built_in">cd</span> frp_0.9.3_linux_amd64</code></pre></div><p>编辑 <code>frps.ini</code> 如下</p><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment"># 通用配置段</span>[common]<span class="hljs-comment"># frp 监听地址</span>bind_addr = 0.0.0.0bind_port = 7000<span class="hljs-comment"># 如果需要代理 web(http) 服务，则开启此端口</span>vhost_http_port = 4080vhost_https_port = 4443<span class="hljs-comment"># frp 控制面板</span>dashboard_port = 7500dashboard_user = admindashboard_pwd = admin<span class="hljs-comment"># 默认日志输出位置(这里输出到标准输出)</span>log_file = /dev/stdout<span class="hljs-comment"># 日志级别，支持: debug, info, warn, error</span>log_level = infolog_max_days = 3<span class="hljs-comment"># 是否开启特权模式(特权模式下，客户端更改配置无需更新服务端)</span>privilege_mode = <span class="hljs-literal">true</span><span class="hljs-comment"># 授权 token 建议随机生成</span>privilege_token = HE7qTtW8Lg83UDKY<span class="hljs-comment"># 特权模式下允许分配的端口(避免端口滥用)</span>privilege_allow_ports = 4000-50000<span class="hljs-comment"># 心跳检测超时</span><span class="hljs-comment"># heartbeat_timeout = 30</span><span class="hljs-comment"># 后端连接池最大连接数量</span>max_pool_count = 100<span class="hljs-comment"># 口令超时时间</span>authentication_timeout = 900<span class="hljs-comment"># 子域名(特权模式需下将 *.domain.com 解析到公网服务器)</span>subdomain_host = domain.com<span class="hljs-comment"># 开启 ssh 穿透(可通过外网链接内网 ssh)</span>[ssh]<span class="hljs-built_in">type</span> = tcpauth_token = M4P2xsH6RuUkbP9dbind_addr = 0.0.0.0listen_port = 6000<span class="hljs-comment"># 开启 dns 查询穿透(个人用不上)</span><span class="hljs-comment">#[dns]</span><span class="hljs-comment">#type = udp</span><span class="hljs-comment">#auth_token = M4P2xsH6RuUkbP9d</span><span class="hljs-comment">#bind_addr = 0.0.0.0</span><span class="hljs-comment">#listen_port = 5353</span></code></pre></div><p><strong>其他具体配置说明请参考frp <a href="https://github.com/fatedier/frp/blob/master/README_zh.md"> README</a> 文档</strong></p><p>设置完成后执行 <code>./frps -c frps.ini</code> 启动即可</p><h3 id="四、客户端配置"><a href="#四、客户端配置" class="headerlink" title="四、客户端配置"></a>四、客户端配置</h3><p>客户端作为发起链接的主动方，只需要正确配置服务器地址，以及要映射客户端的哪些服务端口等即可</p><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment"># 下载 frp 并解压</span>wget https://github.com/fatedier/frp/releases/download/v0.9.3/frp_0.9.3_linux_amd64.tar.gztar -zxvf frp_0.9.3_linux_amd64.tar.gz<span class="hljs-built_in">cd</span> frp_0.9.3_linux_amd64</code></pre></div><p>编辑 <code>frpc.ini</code> 文件</p><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment"># 通用配置</span>[common]<span class="hljs-comment"># 服务端地址及端口</span>server_addr = domain.comserver_port = 7000log_file = /dev/stdoutlog_level = infolog_max_days = 3<span class="hljs-comment"># 授权 token，必须与服务端保持一致方可实现映射</span>auth_token = ouRRXE4tk69oNZ6f<span class="hljs-comment"># 特权模式 token，同样要与服务端一致</span>privilege_token = VfJiyhDVJ38t7Qu6<span class="hljs-comment"># 心跳检测</span><span class="hljs-comment"># heartbeat_interval = 10</span><span class="hljs-comment"># heartbeat_timeout = 30</span><span class="hljs-comment"># 将本地 ssh 映射到服务器</span>[ssh]<span class="hljs-built_in">type</span> = tcplocal_ip = 127.0.0.1local_port = 22<span class="hljs-comment"># 是否开启加密(流量加密，应对防火墙)</span>use_encryption = <span class="hljs-literal">true</span><span class="hljs-comment"># 是否开启压缩</span>use_gzip = <span class="hljs-literal">true</span><span class="hljs-comment"># dns 用不到</span><span class="hljs-comment">#[dns]</span><span class="hljs-comment">#type = udp</span><span class="hljs-comment">#local_ip = 114.114.114.114</span><span class="hljs-comment">#local_port = 53</span><span class="hljs-comment"># 发布本地 web 服务</span>[web01]<span class="hljs-built_in">type</span> = httplocal_ip = 127.0.0.1local_port = 8000<span class="hljs-comment"># 是否启用特权模式(特权模式下服务端无需配置)</span>privilege_mode = <span class="hljs-literal">true</span>use_encryption = <span class="hljs-literal">true</span>use_gzip = <span class="hljs-literal">true</span><span class="hljs-comment"># 连接数量</span>pool_count = 20<span class="hljs-comment"># 是否开启密码访问</span><span class="hljs-comment">#http_user = admin</span><span class="hljs-comment">#http_pwd = admin</span><span class="hljs-comment"># 子域名，当服务端开启特权模式，并且将 &quot;*.domain.com&quot; 解析到服务端 IP后，</span><span class="hljs-comment"># 客户端在选项(privilege_mode)中声明当前映射为特权模式时，服务器端就会</span><span class="hljs-comment"># 给于一个 &quot;subdomain.domain.com&quot; 映射，此示例将在服务端开一个</span><span class="hljs-comment"># &quot;http://test.domain.com/:4080&quot; 的服务映射到内网 8000 端口上</span>subdomain = <span class="hljs-built_in">test</span></code></pre></div><p>最后使用 <code>./frpc -c frpc.ini</code> 启动即可</p><h3 id="五、测试"><a href="#五、测试" class="headerlink" title="五、测试"></a>五、测试</h3><p>服务端和客户端同时开启完成后，即可访问 <code>http://domain.com:7500</code> 进入 frp 控制面板，如下</p><p><img src="https://cdn.oss.link/markdown/1d8pq.jpg" alt="dashboard"></p><p>此时通过 <code>ssh root@domain.com -p 6000</code> 即可连接到内网服务器，通过访问 <code>http://test.domain.com:4080</code> 即可访问内网发布的位于 <code>8000</code> 端口服务</p><h3 id="六、Systemd-管理"><a href="#六、Systemd-管理" class="headerlink" title="六、Systemd 管理"></a>六、Systemd 管理</h3><p>在较新的 Linux 系统中一经采用 Systemd 作为系统服务管理工具，以下为服务端作为服务方式运行的方式</p><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment"># 复制文件</span>cp frps /usr/<span class="hljs-built_in">local</span>/bin/frpsmkdir /etc/frpcp frps.ini /etc/frp/frps.ini<span class="hljs-comment"># 编写 frp service 文件，以 centos7 为例</span>vim /usr/lib/systemd/system/frps.service<span class="hljs-comment"># 内容如下</span>[Unit]Description=frpsAfter=network.target[Service]TimeoutStartSec=30ExecStart=/usr/<span class="hljs-built_in">local</span>/bin/frps -c /etc/frp/frps.iniExecStop=/bin/<span class="hljs-built_in">kill</span> <span class="hljs-variable">$MAINPID</span>[Install]WantedBy=multi-user.target<span class="hljs-comment"># 启动 frp 并设置开机启动</span>systemctl <span class="hljs-built_in">enable</span> frpssystemctl start frpssystemctl status frps</code></pre></div><p>客户端与此类似，这里不再重复编写，更多详细设置(如代理 https 等)请参考 frp 官方文档</p>]]></content>
    
    
    <summary type="html">最近新入了一台小主机，家里还有个树莓派，索性想通过小主机跑点东西，然后通过外网访问家里的设备；不过鉴于大天朝内网环境，没有公网 IP 并且多层路由的情况下只能选择使用内网穿透方案，以下记录了一下使用 frp 进行内网穿透的过程</summary>
    
    
    
    <category term="Linux" scheme="https://mritd.com/categories/linux/"/>
    
    
    <category term="Linux" scheme="https://mritd.com/tags/linux/"/>
    
  </entry>
  
  <entry>
    <title>Git 使用 socks5 代理</title>
    <link href="https://mritd.com/2017/01/12/git-uses-the-socks5-proxy/"/>
    <id>https://mritd.com/2017/01/12/git-uses-the-socks5-proxy/</id>
    <published>2017-01-12T15:03:30.000Z</published>
    <updated>2017-01-12T15:03:30.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>最近伟大的墙又开始搞事情，导致 gayhub 访问奇慢，没办法研究一下 socks5 代理 git，效果还不错</p></blockquote><h3 id="一、Mac-amp-Ubuntu-下代理-git"><a href="#一、Mac-amp-Ubuntu-下代理-git" class="headerlink" title="一、Mac &amp; Ubuntu 下代理 git"></a>一、Mac &amp; Ubuntu 下代理 git</h3><p>git 目前支持 4 种协议: <code>https</code>、<code>ssh</code>、<code>git</code>、本地文件；其中 <code>git</code>协议与 <code>ssh</code> 协议及其类似，暂不清楚底层实现，不过目前发现只需要成功代理 <code>ssh</code> 协议就可以实现代理 <code>git</code>；不清楚两者有什么基情，根据官方描述，<code>git</code> 协议传输非常快，验证基于 <code>ssh</code> 协议，详见 <a href="https://git-scm.com/book/zh/v2/%E6%9C%8D%E5%8A%A1%E5%99%A8%E4%B8%8A%E7%9A%84-Git-%E5%8D%8F%E8%AE%AE">服务器上的 Git - 协议</a></p><p>代理 <code>ssh</code> 协议在 Mac 和 Ubuntu 上可以使用 <code>netcat-openbsd</code> 包中的 <code>nc</code> 命令，这里由于梯子工具问题，所以仅讨论如何使用 <code>nc</code> 代理 <code>ssh</code> 协议到 <code>socks5</code> 上</p><p>Mac 默认就有 <code>nc</code> 命令， Ubuntu 新版本也有，如果较老版本可使用 <code>apt-get install -y netcat-openbsd</code> 安装</p><h4 id="1-1、创建代理命令工具"><a href="#1-1、创建代理命令工具" class="headerlink" title="1.1、创建代理命令工具"></a>1.1、创建代理命令工具</h4><p>首先创建一个代理脚本即可，<code>socks5</code> 地址根据需要更改</p><div class="hljs code-wrapper"><pre><code class="hljs sh">tee /usr/<span class="hljs-built_in">local</span>/bin/proxy-wrapper &lt;&lt;<span class="hljs-string">EOF</span><span class="hljs-string">#!/bin/bash</span><span class="hljs-string">nc -x127.0.0.1:1080 -X5 \$*</span><span class="hljs-string">EOF</span>chmod +x /usr/<span class="hljs-built_in">local</span>/bin/proxy-wrapper</code></pre></div><h4 id="1-2、增加-ssh-配置"><a href="#1-2、增加-ssh-配置" class="headerlink" title="1.2、增加 ssh 配置"></a>1.2、增加 ssh 配置</h4><p>代理 <code>git</code> 协议只需要代理 <code>ssh</code> 即可，其中 <code>Host</code> 后可以跟多个想要被代理的域名，由于代理的是 <code>ssh</code> 协议，所以 <strong>使用 <code>ssh</code> 连接服务器也会根据域名选择是否走代理</strong></p><div class="hljs code-wrapper"><pre><code class="hljs sh">tee ~/.ssh/config &lt;&lt;<span class="hljs-string">EOF</span><span class="hljs-string">Host github github.com mritd.me</span><span class="hljs-string">#Hostname github.com</span><span class="hljs-string">#User git</span><span class="hljs-string">ProxyCommand /usr/local/bin/proxy-wrapper &#x27;%h %p&#x27;</span><span class="hljs-string">EOF</span></code></pre></div><h4 id="1-3、测试"><a href="#1-3、测试" class="headerlink" title="1.3、测试"></a>1.3、测试</h4><p>配置好以后，保证你得 <code>socks5</code> 代理无问题的情况下，使用 <code>git clone git@github.com:xxxxx/xxxxx.git</code> 克隆一个项目即可验证是否成功</p><h3 id="二、CentOS-下代理-git"><a href="#二、CentOS-下代理-git" class="headerlink" title="二、CentOS 下代理 git"></a>二、CentOS 下代理 git</h3><p>默认的 CentOS 下是没有 <code>netcat-openbsd</code> 的，CentOS 下的 netcat 并非 openbsd 版本，所以会出现 <code>nc: invalid option -- &#39;X&#39;</code> 错误；so，用不了 <code>nc</code> 了，不过 Linux 下还有另一款软件可以实现代理 <code>ssh</code> 协议到 <code>socks5</code></p><h4 id="2-1、安装-connect-proxy"><a href="#2-1、安装-connect-proxy" class="headerlink" title="2.1、安装 connect-proxy"></a>2.1、安装 connect-proxy</h4><p>没有 <code>netcat-openbsd</code> 可以安装 <code>connect-proxy</code></p><div class="hljs code-wrapper"><pre><code class="hljs sh">yum install connect-proxy -y</code></pre></div><h4 id="2-2、创建代理脚本"><a href="#2-2、创建代理脚本" class="headerlink" title="2.2、创建代理脚本"></a>2.2、创建代理脚本</h4><p>同上面一样，也最好搞一个脚本</p><div class="hljs code-wrapper"><pre><code class="hljs sh">tee /usr/<span class="hljs-built_in">local</span>/bin/proxy-wrapper &lt;&lt;<span class="hljs-string">EOF</span><span class="hljs-string">#!/bin/bash</span><span class="hljs-string">connect-proxy -S 192.168.1.120:1083 $*</span><span class="hljs-string">EOF</span>chmod +x /usr/<span class="hljs-built_in">local</span>/bin/proxy-wrapper</code></pre></div><h4 id="2-3、增加-ssh-配置"><a href="#2-3、增加-ssh-配置" class="headerlink" title="2.3、增加 ssh 配置"></a>2.3、增加 ssh 配置</h4><p>ssh 配置同上面一样</p><div class="hljs code-wrapper"><pre><code class="hljs sh">tee ~/.ssh/config &lt;&lt;<span class="hljs-string">EOF</span><span class="hljs-string">Host github github.com mritd.me</span><span class="hljs-string">#Hostname github.com</span><span class="hljs-string">#User git</span><span class="hljs-string">ProxyCommand /usr/local/bin/proxy-wrapper &#x27;%h %p&#x27;</span><span class="hljs-string">EOF</span></code></pre></div><h4 id="2-3、测试"><a href="#2-3、测试" class="headerlink" title="2.3、测试"></a>2.3、测试</h4><p>测试掠过……</p><h3 id="三、其他"><a href="#三、其他" class="headerlink" title="三、其他"></a>三、其他</h3><p>对于 <code>https</code> 协议的代理可以参考 <a href="https://mritd.me/2016/07/22/Linux-%E5%91%BD%E4%BB%A4%E8%A1%8C%E4%B8%8B%E4%BD%BF%E7%94%A8-Shadowsocks-%E4%BB%A3%E7%90%86/">Linux 命令行下使用 Shadowsocks 代理</a></p>]]></content>
    
    
    <summary type="html">记录各平台下 git 使用 socks5 代理的方法</summary>
    
    
    
    <category term="Linux" scheme="https://mritd.com/categories/linux/"/>
    
    
    <category term="Linux" scheme="https://mritd.com/tags/linux/"/>
    
    <category term="Git" scheme="https://mritd.com/tags/git/"/>
    
  </entry>
  
  <entry>
    <title>使用 Nexus 搭建 Docker 仓库</title>
    <link href="https://mritd.com/2017/01/08/set-up-docker-registry-by-nexus/"/>
    <id>https://mritd.com/2017/01/08/set-up-docker-registry-by-nexus/</id>
    <published>2017-01-08T15:02:29.000Z</published>
    <updated>2017-01-08T15:02:29.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>nexus 最初用于搭建 maven 私服，提供企业级 maven jar 包管理等功能；2.x 后续支持了 npm、rpm 等包管理；最新版本 3.x 开始支持 Docker 仓库，以下为使用 neuxs 撸一个 docker 仓库的教程</p></blockquote><h3 id="一、环境准备"><a href="#一、环境准备" class="headerlink" title="一、环境准备"></a>一、环境准备</h3><p>初始环境如下</p><ul><li>Centos 7 x86_64</li><li>OpenJDK 8</li><li>Nexus 3.2.0-01</li></ul><p>安装 OpenJDK 命令如下</p><div class="hljs code-wrapper"><pre><code class="hljs sh">yum install java-1.8.0-openjdk -y</code></pre></div><p>安装完成后验证是否安装成功</p><div class="hljs code-wrapper"><pre><code class="hljs sh">➜  ~ java -versionopenjdk version <span class="hljs-string">&quot;1.8.0_111&quot;</span>OpenJDK Runtime Environment (build 1.8.0_111-b15)OpenJDK 64-Bit Server VM (build 25.111-b15, mixed mode)</code></pre></div><p>下载 neuxs3 安装包并解压</p><div class="hljs code-wrapper"><pre><code class="hljs sh">wget --no-check-certificate https://download.sonatype.com/nexus/3/nexus-3.2.0-01-unix.tar.gztar -zxvf nexus-3.2.0-01-unix.tar.gz</code></pre></div><h3 id="二、安装-nexus"><a href="#二、安装-nexus" class="headerlink" title="二、安装 nexus"></a>二、安装 nexus</h3><p>首先将 nexus 移动到任意位置</p><div class="hljs code-wrapper"><pre><code class="hljs sh">mv nexus-3.2.0-01 /usr/<span class="hljs-built_in">local</span></code></pre></div><p>创建 nexus 用户</p><div class="hljs code-wrapper"><pre><code class="hljs sh">adduser -r -s /sbin/nologin -d /data/nexus-data nexus</code></pre></div><p><strong>默认 nexus 运行后会在同级目录下创建一个 <code>sonatype-work</code> 工作目录，并将其数据保存在此目录中，所以为了数据持久化先手动创建并设置其数据存储位置</strong></p><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment"># 创建基本目录结构</span>mkdir -p /usr/<span class="hljs-built_in">local</span>/sonatype-work<span class="hljs-comment"># 创建建数据目录</span>mkdir -p /data/nexus-data/&#123;etc,<span class="hljs-built_in">log</span>,tmp&#125;<span class="hljs-comment"># 将数据目录软连接到工作目录</span>ln -s /data/nexus-data /usr/<span class="hljs-built_in">local</span>/sonatype-work/nexus3<span class="hljs-comment"># 更新所有目录权限</span>chmod -R 755 /usr/<span class="hljs-built_in">local</span>/&#123;sonatype-work,nexus-3.2.0-01&#125; /data/nexus-datachown -R nexus:nexus /usr/<span class="hljs-built_in">local</span>/&#123;sonatype-work,nexus-3.2.0-01&#125; /data/nexus-data</code></pre></div><p>最后启动 nexus 访问 8081 端口即可</p><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment"># 以前台方式运行</span>sudo -u nexus /usr/<span class="hljs-built_in">local</span>/nexus-3.2.0-01/bin/nexus run<span class="hljs-comment"># 后台运行</span>sudo -u nexus /usr/<span class="hljs-built_in">local</span>/nexus-3.2.0-01/bin/nexus start</code></pre></div><p>默认账户 <code>admin</code> 密码 <code>admin123</code>，登录如下</p><p><img src="https://cdn.oss.link/markdown/sb9dw.jpg" alt="nexus_homepage"></p><h3 id="三、创建-docker-仓库"><a href="#三、创建-docker-仓库" class="headerlink" title="三、创建 docker 仓库"></a>三、创建 docker 仓库</h3><p>在设置 <code>Repositories</code> 选项卡中中选择 <code>Create repository</code></p><p><img src="https://cdn.oss.link/markdown/m7m53.jpg" alt="Create repository"></p><p>仓库类型有很多，docker 相关总共有三种类型，其秉承 maven 私服的概念</p><p><img src="https://cdn.oss.link/markdown/pm0r8.jpg" alt="repository type"></p><ul><li>hosted: 本地存储，即同 docker 官方仓库一样提供本地私服功能</li><li>proxy: 提供代理其他仓库的类型，如 docker 中央仓库</li><li>group: 组类型，实质作用是组合多个仓库为一个地址</li></ul><h4 id="3-1、创建一个私服"><a href="#3-1、创建一个私服" class="headerlink" title="3.1、创建一个私服"></a>3.1、创建一个私服</h4><p>选择 <code>hosted</code> 类型仓库，然后输入一个仓库名，<strong>并勾选 HTTP 选项，端口任意即可(下面截图失误，不补了)</strong></p><p><img src="https://cdn.oss.link/markdown/972cl.jpg" alt="create hosted repository"></p><h4 id="3-2、测试私服"><a href="#3-2、测试私服" class="headerlink" title="3.2、测试私服"></a>3.2、测试私服</h4><p>创建好以后更改 docker 参数，测试即可</p><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment"># 增加非安全仓库</span>vim /usr/lib/systemd/system/docker.service<span class="hljs-comment"># 在 ExecStart 后面增加(这里改了 host，上面端口用的 8800)</span>--insecure-registry registry.com:8800<span class="hljs-comment"># 重启 docker</span>systemctl daemon-reloadsystemctl restart docker</code></pre></div><p>测试 push 和 pull 镜像</p><div class="hljs code-wrapper"><pre><code class="hljs sh">➜  ~ docker tag mritd/alpine registry.com:8800/alpine➜  ~ docker push registry.com:8800/alpineThe push refers to a repository [registry.com:8800/alpine]754684812d65: Pushed60ab55d3379d: Pushedlatest: digest: sha256:28f397aca53eb3e8ea1627f4af9c262fca7db17f0c6db492b53adc7bca7d0f91 size: 739➜  ~ docker rmi registry.com:8800/alpineUntagged: registry.com:8800/alpine:latestUntagged: registry.com:8800/alpine@sha256:28f397aca53eb3e8ea1627f4af9c262fca7db17f0c6db492b53adc7bca7d0f91➜  ~ docker rmi mritd/alpineUntagged: mritd/alpine:latestUntagged: mritd/alpine@sha256:28f397aca53eb3e8ea1627f4af9c262fca7db17f0c6db492b53adc7bca7d0f91Deleted: sha256:090c790ee6f28f495d92d5be43641573b0d1b5502b35f7662d88cdbf8d548afdDeleted: sha256:378e2b887fcdffcbd113a7cf6f97e9f8a58851b0a205b31a93acdb887912850d➜  ~ docker pull registry.com:8800/alpineUsing default tag: latestlatest: Pulling from alpine0a8490d0dfd3: Already exists8fb018fb4173: Pull completeDigest: sha256:28f397aca53eb3e8ea1627f4af9c262fca7db17f0c6db492b53adc7bca7d0f91Status: Downloaded newer image <span class="hljs-keyword">for</span> registry.com:8800/alpine:latest</code></pre></div><h4 id="3-2、创建代理仓库"><a href="#3-2、创建代理仓库" class="headerlink" title="3.2、创建代理仓库"></a>3.2、创建代理仓库</h4><p>代理仓库参考官方文档 <a href="http://books.sonatype.com/nexus-book/reference3/docker.html#docker-introduction">点这里</a>，本人不才….没成功，有爱探索的可以尝试一下，如果成功可以探讨一下…..个人怀疑是 index 有问题</p><p> 根据官方文档的这段提示</p><blockquote><p>Just to recap, in order to configure a proxy for Docker Hub you configure the Remote Storage URL to <a href="https://registry-1.docker.io/">https://registry-1.docker.io</a>, enable Docker V1 API support and for the choice of Docker Index select the Use Docker Hub option.</p></blockquote><p>创建仓库类型选择 <code>proxy</code>，Remote storage 填写 <code>https://registry-1.docker.io</code>，Docker index 选择 <code>Use Docker Hub</code>，然后从 代理仓库地址 pull 就可以，但是本人百试不成功，截图如下</p><p><img src="https://cdn.oss.link/markdown/q350r.jpg" alt="proxy registry"></p><h4 id="3-4、创建-group-仓库"><a href="#3-4、创建-group-仓库" class="headerlink" title="3.4、创建 group 仓库"></a>3.4、创建 group 仓库</h4><p>group 不提供具体存储服务，其主要作用就是类似一个前端反代，可以把多个仓库(比如 hosted 私服和 proxy)组合成一个地址提供访问，创建方法基本相同，主要是添加多个 hosted 或者 proxy 类型的其他仓库即可，这里不再详细阐述，截图如下</p><p><img src="https://cdn.oss.link/markdown/2q1qv.jpg" alt="group registry"></p><h3 id="四、其他相关"><a href="#四、其他相关" class="headerlink" title="四、其他相关"></a>四、其他相关</h3><p>由于 nexus 在 maven jar 管理方面已经是很成熟的产品，增加了 docker 等支持以后基本思想没有太大变化，所以关于其他仓库配置这里不再提及，具体可以参考<a href="http://books.sonatype.com/nexus-book/reference3/index.html">官方文档</a>；2.x 可以通过图形界面上传 jar，3.x 目前只能通过 maven deploy 插件实现，可以参考<a href="https://maven.apache.org/guides/mini/guide-3rd-party-jars-remote.html">这里</a></p>]]></content>
    
    
    <summary type="html">使用 Nexus 搭建 Docker 仓库</summary>
    
    
    
    <category term="Docker" scheme="https://mritd.com/categories/docker/"/>
    
    
    <category term="Linux" scheme="https://mritd.com/tags/linux/"/>
    
    <category term="Docker" scheme="https://mritd.com/tags/docker/"/>
    
    <category term="Nexus" scheme="https://mritd.com/tags/nexus/"/>
    
  </entry>
  
  <entry>
    <title>从 WWDC16 ATS 说起</title>
    <link href="https://mritd.com/2016/12/30/configure-nginx-https-under-centos-to-support-ios-ats/"/>
    <id>https://mritd.com/2016/12/30/configure-nginx-https-under-centos-to-support-ios-ats/</id>
    <published>2016-12-30T13:49:42.000Z</published>
    <updated>2016-12-30T13:49:42.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>WWDC16 苹果正式发出公告，2017年1月1号后所有 IOS 应用需要使用 HTTPS 配置，并且 HTTPS 还得符合 ATS 要求，以下记录一下 CentoS 7 下配置 nginx HTTPS 并满足 ATS 过程</p></blockquote><h3 id="一、Nginx-HTTPS-最佳实践"><a href="#一、Nginx-HTTPS-最佳实践" class="headerlink" title="一、Nginx HTTPS 最佳实践"></a>一、Nginx HTTPS 最佳实践</h3><p>随着 HTTPS 呼声越来越高，web 站点 HTTPS 化必不可免；而使用 Nginx 作为前端反向代理服务器配置 HTTPS 时有很多复杂参数，这里采取偷懒办法，直接采用 mozilla 给出的最佳实践参数(如果自己玩的非常溜可以自己自定义)；mozilla 给出了一个生成 HTTPS 配置的 web 页面，基本上给出的 HTTPS 配置已经是很好的最佳实践了，地址 –&gt; <a href="https://mozilla.github.io/server-side-tls/ssl-config-generator/">Generate Mozilla Security Recommended Web Server Configuration Files</a></p><p>服务器选择 Nginx，输入对应 Nginx 版本号和 OpenSSL 版本号；<strong>注意：为了支持 HTTP2，OpenSSL版本必须大于等于 1.0.2</strong>，截图如下</p><p><img src="https://cdn.oss.link/markdown/z5umu.jpg" alt="config"></p><h3 id="二、升级-OpenSSL"><a href="#二、升级-OpenSSL" class="headerlink" title="二、升级 OpenSSL"></a>二、升级 OpenSSL</h3><h4 id="2-1、基础准备"><a href="#2-1、基础准备" class="headerlink" title="2.1、基础准备"></a>2.1、基础准备</h4><p>准备好配置参数以后，需要升级 CentOS 7 默认的 openssl(默认最新版本 1.0.1e)，升级时最好打上 cloudflare 提供的用于支持对移动端比较友好的 chacha20 加密算法补丁，具体过程如下</p><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment"># 安装编译依赖</span>yum install gcc glibc glibc-devel make pcre \        pcre-devel zlib zlib-devel kernel-devel \        curl gnupg libxslt libxslt-devel gd-devel \        geoip-devel perl-devel perl-ExtUtils-Embed \        lua lua-devel patch -y        <span class="hljs-comment"># 下载 openssl 源码</span>wget https://www.openssl.org/<span class="hljs-built_in">source</span>/openssl-1.0.2j.tar.gz<span class="hljs-comment"># 下载 chacha20 补丁</span>wget https://raw.githubusercontent.com/cloudflare/sslconfig/master/patches/openssl__chacha20_poly1305_draft_and_rfc_ossl102j.patch<span class="hljs-comment"># 解压源码</span>tar -zxvf openssl-1.0.2j.tar.gz<span class="hljs-comment"># 打补丁</span>mv openssl__chacha20_poly1305_draft_and_rfc_ossl102j.patch openssl-1.0.2j<span class="hljs-built_in">cd</span> openssl-1.0.2j &amp;&amp; patch -p1 &lt; openssl__chacha20_poly1305_draft_and_rfc_ossl102j.patch</code></pre></div><h4 id="2-2、编译安装"><a href="#2-2、编译安装" class="headerlink" title="2.2、编译安装"></a>2.2、编译安装</h4><p>打过补丁以后就可以安装并进行替换了</p><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment"># 编译并安装</span>./config shared zlib-dynamicmake &amp;&amp; make install<span class="hljs-comment"># 备份原来的 openssl 以防不测</span>mv /usr/bin/openssl  /usr/bin/openssl.oldmv /usr/include/openssl  /usr/include/openssl.old<span class="hljs-comment"># 使用软连接方式替换</span>ln -s /usr/<span class="hljs-built_in">local</span>/ssl/bin/openssl  /usr/bin/opensslln -s /usr/<span class="hljs-built_in">local</span>/ssl/include/openssl  /usr/include/openssl<span class="hljs-comment"># libssl.so 不同操作系统位置不同，建议先 find 一下，然后挨个替换</span>ln -s /usr/<span class="hljs-built_in">local</span>/ssl/lib/libssl.so /usr/lib/libssl.soln -s /usr/<span class="hljs-built_in">local</span>/ssl/lib/libssl.so /usr/<span class="hljs-built_in">local</span>/lib64/libssl.so<span class="hljs-comment"># 刷新 共享库缓存</span><span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;/usr/local/ssl/lib&quot;</span> &gt;&gt; /etc/ld.so.confldconfig -v<span class="hljs-comment"># 最后验证一下 如果都显示为 1.0.2j 表明成功</span>openssl versionstrings /usr/<span class="hljs-built_in">local</span>/lib64/libssl.so |grep OpenSSL</code></pre></div><h3 id="三、编译安装-Nginx"><a href="#三、编译安装-Nginx" class="headerlink" title="三、编译安装 Nginx"></a>三、编译安装 Nginx</h3><p>编译 Nginx 参数有很多，具体的可以自行更改，以下参考官方参数并且安装了 <a href="https://github.com/openresty/lua-nginx-module">lua-nginx</a>、<a href="https://github.com/openresty/headers-more">headers-more</a>、<a href="https://github.com/yaoweibin/nginx_upstream_check_module">upstream_check</a>、<a href="https://github.com/simpl/ngx_devel_kit">ngx_devel_kit</a></p><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment"># 定义版本</span><span class="hljs-built_in">export</span> NGINX_VERSION=<span class="hljs-string">&quot;1.11.6&quot;</span><span class="hljs-built_in">export</span> NGINX_LUA_MODULE_VERSION=<span class="hljs-string">&quot;0.10.7&quot;</span><span class="hljs-built_in">export</span> OPENSSL_VERSION=<span class="hljs-string">&quot;1.0.1t&quot;</span><span class="hljs-built_in">export</span> HEADERS_MORE_VERSION=<span class="hljs-string">&quot;0.32&quot;</span><span class="hljs-built_in">export</span> UPSTREAM_CHECK_VERSION=<span class="hljs-string">&quot;0.3.0&quot;</span><span class="hljs-built_in">export</span> DEVEL_KIT_VERSION=<span class="hljs-string">&quot;0.3.0&quot;</span><span class="hljs-built_in">export</span> LUAJIT_VERSION=<span class="hljs-string">&quot;2.0.4&quot;</span><span class="hljs-built_in">export</span> LUAJIT_MAIN_VERSION=<span class="hljs-string">&quot;2.0&quot;</span><span class="hljs-built_in">export</span> LUAJIT_LIB=<span class="hljs-string">&quot;/usr/local/lib&quot;</span><span class="hljs-built_in">export</span> LUAJIT_INC=<span class="hljs-string">&quot;/usr/local/include/luajit-<span class="hljs-variable">$LUAJIT_MAIN_VERSION</span>&quot;</span><span class="hljs-comment"># 下载相关源码</span>wget http://nginx.org/download/nginx-<span class="hljs-variable">$&#123;NGINX_VERSION&#125;</span>.tar.gzwget https://github.com/openresty/lua-nginx-module/archive/v<span class="hljs-variable">$&#123;NGINX_LUA_MODULE_VERSION&#125;</span>.tar.gz -O lua-nginx-module-v<span class="hljs-variable">$&#123;NGINX_LUA_MODULE_VERSION&#125;</span>.tar.gzwget https://github.com/openresty/headers-more-nginx-module/archive/v<span class="hljs-variable">$&#123;HEADERS_MORE_VERSION&#125;</span>.tar.gzwget https://github.com/yaoweibin/nginx_upstream_check_module/archive/v<span class="hljs-variable">$&#123;UPSTREAM_CHECK_VERSION&#125;</span>.tar.gzwget https://github.com/simpl/ngx_devel_kit/archive/v<span class="hljs-variable">$&#123;DEVEL_KIT_VERSION&#125;</span>.tar.gz -O ngx_devel_kit-v<span class="hljs-variable">$&#123;DEVEL_KIT_VERSION&#125;</span>.tar.gzwget http://luajit.org/download/LuaJIT-<span class="hljs-variable">$LUAJIT_VERSION</span>.tar.gz<span class="hljs-comment"># 解压</span><span class="hljs-keyword">for</span> tgzName <span class="hljs-keyword">in</span> `ls *.tar.gz`;<span class="hljs-keyword">do</span>    tar -zxvf <span class="hljs-variable">$tgzName</span><span class="hljs-keyword">done</span><span class="hljs-comment"># 编译并安装(上面下载是在 /usr/src 下进行的)</span>CONFIG_ARGS=<span class="hljs-string">&quot;\</span><span class="hljs-string">    --prefix=<span class="hljs-variable">$&#123;PREFIX:-/usr/local/nginx&#125;</span> \</span><span class="hljs-string">    --pid-path=/var/run/nginx.pid \</span><span class="hljs-string">    --lock-path=/var/run/nginx.lock \</span><span class="hljs-string">    --with-http_ssl_module \</span><span class="hljs-string">    --with-http_realip_module \</span><span class="hljs-string">    --with-http_addition_module \</span><span class="hljs-string">    --with-http_sub_module \</span><span class="hljs-string">    --with-http_dav_module \</span><span class="hljs-string">    --with-http_flv_module \</span><span class="hljs-string">    --with-http_mp4_module \</span><span class="hljs-string">    --with-http_gunzip_module \</span><span class="hljs-string">    --with-http_gzip_static_module \</span><span class="hljs-string">    --with-http_random_index_module \</span><span class="hljs-string">    --with-http_secure_link_module \</span><span class="hljs-string">    --with-http_stub_status_module \</span><span class="hljs-string">    --with-http_auth_request_module \</span><span class="hljs-string">    --with-http_xslt_module=dynamic \</span><span class="hljs-string">    --with-http_image_filter_module=dynamic \</span><span class="hljs-string">    --with-http_geoip_module=dynamic \</span><span class="hljs-string">    --with-http_perl_module=dynamic \</span><span class="hljs-string">    --with-threads \</span><span class="hljs-string">    --with-stream \</span><span class="hljs-string">    --with-stream_ssl_module \</span><span class="hljs-string">    --with-stream_ssl_preread_module \</span><span class="hljs-string">    --with-stream_realip_module \</span><span class="hljs-string">    --with-stream_geoip_module=dynamic \</span><span class="hljs-string">    --with-http_slice_module \</span><span class="hljs-string">    --with-mail \</span><span class="hljs-string">    --with-mail_ssl_module \</span><span class="hljs-string">    --with-file-aio \</span><span class="hljs-string">    --with-http_v2_module \</span><span class="hljs-string">    --with-openssl=/usr/src/openssl-<span class="hljs-variable">$&#123;OPENSSL_VERSION&#125;</span> \</span><span class="hljs-string">    --add-module=/usr/src/headers-more-nginx-module-<span class="hljs-variable">$&#123;HEADERS_MORE_VERSION&#125;</span> \</span><span class="hljs-string">    --add-module=/usr/src/nginx_upstream_check_module-<span class="hljs-variable">$&#123;UPSTREAM_CHECK_VERSION&#125;</span> \</span><span class="hljs-string">    --add-module=/usr/src/ngx_devel_kit-<span class="hljs-variable">$&#123;DEVEL_KIT_VERSION&#125;</span> \</span><span class="hljs-string">    --add-module=/usr/src/lua-nginx-module-<span class="hljs-variable">$&#123;NGINX_LUA_MODULE_VERSION&#125;</span> \</span><span class="hljs-string">    --http-client-body-temp-path=/tmp/client_body_temp \</span><span class="hljs-string">    --http-proxy-temp-path=/tmp/proxy_temp \</span><span class="hljs-string">    --http-fastcgi-temp-path=/tmp/fastcgi_temp \</span><span class="hljs-string">    --http-uwsgi-temp-path=/tmp/uwsgi_temp \</span><span class="hljs-string">    --http-scgi-temp-path=/tmp/scgi_temp \</span><span class="hljs-string">    &quot;</span><span class="hljs-comment"># 先安装 lua</span><span class="hljs-built_in">cd</span> /usr/src/LuaJIT-<span class="hljs-variable">$LUAJIT_VERSION</span>make -j$(getconf _NPROCESSORS_ONLN)make install<span class="hljs-comment"># 安装 nginx</span><span class="hljs-built_in">cd</span> /usr/src/nginx-<span class="hljs-variable">$NGINX_VERSION</span>./configure <span class="hljs-variable">$CONFIG_ARGS</span> --with-debugmake -j$(getconf _NPROCESSORS_ONLN)make install</code></pre></div><h3 id="四、配置-HTTPS"><a href="#四、配置-HTTPS" class="headerlink" title="四、配置 HTTPS"></a>四、配置 HTTPS</h3><p>主配置参考步骤一的 HTTPS 最佳实践，以下只做简要说明</p><p>前向保密 <code>dhparam.pem</code> 文件通过 <code>openssl dhparam 4096 &gt; dhparam.pem</code> 生成</p><p><code>ssl_trusted_certificate</code> 需要 CA 根证书，请根据具体证书 CA 自行下载</p><h3 id="五、验证-ATS"><a href="#五、验证-ATS" class="headerlink" title="五、验证 ATS"></a>五、验证 ATS</h3><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment"># 验证命令 如果 grep 到 FAIL 则说明配置不通过，</span><span class="hljs-comment"># 需重新检查配置，否则则证明 ATS 通过</span>nscurl --ats-diagnostics --verbose https://mritd.me | grep FAIL</code></pre></div>]]></content>
    
    
    <summary type="html">WWDC16 苹果正式发出公告，2017年1月1号后所有 IOS 应用需要使用 HTTPS 配置，并且 HTTPS 还得符合 ATS 要求，以下记录一下 CentoS 7 下配置 nginx HTTPS 并满足 ATS 过程</summary>
    
    
    
    <category term="Linux" scheme="https://mritd.com/categories/linux/"/>
    
    
    <category term="Linux" scheme="https://mritd.com/tags/linux/"/>
    
  </entry>
  
  <entry>
    <title>Traefik-kubernetes 初试</title>
    <link href="https://mritd.com/2016/12/06/try-traefik-on-kubernetes/"/>
    <id>https://mritd.com/2016/12/06/try-traefik-on-kubernetes/</id>
    <published>2016-12-06T14:38:22.000Z</published>
    <updated>2016-12-06T14:38:22.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>traefik 是一个前端负载均衡器，对于微服务架构尤其是 kubernetes 等编排工具具有良好的支持；同 nginx 等相比，traefik 能够自动感知后端容器变化，从而实现自动服务发现；今天小试了一下，在此记录一下使用过程</p></blockquote><h3 id="一、Kubernetes-服务暴露介绍"><a href="#一、Kubernetes-服务暴露介绍" class="headerlink" title="一、Kubernetes 服务暴露介绍"></a>一、Kubernetes 服务暴露介绍</h3><p>从 kubernetes 1.2 版本开始，kubernetes提供了 Ingress 对象来实现对外暴露服务；到目前为止 kubernetes 总共有三种暴露服务的方式:</p><ul><li>LoadBlancer Service</li><li>NodePort Service</li><li>Ingress</li></ul><h4 id="1-1、LoadBlancer-Service"><a href="#1-1、LoadBlancer-Service" class="headerlink" title="1.1、LoadBlancer Service"></a>1.1、LoadBlancer Service</h4><p>LoadBlancer Service 是 kubernetes 深度结合云平台的一个组件；当使用 LoadBlancer Service 暴露服务时，实际上是通过<strong>向底层云平台申请创建一个负载均衡器</strong>来向外暴露服务；目前 LoadBlancer Service 支持的云平台已经相对完善，比如国外的 GCE、DigitalOcean，国内的 阿里云，私有云 Openstack 等等，由于 LoadBlancer Service 深度结合了云平台，所以只能在一些云平台上来使用</p><h4 id="1-2、NodePort-Service"><a href="#1-2、NodePort-Service" class="headerlink" title="1.2、NodePort Service"></a>1.2、NodePort Service</h4><p>NodePort Service 顾名思义，实质上就是通过在集群的每个 node 上暴露一个端口，然后将这个端口映射到某个具体的 service 来实现的，虽然每个 node 的端口有很多(0~65535)，但是由于安全性和易用性(服务多了就乱了，还有端口冲突问题)实际使用可能并不多</p><h4 id="1-3、Ingress"><a href="#1-3、Ingress" class="headerlink" title="1.3、Ingress"></a>1.3、Ingress</h4><p>Ingress 这个东西是 1.2 后才出现的，通过 Ingress 用户可以实现使用 nginx 等开源的反向代理负载均衡器实现对外暴露服务，以下详细说一下 Ingress，毕竟 traefik 用的就是 Ingress</p><p><strong>使用 Ingress 时一般会有三个组件:</strong></p><ul><li>反向代理负载均衡器</li><li>Ingress Controller</li><li>Ingress</li></ul><h5 id="1-3-1、反向代理负载均衡器"><a href="#1-3-1、反向代理负载均衡器" class="headerlink" title="1.3.1、反向代理负载均衡器"></a>1.3.1、反向代理负载均衡器</h5><p>反向代理负载均衡器很简单，说白了就是 nginx、apache 什么的；在集群中反向代理负载均衡器可以自由部署，可以使用 Replication Controller、Deployment、DaemonSet 等等，不过个人喜欢以 DaemonSet 的方式部署，感觉比较方便</p><h5 id="1-3-2、Ingress-Controller"><a href="#1-3-2、Ingress-Controller" class="headerlink" title="1.3.2、Ingress Controller"></a>1.3.2、Ingress Controller</h5><p>Ingress Controller 实质上可以理解为是个监视器，Ingress Controller 通过不断地跟 kubernetes API 打交道，实时的感知后端 service、pod 等变化，比如新增和减少 pod，service 增加与减少等；当得到这些变化信息后，Ingress Controller 再结合下文的 Ingress 生成配置，然后更新反向代理负载均衡器，并刷新其配置，达到服务发现的作用</p><h5 id="1-3-3、Ingress"><a href="#1-3-3、Ingress" class="headerlink" title="1.3.3、Ingress"></a>1.3.3、Ingress</h5><p>Ingress 简单理解就是个规则定义；比如说某个域名对应某个 service，即当某个域名的请求进来时转发给某个 service;这个规则将与 Ingress Controller 结合，然后 Ingress Controller 将其动态写入到负载均衡器配置中，从而实现整体的服务发现和负载均衡</p><p><strong>有点懵逼，那就看图</strong></p><p><img src="https://cdn.oss.link/markdown/qflqj.jpg" alt="Ingress"></p><p><strong>从上图中可以很清晰的看到，实际上请求进来还是被负载均衡器拦截，比如 nginx，然后 Ingress Controller 通过跟 Ingress 交互得知某个域名对应哪个 service，再通过跟 kubernetes API 交互得知 service 地址等信息；综合以后生成配置文件实时写入负载均衡器，然后负载均衡器 reload 该规则便可实现服务发现，即动态映射</strong></p><p><strong>了解了以上内容以后，这也就很好的说明了我为什么喜欢把负载均衡器部署为 Daemon Set；因为无论如何请求首先是被负载均衡器拦截的，所以在每个 node 上都部署一下，同时 hostport 方式监听 80 端口；那么就解决了其他方式部署不确定 负载均衡器在哪的问题，同时访问每个 node 的 80 都能正确解析请求；如果前端再 放个 nginx 就又实现了一层负载均衡</strong></p><h3 id="二、Traefik-使用"><a href="#二、Traefik-使用" class="headerlink" title="二、Traefik 使用"></a>二、Traefik 使用</h3><p>由于微服务架构以及 Docker 技术和 kubernetes 编排工具最近几年才开始逐渐流行，所以一开始的反向代理服务器比如 nginx、apache 并未提供其支持，毕竟他们也不是先知；所以才会出现 Ingress Controller 这种东西来做 kubernetes 和前端负载均衡器如 nginx 之间做衔接；<strong>即 Ingress Controller 的存在就是为了能跟 kubernetes 交互，又能写 nginx 配置，还能 reload 它，这是一种折中方案</strong>；而最近开始出现的 traefik 天生就是提供了对 kubernetes 的支持，<strong>也就是说 traefik 本身就能跟 kubernetes API 交互，感知后端变化，因此可以得知: 在使用 traefik 时，Ingress Controller 已经无卵用了，所以整体架构如下</strong></p><p><img src="https://cdn.oss.link/markdown/pot7r.jpg" alt="traefik"></p><h4 id="2-1、部署-Traefik"><a href="#2-1、部署-Traefik" class="headerlink" title="2.1、部署 Traefik"></a>2.1、部署 Traefik</h4><p>已经从大体上搞懂了 Ingress 和 traefik，那么部署起来就很简单</p><h5 id="2-1-1、部署-Daemon-Set"><a href="#2-1-1、部署-Daemon-Set" class="headerlink" title="2.1.1、部署 Daemon Set"></a>2.1.1、部署 Daemon Set</h5><p><strong>首先以 Daemon Set 的方式在每个 node 上启动一个 traefik，并使用 hostPort 的方式让其监听每个 node 的 80 端口(有没有感觉这就是个 NodePort? 不过区别就是这个 Port 后面有负载均衡器 –&gt;手动微笑)</strong></p><div class="hljs code-wrapper"><pre><code class="hljs sh">kubectl create -f traefik.ds.yaml<span class="hljs-comment"># Daemon set 文件如下</span>apiVersion: extensions/v1beta1kind: DaemonSetmetadata:  name: traefik-ingress-lb  namespace: kube-system  labels:    k8s-app: traefik-ingress-lbspec:  template:    metadata:      labels:        k8s-app: traefik-ingress-lb        name: traefik-ingress-lb    spec:      terminationGracePeriodSeconds: 60      hostNetwork: <span class="hljs-literal">true</span>      restartPolicy: Always      containers:      - image: traefik        name: traefik-ingress-lb        resources:          limits:            cpu: 200m            memory: 30Mi          requests:            cpu: 100m            memory: 20Mi        ports:        - name: http          containerPort: 80          hostPort: 80        - name: admin          containerPort: 8580        args:        - --web        - --web.address=:8580        - --kubernetes</code></pre></div><p><strong>其中 traefik 监听 node 的 80 和 8580 端口，80 提供正常服务，8580 是其自带的 UI 界面，原本默认是 8080，因为环境里端口冲突了，所以这里临时改一下</strong></p><h5 id="2-1-2、部署-Ingress"><a href="#2-1-2、部署-Ingress" class="headerlink" title="2.1.2、部署 Ingress"></a>2.1.2、部署 Ingress</h5><p>从上面的长篇大论已经得知了 Ingress Controller 是无需部署的，所以直接部署 Ingress 即可</p><div class="hljs code-wrapper"><pre><code class="hljs sh">kubectl create -f traefik.ing.yaml<span class="hljs-comment"># Ingress 文件如下</span>apiVersion: extensions/v1beta1kind: Ingressmetadata:  name: traefik-ingressspec:  rules:  - host: traefik.www.test.com    http:      paths:      - path: /        backend:          serviceName: test-www          servicePort: 8080  - host: traefik.api.test.com    http:      paths:      - path: /        backend:          serviceName: test-api          servicePort: 8080</code></pre></div><p><strong>实际上事先集群中已经存在了相应的名为 test-www 和 test-api 的 service，对应的 service 后端也有很多 pod；所以这里就不在具体写部署实际业务容器(test-www、test-api)的过程了，各位测试时，只需要把这个 test 的 service 替换成自己业务的 service 即可</strong></p><h5 id="2-1-3、部署-Traefik-UI"><a href="#2-1-3、部署-Traefik-UI" class="headerlink" title="2.1.3、部署 Traefik UI"></a>2.1.3、部署 Traefik UI</h5><p>traefik 本身还提供了一套 UI 供我们使用，其同样以 Ingress 方式暴露，只需要创建一下即可</p><div class="hljs code-wrapper"><pre><code class="hljs sh">kubectl create -f ui.yaml<span class="hljs-comment"># ui yaml 如下</span>---apiVersion: v1kind: Servicemetadata:  name: traefik-web-ui  namespace: kube-systemspec:  selector:    k8s-app: traefik-ingress-lb  ports:  - name: web    port: 80    targetPort: 8580---apiVersion: extensions/v1beta1kind: Ingressmetadata:  name: traefik-web-ui  namespace: kube-systemspec:  rules:  - host: traefik-ui.local    http:      paths:      - path: /        backend:          serviceName: traefik-web-ui          servicePort: web</code></pre></div><h5 id="2-1-4、访问测试"><a href="#2-1-4、访问测试" class="headerlink" title="2.1.4、访问测试"></a>2.1.4、访问测试</h5><p>都创建无误以后，只需要将待测试的域名解析到任意一台 node 上即可，页面就不截图了，截图就暴露了…..下面来两张 ui 的</p><p><img src="https://cdn.oss.link/markdown/i32ab.jpg" alt="traefik ui"></p><p><img src="https://cdn.oss.link/markdown/1qtmb.jpg" alt="traefik ui health"></p><h4 id="2-2、健康检查"><a href="#2-2、健康检查" class="headerlink" title="2.2、健康检查"></a>2.2、健康检查</h4><p>关于健康检查，测试可以使用 kubernetes 的 Liveness Probe 实现，如果 Liveness Probe检查失败，则 traefik 会自动移除该 pod，以下是一个 示例</p><p><strong>test 的 deployment，健康检查方式是 <code>cat /tmp/health</code>，容器启动 2 分钟后会删掉这个文件，模拟健康检查失败</strong></p><div class="hljs code-wrapper"><pre><code class="hljs sh">apiVersion: v1kind: DeploymentapiVersion: extensions/v1beta1metadata:  name: <span class="hljs-built_in">test</span>  namespace: default  labels:    <span class="hljs-built_in">test</span>: alpinespec:  replicas: 1  selector:    matchLabels:      <span class="hljs-built_in">test</span>: alpine  template:    metadata:      labels:        <span class="hljs-built_in">test</span>: alpine        name: <span class="hljs-built_in">test</span>    spec:      containers:      - image: mritd/alpine:3.4        name: alpine        resources:          limits:            cpu: 200m            memory: 30Mi          requests:            cpu: 100m            memory: 20Mi        ports:        - name: http          containerPort: 80        args:        <span class="hljs-built_in">command</span>:        - <span class="hljs-string">&quot;bash&quot;</span>        - <span class="hljs-string">&quot;-c&quot;</span>        - <span class="hljs-string">&quot;echo ok &gt; /tmp/health;sleep 120;rm -f /tmp/health&quot;</span>        livenessProbe:          <span class="hljs-built_in">exec</span>:            <span class="hljs-built_in">command</span>:            - cat            - /tmp/health          initialDelaySeconds: 20</code></pre></div><p><strong>test 的 service</strong></p><div class="hljs code-wrapper"><pre><code class="hljs sh">apiVersion: v1kind: Servicemetadata:  name: <span class="hljs-built_in">test</span>   labels:    name: <span class="hljs-built_in">test</span>spec:  ports:  - port: 8123    targetPort: 80  selector:    name: <span class="hljs-built_in">test</span></code></pre></div><p><strong>test 的 Ingress</strong></p><div class="hljs code-wrapper"><pre><code class="hljs sh">apiVersion: extensions/v1beta1kind: Ingressmetadata:  name: <span class="hljs-built_in">test</span>spec:  rules:  - host: test.com    http:      paths:      - path: /        backend:          serviceName: <span class="hljs-built_in">test</span>          servicePort: 8123</code></pre></div><p><strong>全部创建好以后，进入 traefik ui 界面，可以观察到每隔 2 分钟健康检查失败后，kubernetes 重建 pod，同时 traefik 会从后端列表中移除这个 pod</strong></p><p><strong>其他更多玩法请参考 <a href="https://docs.traefik.io/">官方文档</a>(我发现他居然支持 Let’s Entrypt，个人博客福音啊)，如有错误欢迎指正</strong></p>]]></content>
    
    
    <summary type="html">traefik 是一个前端负载均衡器，对于微服务架构尤其是 kubernetes 等编排工具具有良好的支持；同 nginx 等相比，traefik 能够自动感知后端容器变化，从而实现自动服务发现；今天小试了一下，在此记录一下使用过程</summary>
    
    
    
    <category term="Kubernetes" scheme="https://mritd.com/categories/kubernetes/"/>
    
    
    <category term="Linux" scheme="https://mritd.com/tags/linux/"/>
    
    <category term="Docker" scheme="https://mritd.com/tags/docker/"/>
    
    <category term="Kubernetes" scheme="https://mritd.com/tags/kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>LVS 笔记</title>
    <link href="https://mritd.com/2016/12/05/lvs-note/"/>
    <id>https://mritd.com/2016/12/05/lvs-note/</id>
    <published>2016-12-05T14:59:49.000Z</published>
    <updated>2016-12-05T14:59:49.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>LVS(Linux Virtual Server) 意为Linux虚拟服务器，是针对高可伸缩、高可用网络服务要求基于 IP 层和内容请求分发的负载均衡调度解决方案</p></blockquote><h3 id="一、LVS-负载均衡技术"><a href="#一、LVS-负载均衡技术" class="headerlink" title="一、LVS 负载均衡技术"></a>一、LVS 负载均衡技术</h3><h4 id="1-1、Virtual-Server-via-Network-Address-Translation-VS-NAT"><a href="#1-1、Virtual-Server-via-Network-Address-Translation-VS-NAT" class="headerlink" title="1.1、Virtual Server via Network Address Translation(VS/NAT)"></a>1.1、Virtual Server via Network Address Translation(VS/NAT)</h4><p>此种负载均衡技术实质上通过重写请求和响应报文的目标地址及源地址实现网络地址转换的；在请求时，前端负载均衡器首先根据特定算法选择使用哪个后端真实服务器，然后改写请求报文的目标地址再将请求报文转发给后端的真实服务器；后端服务器返回时，前端负载均衡器再次改写响应报文的源地址，并完成响应转发；<strong>由于请求响应报文都需要通过前端负载均衡器进行地址重写，所以此种负载均衡技术对前端负载均衡器性能要求较高，业务量大的情况下前端负载均衡器可能成为瓶颈</strong></p><h4 id="1-2、Virtual-Server-via-IP-Tunneling-VS-TUN"><a href="#1-2、Virtual-Server-via-IP-Tunneling-VS-TUN" class="headerlink" title="1.2、Virtual Server via IP Tunneling(VS/TUN)"></a>1.2、Virtual Server via IP Tunneling(VS/TUN)</h4><p>TUN 模式采用 NAT 技术，在请求时，前端负载均衡器通过 <strong>IP 隧道</strong> 直接转发给后端特定的的真实服务器(根据指定算法)；响应时后端真实服务器直接返回报文给客户端；<strong>由于响应报文不需要再经过前端负载均衡器，所以极大地提高了吞吐量，但是使用此种模式后端服务器必须支持 IP 隧道协议</strong></p><h4 id="1-3、Virtual-Server-via-Direct-Routing-VS-DR"><a href="#1-3、Virtual-Server-via-Direct-Routing-VS-DR" class="headerlink" title="1.3、Virtual Server via Direct Routing(VS/DR)"></a>1.3、Virtual Server via Direct Routing(VS/DR)</h4><p>相对于 VS/TUN 方式，由于 TUN 方式仍需要打开 IP 隧道，所以在请求时仍有很大开销；而此种模式通过直接改写请求报文的 MAC 地址实现，MAC 地址改写后直接根据指定算法转发到后端服务器，后端服务器响应时也直接返回给客户端而不经过前端负载均衡器，所以此种模式吞吐量会更大；<strong>虽然没有了 IP 隧道开销，但是此种模式要求前端负载均衡器必须与后端真实服务器必须在同一网段下，而由于同一网段下机器数量有限，所以也限制了应用范围</strong></p><h3 id="二、负载均衡调度算法"><a href="#二、负载均衡调度算法" class="headerlink" title="二、负载均衡调度算法"></a>二、负载均衡调度算法</h3><h4 id="2-1、轮询算法-RR"><a href="#2-1、轮询算法-RR" class="headerlink" title="2.1、轮询算法(RR)"></a>2.1、轮询算法(RR)</h4><p>轮询(Round Robin) 算法简称 RR，负载均衡器通过轮询调度算法将外部请求轮流分配到集群的各个后端服务器上，对待后端服务器属于 “无差别攻击”，所以也会忽略后端服务器的实际负载等</p><h4 id="2-2、加权轮询算法-WRR"><a href="#2-2、加权轮询算法-WRR" class="headerlink" title="2.2、加权轮询算法(WRR)"></a>2.2、加权轮询算法(WRR)</h4><p>加权轮询(Weighted Round Robin) 根据真实服务器的不同处理能力(自动询问)来处理请求分发，轮询时处理能力强的服务器会得到更多的请求分发</p><h4 id="2-3、最少连接算法-LC"><a href="#2-3、最少连接算法-LC" class="headerlink" title="2.3、最少连接算法(LC)"></a>2.3、最少连接算法(LC)</h4><p>使用最少链接(Least Connections)时，前端负载均衡器会自动选择后端真实服务器中连接数最少的服务器进行请求分发，<strong>如果后端服务器性能相近，则能够很好地负载均衡，否则可能造成性能问题</strong></p><h4 id="2-4、加权最少连接算法"><a href="#2-4、加权最少连接算法" class="headerlink" title="2.4、加权最少连接算法"></a>2.4、加权最少连接算法</h4><p>加权最少连接算法算法(Weighted Least Connections)即在 LC 基础上增加服务器性能权重，通过自动询问后端服务器性能和连接情况综合分发请求</p><h4 id="2-5、基于局部性最少链接算法-LBLC"><a href="#2-5、基于局部性最少链接算法-LBLC" class="headerlink" title="2.5、基于局部性最少链接算法(LBLC)"></a>2.5、基于局部性最少链接算法(LBLC)</h4><p>基于局部性最少连接算法(Locality-Based Least Connections)类似 IP 亲和技术加上最少链接算法；请求分发时，前端负载均衡器根据请求目标 IP 查找该请求目标 IP 最近使用的服务器，如果该服务器没有超载则将请求分发给该服务器；否则采用最小连接算法选择一台服务器分发请求</p><h4 id="2-6、带复制的基于局部性最少连接算法-LBLCR"><a href="#2-6、带复制的基于局部性最少连接算法-LBLCR" class="headerlink" title="2.6、带复制的基于局部性最少连接算法(LBLCR)"></a>2.6、带复制的基于局部性最少连接算法(LBLCR)</h4><p>带复制的基于局部性最少连接算法(Locality-Based Least Connections with Replications) 也是针对 IP 的负载均衡技术；与 LBLC 相比，该算法维护一个从目标 IP 到服务器组的映射关系；分发请求时，根据目标 IP 选择对应的服务器组，然后按照最小链接算法选择一台服务器进行请求分发，如果选中的服务器已经超载，则使用最小链接算法从服务器组外选择一台服务器进行请求分发，同时将该服务器加入到目标 IP 对应的服务器组中</p><h4 id="2-7、目标地址散列算法-DH"><a href="#2-7、目标地址散列算法-DH" class="headerlink" title="2.7、目标地址散列算法(DH)"></a>2.7、目标地址散列算法(DH)</h4><p>目标地址散列算法(Destination Hashing)使用请求的目标 IP 地址作为散列键，然后从静态分配的散列表中找出对应的真实服务器进行请求分发，如果该服务器超载，则返回空</p><h4 id="2-8、源地址散列算法-SH"><a href="#2-8、源地址散列算法-SH" class="headerlink" title="2.8、源地址散列算法(SH)"></a>2.8、源地址散列算法(SH)</h4><p>同 DH 类似，只不过散列键换成请求源 IP 地址而已</p><h3 id="三、LVS-负载均衡配置"><a href="#三、LVS-负载均衡配置" class="headerlink" title="三、LVS 负载均衡配置"></a>三、LVS 负载均衡配置</h3><h4 id="3-1、软件安装"><a href="#3-1、软件安装" class="headerlink" title="3.1、软件安装"></a>3.1、软件安装</h4><div class="hljs code-wrapper"><pre><code class="hljs sh">yum install ipvsadm</code></pre></div><p>安装后总共3个可执行文件，如下</p><table><thead><tr><th>命令</th><th>作用</th></tr></thead><tbody><tr><td><code>ipvsadm</code></td><td>LVS 主程序，负责 RS 天剑、删除和修改</td></tr><tr><td><code>ipvsadm-save</code></td><td>备份 LVS 配置</td></tr><tr><td><code>ipvsadm-restore</code></td><td>恢复 LVS 配置</td></tr></tbody></table><p><strong><code>ipvsadm</code> 常用参数如下</strong></p><table><thead><tr><th>参数</th><th>说明</th></tr></thead><tbody><tr><td><code>-A</code></td><td>在内核虚拟服务器列表中添加一条新的虚拟服务器记录</td></tr><tr><td><code>-E</code></td><td>编辑在内核虚拟服务器列表中的一条虚拟服务器记录</td></tr><tr><td><code>-D</code></td><td>删除内和虚拟服务器列表中的一条虚拟服务器记录</td></tr><tr><td><code>-C</code></td><td>清除内核虚拟服务器列表所有记录</td></tr><tr><td><code>-R</code></td><td>恢复虚拟服务器规则</td></tr><tr><td><code>-S</code></td><td>保存虚拟服务器规则，输出为 <code>-R</code> 可读的格式</td></tr><tr><td><code>-a</code></td><td>在内核虚拟服务器列表中添加一条真实服务器记录</td></tr><tr><td><code>-e</code></td><td>编辑内核虚拟服务器中一条真实服务器记录</td></tr><tr><td><code>-d</code></td><td>删除内核虚拟服务器列表中一条真实服务器记录</td></tr><tr><td><code>-L</code>、<code>-l</code></td><td>显示内核虚拟服务器列表</td></tr><tr><td><code>-Z</code></td><td>内核虚拟服务器列表计数器清零(清除链接数量等)</td></tr><tr><td><code>-set</code></td><td>- tcp tcpfin udp 设置连接超时值</td></tr><tr><td><code>--start-daemon</code></td><td>启动同步守护进程</td></tr><tr><td><code>--stop-daemon</code></td><td>停止同步守护进程</td></tr><tr><td><code>-daemon</code></td><td>显示同步守护进程状态</td></tr><tr><td><code>-h</code></td><td>帮助信息</td></tr><tr><td><code>-t</code></td><td>声明该虚拟服务器提供的是 TCP 服务(用于添加时)</td></tr><tr><td><code>-u</code></td><td>声明该虚拟服务器提供的是 UDP 服务(用于添加时)</td></tr><tr><td><code>-f</code></td><td>声明是经过 <code>iptables</code> 标记过的服务类型</td></tr><tr><td><code>-s</code></td><td>使用的负载均衡调度算法(rr、wrr、lc、wlc、lblc、lblcr、dh、sh、sed、nq)</td></tr><tr><td><code>-p</code></td><td>声明提供持久服务</td></tr><tr><td><code>-r</code></td><td>声明是一台真是的服务器</td></tr><tr><td><code>-g</code></td><td>指定 LVS 工作模式为直接路由模式</td></tr><tr><td><code>-i</code></td><td>指定 LVS 工作模式为隧道模式</td></tr><tr><td><code>-m</code></td><td>指定 LVS 工作模式为 NAT 模式</td></tr><tr><td><code>-w</code></td><td>设置真实服务器的权重值</td></tr><tr><td><code>-c</code></td><td>显示 LVS 目前的连接数</td></tr><tr><td><code>-timeout</code></td><td>显示 tcp tcpfin udp 的超时时间</td></tr><tr><td><code>--stats</code></td><td>显示统计信息</td></tr><tr><td><code>--rate</code></td><td>显示速率信息</td></tr><tr><td><code>--sort</code></td><td>对虚拟服务器和真实服务器排序输出</td></tr><tr><td><code>-n</code></td><td>输出 IP 地址和端口的数字形式</td></tr></tbody></table><h4 id="3-2、基于-NAT-模式"><a href="#3-2、基于-NAT-模式" class="headerlink" title="3.2、基于 NAT 模式"></a>3.2、基于 NAT 模式</h4><p>服务器列表</p><table><thead><tr><th>IP</th><th>节点</th></tr></thead><tbody><tr><td>192.168.1.100</td><td>前端负载均衡器</td></tr><tr><td>192.168.1.150</td><td>虚拟 IP</td></tr><tr><td>192.168.1.104</td><td>真实服务器</td></tr><tr><td>192.168.1.105</td><td>真实服务器</td></tr></tbody></table><h5 id="3-2-1、前端负载均衡器配置"><a href="#3-2-1、前端负载均衡器配置" class="headerlink" title="3.2.1、前端负载均衡器配置"></a>3.2.1、前端负载均衡器配置</h5><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment"># 安装 ipvsadm</span>yum install ipvsadm -y<span class="hljs-comment"># 开启 ip_forward</span><span class="hljs-built_in">echo</span> 1 &gt; /proc/sys/net/ipv4/ip_forward<span class="hljs-comment"># 安装 LVS 服务</span>ipvsadm -A -t 192.168.1.150:80<span class="hljs-comment"># 增加 realserver</span>ipvsadm -a -t 192.168.1.150:80 -r 192.168.1.104:80 -m -w 1ipvsadm -a -t 192.168.1.150:80 -r 192.168.1.105:80 -m -w 1</code></pre></div><h5 id="3-2-2、后端真实服务器配置"><a href="#3-2-2、后端真实服务器配置" class="headerlink" title="3.2.2、后端真实服务器配置"></a>3.2.2、后端真实服务器配置</h5><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment"># 关闭 ip_forward</span><span class="hljs-built_in">echo</span> 0 &gt; /proc/sys/net/ipv4/ip_forward<span class="hljs-comment"># 设置 VIP</span><span class="hljs-comment"># 复制一个网卡配置，在主网卡上绑定一个虚拟网卡</span>cp /etc/sysconfig/network-scripts/ifcfg-enp0s3 /etc/sysconfig/network-scripts/ifcfg-enp0s3:1<span class="hljs-comment"># 编辑虚拟网卡，修改其 IP 为 192.168.1.150，</span><span class="hljs-comment"># 注意重新生成 uuid 并更新 DEVICE 设备名称为 enp0s3:1</span>vim /etc/sysconfig/network-scripts/ifcfg-enp0s3:1<span class="hljs-comment"># 处理 arp 广播问题</span><span class="hljs-built_in">echo</span> 1 &gt; /proc/sys/net/ipv4/conf/enp0s3/arp_ignore<span class="hljs-built_in">echo</span> 2 &gt; /proc/sys/net/ipv4/conf/enp0s3/arp_announce<span class="hljs-built_in">echo</span> 1 &gt; /proc/sys/net/ipv4/conf/all/arp_ignore<span class="hljs-built_in">echo</span> 2 &gt; /proc/sys/net/ipv4/conf/all/arp_announce<span class="hljs-comment"># 设置路由</span>route add -host 192.168.1.150 dev enp0s3</code></pre></div><h5 id="3-2-3、测试-LVS"><a href="#3-2-3、测试-LVS" class="headerlink" title="3.2.3、测试 LVS"></a>3.2.3、测试 LVS</h5><p>在两台后端服务器上分别安装 nginx，并更改 index 页面，打印当前服务器 IP，最后在前端负载均衡器上测试访问虚拟 IP，此时前端负载均衡器将会自动根据算法负载到后端；此例中可以看到不断请求虚拟 IP 时，实际后端在做轮训</p><p><img src="https://cdn.oss.link/markdown/qjo23.jpg" alt="test lvs1"></p><h4 id="3-3、基于-DR-模式"><a href="#3-3、基于-DR-模式" class="headerlink" title="3.3、基于 DR 模式"></a>3.3、基于 DR 模式</h4><h5 id="3-3-1、前端负载均衡器配置"><a href="#3-3-1、前端负载均衡器配置" class="headerlink" title="3.3.1、前端负载均衡器配置"></a>3.3.1、前端负载均衡器配置</h5><p>服务器列表同上面的相同，ipvsadm 安装掠过</p><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment"># 先清除上次的配置</span>ipvsadm -C<span class="hljs-comment"># 开启 ip_forward(实际上面已经操作过)</span><span class="hljs-built_in">echo</span> 1 &gt; /proc/sys/net/ipv4/ip_forward<span class="hljs-comment"># 安装 LVS 服务</span>ipvsadm -A -t 192.168.1.150:80<span class="hljs-comment"># 增加两台 realserver 使用 DR 模式</span>ipvsadm -a -t 192.168.1.150:80 -r 192.168.1.104:80 -g -w 1ipvsadm -a -t 192.168.1.150:80 -r 192.168.1.105:80 -g -w 1</code></pre></div><h5 id="1-3-2、后端真实服务器配置"><a href="#1-3-2、后端真实服务器配置" class="headerlink" title="1.3.2、后端真实服务器配置"></a>1.3.2、后端真实服务器配置</h5><p>后端配置同上，测试掠过</p><h4 id="3-4、基于-TUN-模式"><a href="#3-4、基于-TUN-模式" class="headerlink" title="3.4、基于 TUN 模式"></a>3.4、基于 TUN 模式</h4><p><strong>从上面两个配置可以看到，不同模式实际上只是修改前端负载均衡器创建后端真实服务器的命令参数不通而已(<code>-m</code>、<code>-g</code>、<code>-i</code>)；唯一要注意的是不同模式对后端服务器要求可能不通，比如 DR 要求前端负载均衡器与后端真实服务器在同一网段，TUN 要求后端真实服务器支持 IP 隧道协议等，所以 TUN 模式在此也不在演示；关于上面修改内核参数的做法最好在 <code>sysctl.conf</code> 中修改保证永久生效</strong></p>]]></content>
    
    
    <summary type="html">LVS(Linux Virtual Server) 意为Linux虚拟服务器，是针对高可伸缩、高可用网络服务要求基于 IP 层和内容请求分发的负载均衡调度解决方案</summary>
    
    
    
    <category term="Linux" scheme="https://mritd.com/categories/linux/"/>
    
    
    <category term="Linux" scheme="https://mritd.com/tags/linux/"/>
    
  </entry>
  
  <entry>
    <title>Kubernetes 网络搭建-Calico</title>
    <link href="https://mritd.com/2016/12/01/set-up-kubernetes-cluster-by-calico/"/>
    <id>https://mritd.com/2016/12/01/set-up-kubernetes-cluster-by-calico/</id>
    <published>2016-11-30T23:58:30.000Z</published>
    <updated>2016-11-30T23:58:30.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>接上一篇，大早上试下 Calico，从目前的各种评论上来看 Calico 的性能要更好些，不过由于是纯三层的解决方案，某些用到二层的应用可能无法使用，不过目前还没遇到过，个人理解这种情况应该不多</p></blockquote><h3 id="一、环境准备"><a href="#一、环境准备" class="headerlink" title="一、环境准备"></a>一、环境准备</h3><p>首先有个 kubernetes 集群，集群网络处于未部署状态，集群信息如下</p><table><thead><tr><th>IP地址</th><th>节点</th></tr></thead><tbody><tr><td>192.168.1.101</td><td>master</td></tr><tr><td>192.168.1.102</td><td>node,etcd(单点)</td></tr><tr><td>192.168.1.103</td><td>node</td></tr></tbody></table><h3 id="二、开搞"><a href="#二、开搞" class="headerlink" title="二、开搞"></a>二、开搞</h3><p>至于 kubernetes 集群创建实在不想啰嗦，具体参考上一篇博客</p><p>Calico 官方提供了很好的文档支持，<a href="http://docs.projectcalico.org/v1.6/getting-started/kubernetes/">在这里</a> 基本能找到所有的参考教程，以下直接照着官方文档来</p><p>首先把 Calico 的 yaml 下载下来，这里采用官方文档 kubernetes 页面的 yaml，<strong>非 kubeadm 的</strong>，kubeadm 页面的 yaml 里面 多了创建 etcd 集群信息啥的，没什么卵用</p><div class="hljs code-wrapper"><pre><code class="hljs sh">wget http://docs.projectcalico.org/v1.6/getting-started/kubernetes/installation/hosted/calico.yaml</code></pre></div><p>编辑 <code>calico.yaml</code>，修改 etcd 地址</p><div class="hljs code-wrapper"><pre><code class="hljs sh">vim calico.yaml<span class="hljs-comment"># 将 etcd_endpoints 修改掉即可</span>etcd_endpoints: <span class="hljs-string">&quot;http://192.168.1.102:2379&quot;</span></code></pre></div><p>然后创建网络</p><div class="hljs code-wrapper"><pre><code class="hljs sh">kubectl create -f calico.yaml</code></pre></div><p>创建完成后如下</p><p><img src="https://cdn.oss.link/markdown/ub8yg.jpg" alt="Calico"></p><p>节点测试如下</p><p><img src="https://cdn.oss.link/markdown/p7zlt.jpg" alt="all node"></p><p><img src="https://cdn.oss.link/markdown/ybdw5.jpg" alt="node2"></p><p><img src="https://cdn.oss.link/markdown/3qm8t.jpg" alt="node3"></p><p><strong>更细节的性能体现等可参考 <a href="http://blog.dataman-inc.com/shurenyun-docker-133/">将Docker网络方案进行到底</a></strong></p>]]></content>
    
    
    <summary type="html">接上一篇，大早上试下 Calico，从目前的各种评论上来看 Calico 的性能要更好些，不过由于是纯三层的解决方案，某些用到二层的应用可能无法使用，不过目前还没遇到过，个人理解这种情况应该不多</summary>
    
    
    
    <category term="Kubernetes" scheme="https://mritd.com/categories/kubernetes/"/>
    
    
    <category term="Linux" scheme="https://mritd.com/tags/linux/"/>
    
    <category term="Docker" scheme="https://mritd.com/tags/docker/"/>
    
    <category term="Kubernetes" scheme="https://mritd.com/tags/kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>Kubernetes 网络搭建-flannel</title>
    <link href="https://mritd.com/2016/11/30/set-up-kubernetes-cluster-by-flannel/"/>
    <id>https://mritd.com/2016/11/30/set-up-kubernetes-cluster-by-flannel/</id>
    <published>2016-11-30T14:57:48.000Z</published>
    <updated>2016-11-30T14:57:48.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>一直用 weave，本篇记录一下 kubernetes 使用 flannel 作为网络组件，flannel 以 pod 方式部署</p></blockquote><h3 id="一、环境准备"><a href="#一、环境准备" class="headerlink" title="一、环境准备"></a>一、环境准备</h3><p>首先有个 kubernetes 集群，集群网络处于未部署状态，集群信息如下</p><table><thead><tr><th>IP地址</th><th>节点</th></tr></thead><tbody><tr><td>192.168.1.101</td><td>master</td></tr><tr><td>192.168.1.102</td><td>node,etcd(单点)</td></tr><tr><td>192.168.1.103</td><td>node</td></tr></tbody></table><h3 id="二、开搞"><a href="#二、开搞" class="headerlink" title="二、开搞"></a>二、开搞</h3><h4 id="2-1、创建-kubernetes-集群"><a href="#2-1、创建-kubernetes-集群" class="headerlink" title="2.1、创建 kubernetes 集群"></a>2.1、创建 kubernetes 集群</h4><p>具体各种注意细节这里不再阐述，请参考本博客其他文章，<strong>唯一要注意一点是创建集群(init)时要增加 <code>--pod-network-cidr 10.244.0.0/16</code> 参数；</strong>网段根据需要自己指定，如果不使用 <code>--pod-network-cidr</code>  参数，则 flannel pod 启动后会出现 <code>failed to register network: failed to acquire lease: node &quot;xxxxxx&quot; pod cidr not assigned</code> 错误，以下为部分样例命令</p><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment"># 安装 rpm</span>tee /etc/yum.repos.d/mritd.repo &lt;&lt; <span class="hljs-string">EOF</span><span class="hljs-string">[mritdrepo]</span><span class="hljs-string">name=Mritd Repository</span><span class="hljs-string">baseurl=https://rpm.mritd.me/centos/7/x86_64</span><span class="hljs-string">enabled=1</span><span class="hljs-string">gpgcheck=1</span><span class="hljs-string">gpgkey=https://cdn.oss.link/keys/rpm.public.key</span><span class="hljs-string">EOF</span>yum install -y kubelet kubectl kubernetes-cni kubeadm<span class="hljs-comment"># 处理 hostname</span><span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;192-168-1-101.master&quot;</span> &gt; /etc/hostname<span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;127.0.0.1   192-168-1-101.master&quot;</span> &gt;&gt; /etc/hostssysctl kernel.hostname=<span class="hljs-string">&quot;192-168-1-101.master&quot;</span><span class="hljs-comment"># load 镜像</span>images=(kube-proxy-amd64:v1.4.6 kube-discovery-amd64:1.0 kubedns-amd64:1.7 kube-scheduler-amd64:v1.4.6 kube-controller-manager-amd64:v1.4.6 kube-apiserver-amd64:v1.4.6 etcd-amd64:2.2.5 kube-dnsmasq-amd64:1.3 exechealthz-amd64:1.1 pause-amd64:3.0 kubernetes-dashboard-amd64:v1.4.1)<span class="hljs-keyword">for</span> imageName <span class="hljs-keyword">in</span> <span class="hljs-variable">$&#123;images[@]&#125;</span> ; <span class="hljs-keyword">do</span>  docker pull mritd/<span class="hljs-variable">$imageName</span>  docker tag mritd/<span class="hljs-variable">$imageName</span> gcr.io/google_containers/<span class="hljs-variable">$imageName</span>  docker rmi mritd/<span class="hljs-variable">$imageName</span><span class="hljs-keyword">done</span><span class="hljs-comment"># 其他的什么 dns、etcd 搞完了直接初始化</span>kubeadm init --api-advertise-addresses 192.168.1.101 --external-etcd-endpoints http://192.168.1.102:2379 --use-kubernetes-version v1.4.6 --pod-network-cidr 10.244.0.0/16</code></pre></div><h4 id="2-2、创建-flannel-网络"><a href="#2-2、创建-flannel-网络" class="headerlink" title="2.2、创建 flannel 网络"></a>2.2、创建 flannel 网络</h4><p>前面如果都设置好创建网络很简单，跟 weave 一样</p><div class="hljs code-wrapper"><pre><code class="hljs sh">kubectl create -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml</code></pre></div><p>有兴趣的可以把 yml 搞下来看下，由于他的镜像托管在 <code>quay.io</code>，所以没有墙的问题，也可以提前 load 进来；对于 yml 上面的 <code>ConfigMap</code> 中的 ip 段最好与 <code>--pod-network-cidr</code> 一致(不一致没测试，想作死自己试吧)，然后稍等片刻网络便创建成功，截图如下</p><p><img src="https://cdn.oss.link/markdown/fh723.jpg" alt="flannel"></p><h4 id="2-3、网络测试"><a href="#2-3、网络测试" class="headerlink" title="2.3、网络测试"></a>2.3、网络测试</h4><p>由于环境有限(virtualbox 虚拟机)，所以暂时只测试一下网络互通是否有问题，关于性能啥的由于本人对网络部分也一直是个短板，需要大神们自己来了，如果可以给篇测试报告我也看看 <code>:)</code></p><p><strong>rc 如下</strong></p><div class="hljs code-wrapper"><pre><code class="hljs sh">apiVersion: v1kind: ReplicationControllermetadata:  name: alpine  labels:    name: alpinespec:  replicas: 2  selector:    name: alpine  template:    metadata:      labels:        name: alpine    spec:      containers:        - image: mritd/alpine:3.4          imagePullPolicy: Always          name: alpine          <span class="hljs-built_in">command</span>:             - <span class="hljs-string">&quot;bash&quot;</span>             - <span class="hljs-string">&quot;-c&quot;</span>            - <span class="hljs-string">&quot;while true;do echo test;done&quot;</span>          ports:            - containerPort: 8080              name: alpine</code></pre></div><p><strong>去两个主机上分别进入容器，然后互 ping 集群 IP 可以 ping 通</strong>；图2 ping 错了，不重新截图了，谅解</p><p><img src="https://cdn.oss.link/markdown/x4i0j.jpg" alt="cluster ip"></p><p><img src="https://cdn.oss.link/markdown/v24ju.jpg" alt="node2 ping"></p><p><img src="https://cdn.oss.link/markdown/iukrh.jpg" alt="node3 ping"></p><p><strong>本文只是简单搭建，其他更高级的性能测试交给各位玩网络的大神吧</strong></p>]]></content>
    
    
    <summary type="html">一直用 weave，本篇记录一下 kubernetes 使用 flannel 作为网络组件，flannel 以 pod 方式部署</summary>
    
    
    
    <category term="Kubernetes" scheme="https://mritd.com/categories/kubernetes/"/>
    
    
    <category term="Linux" scheme="https://mritd.com/tags/linux/"/>
    
    <category term="Docker" scheme="https://mritd.com/tags/docker/"/>
    
    <category term="Kubernetes" scheme="https://mritd.com/tags/kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>kubeadm 续坑篇</title>
    <link href="https://mritd.com/2016/11/21/kubeadm-other-problems/"/>
    <id>https://mritd.com/2016/11/21/kubeadm-other-problems/</id>
    <published>2016-11-21T12:16:51.000Z</published>
    <updated>2016-11-21T12:16:51.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>断断续续鼓捣 kubeadm 搭建集群已经很长时间了，目前 kubeadm 已经进入了 beat 阶段，各项功能相对稳定，但是继上篇 <a href="https://mritd.me/2016/10/29/set-up-kubernetes-cluster-by-kubeadm/">kubeadm 搭建 kubernetes 集群</a> 之后还是踩了许多坑，在此记录一下</p></blockquote><h3 id="一、etcd-单点问题"><a href="#一、etcd-单点问题" class="headerlink" title="一、etcd 单点问题"></a>一、etcd 单点问题</h3><p>默认 kubeadm 创建的集群会在内部启动一个单点的 etcd，当然大部分情况下 etcd 还是很稳定的，<strong>但是一但 etcd 由于某种原因挂掉，这个问题会非常严重，会导致整个集群不可用</strong>。具体原因是 etcd 存储着 kubernetes 各种元数据信息；包括 <code>kubectl get pod</code> 等等基础命令实际上全部是调用 RESTful API 从 etcd 中获取的信息；<strong>所以一但 etcd 挂掉以后，基本等同于 <code>kubectl</code> 命令不可用，此时将变为 ‘瞎子’，集群各节点也会因无法从 etcd 获取数据而出现无法调度，最终挂掉</strong>。</p><p><strong>解决办法是在使用 kubeadm 创建集群时使用 <code>--external-etcd-endpoints</code> 参数指定外部 etcd 集群，此时 kubeadm 将不会在内部创建 etcd，转而使用外部我们指定的 etcd 集群，如果外部 etcd 集群配置了 SSL 加密，那么还需要配合 <code>--external-etcd-cafile</code>、<code>--external-etcd-certfile</code>、<code>--external-etcd-keyfile</code> 三个参数指定 etcd 的 CA证书、CA签发的使用证书和私钥文件，命令如下</strong></p><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment"># 非 SSL</span>kubeadm init --external-etcd-endpoints http://192.168.1.100:2379<span class="hljs-comment"># etcd SSL</span>kubeadm init --external-etcd-endpoints https://192.168.1.100:2379 --external-etcd-cafile /path/to/ca --external-etcd-certfile /path/to/cert --external-etcd-keyfile /path/to/privatekey</code></pre></div><h3 id="二、etcd-不可与-master-同在"><a href="#二、etcd-不可与-master-同在" class="headerlink" title="二、etcd 不可与 master 同在"></a>二、etcd 不可与 master 同在</h3><p>‘愿上帝与你同在’……这个坑是由于 kubeadm 的 check 机制的 bug 造成的，目前还没有修复；表现为 <strong>当 etcd 与 master 在同一节点时，kubeadm init 会失败，同时报错信息提示 ‘已经存在了 <code>/var/lib/etcd</code> 目录，或者 2379 端口被占用’<strong>；因为默认 kubeadm 会创建 etcd，而默认的 etcd 会占用这个目录和 2379 端口，</strong>即使你加了 <code>--external-etcd-endpoints</code> 参数，kubeadm 仍然会检测这两项条件是否满足，不满足则禁止 init 操作</strong></p><p><strong>解决办法就是要么外部的 etcd 更换数据目录(<code>/var/lib/etcd</code>)和端口，要么干脆不要和 master 放在同一主机即可</strong></p><h3 id="三、巨大的日志"><a href="#三、巨大的日志" class="headerlink" title="三、巨大的日志"></a>三、巨大的日志</h3><p>熟悉的小伙伴应该清楚，基本上每个 kubernetes 组件都会有个通用的参数 <code>--v</code>；这个参数用于控制 kubernetes 各个组件的日志级别，在早期(alpha)的 kubeadm 版本中，如果不进行调整，默认创建集群所有组件日志级别全部为 <code>--v=4</code> 即最高级别输出，这会导致在业务量大的时候磁盘空间以 <strong>‘我去尼玛’</strong> 的速度增长，尤其是 <code>kube-proxy</code> 组件的容器，会疯狂吃掉你的磁盘空间，然后剩下懵逼的你不知为何。在后续的版本中(beta)发现日志级别已经降到了 <code>--v=2</code>，不过对于完全不怎么看日志的我来说还是无卵用……</p><p><strong>解决办法有两种方案:</strong></p><p><strong>如果已经 <code>--v=4</code> 跑起来了(检查方法就是随便 describe 一个 kube-proxy 的容器，看下 command 字段就能看到)，并且无法停止重建集群，那么最简单的办法就是使用 <code>kubectl edit ds xxx</code> 方式编译一下相关 ds 文件等，然后手动杀掉相关 pod，让 kubernetes 自动重建即可，如果命令行用着不爽也可以通过 dashboard 更改</strong></p><p><strong>如果还没开始搭建，或者可以停掉重建，那么只需在 <code>kubeadm init</code> 之前 <code>export KUBE_COMPONENT_LOGLEVEL=&#39;--v=0&#39;</code> 即可</strong></p><h3 id="四、新节点加入-dns-要你命"><a href="#四、新节点加入-dns-要你命" class="headerlink" title="四、新节点加入 dns 要你命"></a>四、新节点加入 dns 要你命</h3><p>当 kubeadm 创建好集群以后，如果有需要增加新节点，那么在 <code>kubeadm join</code> 之后务必检查 <code>kube-dns</code> 组件，dns 在某些(weave 启动不完整或不正常)情况下，会由于新节点加入而挂掉，此时整个集群 dns 失效，<strong>所以最好 join 完观察一会 dns 状态，如果发现不正常马上杀掉  dns pod，让 kubernetes 自动重建；如果情况允许最好全部 join 完成后直接干掉 dns 让 kubernetes 重建一下</strong></p><h3 id="五、单点的-dns-浪起来让你怕"><a href="#五、单点的-dns-浪起来让你怕" class="headerlink" title="五、单点的 dns 浪起来让你怕"></a>五、单点的 dns 浪起来让你怕</h3><p>kubeadm 创建的 dns 默认也是单点的，而 dns 至关重要，只要一挂瞬间整个集群全部 <code>game over</code>；<strong>不过暂时还是没有发现能在 init 时候创建多个 dns 的方法；不过在集群创建后可以通过 <code>kubectl edit deploy kube-dns</code> 的方式修改其副本数量，让其创建多个副本即可</strong></p><h3 id="六、永远的-v1-4-4"><a href="#六、永远的-v1-4-4" class="headerlink" title="六、永远的 v1.4.4"></a>六、永远的 v1.4.4</h3><p>在一开始 kubeadm 创建集群时，采用的基础组件基本都是写死的，不过现在增加了 <code>--use-kubernetes-version</code> 选项，在 init 时使用该选项可以指定使用的基础组件(kube-proxy、apiserver 等)的版本，如 <code>kubeadm init --use-kubernetes-version v1.4.6</code> 即可使用 1.4.6 的镜像，目前最新版本的 rpm kubelet 版本为 1.4.4，从目前测试来看 1.4.4 的 kubelet 与 1.4.6 的其他版本组件一起运行尚未出现问题，不过最好在准备超版本运行之前，把 kubelet 的二进制文件也换成相应版本的</p><h3 id="七、reset-让你一无所有"><a href="#七、reset-让你一无所有" class="headerlink" title="七、reset 让你一无所有"></a>七、reset 让你一无所有</h3><p>kubeadm 在 <code>alpha</code> 时官方文档页面提供了重建集群脚本，如下</p><div class="hljs code-wrapper"><pre><code class="hljs sh">systemctl stop kubelet;docker rm -f -v $(docker ps -q);find /var/lib/kubelet | xargs -n 1 findmnt -n -t tmpfs -o TARGET -T | uniq | xargs -r umount -v;rm -r -f /etc/kubernetes /var/lib/kubelet /var/lib/etcd;</code></pre></div><p>从脚本上看很容易知道<strong>第二条命令很危险</strong>，他会干掉所有正在运行的容器，如果你的 node 上恰好有 <code>docker-compose</code> 启动的重要服务，这么这一下后果可想而知；<strong>kubeadm 到了 <code>alpha2</code> 之后，提供了 <code>kubeadm reset</code> 命令来重建集群，我已开始以为这是个很好的事情，既然重写了肯定很好用；但是上帝总是跟你讲 ‘Hello World’，经过测试(实际上是躺枪了)我发现 reset 其实就是把这四条 shell 命令封装一起变成一个 <code>kubeadm reset</code> 而已，所以说此命令慎用，不行就用上面的脚本手动档删除，否则一条 <code>reset</code> 倾家荡产</strong></p><p><strong>未完待续，欢迎补充，如果有坑继续添加……</strong></p><blockquote><p>本文参考 <a href="http://kubernetes.io/docs/admin/kubeadm/">kubeadm reference</a></p></blockquote>]]></content>
    
    
    <summary type="html">断断续续鼓捣 kubeadm 搭建集群已经很长时间了，目前 kubeadm 已经进入了 beat 阶段，各项功能相对稳定，但是继上篇 [kubeadm 搭建 kubernetes 集群](https://mritd.me/2016/10/29/set-up-kubernetes-cluster-by-kubeadm/) 之后还是踩了许多坑，在此记录一下</summary>
    
    
    
    <category term="Kubernetes" scheme="https://mritd.com/categories/kubernetes/"/>
    
    
    <category term="Linux" scheme="https://mritd.com/tags/linux/"/>
    
    <category term="Docker" scheme="https://mritd.com/tags/docker/"/>
    
    <category term="Kubernetes" scheme="https://mritd.com/tags/kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>CentOS 升级 kernel</title>
    <link href="https://mritd.com/2016/11/08/update-centos-kernel/"/>
    <id>https://mritd.com/2016/11/08/update-centos-kernel/</id>
    <published>2016-11-08T13:24:31.000Z</published>
    <updated>2016-11-08T13:24:31.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>最紧要鼓捣 Dokcer Swarm，而 Swarm 的 overlay 网络需要 3.15 以上的 kernel，故记录一下升级内核的过程</p></blockquote><h3 id="一、手动档"><a href="#一、手动档" class="headerlink" title="一、手动档"></a>一、手动档</h3><p>手动档就是从源码开始编译内核安装，好处是可以自己选择任意版本的内核，缺点就是耗时长，编译安装消耗系统资源</p><h4 id="1-1、获取-kernel-源码"><a href="#1-1、获取-kernel-源码" class="headerlink" title="1.1、获取 kernel 源码"></a>1.1、获取 kernel 源码</h4><p>这世界上最伟大的 Linux 内核源码下载地址是 <a href="https://kernel.org/">kernel 官网</a>，选择一个稳定版本下载即可</p><p><img src="https://cdn.oss.link/markdown/3se7u.jpg" alt="kernel homepage"></p><h4 id="1-2、解压并清理"><a href="#1-2、解压并清理" class="headerlink" title="1.2、解压并清理"></a>1.2、解压并清理</h4><p>官方要求将其解压到 <code>/usr/src</code> 目录，其实在哪都可以，为了规范一点索性也解压到此位置，然后为了防止编译残留先做一次清理动作</p><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment"># 下载内核源码</span>wget https://cdn.kernel.org/pub/linux/kernel/v4.x/linux-4.8.6.tar.xz<span class="hljs-comment"># 解压并移动到 /usr/src</span>tar -Jxvf linux-4.8.6.tar.xzmv linux-4.8.6 /usr/src/kernels<span class="hljs-comment"># 执行清理(没 gcc 的要装一下)</span><span class="hljs-built_in">cd</span> /usr/src/kernels/linux-4.8.6make mrproper &amp;&amp; make clean</code></pre></div><h4 id="1-3、生成编译配置表"><a href="#1-3、生成编译配置表" class="headerlink" title="1.3、生成编译配置表"></a>1.3、生成编译配置表</h4><p>kernel 在编译时需要一个配置文件(<code>.config</code>)，用于描述开启哪些特性等，该文件一般可通过一下四种途径获得:</p><ul><li>复制当前系统编译配置表，即 <code>cp /boot/config-xxx .config</code>；如果系统有多个内核，那么根据版本号选择最新的即可</li><li>使用 <code>make defconfig</code> 命令获取当前系统编译配置表，该命令会自动写入到 <code>.config</code> 中</li><li>使用 <code>make localmodconfig</code> 命令开启交互模式，然后根据提示生成编译配置表</li><li>使用 <code>make oldconfig</code> 命令根据旧的编译配置表生成新的编译配置表，<strong>刚方式会直接读取旧的便已配置表，并在以前没有设定过的配置时会自动开启交互模式</strong></li></ul><p>这里采用最后一种方式生成</p><p><img src="https://cdn.oss.link/markdown/f9j5r.jpg" alt="create kernel compile param"></p><h4 id="1-4、编译并安装"><a href="#1-4、编译并安装" class="headerlink" title="1.4、编译并安装"></a>1.4、编译并安装</h4><p>内核配置表生成完成后便可进行编译和安装(需要安装 bc、openssl-devel等)</p><div class="hljs code-wrapper"><pre><code class="hljs sh">makemake modulesmake modules_installmake install</code></pre></div><p><strong>最后执行重启验证即可，验证成功后可删除旧的内核(<code>rpm -qa | grep kernel</code>)</strong></p><h3 id="二、自动档"><a href="#二、自动档" class="headerlink" title="二、自动档"></a>二、自动档</h3><p>相对于手动档编译安装，CentOS 还可以通过使用 elrepo 源的方式直接安装最新稳定版 kernel，脚本如下</p><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment"># import key</span>rpm --import https://www.elrepo.org/RPM-GPG-KEY-elrepo.org<span class="hljs-comment"># install elrepo repo</span>rpm -Uvh http://www.elrepo.org/elrepo-release-7.0-2.el7.elrepo.noarch.rpm<span class="hljs-comment"># install kernel</span>yum --enablerepo=elrepo-kernel install  kernel-ml-devel kernel-ml -y<span class="hljs-comment"># modify grub</span>grub2-set-default 0<span class="hljs-comment"># reboot</span>reboot</code></pre></div>]]></content>
    
    
    <summary type="html">最紧要鼓捣 Dokcer Swarm，而 Swarm 的 overlay 网络需要 3.15 以上的 kernel，故记录一下升级内核的过程</summary>
    
    
    
    <category term="Linux" scheme="https://mritd.com/categories/linux/"/>
    
    
    <category term="Linux" scheme="https://mritd.com/tags/linux/"/>
    
  </entry>
  
  <entry>
    <title>晚月</title>
    <link href="https://mritd.com/2016/11/02/wanyue/"/>
    <id>https://mritd.com/2016/11/02/wanyue/</id>
    <published>2016-11-02T15:36:51.000Z</published>
    <updated>2016-11-02T15:36:51.000Z</updated>
    
    <content type="html"><![CDATA[<h4 id="晚月"><a href="#晚月" class="headerlink" title="晚月"></a>晚月</h4><div class="hljs code-wrapper"><pre><code class="hljs sh">轻风细雨秋凉意，枯叶随风去，冷了心头，乱了谁的绪？待到寒风飘雪时，冰凌刺骨痛，旧伤再起，扰了谁的梦？又道凡尘多琐事，不想触则伤断魂；罢了罢了，坎坷千载遇伯乐，何思今朝无故人？</code></pre></div>]]></content>
    
    
    <summary type="html">晚月</summary>
    
    
    
    <category term="随笔" scheme="https://mritd.com/categories/%E9%9A%8F%E7%AC%94/"/>
    
    
    <category term="随笔" scheme="https://mritd.com/tags/%E9%9A%8F%E7%AC%94/"/>
    
  </entry>
  
  <entry>
    <title>kubeadm 搭建 kubernetes 集群</title>
    <link href="https://mritd.com/2016/10/29/set-up-kubernetes-cluster-by-kubeadm/"/>
    <id>https://mritd.com/2016/10/29/set-up-kubernetes-cluster-by-kubeadm/</id>
    <published>2016-10-29T06:58:49.000Z</published>
    <updated>2016-10-29T06:58:49.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>距离上一篇 <a href="https://mritd.me/2016/10/09/kubernetes-1.4-create-cluster/">kubernetes 1.4 集群搭建</a> 发布间隔不算太久，自己也不断地在生产和测试环境鼓捣，有不少 “逗比” 的经历，准备写一下具体的 kubeadm 搭建集群的一些坑和踩坑的经验，如果没有使用过 kubeadm 的同学，最好先看下上面的文章，然后鼓捣一遍，也许并不会成功，但大部分坑再来看此文会有收获</p></blockquote><h3 id="一、环境准备"><a href="#一、环境准备" class="headerlink" title="一、环境准备"></a>一、环境准备</h3><p>首先环境还是三台虚拟机，虚拟机地址如下</p><table><thead><tr><th>IP 地址</th><th>节点</th></tr></thead><tbody><tr><td>192.168.1.167</td><td>master</td></tr><tr><td>192.168.1.189</td><td>node1</td></tr><tr><td>192.168.1.176</td><td>node2</td></tr></tbody></table><p>然后每台机器安装好 docker，至于 rpm 安装包版本下面介绍</p><h3 id="二、说点正经事"><a href="#二、说点正经事" class="headerlink" title="二、说点正经事"></a>二、说点正经事</h3><h4 id="2-1、安装包从哪来"><a href="#2-1、安装包从哪来" class="headerlink" title="2.1、安装包从哪来"></a>2.1、安装包从哪来</h4><p>官方的文档页面更新并不及时，同时他的 yum 源更新也很慢，再者…那他妈可是 Google 的服务器，能特么连上吗？以前总是在国外服务器使用 <code>yumdownloader</code> 下载，然后 <code>scp</code> 到本地，虽然能解决问题，但是蛋碎一地…最后找到了源头，如下</p><p><strong>Kubernetes 编译的各种发行版安装包来源于 Github 上的另一个叫 release 的项目，地址 <a href="https://github.com/kubernetes/release">点这里</a>，把这个项目 <code>clone</code> 下来，由于本人是 Centos 用户，所以进入 rpm 目录，在安装好 docker 的机器上执行那个 <code>docker-build.sh</code> 脚本即可编译 rpm 包，最后会生成到当前目录的 <code>output</code> 目录下,截图如下</strong></p><p><img src="https://cdn.oss.link/markdown/3zs7u.jpg" alt="release"></p><p><img src="https://cdn.oss.link/markdown/8b3a4.jpg" alt="rpm目录"></p><h4 id="2-2、镜像从哪来"><a href="#2-2、镜像从哪来" class="headerlink" title="2.2、镜像从哪来"></a>2.2、镜像从哪来</h4><p>对的，没错，gcr.io 就是 Google 的域名，服务器更不用提，所以在进行 <code>kubeadm init</code> 操作时如果不先把这些镜像 load 进去绝对会卡死不动，以下列出了所需镜像，但是版本号根据 rpm 版本不同可能略有不同，具体怎么看下面介绍</p><table><thead><tr><th>镜像名称</th><th>版本号</th></tr></thead><tbody><tr><td>gcr.io/google_containers/kube-discovery-amd64</td><td>1.0</td></tr><tr><td>gcr.io/google_containers/kubedns-amd64</td><td>1.7</td></tr><tr><td>gcr.io/google_containers/kube-proxy-amd64</td><td>v1.4.1</td></tr><tr><td>gcr.io/google_containers/kube-scheduler-amd64</td><td>v1.4.1</td></tr><tr><td>gcr.io/google_containers/kube-controller-manager-amd64</td><td>v1.4.1</td></tr><tr><td>gcr.io/google_containers/kube-apiserver-amd64</td><td>v1.4.1</td></tr><tr><td>gcr.io/google_containers/etcd-amd64</td><td>2.2.5</td></tr><tr><td>gcr.io/google_containers/kube-dnsmasq-amd64</td><td>1.3</td></tr><tr><td>gcr.io/google_containers/exechealthz-amd64</td><td>1.1</td></tr><tr><td>gcr.io/google_containers/pause-amd64</td><td>3.0</td></tr></tbody></table><p><strong>这些镜像有两种办法可以获取，第一种是利用一台国外的服务器，在上面 pull 下来，然后再 save 成 tar 文件，最后 scp 到本地 load 进去；相对于第一种方式比较坑的是取决于服务器速度，每次搞起来也很蛋疼，第二种方式就是利用 docker hub 做中转，简单的说就是利用 docker hub 的自动构建功能，在 Github 中创建一个 Dockerfile，里面只需要 <code>FROM xxxx</code> 这些 gcr.io 的镜像即可，最后 pull 到本地，然后再 tag 一下</strong></p><p><strong>首先创建一个 github 项目，可以直接 fork 我的即可</strong></p><p><img src="https://cdn.oss.link/markdown/2eo34.jpg" alt="docker-libray"></p><p>其中每个 Dockerfile 只需要 <code>FROM</code> 一下即可</p><p><img src="https://cdn.oss.link/markdown/cxva2.jpg" alt="Dockerfile"></p><p><strong>最后在 Docker Hub 上创建自动构建项目</strong></p><p><img src="https://cdn.oss.link/markdown/p5khs.jpg" alt="createproject"></p><p><img src="https://cdn.oss.link/markdown/gc8vl.jpg" alt="from github"></p><p><img src="https://cdn.oss.link/markdown/9ufnd.jpg" alt="selectproject"></p><p><img src="https://cdn.oss.link/markdown/ud42y.jpg" alt="details"></p><p><strong>最后要手动触发一下，然后 Docker Hub 才会开始给你编译</strong></p><p><img src="https://cdn.oss.link/markdown/phgsg.jpg" alt="Tigger"></p><p><strong>等待完成即可直接 pull 了</strong></p><p><img src="https://cdn.oss.link/markdown/itnw3.jpg" alt="success"></p><h4 id="2-3、镜像版本怎么整"><a href="#2-3、镜像版本怎么整" class="headerlink" title="2.3、镜像版本怎么整"></a>2.3、镜像版本怎么整</h4><p>上面已经解决了镜像获取问题，但是一大心病就是 “我特么怎么知道是哪个版本的”，为了发扬 “刨根问底” 的精神，<strong>先进行一遍 <code>kubeadm init</code>，这时候绝对卡死，此时进入 <code>/etc/kubernetes/manifests</code> 可以看到许多 json 文件，这些文件中定义了需要哪些基础镜像</strong></p><p><img src="https://cdn.oss.link/markdown/3ovg8.jpg" alt="all json"></p><p><img src="https://cdn.oss.link/markdown/uitnd.jpg" alt="image version"></p><p>从上图中基本可以看到 <code>kubeadm init</code> 的时候会拉取哪些基础镜像了，<strong>但是还有一些镜像，仍然无法找到，比如<code>kubedns</code>、<code>pause</code> 等，至于其他的镜像版本，可以从源码中找到，源码位置是 <code>kubernetes/cmd/kubeadm/app/images/images.go</code> 这个文件中，如下所示:</strong> </p><p><img src="https://cdn.oss.link/markdown/ocgu4.jpg" alt="image version"></p><p>剩余的一些镜像，比如 <code>kube-proxy-amd64</code>、<code>kube-discovery-amd64</code> 两个镜像，其中 <code>kube-discovery-amd64</code> 现在一直是 1.0 版本，源码如下所示</p><p><img src="https://cdn.oss.link/markdown/mp3qo.jpg" alt="discovery version"></p><p><code>kube-proxy-amd64</code> 则是一直跟随基础组件的主版本，也就是说如果从 <code>manifests</code> 中看到 controller 等版本是 <code>v.1.4.4</code>，那么 <code>kube-proxy-amd64</code> 也是这个版本，源码如下</p><p><img src="https://cdn.oss.link/markdown/tienu.jpg" alt="proxy version"></p><p>最后根据这些版本去 github 上准备相应的 Dockerfile，在利用 Docker Hub 的自动构建 build 一下，再 pull 下来 tag 成对应的镜像名称即可</p><h3 id="三、搭建集群"><a href="#三、搭建集群" class="headerlink" title="三、搭建集群"></a>三、搭建集群</h3><h4 id="3-1、主机名处理"><a href="#3-1、主机名处理" class="headerlink" title="3.1、主机名处理"></a>3.1、主机名处理</h4><p><strong>经过亲测，节点主机名最好为 <code>xxx.xxx</code> 这种域名格式，否则在某些情况下，POD 中跑的程序使用域名解析时可能出现问题，所以先要处理一下主机名</strong></p><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment"># 写入 hostname(node 节点后缀改成 .node)</span><span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;192-168-1-167.master&quot;</span> &gt; /etc/hostname <span class="hljs-comment"># 加入 hosts</span><span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;127.0.0.1   192-168-1-167.master&quot;</span> &gt;&gt; /etc/hosts<span class="hljs-comment"># 不重启情况下使内核生效</span>sysctl kernel.hostname=192-168-1-167.master<span class="hljs-comment"># 验证是否修改成功</span>➜  ~ hostname192-168-1-167.master</code></pre></div><h4 id="3-2、load-镜像"><a href="#3-2、load-镜像" class="headerlink" title="3.2、load 镜像"></a>3.2、load 镜像</h4><p>由于本人已经在 Docker Hub 上处理好了相关镜像，所以直接 pull 下来 tag 一下即可，</p><div class="hljs code-wrapper"><pre><code class="hljs sh">images=(kube-proxy-amd64:v1.4.4 kube-discovery-amd64:1.0 kubedns-amd64:1.7 kube-scheduler-amd64:v1.4.4 kube-controller-manager-amd64:v1.4.4 kube-apiserver-amd64:v1.4.4 etcd-amd64:2.2.5 kube-dnsmasq-amd64:1.3 exechealthz-amd64:1.1 pause-amd64:3.0 kubernetes-dashboard-amd64:v1.4.1)<span class="hljs-keyword">for</span> imageName <span class="hljs-keyword">in</span> <span class="hljs-variable">$&#123;images[@]&#125;</span> ; <span class="hljs-keyword">do</span>  docker pull mritd/<span class="hljs-variable">$imageName</span>  docker tag mritd/<span class="hljs-variable">$imageName</span> gcr.io/google_containers/<span class="hljs-variable">$imageName</span>  docker rmi mritd/<span class="hljs-variable">$imageName</span><span class="hljs-keyword">done</span></code></pre></div><h4 id="3-3、安装-rpm"><a href="#3-3、安装-rpm" class="headerlink" title="3.3、安装 rpm"></a>3.3、安装 rpm</h4><p>rpm 获取办法上文已经提到，可以自己编译，这里我已经编译好并维护了一个 yum 源，直接yum install 即可(懒)</p><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment"># 添加 yum 源</span>tee /etc/yum.repos.d/mritd.repo &lt;&lt; <span class="hljs-string">EOF</span><span class="hljs-string">[mritdrepo]</span><span class="hljs-string">name=Mritd Repository</span><span class="hljs-string">baseurl=https://rpm.mritd.me/centos/7/x86_64</span><span class="hljs-string">enabled=1</span><span class="hljs-string">gpgcheck=1</span><span class="hljs-string">gpgkey=https://cdn.oss.link/keys/rpm.public.key</span><span class="hljs-string">EOF</span><span class="hljs-comment"># 刷新cache</span>yum makecache<span class="hljs-comment"># 安装</span>yum install -y kubelet kubectl kubernetes-cni kubeadm</code></pre></div><h4 id="3-4、初始化-master"><a href="#3-4、初始化-master" class="headerlink" title="3.4、初始化 master"></a>3.4、初始化 master</h4><p><strong>等会有个坑，kubeadm 等相关 rpm 安装后会生成 <code>/etc/kubernetes</code> 目录，而 kubeadm init 时候又会检测这些目录是否存在，如果存在则停止初始化，所以要先清理一下，以下清理脚本来源于 <a href="http://kubernetes.io/docs/getting-started-guides/kubeadm/">官方文档 Tear down 部分</a>，该脚本同样适用于初始化失败进行重置</strong></p><div class="hljs code-wrapper"><pre><code class="hljs sh">systemctl stop kubelet;<span class="hljs-comment"># 注意: 下面这条命令会干掉所有正在运行的 docker 容器，</span><span class="hljs-comment"># 如果要进行重置操作，最好先确定当前运行的所有容器都能干掉(干掉不影响业务)，</span><span class="hljs-comment"># 否则的话最好手动删除 kubeadm 创建的相关容器(gcr.io 相关的)</span>docker rm -f -v $(docker ps -q);find /var/lib/kubelet | xargs -n 1 findmnt -n -t tmpfs -o TARGET -T | uniq | xargs -r umount -v;rm -r -f /etc/kubernetes /var/lib/kubelet /var/lib/etcd;</code></pre></div><p><strong>还有个坑，初始化以前记得一定要启动 kubelet，虽然你 <code>systemctl status kubelet</code> 看着他是启动失败，但是也得启动，否则绝壁卡死</strong></p><div class="hljs code-wrapper"><pre><code class="hljs sh">systemctl <span class="hljs-built_in">enable</span> kubeletsystemctl start kubelet</code></pre></div><p><strong>等会等会，还有坑，新版本直接 init 会提示 <code>ebtables not found in system path</code> 错误，所以还得先安装一下这个包在初始化</strong></p><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment"># 安装 ebtables</span>yum install -y ebtables</code></pre></div><p><strong>最后见证奇迹的时刻</strong></p><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment"># 初始化并指定 apiserver 监听地址</span>kubeadm init --api-advertise-addresses 192.168.1.167</code></pre></div><p><strong>完美截图如下</strong></p><p><img src="https://cdn.oss.link/markdown/rs2mw.jpg" alt="init master"></p><p><strong>这里再爆料一个坑，底下的 <code>kubeadm join --token=b17964.5d8a3c14e99cf6aa 192.168.1.167</code> 这条命令一定保存好，因为后期没法重现的，你们老大再让你添加机器的时候如果没这个你会哭的</strong></p><h4 id="3-5、加入-node"><a href="#3-5、加入-node" class="headerlink" title="3.5、加入 node"></a>3.5、加入 node</h4><p>上面所有坑大约说的差不多了，直接上命令了</p><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment"># 处理主机名</span><span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;192-168-1-189.node&quot;</span> &gt; /etc/hostname <span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;127.0.0.1   192-168-1-189.node&quot;</span> &gt;&gt; /etc/hostssysctl kernel.hostname=192-168-1-189.node<span class="hljs-comment"># 拉取镜像</span>images=(kube-proxy-amd64:v1.4.4 kube-discovery-amd64:1.0 kubedns-amd64:1.7 kube-scheduler-amd64:v1.4.4 kube-controller-manager-amd64:v1.4.4 kube-apiserver-amd64:v1.4.4 etcd-amd64:2.2.5 kube-dnsmasq-amd64:1.3 exechealthz-amd64:1.1 pause-amd64:3.0 kubernetes-dashboard-amd64:v1.4.1)<span class="hljs-keyword">for</span> imageName <span class="hljs-keyword">in</span> <span class="hljs-variable">$&#123;images[@]&#125;</span> ; <span class="hljs-keyword">do</span>  docker pull mritd/<span class="hljs-variable">$imageName</span>  docker tag mritd/<span class="hljs-variable">$imageName</span> gcr.io/google_containers/<span class="hljs-variable">$imageName</span>  docker rmi mritd/<span class="hljs-variable">$imageName</span><span class="hljs-keyword">done</span><span class="hljs-comment"># 装 rpm</span>tee /etc/yum.repos.d/mritd.repo &lt;&lt; <span class="hljs-string">EOF</span><span class="hljs-string">[mritdrepo]</span><span class="hljs-string">name=Mritd Repository</span><span class="hljs-string">baseurl=https://rpm.mritd.me/centos/7/x86_64</span><span class="hljs-string">enabled=1</span><span class="hljs-string">gpgcheck=1</span><span class="hljs-string">gpgkey=https://cdn.oss.link/keys/rpm.public.key</span><span class="hljs-string">EOF</span>yum makecacheyum install -y kubelet kubectl kubernetes-cni kubeadm ebtables<span class="hljs-comment"># 清理目录(没初始化过只需要删目录)</span>rm -r -f /etc/kubernetes /var/lib/kubelet /var/lib/etcd;<span class="hljs-comment"># 启动 kubelet</span>systemctl <span class="hljs-built_in">enable</span> kubeletsystemctl start kubelet<span class="hljs-comment"># 初始化加入集群</span>kubeadm join --token=b17964.5d8a3c14e99cf6aa 192.168.1.167</code></pre></div><p><strong>同样完美截图</strong></p><p><img src="https://cdn.oss.link/markdown/9c8eu.jpg" alt="join master"></p><p><img src="https://cdn.oss.link/markdown/ri4q9.jpg" alt="get node"></p><h4 id="3-6、部署-weave-网络"><a href="#3-6、部署-weave-网络" class="headerlink" title="3.6、部署 weave 网络"></a>3.6、部署 weave 网络</h4><p>再没部署 weave 时，dns 是启动不了的，如下</p><p><img src="https://cdn.oss.link/markdown/fqjsg.jpg" alt="dns not work"></p><p><strong>官方给出的命令是这样的</strong></p><div class="hljs code-wrapper"><pre><code class="hljs sh">kubectl create -f https://git.io/weave-kube</code></pre></div><p>本着 “刨根问底挖祖坟” 的精神，先把这个 yaml 搞下来</p><div class="hljs code-wrapper"><pre><code class="hljs sh">wget https://git.io/weave-kube -O weave-kube.yaml</code></pre></div><p>然后同样的套路，打开看一下镜像，利用 Docker Hub 做中转，搞下来再 load 进去，然后 <code>create -f</code> 就行了</p><div class="hljs code-wrapper"><pre><code class="hljs sh">docker pull mritd/weave-kube:1.7.2docker tag mritd/weave-kube:1.7.2 weaveworks/weave-kube:1.7.2docker rmi mritd/weave-kube:1.7.2kubectl create -f weave-kube.yaml</code></pre></div><p><strong>完美截图</strong></p><p><img src="https://cdn.oss.link/markdown/0ja5f.jpg" alt="create weave"></p><h4 id="3-7、部署-dashboard"><a href="#3-7、部署-dashboard" class="headerlink" title="3.7、部署 dashboard"></a>3.7、部署 dashboard</h4><p><strong>dashboard 的命令也跟 weave 的一样，不过有个大坑，默认的 yaml 文件中对于 image 拉取策略的定义是 无论何时都会去拉取镜像，导致即使你 load 进去也无卵用，所以还得先把 yaml 搞下来然后改一下镜像拉取策略，最后再 <code>create -f</code> 即可</strong></p><div class="hljs code-wrapper"><pre><code class="hljs sh">wget https://rawgit.com/kubernetes/dashboard/master/src/deploy/kubernetes-dashboard.yaml -O kubernetes-dashboard.yaml</code></pre></div><p><strong>编辑 yaml 改一下 <code>imagePullPolicy</code>，把 <code>Always</code> 改成 <code>IfNotPresent</code>(本地没有再去拉取) 或者 <code>Never</code>(从不去拉取) 即可</strong></p><p><img src="https://cdn.oss.link/markdown/lqvh1.jpg" alt="IfNotPresent"></p><p>最后再利用 Dokcer Hub 中转，然后创建(实际上 dashboard 已经有了 v1.4.1，我这里已经改了)</p><div class="hljs code-wrapper"><pre><code class="hljs sh">kubectl create -f kubernetes-dashboard.yaml</code></pre></div><p><strong>截图如下</strong></p><p><img src="https://cdn.oss.link/markdown/xsn9u.jpg" alt="create dashboard"></p><p><strong>通过 describe 命令我们可以查看其暴露出的 <code>NodePoint</code>,然后便可访问</strong></p><p><img src="https://cdn.oss.link/markdown/5a94q.jpg" alt="describe dashboard"></p><p><img src="https://cdn.oss.link/markdown/xwjvs.jpg" alt="show dashboard"></p><h3 id="四、其他的一些坑"><a href="#四、其他的一些坑" class="headerlink" title="四、其他的一些坑"></a>四、其他的一些坑</h3><p>还有一些其他的坑等着大家去摸索，其中有一个是 DNS 解析错误，表现形式为 <strong>POD 内的程序通过域名访问解析不了，cat 一下容器的 <code>/etc/resolv.conf</code>发现指向的 dns 服务器与 <code>kubectl get svc --namespace=kube-system</code> 中的 kube-dsn 地址不符</strong>；解决办法就是 <strong>编辑节点的 <code>/etc/systemd/system/kubelet.service.d/10-kubeadm.conf</code> 文件，更改 <code>KUBELET_DNS_ARGS</code> 地址为 <code>get svc</code> 中的 kube-dns 地址，然后重启 kubelet 服务，重新杀掉 POD 让 kubernetes 重建即可</strong></p><p><img src="https://cdn.oss.link/markdown/hhozt.jpg" alt="modify kube-dns"></p><p><strong>其他坑欢迎大家补充</strong></p>]]></content>
    
    
    <summary type="html">距离上一篇 [kubernetes 1.4 集群搭建](https://mritd.me/2016/10/09/kubernetes-1.4-create-cluster/) 发布间隔不算太久，自己也不断地在生产和测试环境鼓捣，有不少 &quot;逗比&quot; 的经历，准备写一下具体的 kubeadm 搭建集群的一些坑和踩坑的经验，如果没有使用过 kubeadm 的同学，最好先看下上面的文章，然后鼓捣一遍，也许并不会成功，但大部分坑再来看此文会有收获</summary>
    
    
    
    <category term="Kubernetes" scheme="https://mritd.com/categories/kubernetes/"/>
    
    
    <category term="Linux" scheme="https://mritd.com/tags/linux/"/>
    
    <category term="Docker" scheme="https://mritd.com/tags/docker/"/>
    
    <category term="Kubernetes" scheme="https://mritd.com/tags/kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>人生就是会不断挥手说再见的啊</title>
    <link href="https://mritd.com/2016/10/25/waved-goodbye/"/>
    <id>https://mritd.com/2016/10/25/waved-goodbye/</id>
    <published>2016-10-25T05:50:29.000Z</published>
    <updated>2016-10-25T05:50:29.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>有些人，注定只能陪你一阵子，不是一辈子。人生就是会不断挥手说再见的啊…</p></blockquote><h3 id="1"><a href="#1" class="headerlink" title="1"></a>1</h3><p>前几天失眠，碰上小雨跟我聊天，她说她和男朋友分手了，因为我们习惯了他们俩每次分手后都说老死不相往来，转眼过几天又像连体婴一样出现在大家眼前，所以我对她说：“没事儿的，过几天你们俩就好了。”</p><p>“我今天做饭的时候，刀不小心划破了手，流血的时候我第一反应是拿起电话给他打电话，眼泪都出来了，情绪特别崩溃。”</p><p>“然后呢？”</p><p>“然后我突然想起来，那个可以撒娇的人没有了，以后没人可以让你撒娇了。自己默默回房间拿了个创可贴，发了一会呆就继续做饭了。”</p><p>我觉得有点心疼，除了心疼她被切破的手，更心疼她不能把电话打给想念的人。我想我们都曾有过拿起电话就想打给他却突然意识到，以后你都不能再打给这个人的时候。</p><p>和一个人最远的距离是你知道那个人就在那里，可你再也不能牵他的手，再也不能摸他的头发，你告诉自己：“别哭了，我们的故事结束了。”</p><h3 id="2"><a href="#2" class="headerlink" title="2"></a>2</h3><p>往后的日子里，你过得好就好，我就远远看着不打扰。</p><p>17岁的时候，小雨的身边是他，18岁的时候，小雨一想起就傻笑的人是他，19岁的时候，小雨牵着手的人还是他，今年20岁，小雨往后的生活里，再也没有他了。</p><p>我问她：“分手后悔吗？这么长时间说散就散了。”</p><p>她说：“我不后悔，我也没有遗憾。我曾经所做的事情，流过的的眼泪，都是成长。非要说有什么遗憾的话，可能就是没有和他走到终点。”</p><p>我们不该忘记生命中每一个爱过的人，我们爱过的不只是那个人，更是我们再也回不去的青春。</p><h3 id="3"><a href="#3" class="headerlink" title="3"></a>3</h3><p>之前我问过我的一个异性朋友，我问他：“你分手的时候会很难过很难过吗？”</p><p>他是这么跟我说的：“能说出口的难过都不是真的难过。</p><p>有一天晚上我手机突然响了，电话那头没有人说话，我也没说话，我知道是她，我知道她想我了。我没狠下心挂电话，我担心她害怕。我一根接一根的吸烟，她一声不吭，我们这样持续到我手机没电了自动关机。</p><p>与其说难过，倒不如说是担心。担心她饿了没人带她吃好吃的，担心她睡不着没人陪她说话，担心她的坏脾气没人包容…总之就是担心她以后过得不好，担心她过得不幸福。</p><p>一想到这些我才觉得难过。”</p><p>当这个人再出现在你生活里的时候，你看着她，你知道这个人以前你爱过，但以后你不能继续爱了，你希望别人好好爱她，又害怕别人没那么爱她。</p><p>原来，没有得到过不是最痛的，得到过再失去才是最痛的。</p><h3 id="fin"><a href="#fin" class="headerlink" title="fin"></a>fin</h3><p>对已经逝去的东西最好的尊重是绝口不提。</p><p>人生就是一个不断挥手说再见的过程啊，回不去的时候必须要说：再见。</p><p>说再见的时候不想说珍重，我想说：祝你幸福。</p><p>Author 蒋同学，微信公众号/有故事的蒋同学，ID/meiya54264<br>BGM</p><p><strong>转自 <a href="http://isujin.com/5990">素锦</a></strong></p><audio  autoplay="autoplay">  <source src="https://cdn.oss.link/bgm/waved-goodbye.mp3" type="audio/mpeg" />Your browser does not support the audio element.</audio>]]></content>
    
    
    <summary type="html">有些人，注定只能陪你一阵子，不是一辈子。人生就是会不断挥手说再见的啊...</summary>
    
    
    
    <category term="随笔" scheme="https://mritd.com/categories/%E9%9A%8F%E7%AC%94/"/>
    
    
    <category term="随笔" scheme="https://mritd.com/tags/%E9%9A%8F%E7%AC%94/"/>
    
  </entry>
  
  <entry>
    <title>二十多岁的我们，为什么觉得谈恋爱好难</title>
    <link href="https://mritd.com/2016/10/25/twenty-love-so-hard/"/>
    <id>https://mritd.com/2016/10/25/twenty-love-so-hard/</id>
    <published>2016-10-25T03:19:06.000Z</published>
    <updated>2016-10-25T03:19:06.000Z</updated>
    
    <content type="html"><![CDATA[<h3 id="01"><a href="#01" class="headerlink" title="01"></a>01</h3><p>前两天，我和朋友约在咖啡馆聊天虚度假期。有一个朋友为感情的事愁眉不展，她说在生活的压力下，感觉自己变得难以心动，对一切都感到麻木。</p><p>我也思考起，自己和身边人的感情生活。确实，好像一切都变了，但不是变得难以心动，而是心动之后所面对的一连串现实问题，让我们望而却步了。</p><p>二十多岁的我们，为什么觉得谈恋爱好难</p><p>才想起，过去的我并不觉得恋爱是件难事。</p><p>还记得18岁那年，我拿着卡里仅有的两千块钱，买了迪士尼的套票和酒店，因为那天是她的生日。</p><p>之后的一周时间里，每天吃方便面和咸方包，靠着同学的接济硬是撑了过去。那时候我并没有想太多，只是义无反顾地想让对方开心，再苦也值得。</p><p>四年后的今天，她留在北方的城市工作。有时候她会说：“想你了。可以来找我吗。”我回复一个笑容，然后迅速转移到下一个话题中。</p><p>我在想，如果是18岁时的我，一定会花光积蓄去找她吧。不知不觉，我已经不再会为爱情倾其所有。</p><p>而与此同时，似乎她也开始改变了。我们每天的对话，经常只有三句：“我起来了”，“我下班了”，“早点睡”。</p><p>从前，她事无大小总会跟我分享，但最近她再也没谈起过她的生活。反倒是在朋友圈里，我才看见她参加的各样聚会，跟朋友在周边城市旅游，有时玩到半夜才回家。</p><p>但我们都没有因此而感到难过或不适，反而开始适应了彼此独立的生活，不再盲目地追求当年义无反顾的激情。</p><p>从前我们年轻气盛，认为恋爱大过天，什么阻碍都不放在眼里，只要彼此相爱就好。但是二十多岁了，我们逐渐各自独立，而我再也不敢这样感性地义无反顾下去。</p><p>因为我们都心知肚明，异地恋成功的概率太小，成本太高了。也许，这几年的感情，很快就只能是一段回忆了。</p><p>而那些曾经让人羡慕的校园模范情侣们，到了现在的年纪，好像也走不过这一遭。</p><h3 id="02"><a href="#02" class="headerlink" title="02"></a>02</h3><p>坐在我身边的师姐，她曾经就是人人艳羡的对象。最近，她也分手了。谈起这段感情，她说男朋友的压力太大，向她坦白已经没有太多恋爱的资本了。</p><p>她回忆起上大学的时候，男朋友的父母每个月给他4000块生活费。这在学校可以生活得非常舒适了，他们总会在周末一起到周边城市旅游，或是互赠一些价值不菲的礼物。</p><p>有一次，我跟他们一起到香港。那个男生拿着早已准备好的购物清单，在一家潮牌店里用半个小时刷走了八千多块钱。</p><p>我盯着他那件上千块的T恤和三千多块的球鞋，一时觉得难以置信。而这也是学生时代的我所憧憬的情侣生活：钱袋鼓囊，时间充裕。</p><p>但是毕业以后，好像一切都变了。男生的父母需要他自力更生，于是每个月拿到手的4000块，变成了老板出的工资。</p><p>没有了宿舍以后，两人只能开始租房。为了省钱，他们住在了公司附近的城中村里，每个月1400。但是蟑螂乱窜，甚至要自己捡来半块床板盖厕所。再加上日常基本花销，每个月4000块工资所剩无几。</p><p>以前那些到处旅游购物的日子，再也没有了。平时出门打车的习惯也要改掉，更别说那些上千块的T恤、几千块的球鞋了。</p><p>今年的5月20号，师姐收到他发来的5.2元的红包。师姐说：“想到他如今面对的压力，租房、工作、爱情。要负担的东西变多了，而我们过去的生活也回不去了。”</p><p>终于在上个星期，他们分手了。原因很简单，他们不再有足够的时间和金钱来为彼此的感情付出了。</p><p>“也许分开会更好吧。”师姐喝了一口咖啡，淡淡的说到。“如今的他，更需要为自己的生活打拼，而不是我们互相操心了。”</p><h3 id="03"><a href="#03" class="headerlink" title="03"></a>03</h3><p>与恢复单身的师姐不同，我们的另一个朋友欣欣，最近突然订婚了。她与男朋友阿成从高中开始在一起，八年时间过去，但最终订婚的却不是他。</p><p>还记得在订婚宴上，万众瞩目的欣欣珠光宝气。她妈妈也在一旁不断夸赞着未来女婿，还苦口婆心地教我们，好好计划下自己该怎么过日子。</p><p>也正是因为她妈妈的这一番话，看着眼前不再满脸稚气的欣欣挽着身旁的男生，我意识到：我们都已经迈入到谈婚论嫁的年纪，身边的朋友生下小孩，再也不是一个“意外”。</p><p>毕业以后，身边不少同学开始组建自己的家庭，与此而来的是长辈们的压力。爱情已经不仅是两个人的事，也关乎到两个家庭的方方面面。有时候要面对父母的催婚，有时候还不得不听从父母的意愿。</p><p>回过头看看阿成，他们在一起这么多年，欣欣的妈妈对他并不满意。她的妈妈总说，阿成不会主动帮女生拎行李，不够上进，绝对不会让女儿嫁给这样的人。于是，千方百计地阻扰，并且对欣欣进行思想工作。</p><p>后来，在父母的牵线下，欣欣认识了一位年轻有为的男人。按她的话说，他性格不错，成熟稳重，会是一个可靠有担当的男人。她思前想后，经不住各方面的压力终于向阿成提出了分手。</p><p>然后，见家长，订婚。从结识到敲定婚事，仅仅是半年时间。</p><p>原来，在婚姻、家庭的压力面前，我们已经不能浪费太多时间去自由恋爱了。</p><p>曾经我们可能因为长相，因为性格，也可能是相互感觉不错，就能尝试走到一起。但现在，恋爱却多了很多前提。</p><p>工作是否稳定？</p><p>有房有车吗？</p><p>会不会经常分隔两地？</p><p>家庭环境如何？</p><p>双方父母怎么样？</p><p>在精打细算完一连串的问题之后，爱反而成为了最后才会思考的问题。</p><h3 id="04"><a href="#04" class="headerlink" title="04"></a>04</h3><p>我曾经遇到过十分喜欢的人，常常聊着天就会对着手机不自觉地笑出声。我们喜欢同样的歌曲和食物，喜欢同一座城市，还会幼稚地拍下身边有趣的东西发给对方，有着共同目标。</p><p>但我们都不敢轻易尝试开始，因为于我们而言，要在一起很容易，往后要面对的问题却很难。</p><p>同样刚毕业的我们，养活自己都已经很吃力，更不要说负担两个人的额外花销。更何况工作之余，已没有太多精力顾及另一个人。</p><p>朋友说：“我突然很害怕。害怕以后会迫不得已选择一个会给我良好生活、满足一切前提的人，但不是一个我真正喜欢的人。”</p><p>我也很害怕。我害怕自己无法给对方带来一个好的生活，只剩下我独自面对着现实带来的孤独和折磨。</p><p>昨天晚上，我和紫菜又聊起在咖啡店里听到的这几个故事。</p><p>她说，似乎恋爱中的问题，已经让我们逐渐变得可怕。</p><p>比如当她们得知一个大学朋友，即将嫁给一个三十多岁的有钱有势的人时，身边人的第一反应是羡慕和恭贺，但却没有一个人问她是否喜欢对方。</p><p>在那一瞬间，她觉得自己“这么多年来的书都白读了”。因为在现实面前，我们都不经意变得肤浅起来。</p><p>我反复思考她所说的话，一直到凌晨四点都没睡着。</p><p>也许，我们真的过了只要努力一下就能在一起的年纪了，过去藏在校服和学习背后的东西，终究被扯了出来。</p><p>Author：KC<br>BGM：天空の城ラピュタ~君をのせて by 高嶋ちさ子</p><p><strong>本文转自 <a href="http://isujin.com/6117">素锦</a></strong></p><audio  autoplay="autoplay">  <source src="https://cdn.oss.link/bgm/高嶋ちさ子-天空の城ラピュタ~君をのせて.mp3" type="audio/mpeg" />Your browser does not support the audio element.</audio>]]></content>
    
    
    <summary type="html">二十多岁的我们，为什么觉得谈恋爱好难</summary>
    
    
    
    <category term="随笔" scheme="https://mritd.com/categories/%E9%9A%8F%E7%AC%94/"/>
    
    
    <category term="随笔" scheme="https://mritd.com/tags/%E9%9A%8F%E7%AC%94/"/>
    
  </entry>
  
  <entry>
    <title>StrongSwan 搭建 VPN</title>
    <link href="https://mritd.com/2016/10/18/set-up-vpn-with-strongswan/"/>
    <id>https://mritd.com/2016/10/18/set-up-vpn-with-strongswan/</id>
    <published>2016-10-17T16:14:04.000Z</published>
    <updated>2016-10-17T16:14:04.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>由于工作需要，记录一下使用 StrongSwan 搭建 VPN 的过程，支持 L2TP、IKEv2 PSK/CERT、IPsec 连接，基本上兼容大部分设备</p></blockquote><h3 id="一、环境准备"><a href="#一、环境准备" class="headerlink" title="一、环境准备"></a>一、环境准备</h3><p>基本环境如下</p><ul><li>CentOS 7 X64</li><li>StrongSwan 5.5</li></ul><h3 id="二、搭建-VPN"><a href="#二、搭建-VPN" class="headerlink" title="二、搭建 VPN"></a>二、搭建 VPN</h3><h4 id="2-1、安装依赖"><a href="#2-1、安装依赖" class="headerlink" title="2.1、安装依赖"></a>2.1、安装依赖</h4><p>以下采用源码编译安装，需要安装编译依赖环境</p><div class="hljs code-wrapper"><pre><code class="hljs sh">yum install -y gmp-devel xl2tpd module-init-tools gcc openssl-devel</code></pre></div><h4 id="2-2、编译安装"><a href="#2-2、编译安装" class="headerlink" title="2.2、编译安装"></a>2.2、编译安装</h4><p>首先下载源码</p><div class="hljs code-wrapper"><pre><code class="hljs sh">wget https://download.strongswan.org/strongswan-5.5.0.tar.gz -O /tmp/strongswan-5.5.0.tar.gz</code></pre></div><p>解压并编译安装</p><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-built_in">cd</span> /tmp &amp;&amp; tar -zxvf strongswan-5.5.0.tar.gz<span class="hljs-built_in">cd</span> /tmp/strongswan-5.5.0 &amp;&amp; ./configure --prefix=/usr --sysconfdir=/etc \--enable-eap-radius \--enable-eap-mschapv2 \--enable-eap-identity \--enable-eap-md5 \--enable-eap-mschapv2 \--enable-eap-tls \--enable-eap-ttls \--enable-eap-peap \--enable-eap-tnc \--enable-eap-dynamic \--enable-xauth-eap \--enable-openssl \&amp;&amp; make -j \&amp;&amp; make install</code></pre></div><h4 id="2-3、基础配置"><a href="#2-3、基础配置" class="headerlink" title="2.3、基础配置"></a>2.3、基础配置</h4><p>StrongSwan 的配置主要为 <code>ipsec.conf</code>、<code>strongswan.conf</code>、<code>xl2tpd.conf</code>、<code>options.xl2tpd</code> 这四个配置文件，以下为四个配置文件样例</p><h5 id="2-3-1、ipsec-conf"><a href="#2-3-1、ipsec-conf" class="headerlink" title="2.3.1、ipsec.conf"></a>2.3.1、ipsec.conf</h5><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment"># ipsec.conf - strongSwan IPsec configuration file</span>config setupuniqueids=nocharondebug=<span class="hljs-string">&quot;cfg 2, dmn 2, ike 2, net 0&quot;</span>conn %defaultdpdaction=cleardpddelay=300srekey=noleft=%defaultrouteleftfirewall=yesright=%anyikelifetime=60mkeylife=20mrekeymargin=3mkeyingtries=1auto=add<span class="hljs-comment">#######################################</span><span class="hljs-comment"># L2TP Connections</span><span class="hljs-comment">#######################################</span>conn L2TP-IKEv1-PSK<span class="hljs-built_in">type</span>=transportkeyexchange=ikev1authby=secretleftprotoport=udp/l2tpleft=%anyright=%anyrekey=noforceencaps=yes<span class="hljs-comment">#######################################</span><span class="hljs-comment"># Default non L2TP Connections</span><span class="hljs-comment">#######################################</span>conn Non-L2TPleftsubnet=0.0.0.0/0rightsubnet=10.0.0.0/24rightsourceip=10.0.0.0/24<span class="hljs-comment">#######################################</span><span class="hljs-comment"># EAP Connections</span><span class="hljs-comment">#######################################</span><span class="hljs-comment"># This detects a supported EAP method</span>conn IKEv2-EAPalso=Non-L2TPkeyexchange=ikev2eap_identity=%anyrightauth=eap-dynamic<span class="hljs-comment">#######################################</span><span class="hljs-comment"># PSK Connections</span><span class="hljs-comment">#######################################</span>conn IKEv2-PSKalso=Non-L2TPkeyexchange=ikev2authby=secret<span class="hljs-comment"># Cisco IPSec</span>conn IKEv1-PSK-XAuthalso=Non-L2TPkeyexchange=ikev1leftauth=pskrightauth=pskrightauth2=xauth<span class="hljs-comment">#######################################</span><span class="hljs-comment"># Certificate Connections</span><span class="hljs-comment">#######################################</span>conn windows7    keyexchange=ikev2    ike=aes256-sha1-modp1024!    rekey=no    left=%defaultroute    leftauth=pubkey    leftsubnet=0.0.0.0/0    leftcert=server.cert.pem    right=%any    rightauth=eap-mschapv2    rightsourceip=10.0.0.0/24    rightsendcert=never    eap_identity=%any    auto=add</code></pre></div><h5 id="2-3-2、options-xl2tpd"><a href="#2-3-2、options-xl2tpd" class="headerlink" title="2.3.2、options.xl2tpd"></a>2.3.2、options.xl2tpd</h5><div class="hljs code-wrapper"><pre><code class="hljs sh">ipcp-accept-localipcp-accept-remotems-dns 8.8.8.8ms-dns 8.8.4.4noccpauthcrtsctsidle 1800mtu 1280mru 1280locklcp-echo-failure 10lcp-echo-interval 60connect-delay 5000</code></pre></div><h5 id="2-3-3、strongswan-conf"><a href="#2-3-3、strongswan-conf" class="headerlink" title="2.3.3、strongswan.conf"></a>2.3.3、strongswan.conf</h5><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment"># /etc/strongswan.conf - strongSwan configuration file</span><span class="hljs-comment"># strongswan.conf - strongSwan configuration file</span><span class="hljs-comment">#</span><span class="hljs-comment"># Refer to the strongswan.conf(5) manpage for details</span>charon &#123;load_modular = yessend_vendor_id = yesplugins &#123;include strongswan.d/charon/*.confattr &#123;dns = 8.8.8.8, 8.8.4.4&#125;&#125;&#125;include strongswan.d/*.conf</code></pre></div><h5 id="2-3-4、xl2tpd-conf"><a href="#2-3-4、xl2tpd-conf" class="headerlink" title="2.3.4、xl2tpd.conf"></a>2.3.4、xl2tpd.conf</h5><div class="hljs code-wrapper"><pre><code class="hljs sh">[global]port = 1701auth file = /etc/ppp/l2tp-secretsdebug avp = yesdebug network = yesdebug state = yesdebug tunnel = yes[lns default]ip range = 10.1.0.2-10.1.0.254<span class="hljs-built_in">local</span> ip = 10.1.0.1require chap = yesrefuse pap = yesrequire authentication = yesname = l2tpd;ppp debug = yespppoptfile = /etc/ppp/options.xl2tpdlength bit = yes</code></pre></div><p><strong>创建好四个配置文件后将其复制到指定位置即可</strong></p><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment"># Strongswan Configuration</span>cp ipsec.conf /etc/ipsec.confcp strongswan.conf /etc/strongswan.conf<span class="hljs-comment"># XL2TPD Configuration</span>cp xl2tpd.conf /etc/xl2tpd/xl2tpd.confcp options.xl2tpd /etc/ppp/options.xl2tpd</code></pre></div><h4 id="2-4、创建证书"><a href="#2-4、创建证书" class="headerlink" title="2.4、创建证书"></a>2.4、创建证书</h4><p>对于 Windows、Android 等设备可能不支持某些登录方式，比如 IKEv2 PSK，这是需要创建证书，以支持使用 IKEv2 证书登录</p><h5 id="2-4-1、自签-CA"><a href="#2-4-1、自签-CA" class="headerlink" title="2.4.1、自签 CA"></a>2.4.1、自签 CA</h5><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment"># create CA certificate</span>ipsec pki --gen --outform pem &gt; ca.key.pemipsec pki --self --<span class="hljs-keyword">in</span> ca.key.pem --dn <span class="hljs-string">&quot;C=CN, O=StrongSwan, CN=StrongSwan CA&quot;</span> --ca --outform pem &gt; ca.cert.pem</code></pre></div><h5 id="2-4-2、创建服务器证书"><a href="#2-4-2、创建服务器证书" class="headerlink" title="2.4.2、创建服务器证书"></a>2.4.2、创建服务器证书</h5><p><strong>其中 <code>--san</code> 可以指定多个，但一般为一个是域名，一个是外网 IP，如果经过了路由，那么只需要写本机的对外暴露网卡的 IP 即可</strong></p><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-comment"># create server certificate</span>ipsec pki --gen --outform pem &gt; server.key.pemipsec pki --pub --<span class="hljs-keyword">in</span> server.key.pem | ipsec pki --issue --cacert ca.cert.pem \  --cakey ca.key.pem --dn <span class="hljs-string">&quot;C=CN, O=StrongSwan, CN=服务器域名&quot;</span> \  --san=<span class="hljs-string">&quot;服务器域名&quot;</span> --san=<span class="hljs-string">&quot;网卡IP&quot;</span> --flag serverAuth --flag ikeIntermediate \  --outform pem &gt; server.cert.pem</code></pre></div><h5 id="2-4-3、创建客户端证书"><a href="#2-4-3、创建客户端证书" class="headerlink" title="2.4.3、创建客户端证书"></a>2.4.3、创建客户端证书</h5><div class="hljs code-wrapper"><pre><code class="hljs sh">ipsec pki --gen --outform pem &gt; client.key.pemipsec pki --pub --<span class="hljs-keyword">in</span> client.key.pem | ipsec pki --issue --cacert ca.cert.pem \  --cakey ca.key.pem --dn <span class="hljs-string">&quot;C=CN, O=StrongSwan, CN=Client&quot;</span> \  --outform pem &gt; client.cert.pem</code></pre></div><h5 id="2-4-4、生成-p12"><a href="#2-4-4、生成-p12" class="headerlink" title="2.4.4、生成 p12"></a>2.4.4、生成 p12</h5><p>安卓等设备是不支持直接导入客户端证书的，需要转换成 p12 格式，转换过程中需要输入两次密码，该密码为证书使用密码，导入时需要输入</p><div class="hljs code-wrapper"><pre><code class="hljs sh">openssl pkcs12 -<span class="hljs-built_in">export</span> -inkey client.key.pem -<span class="hljs-keyword">in</span> client.cert.pem -name <span class="hljs-string">&quot;Client&quot;</span> \  -certfile ca.cert.pem -caname <span class="hljs-string">&quot;StrongSwan CA&quot;</span> -out client.cert.p12</code></pre></div><h5 id="2-4-5、安装证书"><a href="#2-4-5、安装证书" class="headerlink" title="2.4.5、安装证书"></a>2.4.5、安装证书</h5><p>创建完成后将证书复制到指定目录即可</p><div class="hljs code-wrapper"><pre><code class="hljs sh">cp -r ca.cert.pem /etc/ipsec.d/cacerts/cp -r server.cert.pem /etc/ipsec.d/certs/cp -r server.key.pem /etc/ipsec.d/private/cp -r client.cert.pem /etc/ipsec.d/certs/cp -r client.key.pem /etc/ipsec.d/private/</code></pre></div><h4 id="2-5、创建用户"><a href="#2-5、创建用户" class="headerlink" title="2.5、创建用户"></a>2.5、创建用户</h4><p>关于用户的登陆模式，比如使用 L2TP、IPsec、IKEv2 等请自行 Google，以下提供了一个简单的创建用户的脚本</p><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-meta">#!/bin/sh</span>vpn_user=<span class="hljs-variable">$1</span>vpn_password=<span class="hljs-variable">$2</span><span class="hljs-keyword">if</span> [ -z <span class="hljs-variable">$&#123;vpn_user&#125;</span> ] || [ -z <span class="hljs-variable">$&#123;vpn_password&#125;</span> ]; <span class="hljs-keyword">then</span><span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;Usage: <span class="hljs-variable">$0</span> user password&quot;</span><span class="hljs-built_in">exit</span> 1<span class="hljs-keyword">fi</span>vpn_deluser <span class="hljs-variable">$&#123;vpn_user&#125;</span>cat &gt;&gt; /etc/ipsec.d/l2tp-secrets &lt;&lt;<span class="hljs-string">EOF</span><span class="hljs-string">&quot;$&#123;vpn_user&#125;&quot; &quot;*&quot; &quot;$&#123;vpn_password&#125;&quot; &quot;*&quot;</span><span class="hljs-string">EOF</span>cat &gt;&gt; /etc/ipsec.d/ipsec.secrets &lt;&lt;<span class="hljs-string">EOF</span><span class="hljs-string">$&#123;vpn_user&#125; : EAP &quot;$&#123;vpn_password&#125;&quot;</span><span class="hljs-string">$&#123;vpn_user&#125; : XAUTH &quot;$&#123;vpn_password&#125;&quot;</span><span class="hljs-string">EOF</span></code></pre></div><p><strong>将其保存为 <code>vpn_adduser.sh</code>，执行 <code>./vpn_adduser.sh USERNAME PASSWD</code> 即可添加用户</strong></p><h4 id="2-6、设置-PSK"><a href="#2-6、设置-PSK" class="headerlink" title="2.6、设置 PSK"></a>2.6、设置 PSK</h4><p>同样 PSK 也用于登录，如 IKEv2 PSK 登录，使用同样自行 Google，以下为设置 PSK 的脚本</p><div class="hljs code-wrapper"><pre><code class="hljs sh"><span class="hljs-meta">#!/bin/sh</span>psk=<span class="hljs-variable">$1</span><span class="hljs-keyword">if</span> [ -z <span class="hljs-variable">$&#123;psk&#125;</span> ]; <span class="hljs-keyword">then</span><span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;Usage: <span class="hljs-variable">$0</span> psk&quot;</span><span class="hljs-built_in">exit</span> 1<span class="hljs-keyword">fi</span>vpn_unsetpsktouch /etc/ipsec.d/ipsec.secretscat &gt;&gt; /etc/ipsec.d/ipsec.secrets &lt;&lt;<span class="hljs-string">EOF</span><span class="hljs-string">: PSK &quot;$&#123;psk&#125;&quot;</span><span class="hljs-string">EOF</span></code></pre></div><p>最后启动 VPN 连接即可</p><div class="hljs code-wrapper"><pre><code class="hljs sh">/usr/sbin/xl2tpd -c /etc/xl2tpd/xl2tpd.confipsec start</code></pre></div>]]></content>
    
    
    <summary type="html">由于工作需要，记录一下使用 StrongSwan 搭建 VPN 的过程，支持 L2TP、IKEv2 PSK/CERT、IPsec 连接，基本上兼容大部分设备</summary>
    
    
    
    <category term="Linux" scheme="https://mritd.com/categories/linux/"/>
    
    
    <category term="Linux" scheme="https://mritd.com/tags/linux/"/>
    
  </entry>
  
</feed>
